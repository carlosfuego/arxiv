[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v4",
                "updated": "2025-01-25T20:50:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    20,
                    50,
                    22,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v1",
                "updated": "2025-01-25T01:45:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14795v1",
                "updated": "2025-01-11T12:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T12:22:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL"
                },
                "summary": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation."
                },
                "authors": [
                    {
                        "name": "Nicolas Lee Guidotti"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Lee Guidotti"
                },
                "author": "Nicolas Lee Guidotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.17157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17157v1",
                "updated": "2025-01-28T18:56:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    56,
                    47,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    56,
                    47,
                    1,
                    28,
                    0
                ],
                "title": "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry"
                },
                "summary": "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions."
                },
                "authors": [
                    {
                        "name": "Jan Jakob"
                    }
                ],
                "author_detail": {
                    "name": "Jan Jakob"
                },
                "author": "Jan Jakob",
                "arxiv_comment": "Preprint, 7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17148v1",
                "updated": "2025-01-28T18:51:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:51:24Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders"
                },
                "summary": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean."
                },
                "authors": [
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17147v1",
                "updated": "2025-01-28T18:49:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    49,
                    57,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:49:57Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    49,
                    57,
                    1,
                    28,
                    0
                ],
                "title": "Non-halo structures and their effects on gravitationally lensed galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-halo structures and their effects on gravitationally lensed galaxies"
                },
                "summary": "While the $\\Lambda$CDM model succeeds on large scales, its validity on\nsmaller scales remains uncertain. Recent works suggest that non-halo dark\nmatter structures, such as filaments and walls, could significantly influence\ngravitational lensing and that the importance of these effects depends on the\ndark matter model: in warm dark matter scenarios, fewer low-mass objects form\nand thus their mass is redistributed into the cosmic-web. We investigate these\neffects on galaxy-galaxy lensing using fragmentation-free Warm Dark Matter\n(WDM) simulations with particle masses of m$_{\\chi}$ = 1 keV and m$_{\\chi}$ = 3\nkeV. Although these cosmological scenarios are already observationally\nexcluded, the fraction of mass falling outside of haloes grows with the thermal\nvelocity of the dark matter particles, which allows for the search for\nfirst-order effects. We create mock datasets, based on gravitationally-lensed\nsystems from the BELLS-Gallery, incorporating non-halo contributions from these\nsimulations to study their impact in comparison to mocks where the lens has a\nsmooth mass distribution. Using Bayesian modelling, we find that perturbations\nfrom WDM non-halo structures produce an effect on the inferred parameters of\nthe main lens and shift the reconstructed source position. However, these\nvariations are subtle and are effectively absorbed by standard elliptical\npower-law lens models, making them challenging to distinguish from intrinsic\nlensing features. Most importantly, non-halo perturbation does not appear as a\nstrong external shear term, which is commonly used in gravitational lensing\nanalyses to represent large-scale perturbations. Our results demonstrate that\nwhile non-halo structures can affect the lensing analysis, the overall impact\nremains indistinguishable from variations of the main lens in colder WDM and\nCDM scenarios, where non-halo contributions are smaller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the $\\Lambda$CDM model succeeds on large scales, its validity on\nsmaller scales remains uncertain. Recent works suggest that non-halo dark\nmatter structures, such as filaments and walls, could significantly influence\ngravitational lensing and that the importance of these effects depends on the\ndark matter model: in warm dark matter scenarios, fewer low-mass objects form\nand thus their mass is redistributed into the cosmic-web. We investigate these\neffects on galaxy-galaxy lensing using fragmentation-free Warm Dark Matter\n(WDM) simulations with particle masses of m$_{\\chi}$ = 1 keV and m$_{\\chi}$ = 3\nkeV. Although these cosmological scenarios are already observationally\nexcluded, the fraction of mass falling outside of haloes grows with the thermal\nvelocity of the dark matter particles, which allows for the search for\nfirst-order effects. We create mock datasets, based on gravitationally-lensed\nsystems from the BELLS-Gallery, incorporating non-halo contributions from these\nsimulations to study their impact in comparison to mocks where the lens has a\nsmooth mass distribution. Using Bayesian modelling, we find that perturbations\nfrom WDM non-halo structures produce an effect on the inferred parameters of\nthe main lens and shift the reconstructed source position. However, these\nvariations are subtle and are effectively absorbed by standard elliptical\npower-law lens models, making them challenging to distinguish from intrinsic\nlensing features. Most importantly, non-halo perturbation does not appear as a\nstrong external shear term, which is commonly used in gravitational lensing\nanalyses to represent large-scale perturbations. Our results demonstrate that\nwhile non-halo structures can affect the lensing analysis, the overall impact\nremains indistinguishable from variations of the main lens in colder WDM and\nCDM scenarios, where non-halo contributions are smaller."
                },
                "authors": [
                    {
                        "name": "Baptiste Jego"
                    },
                    {
                        "name": "Giulia Despali"
                    },
                    {
                        "name": "Tamara Richardson"
                    },
                    {
                        "name": "Jens Stücker"
                    }
                ],
                "author_detail": {
                    "name": "Jens Stücker"
                },
                "author": "Jens Stücker",
                "arxiv_comment": "13+4 pages, submitted to A&A, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17144v1",
                "updated": "2025-01-28T18:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:45:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data"
                },
                "summary": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size."
                },
                "authors": [
                    {
                        "name": "Deren Lei"
                    },
                    {
                        "name": "Yaxi Li"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Mengya Hu"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Ken Archer"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Emily Ching"
                    },
                    {
                        "name": "Alex Deng"
                    }
                ],
                "author_detail": {
                    "name": "Alex Deng"
                },
                "author": "Alex Deng",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07272v2",
                "updated": "2025-01-28T18:40:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    40,
                    26,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-14T03:42:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "Abstract Operations Research Modeling Using Natural Language Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Operations Research Modeling Using Natural Language Inputs"
                },
                "summary": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Ryan Wickman"
                    },
                    {
                        "name": "Sahil Bhatnagar"
                    },
                    {
                        "name": "Raj Kumar Maity"
                    },
                    {
                        "name": "Arko Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Arko Mukherjee"
                },
                "author": "Arko Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10587v2",
                "updated": "2025-01-28T18:36:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    36,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-17T22:39:06Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    22,
                    39,
                    6,
                    4,
                    17,
                    0
                ],
                "title": "Selecting samples of galaxies with fewer Fingers-of-God",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting samples of galaxies with fewer Fingers-of-God"
                },
                "summary": "The radial positions of galaxies inferred from their measured redshift appear\ndistorted due to their peculiar velocities. We argue that the contribution from\nstochastic velocities -- which gives rise to `Fingers-of-God' (FoG) anisotropy\nin the inferred maps -- does not lend itself to perturbative modelling already\non scales targeted by current experiments. To get around this limitation, we\npropose to remove FoG using data-driven indicators of their abundance that are\nlocal in nature and thus avoid selection biases. In particular, we show that\nthe scale where the measured power spectrum quadrupole changes sign is tightly\nanti-correlated with both the satellite fraction and the velocity dispersion,\nand can thus be used to select galaxy samples with fewer FoG. In addition, we\nshow that maps of the thermal Sunyaev-Zel'dovich distortion of the cosmic\nmicrowave background frequency spectrum can be used to identify and discard\nmany of the most problematic galaxies. These techniques could potentially\nextend the reach of perturbative models for galaxy clustering and improve\nreconstructions of the large-scale velocity and displacement fields from the\nredshift-space positions of galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The radial positions of galaxies inferred from their measured redshift appear\ndistorted due to their peculiar velocities. We argue that the contribution from\nstochastic velocities -- which gives rise to `Fingers-of-God' (FoG) anisotropy\nin the inferred maps -- does not lend itself to perturbative modelling already\non scales targeted by current experiments. To get around this limitation, we\npropose to remove FoG using data-driven indicators of their abundance that are\nlocal in nature and thus avoid selection biases. In particular, we show that\nthe scale where the measured power spectrum quadrupole changes sign is tightly\nanti-correlated with both the satellite fraction and the velocity dispersion,\nand can thus be used to select galaxy samples with fewer FoG. In addition, we\nshow that maps of the thermal Sunyaev-Zel'dovich distortion of the cosmic\nmicrowave background frequency spectrum can be used to identify and discard\nmany of the most problematic galaxies. These techniques could potentially\nextend the reach of perturbative models for galaxy clustering and improve\nreconstructions of the large-scale velocity and displacement fields from the\nredshift-space positions of galaxies."
                },
                "authors": [
                    {
                        "name": "Antón Baleato Lizancos"
                    },
                    {
                        "name": "Uroš Seljak"
                    },
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Marco Bonici"
                    },
                    {
                        "name": "Simone Ferraro"
                    }
                ],
                "author_detail": {
                    "name": "Simone Ferraro"
                },
                "author": "Simone Ferraro",
                "arxiv_comment": "33 pages + appendices & bibliography. 19 figures. Matches version\n  submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09443v2",
                "updated": "2025-01-28T18:26:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    26,
                    17,
                    1,
                    28,
                    0
                ],
                "published": "2024-07-12T17:28:08Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    28,
                    8,
                    4,
                    194,
                    0
                ],
                "title": "Addressing Confounding and Continuous Exposure Measurement Error Using\n  Corrected Score Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Confounding and Continuous Exposure Measurement Error Using\n  Corrected Score Functions"
                },
                "summary": "Confounding and exposure measurement error can introduce bias when drawing\ninference about the marginal effect of an exposure on an outcome of interest.\nWhile there are broad methodologies for addressing each source of bias\nindividually, confounding and exposure measurement error frequently co-occur,\nand there is a need for methods that address them simultaneously. In this\npaper, corrected score methods are derived under classical additive measurement\nerror to draw inference about marginal exposure effects using only measured\nvariables. Three estimators are proposed based on g-formula, inverse\nprobability weighting, and doubly-robust estimation techniques. The estimators\nare shown to be consistent and asymptotically normal, and the doubly-robust\nestimator is shown to exhibit its namesake property. The methods, which are\nimplemented in the R package mismex, perform well in finite samples under both\nconfounding and measurement error as demonstrated by simulation studies. The\nproposed doubly-robust estimator is applied to study the effects of two\nbiomarkers on HIV-1 infection using data from the HVTN 505 preventative vaccine\ntrial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confounding and exposure measurement error can introduce bias when drawing\ninference about the marginal effect of an exposure on an outcome of interest.\nWhile there are broad methodologies for addressing each source of bias\nindividually, confounding and exposure measurement error frequently co-occur,\nand there is a need for methods that address them simultaneously. In this\npaper, corrected score methods are derived under classical additive measurement\nerror to draw inference about marginal exposure effects using only measured\nvariables. Three estimators are proposed based on g-formula, inverse\nprobability weighting, and doubly-robust estimation techniques. The estimators\nare shown to be consistent and asymptotically normal, and the doubly-robust\nestimator is shown to exhibit its namesake property. The methods, which are\nimplemented in the R package mismex, perform well in finite samples under both\nconfounding and measurement error as demonstrated by simulation studies. The\nproposed doubly-robust estimator is applied to study the effects of two\nbiomarkers on HIV-1 infection using data from the HVTN 505 preventative vaccine\ntrial."
                },
                "authors": [
                    {
                        "name": "Brian D. Richardson"
                    },
                    {
                        "name": "Bryan S. Blette"
                    },
                    {
                        "name": "Peter B. Gilbert"
                    },
                    {
                        "name": "Michael G. Hudgens"
                    }
                ],
                "author_detail": {
                    "name": "Michael G. Hudgens"
                },
                "author": "Michael G. Hudgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17132v1",
                "updated": "2025-01-28T18:25:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:25:11Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "title": "ASTRAL: Automated Safety Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRAL: Automated Safety Testing of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors."
                },
                "authors": [
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    },
                    {
                        "name": "Aitor Arrieta"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Arrieta"
                },
                "author": "Aitor Arrieta",
                "arxiv_journal_ref": "The 6th ACM/IEEE International Conference on Automation of\n  Software Test (AST 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17117v1",
                "updated": "2025-01-28T18:07:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:07:30Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Histoires Morales: A French Dataset for Assessing Moral Alignment"
                },
                "summary": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata."
                },
                "authors": [
                    {
                        "name": "Thibaud Leteno"
                    },
                    {
                        "name": "Irina Proskurina"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Julien Velcin"
                    },
                    {
                        "name": "Charlotte Laclau"
                    },
                    {
                        "name": "Guillaume Metzler"
                    },
                    {
                        "name": "Christophe Gravier"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Gravier"
                },
                "author": "Christophe Gravier",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17116v1",
                "updated": "2025-01-28T18:04:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:04:50Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Large Language Model Training Using FP4 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Model Training Using FP4 Quantization"
                },
                "summary": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training."
                },
                "authors": [
                    {
                        "name": "Ruizhe Wang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Baining Guo"
                    },
                    {
                        "name": "Zhengjun Zha"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v2",
                "updated": "2025-01-28T18:00:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    0,
                    22,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17112v1",
                "updated": "2025-01-28T17:59:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    59,
                    56,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:59:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    59,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction"
                },
                "summary": "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Carl-Leander Henneking"
                    },
                    {
                        "name": "Claas Beger"
                    }
                ],
                "author_detail": {
                    "name": "Claas Beger"
                },
                "author": "Claas Beger",
                "arxiv_comment": "8 Pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14925v2",
                "updated": "2025-01-28T17:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-24T21:31:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    21,
                    31,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "Profiling Apple Silicon Performance for ML Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Apple Silicon Performance for ML Training"
                },
                "summary": "Apple Silicon has attracted much attention for its performance and role in\nmachine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally\ndominated ML training, Apple Silicon has a significant difference in memory\narchitecture. It uses Unified Memory, which integrates CPU and GPU memory\ninstead of separate CPU memory and GPU VRAM. However, it is difficult to tell\nwhether Unified Memory means more performance benefits.\n  This paper investigates the performance differences by training several large\nlanguage model (LLM) workloads end-to-end under different memory scenarios. The\nresults show a significant performance gap between Apple Silicon and NVIDIA\nGPUs. This paper attributes this gap to system-level factors such as page\nfaults, power consumption, and kernel launch time. In addition, the performance\ndifference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and\nApple Silicon chips is analyzed to further explain the observed gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Silicon has attracted much attention for its performance and role in\nmachine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally\ndominated ML training, Apple Silicon has a significant difference in memory\narchitecture. It uses Unified Memory, which integrates CPU and GPU memory\ninstead of separate CPU memory and GPU VRAM. However, it is difficult to tell\nwhether Unified Memory means more performance benefits.\n  This paper investigates the performance differences by training several large\nlanguage model (LLM) workloads end-to-end under different memory scenarios. The\nresults show a significant performance gap between Apple Silicon and NVIDIA\nGPUs. This paper attributes this gap to system-level factors such as page\nfaults, power consumption, and kernel launch time. In addition, the performance\ndifference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and\nApple Silicon chips is analyzed to further explain the observed gap."
                },
                "authors": [
                    {
                        "name": "Dahua Feng"
                    },
                    {
                        "name": "Zhiming Xu"
                    },
                    {
                        "name": "Rongxiang Wang"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17107v1",
                "updated": "2025-01-28T17:48:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    48,
                    17,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:48:17Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    48,
                    17,
                    1,
                    28,
                    0
                ],
                "title": "Goodness of Fit for Bayesian Generative Models with Applications in\n  Population Genetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goodness of Fit for Bayesian Generative Models with Applications in\n  Population Genetics"
                },
                "summary": "In population genetics and other application fields, models with intractable\nlikelihood are common. Approximate Bayesian Computation (ABC) or more generally\nSimulation-Based Inference (SBI) methods work by simulating instrumental data\nsets from the models under study and comparing them with the observed data set,\nusing advanced machine learning tools for tasks such as model selection and\nparameter inference. The present work focuses on model criticism, and more\nspecifically on Goodness of fit (GoF) tests, for intractable likelihood models.\nWe introduce two new GoF tests: the pre-inference \\gof tests whether the\nobserved dataset is distributed from the prior predictive distribution, while\nthe post-inference GoF tests whether there is a parameter value such that the\nobserved dataset is distributed from the likelihood with that value. The\npre-inference test can be used to prune a large set of models using a limited\namount of simulations, while the post-inference test is used to assess the fit\nof a selected model. Both tests are based on the Local Outlier Factor (LOF,\nBreunig et al., 2000). This indicator was initially defined for outlier and\nnovelty detection. It is able to quantify local density deviations, capturing\nsubtleties that a more traditional k-NN-based approach may miss. We evaluated\nthe performance of our two GoF tests on simulated datasets from three different\nmodel settings of varying complexity. We then illustrate the utility of these\napproaches on a dataset of single nucleotide polymorphism (SNP) markers for the\nevaluation of complex evolutionary scenarios of modern human populations. Our\ndual-test GoF approach highlights the flexibility of our method: the\npre-inference \\gof test provides insight into model validity from a Bayesian\nperspective, while the post-inference test provides a more general and\ntraditional view of assessing goodness of fit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In population genetics and other application fields, models with intractable\nlikelihood are common. Approximate Bayesian Computation (ABC) or more generally\nSimulation-Based Inference (SBI) methods work by simulating instrumental data\nsets from the models under study and comparing them with the observed data set,\nusing advanced machine learning tools for tasks such as model selection and\nparameter inference. The present work focuses on model criticism, and more\nspecifically on Goodness of fit (GoF) tests, for intractable likelihood models.\nWe introduce two new GoF tests: the pre-inference \\gof tests whether the\nobserved dataset is distributed from the prior predictive distribution, while\nthe post-inference GoF tests whether there is a parameter value such that the\nobserved dataset is distributed from the likelihood with that value. The\npre-inference test can be used to prune a large set of models using a limited\namount of simulations, while the post-inference test is used to assess the fit\nof a selected model. Both tests are based on the Local Outlier Factor (LOF,\nBreunig et al., 2000). This indicator was initially defined for outlier and\nnovelty detection. It is able to quantify local density deviations, capturing\nsubtleties that a more traditional k-NN-based approach may miss. We evaluated\nthe performance of our two GoF tests on simulated datasets from three different\nmodel settings of varying complexity. We then illustrate the utility of these\napproaches on a dataset of single nucleotide polymorphism (SNP) markers for the\nevaluation of complex evolutionary scenarios of modern human populations. Our\ndual-test GoF approach highlights the flexibility of our method: the\npre-inference \\gof test provides insight into model validity from a Bayesian\nperspective, while the post-inference test provides a more general and\ntraditional view of assessing goodness of fit"
                },
                "authors": [
                    {
                        "name": "Guillaume Le Mailloux"
                    },
                    {
                        "name": "Paul Bastide"
                    },
                    {
                        "name": "Jean-Michel Marin"
                    },
                    {
                        "name": "Arnaud Estoup"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Estoup"
                },
                "author": "Arnaud Estoup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08099v3",
                "updated": "2025-01-28T17:33:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    33,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-11T04:53:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    53,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications."
                },
                "authors": [
                    {
                        "name": "Fuqiang Liu"
                    },
                    {
                        "name": "Sicong Jiang"
                    },
                    {
                        "name": "Luis Miranda-Moreno"
                    },
                    {
                        "name": "Seongjin Choi"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "arxiv_comment": "AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04430v2",
                "updated": "2025-01-28T17:32:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    32,
                    33,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-08T12:57:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    12,
                    57,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "Large Language Models for cross-language code clone detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for cross-language code clone detection"
                },
                "summary": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection."
                },
                "authors": [
                    {
                        "name": "Micheline Bénédicte Moumoula"
                    },
                    {
                        "name": "Abdoul Kader Kabore"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé Bissyande"
                },
                "author": "Tegawendé Bissyande",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17088v1",
                "updated": "2025-01-28T17:22:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    22,
                    1,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:22:01Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    22,
                    1,
                    1,
                    28,
                    0
                ],
                "title": "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models"
                },
                "summary": "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning."
                },
                "authors": [
                    {
                        "name": "J. Pablo Muñoz"
                    },
                    {
                        "name": "Jinjie Yuan"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "arxiv_comment": "NAACL-25 - Main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17084v1",
                "updated": "2025-01-28T17:11:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:11:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on\n  Advanced Mathematical Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on\n  Advanced Mathematical Problem-Solving"
                },
                "summary": "Large language models (LLMs) excel in many natural language tasks, yet they\nstruggle with complex mathemat-ical problem-solving, particularly in symbolic\nreasoning and maintaining consistent output. This study evalu-ates 10 LLMs with\n7 to 8 billion parameters using 945 competition-level problems from the MATH\ndataset. The focus is on their ability to generate executable Python code as a\nstep in their reasoning process, involving over 9,450 code executions. The\nresearch introduces an evaluation framework using mistral-large-2411 to rate\nanswers on a 5-point scale, which helps address inconsistencies in mathematical\nnotation. It also examines the impact of regenerating output token-by-token on\nrefining results. The findings reveal a significant 34.5% per-formance gap\nbetween the top commercial model (gpt-4o-mini, scoring 83.7%) and the least\neffective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This\ndisparity is especially noticeable in complex areas like Number Theory. While\ntoken-by-token regeneration slightly improved accuracy (+0.8%) for the model\nllama3.1:8b, it also reduced code execution time by 36.7%, highlighting a\ntrade-off between efficiency and precision. The study also noted a consistent\ntrend where harder problems correlated with lower accuracy across all models.\nDespite using controlled execution environments, less than 1% of the generated\ncode was unsafe, and 3.17% of problems remained unsolved after 10 attempts,\nsuggesting that hybrid reasoning methods may be beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many natural language tasks, yet they\nstruggle with complex mathemat-ical problem-solving, particularly in symbolic\nreasoning and maintaining consistent output. This study evalu-ates 10 LLMs with\n7 to 8 billion parameters using 945 competition-level problems from the MATH\ndataset. The focus is on their ability to generate executable Python code as a\nstep in their reasoning process, involving over 9,450 code executions. The\nresearch introduces an evaluation framework using mistral-large-2411 to rate\nanswers on a 5-point scale, which helps address inconsistencies in mathematical\nnotation. It also examines the impact of regenerating output token-by-token on\nrefining results. The findings reveal a significant 34.5% per-formance gap\nbetween the top commercial model (gpt-4o-mini, scoring 83.7%) and the least\neffective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This\ndisparity is especially noticeable in complex areas like Number Theory. While\ntoken-by-token regeneration slightly improved accuracy (+0.8%) for the model\nllama3.1:8b, it also reduced code execution time by 36.7%, highlighting a\ntrade-off between efficiency and precision. The study also noted a consistent\ntrend where harder problems correlated with lower accuracy across all models.\nDespite using controlled execution environments, less than 1% of the generated\ncode was unsafe, and 3.17% of problems remained unsolved after 10 attempts,\nsuggesting that hybrid reasoning methods may be beneficial."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16239v2",
                "updated": "2025-01-28T17:09:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    9,
                    41,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T17:35:39Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    35,
                    39,
                    0,
                    27,
                    0
                ],
                "title": "Distilling foundation models for robust and efficient models in digital\n  pathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling foundation models for robust and efficient models in digital\n  pathology"
                },
                "summary": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
                },
                "authors": [
                    {
                        "name": "Alexandre Filiot"
                    },
                    {
                        "name": "Nicolas Dop"
                    },
                    {
                        "name": "Oussama Tchita"
                    },
                    {
                        "name": "Auriane Riou"
                    },
                    {
                        "name": "Rémy Dubois"
                    },
                    {
                        "name": "Thomas Peeters"
                    },
                    {
                        "name": "Daria Valter"
                    },
                    {
                        "name": "Marin Scalbert"
                    },
                    {
                        "name": "Charlie Saillard"
                    },
                    {
                        "name": "Geneviève Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17081v1",
                "updated": "2025-01-28T17:06:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    6,
                    9,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:06:09Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    6,
                    9,
                    1,
                    28,
                    0
                ],
                "title": "Graph Transformers for inverse physics: reconstructing flows around\n  arbitrary 2D airfoils",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Transformers for inverse physics: reconstructing flows around\n  arbitrary 2D airfoils"
                },
                "summary": "We introduce a Graph Transformer framework that serves as a general inverse\nphysics engine on meshes, demonstrated through the challenging task of\nreconstructing aerodynamic flow fields from sparse surface measurements. While\ndeep learning has shown promising results in forward physics simulation,\ninverse problems remain particularly challenging due to their ill-posed nature\nand the difficulty of propagating information from limited boundary\nobservations. Our approach addresses these challenges by combining the\ngeometric expressiveness of message-passing neural networks with the global\nreasoning of Transformers, enabling efficient learning of inverse mappings from\nboundary conditions to complete states. We evaluate this framework on a\ncomprehensive dataset of steady-state RANS simulations around diverse airfoil\ngeometries, where the task is to reconstruct full pressure and velocity fields\nfrom surface pressure measurements alone. The architecture achieves high\nreconstruction accuracy while maintaining fast inference times. We conduct\nexperiments and provide insights into the relative importance of local\ngeometric processing and global attention mechanisms in mesh-based inverse\nproblems. We also find that the framework is robust to reduced sensor coverage.\nThese results suggest that Graph Transformers can serve as effective inverse\nphysics engines across a broader range of applications where complete system\nstates must be reconstructed from limited boundary observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Graph Transformer framework that serves as a general inverse\nphysics engine on meshes, demonstrated through the challenging task of\nreconstructing aerodynamic flow fields from sparse surface measurements. While\ndeep learning has shown promising results in forward physics simulation,\ninverse problems remain particularly challenging due to their ill-posed nature\nand the difficulty of propagating information from limited boundary\nobservations. Our approach addresses these challenges by combining the\ngeometric expressiveness of message-passing neural networks with the global\nreasoning of Transformers, enabling efficient learning of inverse mappings from\nboundary conditions to complete states. We evaluate this framework on a\ncomprehensive dataset of steady-state RANS simulations around diverse airfoil\ngeometries, where the task is to reconstruct full pressure and velocity fields\nfrom surface pressure measurements alone. The architecture achieves high\nreconstruction accuracy while maintaining fast inference times. We conduct\nexperiments and provide insights into the relative importance of local\ngeometric processing and global attention mechanisms in mesh-based inverse\nproblems. We also find that the framework is robust to reduced sensor coverage.\nThese results suggest that Graph Transformers can serve as effective inverse\nphysics engines across a broader range of applications where complete system\nstates must be reconstructed from limited boundary observations."
                },
                "authors": [
                    {
                        "name": "Gregory Duthé"
                    },
                    {
                        "name": "Imad Abdallah"
                    },
                    {
                        "name": "Eleni Chatzi"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Chatzi"
                },
                "author": "Eleni Chatzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17077v1",
                "updated": "2025-01-28T17:02:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    2,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:02:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    2,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "Induced Modularity and Community Detection for Functionally\n  Interpretable Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induced Modularity and Community Detection for Functionally\n  Interpretable Reinforcement Learning"
                },
                "summary": "Interpretability in reinforcement learning is crucial for ensuring AI systems\nalign with human values and fulfill the diverse related requirements including\nsafety, robustness and fairness. Building on recent approaches to encouraging\nsparsity and locality in neural networks, we demonstrate how the penalisation\nof non-local weights leads to the emergence of functionally independent modules\nin the policy network of a reinforcement learning agent. To illustrate this, we\ndemonstrate the emergence of two parallel modules for assessment of movement\nalong the X and Y axes in a stochastic Minigrid environment. Through the novel\napplication of community detection algorithms, we show how these modules can be\nautomatically identified and their functional roles verified through direct\nintervention on the network weights prior to inference. This establishes a\nscalable framework for reinforcement learning interpretability through\nfunctional modularity, addressing challenges regarding the trade-off between\ncompleteness and cognitive tractability of reinforcement learning explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability in reinforcement learning is crucial for ensuring AI systems\nalign with human values and fulfill the diverse related requirements including\nsafety, robustness and fairness. Building on recent approaches to encouraging\nsparsity and locality in neural networks, we demonstrate how the penalisation\nof non-local weights leads to the emergence of functionally independent modules\nin the policy network of a reinforcement learning agent. To illustrate this, we\ndemonstrate the emergence of two parallel modules for assessment of movement\nalong the X and Y axes in a stochastic Minigrid environment. Through the novel\napplication of community detection algorithms, we show how these modules can be\nautomatically identified and their functional roles verified through direct\nintervention on the network weights prior to inference. This establishes a\nscalable framework for reinforcement learning interpretability through\nfunctional modularity, addressing challenges regarding the trade-off between\ncompleteness and cognitive tractability of reinforcement learning explanations."
                },
                "authors": [
                    {
                        "name": "Anna Soligo"
                    },
                    {
                        "name": "Pietro Ferraro"
                    },
                    {
                        "name": "David Boyle"
                    }
                ],
                "author_detail": {
                    "name": "David Boyle"
                },
                "author": "David Boyle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17069v1",
                "updated": "2025-01-28T16:53:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    53,
                    28,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:53:28Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    53,
                    28,
                    1,
                    28,
                    0
                ],
                "title": "Directly probing work extraction from a single qubit engine fueled by\n  quantum measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directly probing work extraction from a single qubit engine fueled by\n  quantum measurements"
                },
                "summary": "Recent progress in manipulating individual quantum systems enables the\nexploration of resources of quantum origin in engines. One of the main\nremaining challenges consists in directly measuring the work extracted from\nsuch quantum engines. Here we probe the work extracted from a transmon\nsuperconducting qubit acting as a working medium. The engine is fueled by\nperforming quantum measurements of an observable which does not commute with\nthe bare qubit Hamiltonian. Using feedback, the engine acts as a quantum\nMaxwell demon working without a hot thermal source. In our implementation, the\nqubit working medium amplifies a coherent microwave field from which work is\nextracted. The latter is not only inferred from tomographic measurements of the\nqubit state but also directly monitored via heterodyne detection of the field\nquadratures. We demonstrate the long-term stability of the engine as well as\nits robustness to transmon decoherence, losses and drifts. To emphasize the\nrole of feedback, we also study the breakdown of the engine in the open-loop\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in manipulating individual quantum systems enables the\nexploration of resources of quantum origin in engines. One of the main\nremaining challenges consists in directly measuring the work extracted from\nsuch quantum engines. Here we probe the work extracted from a transmon\nsuperconducting qubit acting as a working medium. The engine is fueled by\nperforming quantum measurements of an observable which does not commute with\nthe bare qubit Hamiltonian. Using feedback, the engine acts as a quantum\nMaxwell demon working without a hot thermal source. In our implementation, the\nqubit working medium amplifies a coherent microwave field from which work is\nextracted. The latter is not only inferred from tomographic measurements of the\nqubit state but also directly monitored via heterodyne detection of the field\nquadratures. We demonstrate the long-term stability of the engine as well as\nits robustness to transmon decoherence, losses and drifts. To emphasize the\nrole of feedback, we also study the breakdown of the engine in the open-loop\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Rémy Dassonneville"
                    },
                    {
                        "name": "Cyril Elouard"
                    },
                    {
                        "name": "Romain Cazali"
                    },
                    {
                        "name": "Réouven Assouly"
                    },
                    {
                        "name": "Audrey Bienfait"
                    },
                    {
                        "name": "Alexia Auffèves"
                    },
                    {
                        "name": "Benjamin Huard"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Huard"
                },
                "author": "Benjamin Huard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13631v2",
                "updated": "2025-01-28T16:42:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    42,
                    59,
                    1,
                    28,
                    0
                ],
                "published": "2024-06-19T15:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    28,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "On AI-Inspired UI-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On AI-Inspired UI-Design"
                },
                "summary": "Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches."
                },
                "authors": [
                    {
                        "name": "Jialiang Wei"
                    },
                    {
                        "name": "Anne-Lise Courbis"
                    },
                    {
                        "name": "Thomas Lambolais"
                    },
                    {
                        "name": "Gérard Dray"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17062v1",
                "updated": "2025-01-28T16:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    40,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    40,
                    40,
                    1,
                    28,
                    0
                ],
                "title": "EdgeMLOps: Operationalizing ML models with Cumulocity IoT and\n  thin-edge.io for Visual quality Inspection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeMLOps: Operationalizing ML models with Cumulocity IoT and\n  thin-edge.io for Visual quality Inspection"
                },
                "summary": "This paper introduces EdgeMLOps, a framework leveraging Cumulocity IoT and\nthin-edge.io for deploying and managing machine learning models on\nresource-constrained edge devices. We address the challenges of model\noptimization, deployment, and lifecycle management in edge environments. The\nframework's efficacy is demonstrated through a visual quality inspection (VQI)\nuse case where images of assets are processed on edge devices, enabling\nreal-time condition updates within an asset management system. Furthermore, we\nevaluate the performance benefits of different quantization methods,\nspecifically static and dynamic signed-int8, on a Raspberry Pi 4, demonstrating\nsignificant inference time reductions compared to FP32 precision. Our results\nhighlight the potential of EdgeMLOps to enable efficient and scalable AI\ndeployments at the edge for industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EdgeMLOps, a framework leveraging Cumulocity IoT and\nthin-edge.io for deploying and managing machine learning models on\nresource-constrained edge devices. We address the challenges of model\noptimization, deployment, and lifecycle management in edge environments. The\nframework's efficacy is demonstrated through a visual quality inspection (VQI)\nuse case where images of assets are processed on edge devices, enabling\nreal-time condition updates within an asset management system. Furthermore, we\nevaluate the performance benefits of different quantization methods,\nspecifically static and dynamic signed-int8, on a Raspberry Pi 4, demonstrating\nsignificant inference time reductions compared to FP32 precision. Our results\nhighlight the potential of EdgeMLOps to enable efficient and scalable AI\ndeployments at the edge for industrial applications."
                },
                "authors": [
                    {
                        "name": "Kanishk Chaturvedi"
                    },
                    {
                        "name": "Johannes Gasthuber"
                    },
                    {
                        "name": "Mohamed Abdelaal"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Abdelaal"
                },
                "author": "Mohamed Abdelaal",
                "arxiv_journal_ref": "Industry Track of the 21st Conference on Database Systems for\n  Business, Technology and Web (BTW'25, Bamberg, Germany), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17127v2",
                "updated": "2025-01-28T16:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    31,
                    51,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-22T16:00:26Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    0,
                    26,
                    1,
                    296,
                    0
                ],
                "title": "PAPILLON: Privacy Preservation from Internet-based and Local Language\n  Model Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAPILLON: Privacy Preservation from Internet-based and Local Language\n  Model Ensembles"
                },
                "summary": "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON."
                },
                "authors": [
                    {
                        "name": "Li Siyan"
                    },
                    {
                        "name": "Vethavikashini Chithrra Raghuram"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07727v3",
                "updated": "2025-01-28T16:31:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    31,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2023-12-12T20:44:07Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    20,
                    44,
                    7,
                    1,
                    346,
                    0
                ],
                "title": "Two-sample inference for sparse functional data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-sample inference for sparse functional data"
                },
                "summary": "We propose a novel test procedure for comparing mean functions across two\ngroups within the reproducing kernel Hilbert space (RKHS) framework. Our\nproposed method is adept at handling sparsely and irregularly sampled\nfunctional data when observation times are random for each subject.\nConventional approaches, which are built upon functional principal components\nanalysis, usually assume a homogeneous covariance structure across groups.\nNonetheless, justifying this assumption in real-world scenarios can be\nchallenging. To eliminate the need for a homogeneous covariance structure, we\nfirst develop a linear approximation for the mean estimator under the RKHS\nframework; this approximation is a sum of i.i.d. random elements, which\nnaturally leads to the desirable pointwise limiting distributions. Moreover, we\nestablish weak convergence for the mean estimator, allowing us to construct a\ntest statistic for the mean difference. Our method is easily implementable and\noutperforms some conventional tests in controlling type I errors across various\nsettings. We demonstrate the finite sample performance of our approach through\nextensive simulations and two real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel test procedure for comparing mean functions across two\ngroups within the reproducing kernel Hilbert space (RKHS) framework. Our\nproposed method is adept at handling sparsely and irregularly sampled\nfunctional data when observation times are random for each subject.\nConventional approaches, which are built upon functional principal components\nanalysis, usually assume a homogeneous covariance structure across groups.\nNonetheless, justifying this assumption in real-world scenarios can be\nchallenging. To eliminate the need for a homogeneous covariance structure, we\nfirst develop a linear approximation for the mean estimator under the RKHS\nframework; this approximation is a sum of i.i.d. random elements, which\nnaturally leads to the desirable pointwise limiting distributions. Moreover, we\nestablish weak convergence for the mean estimator, allowing us to construct a\ntest statistic for the mean difference. Our method is easily implementable and\noutperforms some conventional tests in controlling type I errors across various\nsettings. We demonstrate the finite sample performance of our approach through\nextensive simulations and two real-world applications."
                },
                "authors": [
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Peijun Sang"
                    },
                    {
                        "name": "Yingli Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yingli Qin"
                },
                "author": "Yingli Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17032v2",
                "updated": "2025-01-28T16:28:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    28,
                    10,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-22T14:17:12Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    14,
                    17,
                    12,
                    6,
                    357,
                    0
                ],
                "title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17049v1",
                "updated": "2025-01-28T16:17:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    17,
                    9,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    17,
                    9,
                    1,
                    28,
                    0
                ],
                "title": "Hellinger-Kantorovich Gradient Flows: Global Exponential Decay of\n  Entropy Functionals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hellinger-Kantorovich Gradient Flows: Global Exponential Decay of\n  Entropy Functionals"
                },
                "summary": "We investigate a family of gradient flows of positive and probability\nmeasures, focusing on the Hellinger-Kantorovich (HK) geometry, which unifies\ntransport mechanism of Otto-Wasserstein, and the birth-death mechanism of\nHellinger (or Fisher-Rao). A central contribution is a complete\ncharacterization of global exponential decay behaviors of entropy functionals\n(e.g. KL, $\\chi^2$) under Otto-Wasserstein and Hellinger-type gradient flows.\nIn particular, for the more challenging analysis of HK gradient flows on\npositive measures -- where the typical log-Sobolev arguments fail -- we develop\na specialized shape-mass decomposition that enables new analysis results. Our\napproach also leverages the (Polyak-)\\L{}ojasiewicz-type functional\ninequalities and a careful extension of classical dissipation estimates. These\nfindings provide a unified and complete theoretical framework for gradient\nflows and underpin applications in computational algorithms for statistical\ninference, optimization, and machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a family of gradient flows of positive and probability\nmeasures, focusing on the Hellinger-Kantorovich (HK) geometry, which unifies\ntransport mechanism of Otto-Wasserstein, and the birth-death mechanism of\nHellinger (or Fisher-Rao). A central contribution is a complete\ncharacterization of global exponential decay behaviors of entropy functionals\n(e.g. KL, $\\chi^2$) under Otto-Wasserstein and Hellinger-type gradient flows.\nIn particular, for the more challenging analysis of HK gradient flows on\npositive measures -- where the typical log-Sobolev arguments fail -- we develop\na specialized shape-mass decomposition that enables new analysis results. Our\napproach also leverages the (Polyak-)\\L{}ojasiewicz-type functional\ninequalities and a careful extension of classical dissipation estimates. These\nfindings provide a unified and complete theoretical framework for gradient\nflows and underpin applications in computational algorithms for statistical\ninference, optimization, and machine learning."
                },
                "authors": [
                    {
                        "name": "Alexander Mielke"
                    },
                    {
                        "name": "Jia-Jie Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jia-Jie Zhu"
                },
                "author": "Jia-Jie Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49Q22 (Primary) 35Q49 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17044v1",
                "updated": "2025-01-28T16:09:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    9,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:09:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    9,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with\n  Transformers"
                },
                "summary": "We generate abstractions of buildings, reflecting the essential aspects of\ntheir geometry and structure, by learning to invert procedural models. We first\nbuild a dataset of abstract procedural building models paired with simulated\npoint clouds and then learn the inverse mapping through a transformer. Given a\npoint cloud, the trained transformer then infers the corresponding abstracted\nbuilding in terms of a programmatic language description. This approach\nleverages expressive procedural models developed for gaming and animation, and\nthereby retains desirable properties such as efficient rendering of the\ninferred abstractions and strong priors for regularity and symmetry. Our\napproach achieves good reconstruction accuracy in terms of geometry and\nstructure, as well as structurally consistent inpainting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We generate abstractions of buildings, reflecting the essential aspects of\ntheir geometry and structure, by learning to invert procedural models. We first\nbuild a dataset of abstract procedural building models paired with simulated\npoint clouds and then learn the inverse mapping through a transformer. Given a\npoint cloud, the trained transformer then infers the corresponding abstracted\nbuilding in terms of a programmatic language description. This approach\nleverages expressive procedural models developed for gaming and animation, and\nthereby retains desirable properties such as efficient rendering of the\ninferred abstractions and strong priors for regularity and symmetry. Our\napproach achieves good reconstruction accuracy in terms of geometry and\nstructure, as well as structurally consistent inpainting."
                },
                "authors": [
                    {
                        "name": "Max Dax"
                    },
                    {
                        "name": "Jordi Berbel"
                    },
                    {
                        "name": "Jan Stria"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Urs Bergmann"
                    }
                ],
                "author_detail": {
                    "name": "Urs Bergmann"
                },
                "author": "Urs Bergmann",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17039v1",
                "updated": "2025-01-28T16:03:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    3,
                    52,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:03:52Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    3,
                    52,
                    1,
                    28,
                    0
                ],
                "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block\n  Representations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block\n  Representations with Large Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated exceptional\npower in various domains, including information retrieval. Most of the previous\npractices involve leveraging these models to create a single embedding for each\nquery, each passage, or each document individually, a strategy exemplified and\nused by the Retrieval-Augmented Generation (RAG) framework. While this method\nhas proven effective, we argue that it falls short in fully capturing the\nnuanced intricacies of document-level texts due to its reliance on a relatively\ncoarse-grained representation. To address this limitation, we introduce a\nnovel, fine-grained approach aimed at enhancing the accuracy of relevance\nscoring for long documents. Our methodology firstly segments a long document\ninto blocks, each of which is embedded using an LLM, for matching with the\nquery representation. When calculating the relevance score, we aggregate the\nquery-block relevance scores through a weighted sum method, yielding a\ncomprehensive score for the query with the entire document. Despite its\napparent simplicity, our experimental findings reveal that this approach\noutperforms standard representation methods and achieves a significant\nreduction in embedding generation latency. Moreover, by carefully optimizing\npairwise loss functions, superior performances have been achieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated exceptional\npower in various domains, including information retrieval. Most of the previous\npractices involve leveraging these models to create a single embedding for each\nquery, each passage, or each document individually, a strategy exemplified and\nused by the Retrieval-Augmented Generation (RAG) framework. While this method\nhas proven effective, we argue that it falls short in fully capturing the\nnuanced intricacies of document-level texts due to its reliance on a relatively\ncoarse-grained representation. To address this limitation, we introduce a\nnovel, fine-grained approach aimed at enhancing the accuracy of relevance\nscoring for long documents. Our methodology firstly segments a long document\ninto blocks, each of which is embedded using an LLM, for matching with the\nquery representation. When calculating the relevance score, we aggregate the\nquery-block relevance scores through a weighted sum method, yielding a\ncomprehensive score for the query with the entire document. Despite its\napparent simplicity, our experimental findings reveal that this approach\noutperforms standard representation methods and achieves a significant\nreduction in embedding generation latency. Moreover, by carefully optimizing\npairwise loss functions, superior performances have been achieved."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17030v1",
                "updated": "2025-01-28T15:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    52,
                    51,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T15:52:51Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    52,
                    51,
                    1,
                    28,
                    0
                ],
                "title": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented."
                },
                "authors": [
                    {
                        "name": "Manojkumar Parmar"
                    },
                    {
                        "name": "Yuvaraj Govindarajulu"
                    }
                ],
                "author_detail": {
                    "name": "Yuvaraj Govindarajulu"
                },
                "author": "Yuvaraj Govindarajulu",
                "arxiv_comment": "9 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17024v1",
                "updated": "2025-01-28T15:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    41,
                    54,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T15:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    41,
                    54,
                    1,
                    28,
                    0
                ],
                "title": "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs"
                },
                "summary": "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required."
                },
                "authors": [
                    {
                        "name": "Alessandro Midolo"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Di Penta"
                },
                "author": "Massimiliano Di Penta",
                "arxiv_journal_ref": "Proceedings of the 33rd IEEE/ACM International Conference on\n  Program Comprehension (ICPC 2025), April 27-28 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v2",
                "updated": "2025-01-28T15:11:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    11,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "arxiv_comment": "Accepted to ICLR 2025 main conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16998v1",
                "updated": "2025-01-28T14:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T14:52:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "Large Language Models for Code Generation: The Practitioners Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code Generation: The Practitioners Perspective"
                },
                "summary": "Large Language Models (LLMs) have emerged as coding assistants, capable of\ngenerating source code from natural language prompts. With the increasing\nadoption of LLMs in software development, academic research and industry based\nprojects are developing various tools, benchmarks, and metrics to evaluate the\neffectiveness of LLM-generated code. However, there is a lack of solutions\nevaluated through empirically grounded methods that incorporate practitioners\nperspectives to assess functionality, syntax, and accuracy in real world\napplications. To address this gap, we propose and develop a multi-model unified\nplatform to generate and execute code based on natural language prompts. We\nconducted a survey with 60 software practitioners from 11 countries across four\ncontinents working in diverse professional roles and domains to evaluate the\nusability, performance, strengths, and limitations of each model. The results\npresent practitioners feedback and insights into the use of LLMs in software\ndevelopment, including their strengths and weaknesses, key aspects overlooked\nby benchmarks and metrics, and a broader understanding of their practical\napplicability. These findings can help researchers and practitioners make\ninformed decisions for systematically selecting and using LLMs in software\ndevelopment projects. Future research will focus on integrating more diverse\nmodels into the proposed system, incorporating additional case studies, and\nconducting developer interviews for deeper empirical insights into LLM-driven\nsoftware development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as coding assistants, capable of\ngenerating source code from natural language prompts. With the increasing\nadoption of LLMs in software development, academic research and industry based\nprojects are developing various tools, benchmarks, and metrics to evaluate the\neffectiveness of LLM-generated code. However, there is a lack of solutions\nevaluated through empirically grounded methods that incorporate practitioners\nperspectives to assess functionality, syntax, and accuracy in real world\napplications. To address this gap, we propose and develop a multi-model unified\nplatform to generate and execute code based on natural language prompts. We\nconducted a survey with 60 software practitioners from 11 countries across four\ncontinents working in diverse professional roles and domains to evaluate the\nusability, performance, strengths, and limitations of each model. The results\npresent practitioners feedback and insights into the use of LLMs in software\ndevelopment, including their strengths and weaknesses, key aspects overlooked\nby benchmarks and metrics, and a broader understanding of their practical\napplicability. These findings can help researchers and practitioners make\ninformed decisions for systematically selecting and using LLMs in software\ndevelopment projects. Future research will focus on integrating more diverse\nmodels into the proposed system, incorporating additional case studies, and\nconducting developer interviews for deeper empirical insights into LLM-driven\nsoftware development."
                },
                "authors": [
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai Kristian Kemell"
                    },
                    {
                        "name": "Aakash Ahmad"
                    },
                    {
                        "name": "Malik Abdul Sami"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Kari Systä"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "20 pages, 4 figures, 2 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15050v3",
                "updated": "2025-01-28T14:33:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    33,
                    42,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-19T16:57:45Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    57,
                    45,
                    3,
                    354,
                    0
                ],
                "title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion"
                },
                "summary": "Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area."
                },
                "authors": [
                    {
                        "name": "Zhifei Chen"
                    },
                    {
                        "name": "Tianshuo Xu"
                    },
                    {
                        "name": "Wenhang Ge"
                    },
                    {
                        "name": "Leyi Wu"
                    },
                    {
                        "name": "Dongyu Yan"
                    },
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Luozhou Wang"
                    },
                    {
                        "name": "Lu Zeng"
                    },
                    {
                        "name": "Shunsi Zhang"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18644v3",
                "updated": "2025-01-28T14:23:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    23,
                    17,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-24T16:06:53Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    6,
                    53,
                    1,
                    359,
                    0
                ],
                "title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation"
                },
                "summary": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    }
                ],
                "author_detail": {
                    "name": "Karishma Thakrar"
                },
                "author": "Karishma Thakrar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16975v1",
                "updated": "2025-01-28T14:15:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    15,
                    42,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T14:15:42Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    15,
                    42,
                    1,
                    28,
                    0
                ],
                "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling"
                },
                "summary": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs."
                },
                "authors": [
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Defa Zhu"
                    },
                    {
                        "name": "Banggu Wu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15602v2",
                "updated": "2025-01-28T14:14:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    14,
                    3,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-26T17:05:16Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    17,
                    5,
                    16,
                    6,
                    26,
                    0
                ],
                "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning"
                },
                "summary": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability."
                },
                "authors": [
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Yun Liao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23167v2",
                "updated": "2025-01-28T13:51:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    51,
                    20,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-30T16:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "BPASS stellar evolution models incorporating $α$-enhanced\n  composition -- I. Single star models from 0.1 to 316 M$_\\odot$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPASS stellar evolution models incorporating $α$-enhanced\n  composition -- I. Single star models from 0.1 to 316 M$_\\odot$"
                },
                "summary": "Stellar evolution modelling is fundamental to many areas of astrophysics\nincluding stellar populations in both nearby and distant galaxies. It is\nheavily influenced by chemical composition. Observations of distant galaxies\nand nucleosynthesis calculations show that $\\alpha$-process elements are\nenriched faster than iron group elements. We present a dense grid of\nsingle-star models calculated using the BPASS stellar evolution code and\ncovering masses ($0.1\\le\\mathrm{M/M}_\\odot\\le316$), metallicity mass fractions\n($10^{-5} \\le Z \\le 0.04$) and $\\alpha$-to-iron abundance ratios\n($-0.2\\le[\\alpha/\\mathrm{Fe}]\\le+0.6$). By comparing Solar-scaled models to\nones enriched in $\\alpha$-process elements, we find that stellar radii, surface\ntemperatures, Main Sequence lifetimes, supernova progenitor properties and\nsupernova rates are all sensitive to changes in [$\\alpha$/Fe]. Lifetimes of\nlow-mass stars differ by up to 0.4 dex, while surface temperatures of massive\nstars at the end of the Main Sequence also differ by around 0.4 dex. Inferred\nsupernova rates when [Fe/H] is unknown can be highly uncertain. Models with\ndifferent [$\\alpha$/Fe] but comparable iron abundances show smaller variations,\nindicating that while iron primarily defines the course of evolution;\n$\\alpha$-enhancement nonetheless has an impact of up to 0.1 dex on stellar\nproperties. Such changes are small for individual stars, but have a large\ncumulative effect when considering an entire stellar population as demonstrated\nby isochrone fitting to nearby clusters. Changes in radii and lifetimes have\nfurther consequences for a stellar population including binary stars, as they\ninfluence the timing, nature and occurrence rate of mass transfer events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar evolution modelling is fundamental to many areas of astrophysics\nincluding stellar populations in both nearby and distant galaxies. It is\nheavily influenced by chemical composition. Observations of distant galaxies\nand nucleosynthesis calculations show that $\\alpha$-process elements are\nenriched faster than iron group elements. We present a dense grid of\nsingle-star models calculated using the BPASS stellar evolution code and\ncovering masses ($0.1\\le\\mathrm{M/M}_\\odot\\le316$), metallicity mass fractions\n($10^{-5} \\le Z \\le 0.04$) and $\\alpha$-to-iron abundance ratios\n($-0.2\\le[\\alpha/\\mathrm{Fe}]\\le+0.6$). By comparing Solar-scaled models to\nones enriched in $\\alpha$-process elements, we find that stellar radii, surface\ntemperatures, Main Sequence lifetimes, supernova progenitor properties and\nsupernova rates are all sensitive to changes in [$\\alpha$/Fe]. Lifetimes of\nlow-mass stars differ by up to 0.4 dex, while surface temperatures of massive\nstars at the end of the Main Sequence also differ by around 0.4 dex. Inferred\nsupernova rates when [Fe/H] is unknown can be highly uncertain. Models with\ndifferent [$\\alpha$/Fe] but comparable iron abundances show smaller variations,\nindicating that while iron primarily defines the course of evolution;\n$\\alpha$-enhancement nonetheless has an impact of up to 0.1 dex on stellar\nproperties. Such changes are small for individual stars, but have a large\ncumulative effect when considering an entire stellar population as demonstrated\nby isochrone fitting to nearby clusters. Changes in radii and lifetimes have\nfurther consequences for a stellar population including binary stars, as they\ninfluence the timing, nature and occurrence rate of mass transfer events."
                },
                "authors": [
                    {
                        "name": "Conor M Byrne"
                    },
                    {
                        "name": "Jan J Eldridge"
                    },
                    {
                        "name": "Elizabeth R Stanway"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth R Stanway"
                },
                "author": "Elizabeth R Stanway",
                "arxiv_comment": "Accepted for publication in MNRAS. 20 pages, 16 figures.\n  Supplementary material included in the form of an appendix (8 pages, 9\n  additional figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16952v1",
                "updated": "2025-01-28T13:49:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    49,
                    39,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:49:39Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    49,
                    39,
                    1,
                    28,
                    0
                ],
                "title": "Multiple Abstraction Level Retrieve Augment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Abstraction Level Retrieve Augment Generation"
                },
                "summary": "A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers."
                },
                "authors": [
                    {
                        "name": "Zheng Zheng"
                    },
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Pengyu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Hong"
                },
                "arxiv_affiliation": "Brandeis University",
                "author": "Pengyu Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16945v1",
                "updated": "2025-01-28T13:42:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    42,
                    33,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:42:33Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    42,
                    33,
                    1,
                    28,
                    0
                ],
                "title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations"
                },
                "summary": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows."
                },
                "authors": [
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Qiuyang Wang"
                    },
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Pengyu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Hong"
                },
                "arxiv_affiliation": "Brandeis University",
                "author": "Pengyu Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15361v3",
                "updated": "2025-01-28T13:22:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    22,
                    47,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-19T19:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    19,
                    47,
                    35,
                    3,
                    354,
                    0
                ],
                "title": "A Generative Framework for Probabilistic, Spatiotemporally Coherent\n  Downscaling of Climate Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Framework for Probabilistic, Spatiotemporally Coherent\n  Downscaling of Climate Simulation"
                },
                "summary": "Local climate information is crucial for impact assessment and\ndecision-making, yet coarse global climate simulations cannot capture\nsmall-scale phenomena. Current statistical downscaling methods infer these\nphenomena as temporally decoupled spatial patches. However, to preserve\nphysical properties, estimating spatio-temporally coherent high-resolution\nweather dynamics for multiple variables across long time horizons is crucial.\nWe present a novel generative framework that uses a score-based diffusion model\ntrained on high-resolution reanalysis data to capture the statistical\nproperties of local weather dynamics. After training, we condition on coarse\nclimate model data to generate weather patterns consistent with the aggregate\ninformation. As this predictive task is inherently uncertain, we leverage the\nprobabilistic nature of diffusion models and sample multiple trajectories. We\nevaluate our approach with high-resolution reanalysis information before\napplying it to the climate model downscaling task. We then demonstrate that the\nmodel generates spatially and temporally coherent weather dynamics that align\nwith global climate output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local climate information is crucial for impact assessment and\ndecision-making, yet coarse global climate simulations cannot capture\nsmall-scale phenomena. Current statistical downscaling methods infer these\nphenomena as temporally decoupled spatial patches. However, to preserve\nphysical properties, estimating spatio-temporally coherent high-resolution\nweather dynamics for multiple variables across long time horizons is crucial.\nWe present a novel generative framework that uses a score-based diffusion model\ntrained on high-resolution reanalysis data to capture the statistical\nproperties of local weather dynamics. After training, we condition on coarse\nclimate model data to generate weather patterns consistent with the aggregate\ninformation. As this predictive task is inherently uncertain, we leverage the\nprobabilistic nature of diffusion models and sample multiple trajectories. We\nevaluate our approach with high-resolution reanalysis information before\napplying it to the climate model downscaling task. We then demonstrate that the\nmodel generates spatially and temporally coherent weather dynamics that align\nwith global climate output."
                },
                "authors": [
                    {
                        "name": "Jonathan Schmidt"
                    },
                    {
                        "name": "Luca Schmidt"
                    },
                    {
                        "name": "Felix Strnad"
                    },
                    {
                        "name": "Nicole Ludwig"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "15 pages, 6 figures, additional supplementary text and figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09560v2",
                "updated": "2025-01-28T13:17:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    17,
                    29,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-12T18:46:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "Foundational Large Language Models for Materials Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models for Materials Research"
                },
                "summary": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems."
                },
                "authors": [
                    {
                        "name": "Vaibhav Mishra"
                    },
                    {
                        "name": "Somaditya Singh"
                    },
                    {
                        "name": "Dhruv Ahlawat"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Vaibhav Bihani"
                    },
                    {
                        "name": "Hargun Singh Grover"
                    },
                    {
                        "name": "Biswajit Mishra"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Mausam"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1801.09637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1801.09637v2",
                "updated": "2025-01-28T12:54:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    54,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2018-01-29T17:24:27Z",
                "published_parsed": [
                    2018,
                    1,
                    29,
                    17,
                    24,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "Geospatial distributions reflect rates of evolution of features of\n  language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial distributions reflect rates of evolution of features of\n  language"
                },
                "summary": "Quantifying the speed of linguistic change is challenging due to the fact\nthat the historical evolution of languages is sparsely documented.\nConsequently, traditional methods rely on phylogenetic reconstruction. In this\npaper, we propose a model-based approach to the problem through the analysis of\nlanguage change as a stochastic process combining vertical descent, spatial\ninteractions, and mutations in both dimensions. A notion of linguistic\ntemperature emerges naturally from this analysis as a dimensionless measure of\nthe propensity of a linguistic feature to undergo change. We demonstrate how\ntemperatures of linguistic features can be inferred from their present-day\ngeospatial distributions, without recourse to information about their\nphylogenies. Thus the evolutionary dynamics of language, operating across\nthousands of years, leaves a measurable geospatial signature. This signature\nlicenses inferences about the historical evolution of languages even in the\nabsence of longitudinal data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the speed of linguistic change is challenging due to the fact\nthat the historical evolution of languages is sparsely documented.\nConsequently, traditional methods rely on phylogenetic reconstruction. In this\npaper, we propose a model-based approach to the problem through the analysis of\nlanguage change as a stochastic process combining vertical descent, spatial\ninteractions, and mutations in both dimensions. A notion of linguistic\ntemperature emerges naturally from this analysis as a dimensionless measure of\nthe propensity of a linguistic feature to undergo change. We demonstrate how\ntemperatures of linguistic features can be inferred from their present-day\ngeospatial distributions, without recourse to information about their\nphylogenies. Thus the evolutionary dynamics of language, operating across\nthousands of years, leaves a measurable geospatial signature. This signature\nlicenses inferences about the historical evolution of languages even in the\nabsence of longitudinal data."
                },
                "authors": [
                    {
                        "name": "Henri Kauhanen"
                    },
                    {
                        "name": "Deepthi Gopal"
                    },
                    {
                        "name": "Tobias Galla"
                    },
                    {
                        "name": "Ricardo Bermúdez-Otero"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Bermúdez-Otero"
                },
                "author": "Ricardo Bermúdez-Otero",
                "arxiv_doi": "10.1126/sciadv.abe6540",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1126/sciadv.abe6540",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/1801.09637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1801.09637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "44 pages, of which 24 pages Supplementary Information, 8 figures, 3\n  tables",
                "arxiv_journal_ref": "Sci. Adv. 2021; 7 : eabe6540",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16904v1",
                "updated": "2025-01-28T12:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    44,
                    27,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:44:27Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    44,
                    27,
                    1,
                    28,
                    0
                ],
                "title": "Adversarial Masked Autoencoder Purifier with Defense Transferability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Masked Autoencoder Purifier with Defense Transferability"
                },
                "summary": "The study of adversarial defense still struggles to combat with advanced\nadversarial attacks. In contrast to most prior studies that rely on the\ndiffusion model for test-time defense to remarkably increase the inference\ntime, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked\nAutoEncoder (MAE) into an adversarial purifier framework for test-time\npurification. While MAEP achieves promising adversarial robustness, it\nparticularly features model defense transferability and attack generalization\nwithout relying on using additional data that is different from the training\ndataset. To our knowledge, MAEP is the first study of adversarial purifier\nbased on MAE. Extensive experimental results demonstrate that our method can\nnot only maintain clear accuracy with only a slight drop but also exhibit a\nclose gap between the clean and robust accuracy. Notably, MAEP trained on\nCIFAR10 achieves state-of-the-art performance even when tested directly on\nImageNet, outperforming existing diffusion-based models trained specifically on\nImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of adversarial defense still struggles to combat with advanced\nadversarial attacks. In contrast to most prior studies that rely on the\ndiffusion model for test-time defense to remarkably increase the inference\ntime, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked\nAutoEncoder (MAE) into an adversarial purifier framework for test-time\npurification. While MAEP achieves promising adversarial robustness, it\nparticularly features model defense transferability and attack generalization\nwithout relying on using additional data that is different from the training\ndataset. To our knowledge, MAEP is the first study of adversarial purifier\nbased on MAE. Extensive experimental results demonstrate that our method can\nnot only maintain clear accuracy with only a slight drop but also exhibit a\nclose gap between the clean and robust accuracy. Notably, MAEP trained on\nCIFAR10 achieves state-of-the-art performance even when tested directly on\nImageNet, outperforming existing diffusion-based models trained specifically on\nImageNet."
                },
                "authors": [
                    {
                        "name": "Yuan-Chih Chen"
                    },
                    {
                        "name": "Chun-Shien Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Shien Lu"
                },
                "author": "Chun-Shien Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16899v1",
                "updated": "2025-01-28T12:35:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    35,
                    6,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:35:06Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    35,
                    6,
                    1,
                    28,
                    0
                ],
                "title": "RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with\n  Enhanced Contextual Awareness in Specific Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with\n  Enhanced Contextual Awareness in Specific Domains"
                },
                "summary": "Large language models (LLMs) represent a significant advancement in\nintegrating physical robots with AI-driven systems. We showcase the\ncapabilities of our framework within the context of the real-world household\ncompetition. This research introduces a framework that utilizes RDMM (Robotics\nDecision-Making Models), which possess the capacity for decision-making within\ndomain-specific contexts, as well as an awareness of their personal knowledge\nand capabilities. The framework leverages information to enhance the autonomous\ndecision-making of the system. In contrast to other approaches, our focus is on\nreal-time, on-device solutions, successfully operating on hardware with as\nlittle as 8GB of memory. Our framework incorporates visual perception models\nequipping robots with understanding of their environment. Additionally, the\nframework has integrated real-time speech recognition capabilities, thus\nenhancing the human-robot interaction experience. Experimental results\ndemonstrate that the RDMM framework can plan with an 93\\% accuracy.\nFurthermore, we introduce a new dataset consisting of 27k planning instances,\nas well as 1.3k text-image annotated samples derived from the competition. The\nframework, benchmarks, datasets, and models developed in this work are publicly\navailable on our GitHub repository at https://github.com/shadynasrat/RDMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a significant advancement in\nintegrating physical robots with AI-driven systems. We showcase the\ncapabilities of our framework within the context of the real-world household\ncompetition. This research introduces a framework that utilizes RDMM (Robotics\nDecision-Making Models), which possess the capacity for decision-making within\ndomain-specific contexts, as well as an awareness of their personal knowledge\nand capabilities. The framework leverages information to enhance the autonomous\ndecision-making of the system. In contrast to other approaches, our focus is on\nreal-time, on-device solutions, successfully operating on hardware with as\nlittle as 8GB of memory. Our framework incorporates visual perception models\nequipping robots with understanding of their environment. Additionally, the\nframework has integrated real-time speech recognition capabilities, thus\nenhancing the human-robot interaction experience. Experimental results\ndemonstrate that the RDMM framework can plan with an 93\\% accuracy.\nFurthermore, we introduce a new dataset consisting of 27k planning instances,\nas well as 1.3k text-image annotated samples derived from the competition. The\nframework, benchmarks, datasets, and models developed in this work are publicly\navailable on our GitHub repository at https://github.com/shadynasrat/RDMM."
                },
                "authors": [
                    {
                        "name": "Shady Nasrat"
                    },
                    {
                        "name": "Myungsu Kim"
                    },
                    {
                        "name": "Seonil Lee"
                    },
                    {
                        "name": "Jiho Lee"
                    },
                    {
                        "name": "Yeoncheol Jang"
                    },
                    {
                        "name": "Seung-joon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Seung-joon Yi"
                },
                "author": "Seung-joon Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16884v1",
                "updated": "2025-01-28T12:13:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    13,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:13:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    13,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irony Detection, Reasoning and Understanding in Zero-shot Learning"
                },
                "summary": "Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs."
                },
                "authors": [
                    {
                        "name": "Peiling Yi"
                    },
                    {
                        "name": "Yuhan Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Xia"
                },
                "author": "Yuhan Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07839v2",
                "updated": "2025-01-28T11:42:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    42,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T11:58:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Semantic Self-Consistency: Enhancing Language Model Reasoning via\n  Semantic Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Self-Consistency: Enhancing Language Model Reasoning via\n  Semantic Weighting"
                },
                "summary": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al.'s self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the semantic information detailed in the step-by-step reasoning\npaths. Our work introduces semantic self-consistency, enhancing this approach\nby incorporating and analyzing both the reasoning paths of these rationales in\naddition to their final decisions before taking a majority vote. These methods\nnot only improve the reliability of reasoning paths but also cause more robust\nperformance on complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al.'s self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the semantic information detailed in the step-by-step reasoning\npaths. Our work introduces semantic self-consistency, enhancing this approach\nby incorporating and analyzing both the reasoning paths of these rationales in\naddition to their final decisions before taking a majority vote. These methods\nnot only improve the reliability of reasoning paths but also cause more robust\nperformance on complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Tim Knappe"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Ayush Chauhan"
                    },
                    {
                        "name": "Kaylee Chhua"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted to MATH-AI at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16865v1",
                "updated": "2025-01-28T11:30:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    30,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T11:30:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    30,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience"
                },
                "summary": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com/Zzoay/JRE-L.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com/Zzoay/JRE-L."
                },
                "authors": [
                    {
                        "name": "Gongyao Jiang"
                    },
                    {
                        "name": "Xinran Shi"
                    },
                    {
                        "name": "Qiong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Qiong Luo"
                },
                "author": "Qiong Luo",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.09756",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16857v1",
                "updated": "2025-01-28T11:11:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T11:11:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "Comparing Human and LLM Generated Code: The Jury is Still Out!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Human and LLM Generated Code: The Jury is Still Out!"
                },
                "summary": "Much is promised in relation to AI-supported software development. However,\nthere has been limited evaluation effort in the research domain aimed at\nvalidating the true utility of such techniques, especially when compared to\nhuman coding outputs. We bridge this gap, where a benchmark dataset comprising\n72 distinct software engineering tasks is used to compare the effectiveness of\nlarge language models (LLMs) and human programmers in producing Python software\ncode. GPT-4 is used as a representative LLM, where for the code generated by\nhumans and this LLM, we evaluate code quality and adherence to Python coding\nstandards, code security and vulnerabilities, code complexity and functional\ncorrectness. We use various static analysis benchmarks, including Pylint,\nRadon, Bandit and test cases. Among the notable outcomes, results show that\nhuman-generated code recorded higher ratings for adhering to coding standards\nthan GPT-4. We observe security flaws in code generated by both humans and\nGPT-4, however, code generated by humans shows a greater variety of problems,\nbut GPT-4 code included more severe outliers. Our results show that although\nGPT-4 is capable of producing coding solutions, it frequently produces more\ncomplex code that may need more reworking to ensure maintainability. On the\ncontrary however, our outcomes show that a higher number of test cases passed\nfor code generated by GPT-4 across a range of tasks than code that was\ngenerated by humans. That said, GPT-4 frequently struggles with complex\nproblem-solving that involve in-depth domain knowledge. This study highlights\nthe potential utility of LLMs for supporting software development, however,\ntasks requiring comprehensive, innovative or unconventional solutions, and\ncareful debugging and error correction seem to be better developed by human\nprogrammers. We plot an agenda for the software engineering community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much is promised in relation to AI-supported software development. However,\nthere has been limited evaluation effort in the research domain aimed at\nvalidating the true utility of such techniques, especially when compared to\nhuman coding outputs. We bridge this gap, where a benchmark dataset comprising\n72 distinct software engineering tasks is used to compare the effectiveness of\nlarge language models (LLMs) and human programmers in producing Python software\ncode. GPT-4 is used as a representative LLM, where for the code generated by\nhumans and this LLM, we evaluate code quality and adherence to Python coding\nstandards, code security and vulnerabilities, code complexity and functional\ncorrectness. We use various static analysis benchmarks, including Pylint,\nRadon, Bandit and test cases. Among the notable outcomes, results show that\nhuman-generated code recorded higher ratings for adhering to coding standards\nthan GPT-4. We observe security flaws in code generated by both humans and\nGPT-4, however, code generated by humans shows a greater variety of problems,\nbut GPT-4 code included more severe outliers. Our results show that although\nGPT-4 is capable of producing coding solutions, it frequently produces more\ncomplex code that may need more reworking to ensure maintainability. On the\ncontrary however, our outcomes show that a higher number of test cases passed\nfor code generated by GPT-4 across a range of tasks than code that was\ngenerated by humans. That said, GPT-4 frequently struggles with complex\nproblem-solving that involve in-depth domain knowledge. This study highlights\nthe potential utility of LLMs for supporting software development, however,\ntasks requiring comprehensive, innovative or unconventional solutions, and\ncareful debugging and error correction seem to be better developed by human\nprogrammers. We plot an agenda for the software engineering community."
                },
                "authors": [
                    {
                        "name": "Sherlock A. Licorish"
                    },
                    {
                        "name": "Ansh Bajpai"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Fanyu Wang"
                    },
                    {
                        "name": "Kla Tantithamthavorn"
                    }
                ],
                "author_detail": {
                    "name": "Kla Tantithamthavorn"
                },
                "author": "Kla Tantithamthavorn",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; D.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v3",
                "updated": "2025-01-28T11:05:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    5,
                    18,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16852v1",
                "updated": "2025-01-28T10:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    58,
                    54,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T10:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    58,
                    54,
                    1,
                    28,
                    0
                ],
                "title": "Generalized framework for likelihood-based field-level inference of\n  growth rate from velocity and density fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized framework for likelihood-based field-level inference of\n  growth rate from velocity and density fields"
                },
                "summary": "Measuring the growth rate of large-scale structures ($f$) as a function of\nredshift has the potential to break degeneracies between modified gravity and\ndark energy models, when combined with expansion-rate probes. Direct estimates\nof peculiar velocities of galaxies have gained interest to estimate\n$f\\sigma_8$. In particular, field-level methods can be used to fit the field\nnuisance parameter along with cosmological parameters simultaneously. This\narticle aims to provide the community with an unified framework for the\ntheoretical modeling of the likelihood-based field-level inference by\nperforming fast field covariance calculations for velocity and density fields.\nOur purpose is to lay the foundations for non-linear extension of the\nlikelihood-based method at the field level. We develop a generalized framework,\nimplemented in the dedicated software flip to perform a likelihood-based\ninference of $f\\sigma_8$. We derive a new field covariance model, which\nincludes wide-angle corrections. We also include the models previously\ndescribed in the literature inside our framework. We compare their performance\nagainst ours, we validate our model by comparing it with the two-point\nstatistics of a recent N-body simulation. The tests we perform allow us to\nvalidate our software and determine the appropriate wavenumber range to\nintegrate our covariance model and its validity in terms of separation. Our\nframework allows for a wider wavenumber coverage used in our calculations than\nprevious works, which is particularly interesting for non-linear model\nextensions. Finally, our generalized framework allows us to efficiently perform\na survey geometry-dependent Fisher forecast of the $f\\sigma_8$ parameter. We\nshow that the Fisher forecast method we developed gives an error bar that is 30\n% closer to a full likelihood-based estimation than a standard volume Fisher\nforecast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the growth rate of large-scale structures ($f$) as a function of\nredshift has the potential to break degeneracies between modified gravity and\ndark energy models, when combined with expansion-rate probes. Direct estimates\nof peculiar velocities of galaxies have gained interest to estimate\n$f\\sigma_8$. In particular, field-level methods can be used to fit the field\nnuisance parameter along with cosmological parameters simultaneously. This\narticle aims to provide the community with an unified framework for the\ntheoretical modeling of the likelihood-based field-level inference by\nperforming fast field covariance calculations for velocity and density fields.\nOur purpose is to lay the foundations for non-linear extension of the\nlikelihood-based method at the field level. We develop a generalized framework,\nimplemented in the dedicated software flip to perform a likelihood-based\ninference of $f\\sigma_8$. We derive a new field covariance model, which\nincludes wide-angle corrections. We also include the models previously\ndescribed in the literature inside our framework. We compare their performance\nagainst ours, we validate our model by comparing it with the two-point\nstatistics of a recent N-body simulation. The tests we perform allow us to\nvalidate our software and determine the appropriate wavenumber range to\nintegrate our covariance model and its validity in terms of separation. Our\nframework allows for a wider wavenumber coverage used in our calculations than\nprevious works, which is particularly interesting for non-linear model\nextensions. Finally, our generalized framework allows us to efficiently perform\na survey geometry-dependent Fisher forecast of the $f\\sigma_8$ parameter. We\nshow that the Fisher forecast method we developed gives an error bar that is 30\n% closer to a full likelihood-based estimation than a standard volume Fisher\nforecast."
                },
                "authors": [
                    {
                        "name": "Corentin Ravoux"
                    },
                    {
                        "name": "Bastien Carreres"
                    },
                    {
                        "name": "Damiano Rosselli"
                    },
                    {
                        "name": "Julian Bautista"
                    },
                    {
                        "name": "Anthony Carr"
                    },
                    {
                        "name": "Tyann Dummerchat"
                    },
                    {
                        "name": "Alex G. Kim"
                    },
                    {
                        "name": "David Parkinson"
                    },
                    {
                        "name": "Benjamin Racine"
                    },
                    {
                        "name": "Dominique Fouchez"
                    },
                    {
                        "name": "Fabrice Feinstein"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Feinstein"
                },
                "author": "Fabrice Feinstein",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16842v1",
                "updated": "2025-01-28T10:33:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    33,
                    1,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T10:33:01Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    33,
                    1,
                    1,
                    28,
                    0
                ],
                "title": "Adapting Network Information to Semantics for Generalizable and\n  Plug-and-Play Multi-Scenario Network Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Network Information to Semantics for Generalizable and\n  Plug-and-Play Multi-Scenario Network Diagnosis"
                },
                "summary": "Network fault diagnosis is a core challenge in ensuring the stability and\nreliability of modern network operations. Traditional approaches, limited by\ntheir training on specific performance metrics for predefined scenarios,\nstruggle to generalize across diverse faults and anomalies in varying network\nenvironments. In recent years, large language models (LLMs) have demonstrated\nstrong generalization capabilities across various domains. Building on this\nsuccess, we propose NetSemantic, a plug-and-play intelligent network fault\ndiagnosis framework based on LLMs. NetSemantic transforms multimodal network\ninformation into unified textual representations, enabling LLMs to perform\nreasoning and generate efficient fault resolutions and health assessment\nreports. To further enhance the logical reasoning capabilities of LLMs, we\nintroduce a novel symbolic representation method that transforms logically\nstrong network information into symbols. Additionally, we propose a\nself-adaptive data updating mechanism that dynamically incorporates network\ninformation into a knowledge graph to ensure the validity and timeliness of the\nknowledge base. Experimental results demonstrate that NetSemantic excels in\nnetwork fault diagnosis across various complex scenarios, significantly\nimproving diagnostic accuracy and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network fault diagnosis is a core challenge in ensuring the stability and\nreliability of modern network operations. Traditional approaches, limited by\ntheir training on specific performance metrics for predefined scenarios,\nstruggle to generalize across diverse faults and anomalies in varying network\nenvironments. In recent years, large language models (LLMs) have demonstrated\nstrong generalization capabilities across various domains. Building on this\nsuccess, we propose NetSemantic, a plug-and-play intelligent network fault\ndiagnosis framework based on LLMs. NetSemantic transforms multimodal network\ninformation into unified textual representations, enabling LLMs to perform\nreasoning and generate efficient fault resolutions and health assessment\nreports. To further enhance the logical reasoning capabilities of LLMs, we\nintroduce a novel symbolic representation method that transforms logically\nstrong network information into symbols. Additionally, we propose a\nself-adaptive data updating mechanism that dynamically incorporates network\ninformation into a knowledge graph to ensure the validity and timeliness of the\nknowledge base. Experimental results demonstrate that NetSemantic excels in\nnetwork fault diagnosis across various complex scenarios, significantly\nimproving diagnostic accuracy and reliability."
                },
                "authors": [
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Ming Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhao"
                },
                "author": "Ming Zhao",
                "arxiv_comment": "10 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16829v1",
                "updated": "2025-01-28T10:21:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    21,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T10:21:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    21,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "Multi-Messenger and Cosmological Constraints on Dark Matter through\n  Two-Fluid Neutron Star Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Messenger and Cosmological Constraints on Dark Matter through\n  Two-Fluid Neutron Star Modeling"
                },
                "summary": "In this study, we investigate the impact of dark matter (DM) on neutron stars\n(NSs) using a two-fluid formalism that treats nuclear matter (NM) and DM as\ngravitationally coupled components. Employing NM equations of state spanning a\nwide range of stiffness and a self-interacting asymmetric fermionic DM\nframework, we explore the emergence of DM core- and halo-dominated structures\nand their observational implications. Constraints from gravitational waves\n(GW170817), NICER X-ray measurements (PSR J0030+0451), and pulsar mass limits\n(PSR J0740+6620) delineate a consistent parameter space for DM properties\nderived from these multi-messenger observations. DM halo-dominated\nconfigurations, while consistent with PSR J0740+6620's mass limits and NICER's\nradius measurements for PSR J0030+0451, are ruled out by the tidal\ndeformability bounds inferred from the GW170817 event. Consequently, the\ncombined limits inferred from the observational data of GW170817, PSR\nJ0030+0451, and PSR J0740+6620 support the plausibility of DM core-dominated\nconfigurations. Constraints on the DM self-interaction strength from galaxy\ncluster dynamics further refine the DM parameter space permitted by NS\nobservations. This work bridges multi-messenger astrophysics and cosmology,\nproviding insights into DM interactions and their implications for NS\nstructure, evolution, and observational signatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the impact of dark matter (DM) on neutron stars\n(NSs) using a two-fluid formalism that treats nuclear matter (NM) and DM as\ngravitationally coupled components. Employing NM equations of state spanning a\nwide range of stiffness and a self-interacting asymmetric fermionic DM\nframework, we explore the emergence of DM core- and halo-dominated structures\nand their observational implications. Constraints from gravitational waves\n(GW170817), NICER X-ray measurements (PSR J0030+0451), and pulsar mass limits\n(PSR J0740+6620) delineate a consistent parameter space for DM properties\nderived from these multi-messenger observations. DM halo-dominated\nconfigurations, while consistent with PSR J0740+6620's mass limits and NICER's\nradius measurements for PSR J0030+0451, are ruled out by the tidal\ndeformability bounds inferred from the GW170817 event. Consequently, the\ncombined limits inferred from the observational data of GW170817, PSR\nJ0030+0451, and PSR J0740+6620 support the plausibility of DM core-dominated\nconfigurations. Constraints on the DM self-interaction strength from galaxy\ncluster dynamics further refine the DM parameter space permitted by NS\nobservations. This work bridges multi-messenger astrophysics and cosmology,\nproviding insights into DM interactions and their implications for NS\nstructure, evolution, and observational signatures."
                },
                "authors": [
                    {
                        "name": "Ankit Kumar"
                    },
                    {
                        "name": "Sudhakantha Girmohanta"
                    },
                    {
                        "name": "Hajime Sotani"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Sotani"
                },
                "author": "Hajime Sotani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16220v2",
                "updated": "2025-01-28T10:16:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    16,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T17:09:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "DBRouting: Routing End User Queries to Databases for Answerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBRouting: Routing End User Queries to Databases for Answerability"
                },
                "summary": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
                },
                "authors": [
                    {
                        "name": "Priyangshu Mandal"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "arxiv_comment": "Accepted at 1st Workshop on GenAI and RAG Systems for Enterprise at\n  CIKM 2024 Conference. 10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16825v1",
                "updated": "2025-01-28T10:04:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    4,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T10:04:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    4,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Can Transformers Learn Full Bayesian Inference in Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformers Learn Full Bayesian Inference in Context?"
                },
                "summary": "Transformers have emerged as the dominant architecture in the field of deep\nlearning, with a broad range of applications and remarkable in-context learning\n(ICL) capabilities. While not yet fully understood, ICL has already proved to\nbe an intriguing phenomenon, allowing transformers to learn in context --\nwithout requiring further training. In this paper, we further advance the\nunderstanding of ICL by demonstrating that transformers can perform full\nBayesian inference for commonly used statistical models in context. More\nspecifically, we introduce a general framework that builds on ideas from prior\nfitted networks and continuous normalizing flows which enables us to infer\ncomplex posterior distributions for methods such as generalized linear models\nand latent factor models. Extensive experiments on real-world datasets\ndemonstrate that our ICL approach yields posterior samples that are similar in\nquality to state-of-the-art MCMC or variational inference methods not operating\nin context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the dominant architecture in the field of deep\nlearning, with a broad range of applications and remarkable in-context learning\n(ICL) capabilities. While not yet fully understood, ICL has already proved to\nbe an intriguing phenomenon, allowing transformers to learn in context --\nwithout requiring further training. In this paper, we further advance the\nunderstanding of ICL by demonstrating that transformers can perform full\nBayesian inference for commonly used statistical models in context. More\nspecifically, we introduce a general framework that builds on ideas from prior\nfitted networks and continuous normalizing flows which enables us to infer\ncomplex posterior distributions for methods such as generalized linear models\nand latent factor models. Extensive experiments on real-world datasets\ndemonstrate that our ICL approach yields posterior samples that are similar in\nquality to state-of-the-art MCMC or variational inference methods not operating\nin context."
                },
                "authors": [
                    {
                        "name": "Arik Reuter"
                    },
                    {
                        "name": "Tim G. J. Rudner"
                    },
                    {
                        "name": "Vincent Fortuin"
                    },
                    {
                        "name": "David Rügamer"
                    }
                ],
                "author_detail": {
                    "name": "David Rügamer"
                },
                "author": "David Rügamer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19420v3",
                "updated": "2025-01-28T09:56:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    56,
                    52,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-25T09:23:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches"
                },
                "summary": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Paola Leaci"
                    },
                    {
                        "name": "Pia Astone"
                    },
                    {
                        "name": "Stefano Dal Pra"
                    },
                    {
                        "name": "Sabrina D'Antonio"
                    },
                    {
                        "name": "Luca D'Onofrio"
                    },
                    {
                        "name": "Sergio Frasca"
                    },
                    {
                        "name": "Federico Muciaccia"
                    },
                    {
                        "name": "Cristiano Palomba"
                    },
                    {
                        "name": "Lorenzo Pierini"
                    },
                    {
                        "name": "Francesco Safai Tehrani"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Safai Tehrani"
                },
                "author": "Francesco Safai Tehrani",
                "arxiv_comment": "13 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16751v2",
                "updated": "2025-01-28T09:45:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    45,
                    41,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-25T09:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL"
                },
                "summary": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set."
                },
                "authors": [
                    {
                        "name": "Hasan Alp Caferoğlu"
                    },
                    {
                        "name": "Özgür Ulusoy"
                    }
                ],
                "author_detail": {
                    "name": "Özgür Ulusoy"
                },
                "author": "Özgür Ulusoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07739v3",
                "updated": "2025-01-28T09:26:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    26,
                    27,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T09:16:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    16,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture"
                },
                "summary": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Weibo Zheng"
                    },
                    {
                        "name": "Donghong Han"
                    }
                ],
                "author_detail": {
                    "name": "Donghong Han"
                },
                "author": "Donghong Han",
                "arxiv_comment": "13 pages, 7 figures, 4 tables; Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15830v2",
                "updated": "2025-01-28T09:25:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    25,
                    31,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T07:34:33Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    34,
                    33,
                    0,
                    27,
                    0
                ],
                "title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model"
                },
                "summary": "In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Xinyi Ye"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "JiaYuan Gu"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v7",
                "updated": "2025-01-28T09:15:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    15,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17840v2",
                "updated": "2025-01-28T09:09:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    9,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-23T13:05:46Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs"
                },
                "summary": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces."
                },
                "authors": [
                    {
                        "name": "Ferdi Kossmann"
                    },
                    {
                        "name": "Bruce Fontaine"
                    },
                    {
                        "name": "Daya Khudia"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Samuel Madden"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Madden"
                },
                "author": "Samuel Madden",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15577v2",
                "updated": "2025-01-28T08:46:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    46,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2024-06-21T18:25:12Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    25,
                    12,
                    4,
                    173,
                    0
                ],
                "title": "Decoherence of Histories: Chaotic Versus Integrable Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoherence of Histories: Chaotic Versus Integrable Systems"
                },
                "summary": "We study the emergence of decoherent histories in isolated systems based on\nexact numerical integration of the Schr\\\"odinger equation for a Heisenberg\nchain. We reveal that the nature of the system, which we switch from (i)\nchaotic to (ii) interacting integrable to (iii) non-interacting integrable,\nstrongly impacts decoherence. From a finite size scaling law we infer a strong\nexponential suppression of coherences for (i), a weak exponential suppression\nfor (ii) and no exponential suppression for (iii) on a relevant short\n(nonequilibrium) time scale. Moreover, for longer times we find stronger\ndecoherence for (i) but the opposite for (ii), hinting even at a possible\npower-law decay for (ii) at equilibrium time scales. This behaviour is encoded\nin the multi-time properties of the quantum histories and it can not be\nexplained by environmentally induced decoherence. Our results suggest that\nchaoticity plays a crucial role in the emergence of classicality in finite size\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the emergence of decoherent histories in isolated systems based on\nexact numerical integration of the Schr\\\"odinger equation for a Heisenberg\nchain. We reveal that the nature of the system, which we switch from (i)\nchaotic to (ii) interacting integrable to (iii) non-interacting integrable,\nstrongly impacts decoherence. From a finite size scaling law we infer a strong\nexponential suppression of coherences for (i), a weak exponential suppression\nfor (ii) and no exponential suppression for (iii) on a relevant short\n(nonequilibrium) time scale. Moreover, for longer times we find stronger\ndecoherence for (i) but the opposite for (ii), hinting even at a possible\npower-law decay for (ii) at equilibrium time scales. This behaviour is encoded\nin the multi-time properties of the quantum histories and it can not be\nexplained by environmentally induced decoherence. Our results suggest that\nchaoticity plays a crucial role in the emergence of classicality in finite size\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiaozi Wang"
                    },
                    {
                        "name": "Philipp Strasberg"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Strasberg"
                },
                "author": "Philipp Strasberg",
                "arxiv_comment": "11 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16788v1",
                "updated": "2025-01-28T08:35:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    35,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:35:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    35,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Statistical biases in parameterized searches for gravitational-wave\n  polarizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical biases in parameterized searches for gravitational-wave\n  polarizations"
                },
                "summary": "In tests of gravity using gravitational waves (GWs), GW events analyzed are\noften selected based on specific criteria, particularly the signal-to-noise\nratio (SNR). However, such event selection can introduce bias into parameter\nestimation unless the selection effect is appropriately taken into account in\nthe analysis. In this paper, we investigate how event selection with certain\nprior information affects parameter inference within the scalar-tensor\npolarization framework, focusing on the measurement of the scalar mode\namplitude parameters. We find that for the Tensor+Scalar(dipole) model, the\namplitude of the scalar dipole radiation is overestimated when its true value\nis nonzero while there is no false deviation in the absence of the scalar mode.\nThe same bias is expected to occur also for the Tensor+Scalar(quadrupole)\nmodel. However, error typically exceeds the bias as the scalar quadrupole mode\nis difficult to be distinguished from the tensor mode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In tests of gravity using gravitational waves (GWs), GW events analyzed are\noften selected based on specific criteria, particularly the signal-to-noise\nratio (SNR). However, such event selection can introduce bias into parameter\nestimation unless the selection effect is appropriately taken into account in\nthe analysis. In this paper, we investigate how event selection with certain\nprior information affects parameter inference within the scalar-tensor\npolarization framework, focusing on the measurement of the scalar mode\namplitude parameters. We find that for the Tensor+Scalar(dipole) model, the\namplitude of the scalar dipole radiation is overestimated when its true value\nis nonzero while there is no false deviation in the absence of the scalar mode.\nThe same bias is expected to occur also for the Tensor+Scalar(quadrupole)\nmodel. However, error typically exceeds the bias as the scalar quadrupole mode\nis difficult to be distinguished from the tensor mode."
                },
                "authors": [
                    {
                        "name": "Hayato Imafuku"
                    },
                    {
                        "name": "Hiroki Takeda"
                    },
                    {
                        "name": "Atsushi Nishizawa"
                    },
                    {
                        "name": "Daiki Watarai"
                    },
                    {
                        "name": "Kipp Cannon"
                    }
                ],
                "author_detail": {
                    "name": "Kipp Cannon"
                },
                "author": "Kipp Cannon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16786v1",
                "updated": "2025-01-28T08:30:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    30,
                    58,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:30:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    30,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large\n  Language Models for Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large\n  Language Models for Video Understanding"
                },
                "summary": "Applying Multimodal Large Language Models (MLLMs) to video understanding\npresents significant challenges due to the need to model temporal relations\nacross frames. Existing approaches adopt either implicit temporal modeling,\nrelying solely on the LLM decoder, or explicit temporal modeling, employing\nauxiliary temporal encoders. To investigate this debate between the two\nparadigms, we propose the Stackable Temporal Encoder (STE). STE enables\nflexible explicit temporal modeling with adjustable temporal receptive fields\nand token compression ratios. Using STE, we systematically compare implicit and\nexplicit temporal modeling across dimensions such as overall performance, token\ncompression effectiveness, and temporal-specific understanding. We also explore\nSTE's design considerations and broader impacts as a plug-in module and in\nimage modalities. Our findings emphasize the critical role of explicit temporal\nmodeling, providing actionable insights to advance video MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Multimodal Large Language Models (MLLMs) to video understanding\npresents significant challenges due to the need to model temporal relations\nacross frames. Existing approaches adopt either implicit temporal modeling,\nrelying solely on the LLM decoder, or explicit temporal modeling, employing\nauxiliary temporal encoders. To investigate this debate between the two\nparadigms, we propose the Stackable Temporal Encoder (STE). STE enables\nflexible explicit temporal modeling with adjustable temporal receptive fields\nand token compression ratios. Using STE, we systematically compare implicit and\nexplicit temporal modeling across dimensions such as overall performance, token\ncompression effectiveness, and temporal-specific understanding. We also explore\nSTE's design considerations and broader impacts as a plug-in module and in\nimage modalities. Our findings emphasize the critical role of explicit temporal\nmodeling, providing actionable insights to advance video MLLMs."
                },
                "authors": [
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yajing Kong"
                    },
                    {
                        "name": "Guangrui Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Chao Bian"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Zhenbang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenbang Sun"
                },
                "author": "Zhenbang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16784v1",
                "updated": "2025-01-28T08:13:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    13,
                    2,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:13:02Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    13,
                    2,
                    1,
                    28,
                    0
                ],
                "title": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT\n  Devices Concealed within the Tor Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT\n  Devices Concealed within the Tor Network"
                },
                "summary": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views."
                },
                "authors": [
                    {
                        "name": "Yumingzhi Pan"
                    },
                    {
                        "name": "Zhen Ling"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Hongze Wang"
                    },
                    {
                        "name": "Guangchi Liu"
                    },
                    {
                        "name": "Junzhou Luo"
                    },
                    {
                        "name": "Xinwen Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwen Fu"
                },
                "author": "Xinwen Fu",
                "arxiv_comment": "27 pages, 14 figure, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.06360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.06360v2",
                "updated": "2025-01-28T08:10:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    10,
                    15,
                    1,
                    28,
                    0
                ],
                "published": "2021-06-11T13:01:03Z",
                "published_parsed": [
                    2021,
                    6,
                    11,
                    13,
                    1,
                    3,
                    4,
                    162,
                    0
                ],
                "title": "Conterfactual Generative Zero-Shot Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conterfactual Generative Zero-Shot Semantic Segmentation"
                },
                "summary": "zero-shot learning is an essential part of computer vision. As a classical\ndownstream task, zero-shot semantic segmentation has been studied because of\nits applicant value. One of the popular zero-shot semantic segmentation methods\nis based on the generative model Most new proposed works added structures on\nthe same architecture to enhance this model. However, we found that, from the\nview of causal inference, the result of the original model has been influenced\nby spurious statistical relationships. Thus the performance of the prediction\nshows severe bias. In this work, we consider counterfactual methods to avoid\nthe confounder in the original model. Based on this method, we proposed a new\nframework for zero-shot semantic segmentation. Our model is compared with\nbaseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The\nexperiment results show proposed models can surpass previous confounded models\nand can still make use of additional structures to improve the performance. We\nalso design a simple structure based on Graph Convolutional Networks (GCN) in\nthis work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zero-shot learning is an essential part of computer vision. As a classical\ndownstream task, zero-shot semantic segmentation has been studied because of\nits applicant value. One of the popular zero-shot semantic segmentation methods\nis based on the generative model Most new proposed works added structures on\nthe same architecture to enhance this model. However, we found that, from the\nview of causal inference, the result of the original model has been influenced\nby spurious statistical relationships. Thus the performance of the prediction\nshows severe bias. In this work, we consider counterfactual methods to avoid\nthe confounder in the original model. Based on this method, we proposed a new\nframework for zero-shot semantic segmentation. Our model is compared with\nbaseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The\nexperiment results show proposed models can surpass previous confounded models\nand can still make use of additional structures to improve the performance. We\nalso design a simple structure based on Graph Convolutional Networks (GCN) in\nthis work."
                },
                "authors": [
                    {
                        "name": "Feihong Shen"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Hu"
                },
                "author": "Ping Hu",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.06360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.06360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16783v1",
                "updated": "2025-01-28T08:08:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    8,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:08:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    8,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling\n  Severity Drift as a Critical Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling\n  Severity Drift as a Critical Process"
                },
                "summary": "This paper introduces a continuous-time stochastic dynamical framework for\nunderstanding how large language models (LLMs) may self-amplify latent biases\nor toxicity through their own chain-of-thought reasoning. The model posits an\ninstantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic\ndifferential equation (SDE) with a drift term $\\mu(x)$ and diffusion\n$\\sigma(x)$. Crucially, such a process can be consistently analyzed via the\nFokker--Planck approach if each incremental step behaves nearly Markovian in\nseverity space. The analysis investigates critical phenomena, showing that\ncertain parameter regimes create phase transitions from subcritical\n(self-correcting) to supercritical (runaway severity). The paper derives\nstationary distributions, first-passage times to harmful thresholds, and\nscaling laws near critical points. Finally, it highlights implications for\nagents and extended LLM reasoning models: in principle, these equations might\nserve as a basis for formal verification of whether a model remains stable or\npropagates bias over repeated inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a continuous-time stochastic dynamical framework for\nunderstanding how large language models (LLMs) may self-amplify latent biases\nor toxicity through their own chain-of-thought reasoning. The model posits an\ninstantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic\ndifferential equation (SDE) with a drift term $\\mu(x)$ and diffusion\n$\\sigma(x)$. Crucially, such a process can be consistently analyzed via the\nFokker--Planck approach if each incremental step behaves nearly Markovian in\nseverity space. The analysis investigates critical phenomena, showing that\ncertain parameter regimes create phase transitions from subcritical\n(self-correcting) to supercritical (runaway severity). The paper derives\nstationary distributions, first-passage times to harmful thresholds, and\nscaling laws near critical points. Finally, it highlights implications for\nagents and extended LLM reasoning models: in principle, these equations might\nserve as a basis for formal verification of whether a model remains stable or\npropagates bias over repeated inferences."
                },
                "authors": [
                    {
                        "name": "Jack David Carson"
                    }
                ],
                "author_detail": {
                    "name": "Jack David Carson"
                },
                "author": "Jack David Carson",
                "arxiv_comment": "Experimental verification and more formal argument for Markov\n  approximation of bias propagation to be released soon. Primarily pushed now\n  to establish novelty and ease of sharing. Please do not cite this work until\n  the forthcoming experimental validation and updated mathematical model are\n  provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00989v3",
                "updated": "2025-01-28T07:45:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    45,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-02T03:25:20Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    3,
                    25,
                    20,
                    4,
                    215,
                    0
                ],
                "title": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty\n  Agents"
                },
                "summary": "Large language model-based multi-agent systems have shown great abilities\nacross various tasks due to the collaboration of expert agents, each focusing\non a specific domain. However, the impact of clumsy or even malicious agents,\ni.e., those who frequently make errors in their tasks, on the overall\nperformance of the system remains underexplored. This paper investigates: (1)\nWhat is the resilience of various system structures (e.g.,\nA$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under\nfaulty agents, on different downstream tasks? (2) How can we increase system\nresilience to defend against these agents? To simulate faulty agents, we\npropose two approaches, AutoTransform and AutoInject, which introduce mistakes\ninto the agents' responses. We select four downstream tasks, including code\ngeneration, math problems, translation, and text evaluation. Results suggest\nthat the hierarchical structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C),\nexhibits superior resilience with the lowest performance drop of $9.2\\%$,\ncompared to $26.0\\%$ and $31.2\\%$ of other two structures. Additionally, we\nimprove the system resilience with two methods, introducing a mechanism for\neach agent to challenge others' outputs, and an additional agent to review and\ncorrect messages. Our code and data are available at\nhttps://github.com/CUHK-ARISE/MAS-Resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based multi-agent systems have shown great abilities\nacross various tasks due to the collaboration of expert agents, each focusing\non a specific domain. However, the impact of clumsy or even malicious agents,\ni.e., those who frequently make errors in their tasks, on the overall\nperformance of the system remains underexplored. This paper investigates: (1)\nWhat is the resilience of various system structures (e.g.,\nA$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under\nfaulty agents, on different downstream tasks? (2) How can we increase system\nresilience to defend against these agents? To simulate faulty agents, we\npropose two approaches, AutoTransform and AutoInject, which introduce mistakes\ninto the agents' responses. We select four downstream tasks, including code\ngeneration, math problems, translation, and text evaluation. Results suggest\nthat the hierarchical structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C),\nexhibits superior resilience with the lowest performance drop of $9.2\\%$,\ncompared to $26.0\\%$ and $31.2\\%$ of other two structures. Additionally, we\nimprove the system resilience with two methods, introducing a mechanism for\neach agent to challenge others' outputs, and an additional agent to review and\ncorrect messages. Our code and data are available at\nhttps://github.com/CUHK-ARISE/MAS-Resilience."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Jiaxu Zhou"
                    },
                    {
                        "name": "Tailin Jin"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "9 pages of main text; 11 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19272v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19272v4",
                "updated": "2025-01-28T07:36:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    36,
                    15,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-28T07:13:33Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    7,
                    13,
                    33,
                    5,
                    272,
                    0
                ],
                "title": "Perception Compressor: A Training-Free Prompt Compression Framework in\n  Long Context Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Compressor: A Training-Free Prompt Compression Framework in\n  Long Context Scenarios"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information in long context scenarios. To\naddress these challenges, we present Perception Compressor, a training-free\nprompt compression framework. It includes a perception retriever that leverages\nguiding questions and instruction to retrieve the most relevant demonstrations,\na dual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information in long context scenarios. To\naddress these challenges, we present Perception Compressor, a training-free\nprompt compression framework. It includes a perception retriever that leverages\nguiding questions and instruction to retrieve the most relevant demonstrations,\na dual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Lin Hai"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "arxiv_comment": "Accepted at NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19272v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16750v1",
                "updated": "2025-01-28T07:00:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    0,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T07:00:45Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    0,
                    45,
                    1,
                    28,
                    0
                ],
                "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content\n  and Hate Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content\n  and Hate Campaigns"
                },
                "summary": "Large Language Models (LLMs) have raised increasing concerns about their\nmisuse in generating hate speech. Among all the efforts to address this issue,\nhate speech detectors play a crucial role. However, the effectiveness of\ndifferent detectors against LLM-generated hate speech remains largely unknown.\nIn this paper, we propose HateBench, a framework for benchmarking hate speech\ndetectors on LLM-generated hate speech. We first construct a hate speech\ndataset of 7,838 samples generated by six widely-used LLMs covering 34 identity\ngroups, with meticulous annotations by three labelers. We then assess the\neffectiveness of eight representative hate speech detectors on the\nLLM-generated dataset. Our results show that while detectors are generally\neffective in identifying LLM-generated hate speech, their performance degrades\nwith newer versions of LLMs. We also reveal the potential of LLM-driven hate\ncampaigns, a new threat that LLMs bring to the field of hate speech detection.\nBy leveraging advanced techniques like adversarial attacks and model stealing\nattacks, the adversary can intentionally evade the detector and automate hate\ncampaigns online. The most potent adversarial attack achieves an attack success\nrate of 0.966, and its attack efficiency can be further improved by\n$13-21\\times$ through model stealing attacks with acceptable attack\nperformance. We hope our study can serve as a call to action for the research\ncommunity and platform moderators to fortify defenses against these emerging\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have raised increasing concerns about their\nmisuse in generating hate speech. Among all the efforts to address this issue,\nhate speech detectors play a crucial role. However, the effectiveness of\ndifferent detectors against LLM-generated hate speech remains largely unknown.\nIn this paper, we propose HateBench, a framework for benchmarking hate speech\ndetectors on LLM-generated hate speech. We first construct a hate speech\ndataset of 7,838 samples generated by six widely-used LLMs covering 34 identity\ngroups, with meticulous annotations by three labelers. We then assess the\neffectiveness of eight representative hate speech detectors on the\nLLM-generated dataset. Our results show that while detectors are generally\neffective in identifying LLM-generated hate speech, their performance degrades\nwith newer versions of LLMs. We also reveal the potential of LLM-driven hate\ncampaigns, a new threat that LLMs bring to the field of hate speech detection.\nBy leveraging advanced techniques like adversarial attacks and model stealing\nattacks, the adversary can intentionally evade the detector and automate hate\ncampaigns online. The most potent adversarial attack achieves an attack success\nrate of 0.966, and its attack efficiency can be further improved by\n$13-21\\times$ through model stealing attacks with acceptable attack\nperformance. We hope our study can serve as a call to action for the research\ncommunity and platform moderators to fortify defenses against these emerging\nthreats."
                },
                "authors": [
                    {
                        "name": "Xinyue Shen"
                    },
                    {
                        "name": "Yixin Wu"
                    },
                    {
                        "name": "Yiting Qu"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Savvas Zannettou"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16748v1",
                "updated": "2025-01-28T06:58:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:58:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03168v3",
                "updated": "2025-01-28T06:55:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    55,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-04T06:01:27Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    1,
                    27,
                    4,
                    278,
                    0
                ],
                "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?"
                },
                "summary": "Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Sheng Guan"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "28 pages, 5 figures, 11 tables Published as a conference paper at\n  ICLR 2025 Github link:\n  https://github.com/THU-BPM/Watermarked_LLM_Identification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16744v1",
                "updated": "2025-01-28T06:41:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    41,
                    37,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:41:37Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    41,
                    37,
                    1,
                    28,
                    0
                ],
                "title": "LLM Assisted Anomaly Detection Service for Site Reliability Engineers:\n  Enhancing Cloud Infrastructure Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assisted Anomaly Detection Service for Site Reliability Engineers:\n  Enhancing Cloud Infrastructure Resilience"
                },
                "summary": "This paper introduces a scalable Anomaly Detection Service with a\ngeneralizable API tailored for industrial time-series data, designed to assist\nSite Reliability Engineers (SREs) in managing cloud infrastructure. The service\nenables efficient anomaly detection in complex data streams, supporting\nproactive identification and resolution of issues. Furthermore, it presents an\ninnovative approach to anomaly modeling in cloud infrastructure by utilizing\nLarge Language Models (LLMs) to understand key components, their failure modes,\nand behaviors. A suite of algorithms for detecting anomalies is offered in\nunivariate and multivariate time series data, including regression-based,\nmixture-model-based, and semi-supervised approaches. We provide insights into\nthe usage patterns of the service, with over 500 users and 200,000 API calls in\na year. The service has been successfully applied in various industrial\nsettings, including IoT-based AI applications. We have also evaluated our\nsystem on public anomaly benchmarks to show its effectiveness. By leveraging\nit, SREs can proactively identify potential issues before they escalate,\nreducing downtime and improving response times to incidents, ultimately\nenhancing the overall customer experience. We plan to extend the system to\ninclude time series foundation models, enabling zero-shot anomaly detection\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a scalable Anomaly Detection Service with a\ngeneralizable API tailored for industrial time-series data, designed to assist\nSite Reliability Engineers (SREs) in managing cloud infrastructure. The service\nenables efficient anomaly detection in complex data streams, supporting\nproactive identification and resolution of issues. Furthermore, it presents an\ninnovative approach to anomaly modeling in cloud infrastructure by utilizing\nLarge Language Models (LLMs) to understand key components, their failure modes,\nand behaviors. A suite of algorithms for detecting anomalies is offered in\nunivariate and multivariate time series data, including regression-based,\nmixture-model-based, and semi-supervised approaches. We provide insights into\nthe usage patterns of the service, with over 500 users and 200,000 API calls in\na year. The service has been successfully applied in various industrial\nsettings, including IoT-based AI applications. We have also evaluated our\nsystem on public anomaly benchmarks to show its effectiveness. By leveraging\nit, SREs can proactively identify potential issues before they escalate,\nreducing downtime and improving response times to incidents, ultimately\nenhancing the overall customer experience. We plan to extend the system to\ninclude time series foundation models, enabling zero-shot anomaly detection\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Nimesh Jha"
                    },
                    {
                        "name": "Shuxin Lin"
                    },
                    {
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "name": "Kyle Frohling"
                    },
                    {
                        "name": "Christodoulos Constantinides"
                    },
                    {
                        "name": "Dhaval Patel"
                    }
                ],
                "author_detail": {
                    "name": "Dhaval Patel"
                },
                "author": "Dhaval Patel",
                "arxiv_comment": "Accepted at the AAAI-2025 Deployable AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15453v2",
                "updated": "2025-01-28T06:35:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    35,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-26T08:49:46Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    8,
                    49,
                    46,
                    6,
                    26,
                    0
                ],
                "title": "Data-adaptive Safety Rules for Training Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-adaptive Safety Rules for Training Reward Models"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is commonly employed to\ntailor models to human preferences, especially to improve the safety of outputs\nfrom large language models (LLMs). Traditionally, this method depends on\nselecting preferred responses from pairs. However, due to the variability in\nhuman opinions and the challenges in directly comparing two responses, there is\nan increasing trend towards fine-grained annotation approaches that evaluate\nresponses using multiple targeted metrics or rules. The challenge lies in\nefficiently choosing and applying these rules to handle the diverse range of\npreference data. In this paper, we propose a dynamic method that adaptively\nselects the most important rules for each response pair. We introduce a\nmathematical framework that utilizes the maximum discrepancy across paired\nresponses and demonstrate theoretically that this approach maximizes the mutual\ninformation between the rule-based annotations and the underlying true\npreferences. We then train an 8B reward model using this adaptively labeled\npreference dataset and assess its efficacy using RewardBench. As of January 25,\n2025, our model achieved the highest safety performance on the leaderboard,\nsurpassing various larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is commonly employed to\ntailor models to human preferences, especially to improve the safety of outputs\nfrom large language models (LLMs). Traditionally, this method depends on\nselecting preferred responses from pairs. However, due to the variability in\nhuman opinions and the challenges in directly comparing two responses, there is\nan increasing trend towards fine-grained annotation approaches that evaluate\nresponses using multiple targeted metrics or rules. The challenge lies in\nefficiently choosing and applying these rules to handle the diverse range of\npreference data. In this paper, we propose a dynamic method that adaptively\nselects the most important rules for each response pair. We introduce a\nmathematical framework that utilizes the maximum discrepancy across paired\nresponses and demonstrate theoretically that this approach maximizes the mutual\ninformation between the rule-based annotations and the underlying true\npreferences. We then train an 8B reward model using this adaptively labeled\npreference dataset and assess its efficacy using RewardBench. As of January 25,\n2025, our model achieved the highest safety performance on the leaderboard,\nsurpassing various larger models."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Weiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiyu Li"
                },
                "author": "Weiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16734v1",
                "updated": "2025-01-28T06:19:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:19:29Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "title": "Distilling Large Language Models for Network Active Queue Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Large Language Models for Network Active Queue Management"
                },
                "summary": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems."
                },
                "authors": [
                    {
                        "name": "Deol Satish"
                    },
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Jonathan Kua"
                    },
                    {
                        "name": "Anwar Walid"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Walid"
                },
                "author": "Anwar Walid",
                "arxiv_comment": "11 pages",
                "arxiv_journal_ref": "IEEE Trans on Networking, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16727v1",
                "updated": "2025-01-28T06:07:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking"
                },
                "summary": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Shuaimin Li"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14848v2",
                "updated": "2025-01-28T06:00:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    0,
                    44,
                    1,
                    28,
                    0
                ],
                "published": "2024-06-21T03:33:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    3,
                    33,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models"
                },
                "summary": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "arxiv_comment": "Accepted by WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15747v2",
                "updated": "2025-01-28T04:56:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    56,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T03:19:03Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    19,
                    3,
                    0,
                    27,
                    0
                ],
                "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task\n  Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task\n  Language Understanding"
                },
                "summary": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models."
                },
                "authors": [
                    {
                        "name": "Sankalp KJ"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Laxmaan Balaji"
                    },
                    {
                        "name": "Nikunj Kotecha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    }
                ],
                "author_detail": {
                    "name": "Sreyoshi Bhaduri"
                },
                "author": "Sreyoshi Bhaduri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16703v1",
                "updated": "2025-01-28T04:44:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    44,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T04:44:00Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    44,
                    0,
                    1,
                    28,
                    0
                ],
                "title": "Consistent support recovery for high-dimensional diffusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent support recovery for high-dimensional diffusions"
                },
                "summary": "Statistical inference for stochastic processes has advanced significantly due\nto applications in diverse fields, but challenges remain in high-dimensional\nsettings where parameters are allowed to grow with the sample size. This paper\nanalyzes a d-dimensional ergodic diffusion process under sparsity constraints,\nfocusing on the adaptive Lasso estimator, which improves variable selection and\nbias over the standard Lasso. We derive conditions under which the adaptive\nLasso achieves support recovery property and asymptotic normality for the drift\nparameter, with a focus on linear models. Explicit parameter relationships\nguide tuning for optimal performance, and a marginal estimator is proposed for\np>>d scenarios under partial orthogonality assumption. Numerical studies\nconfirm the adaptive Lasso's superiority over standard Lasso and MLE in\naccuracy and support recovery, providing robust solutions for high-dimensional\nstochastic processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for stochastic processes has advanced significantly due\nto applications in diverse fields, but challenges remain in high-dimensional\nsettings where parameters are allowed to grow with the sample size. This paper\nanalyzes a d-dimensional ergodic diffusion process under sparsity constraints,\nfocusing on the adaptive Lasso estimator, which improves variable selection and\nbias over the standard Lasso. We derive conditions under which the adaptive\nLasso achieves support recovery property and asymptotic normality for the drift\nparameter, with a focus on linear models. Explicit parameter relationships\nguide tuning for optimal performance, and a marginal estimator is proposed for\np>>d scenarios under partial orthogonality assumption. Numerical studies\nconfirm the adaptive Lasso's superiority over standard Lasso and MLE in\naccuracy and support recovery, providing robust solutions for high-dimensional\nstochastic processes."
                },
                "authors": [
                    {
                        "name": "Dmytro Marushkevych"
                    },
                    {
                        "name": "Francisco Pina"
                    },
                    {
                        "name": "Mark Podolskij"
                    }
                ],
                "author_detail": {
                    "name": "Mark Podolskij"
                },
                "author": "Mark Podolskij",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16698v1",
                "updated": "2025-01-28T04:31:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    19,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T04:31:19Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    19,
                    1,
                    28,
                    0
                ],
                "title": "3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow"
                },
                "summary": "3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters."
                },
                "authors": [
                    {
                        "name": "Yueen Ma"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14211v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14211v3",
                "updated": "2025-01-28T04:31:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-18T06:57:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    57,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "arxiv_comment": "Accepted by The Web Conference 2025 (WWW, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14211v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15757v2",
                "updated": "2025-01-28T04:26:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    26,
                    12,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T04:00:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    0,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A\n  Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A\n  Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular\n  Classification"
                },
                "summary": "Algorithmic level developments like Convolutional Neural Networks,\ntransformers, attention mechanism, Retrieval Augmented Generation and so on\nhave changed Artificial Intelligence. Recent such development was observed by\nKolmogorov-Arnold Networks that suggested to challenge the fundamental concept\nof a Neural Network, thus change Multilayer Perceptron, and Convolutional\nNeural Networks. They received a good reception in terms of scientific\nmodeling, yet had some drawbacks in terms of efficiency. In this paper, we\ntrain Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k\ndataset with 1.3 million images, MNIST dataset with 60k images and a tabular\nbiological science related MoA dataset and test the promise of CKANs in terms\nof FLOPS, Inference Time, number of trainable parameters and training time\nagainst the accuracy, precision, recall and f-1 score they produce against the\nstandard industry practice on CNN models. We show that the CKANs perform fair\nyet slower than CNNs in small size dataset like MoA and MNIST but are not\nnearly comparable as the dataset gets larger and more complex like the\nImageNet. The code implementation of this paper can be found on the link:\n\\href{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic level developments like Convolutional Neural Networks,\ntransformers, attention mechanism, Retrieval Augmented Generation and so on\nhave changed Artificial Intelligence. Recent such development was observed by\nKolmogorov-Arnold Networks that suggested to challenge the fundamental concept\nof a Neural Network, thus change Multilayer Perceptron, and Convolutional\nNeural Networks. They received a good reception in terms of scientific\nmodeling, yet had some drawbacks in terms of efficiency. In this paper, we\ntrain Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k\ndataset with 1.3 million images, MNIST dataset with 60k images and a tabular\nbiological science related MoA dataset and test the promise of CKANs in terms\nof FLOPS, Inference Time, number of trainable parameters and training time\nagainst the accuracy, precision, recall and f-1 score they produce against the\nstandard industry practice on CNN models. We show that the CKANs perform fair\nyet slower than CNNs in small size dataset like MoA and MNIST but are not\nnearly comparable as the dataset gets larger and more complex like the\nImageNet. The code implementation of this paper can be found on the link:\n\\href{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}"
                },
                "authors": [
                    {
                        "name": "Ashim Dahal"
                    },
                    {
                        "name": "Saydul Akbar Murad"
                    },
                    {
                        "name": "Nick Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Nick Rahimi"
                },
                "author": "Nick Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11900v2",
                "updated": "2025-01-28T04:04:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    4,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-21T05:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation"
                },
                "summary": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines."
                },
                "authors": [
                    {
                        "name": "Junhong Lian"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_doi": "10.1145/3701716.3715539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to The ACM Web Conference 2025 (WWW'25, short paper)",
                "arxiv_journal_ref": "In Companion Proceedings of the ACM Web Conference 2025 (WWW\n  Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16692v1",
                "updated": "2025-01-28T04:00:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T04:00:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation"
                },
                "summary": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization."
                },
                "authors": [
                    {
                        "name": "Manish Acharya"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Kevin Leach"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leach"
                },
                "author": "Kevin Leach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09003v3",
                "updated": "2025-01-28T03:59:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    59,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2024-11-13T20:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "title": "Refusal in LLMs is an Affine Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal in LLMs is an Affine Function"
                },
                "summary": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 ."
                },
                "authors": [
                    {
                        "name": "Thomas Marshall"
                    },
                    {
                        "name": "Adam Scherlis"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16689v1",
                "updated": "2025-01-28T03:57:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:57:22Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "title": "MACI: Multi-Agent Collaborative Intelligence for Robust Reasoning and\n  Temporal Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACI: Multi-Agent Collaborative Intelligence for Robust Reasoning and\n  Temporal Planning"
                },
                "summary": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities beyond the pattern-matching\nstrengths of LLMs. LLMs struggle with planning tasks because of their reliance\non associative reasoning, inability to self-verify, and inconsistent constraint\nawareness. We propose Multi-Agent Collaborative Intelligence (MACI), a\nframework centered on a meta-planner (MP) that orchestrates multiple agents to\ngenerate planner templates that define roles and constraints. These planners\nproduce actionable workflows of role nodes and dependency constraints, enabling\nadvanced temporal reasoning and adaptability.\n  MACI's three-tier architecture includes a meta-planning module for planner\nconstruction, common agents for general reasoning, and specialized agents for\ndomain expertise. By decoupling planning from validation, it overcomes key LLM\nlimitations. Evaluations demonstrate MACI's effective constraint satisfaction,\nconflict detection, and reasoning, positioning it as a robust solution for\ncomplex reasoning and planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities beyond the pattern-matching\nstrengths of LLMs. LLMs struggle with planning tasks because of their reliance\non associative reasoning, inability to self-verify, and inconsistent constraint\nawareness. We propose Multi-Agent Collaborative Intelligence (MACI), a\nframework centered on a meta-planner (MP) that orchestrates multiple agents to\ngenerate planner templates that define roles and constraints. These planners\nproduce actionable workflows of role nodes and dependency constraints, enabling\nadvanced temporal reasoning and adaptability.\n  MACI's three-tier architecture includes a meta-planning module for planner\nconstruction, common agents for general reasoning, and specialized agents for\ndomain expertise. By decoupling planning from validation, it overcomes key LLM\nlimitations. Evaluations demonstrate MACI's effective constraint satisfaction,\nconflict detection, and reasoning, positioning it as a robust solution for\ncomplex reasoning and planning tasks."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "22 pages, 21 tables",
                "arxiv_journal_ref": "Stanford University InfoLab Technical Report, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15754v2",
                "updated": "2025-01-28T03:42:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    42,
                    27,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T03:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    45,
                    29,
                    0,
                    27,
                    0
                ],
                "title": "Weight-based Analysis of Detokenization in Language Models:\n  Understanding the First Stage of Inference Without Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight-based Analysis of Detokenization in Language Models:\n  Understanding the First Stage of Inference Without Inference"
                },
                "summary": "According to the stages-of-inference hypothesis, early layers of language\nmodels map their subword-tokenized input, which does not necessarily correspond\nto a linguistically meaningful segmentation, to more meaningful representations\nthat form the model's ``inner vocabulary''. Prior analysis of this\ndetokenization stage has predominantly relied on probing and interventions such\nas path patching, which involve selecting particular inputs, choosing a subset\nof components that will be patched, and then observing changes in model\nbehavior. Here, we show that several important aspects of the detokenization\nstage can be understood purely by analyzing model weights, without performing\nany model inference steps. Specifically, we introduce an analytical\ndecomposition of first-layer attention in GPT-2. Our decomposition yields\ninterpretable terms that quantify the relative contributions of\nposition-related, token-related, and mixed effects. By focusing on terms in\nthis decomposition, we discover weight-based explanations of attention bias\ntoward close tokens and attention for detokenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to the stages-of-inference hypothesis, early layers of language\nmodels map their subword-tokenized input, which does not necessarily correspond\nto a linguistically meaningful segmentation, to more meaningful representations\nthat form the model's ``inner vocabulary''. Prior analysis of this\ndetokenization stage has predominantly relied on probing and interventions such\nas path patching, which involve selecting particular inputs, choosing a subset\nof components that will be patched, and then observing changes in model\nbehavior. Here, we show that several important aspects of the detokenization\nstage can be understood purely by analyzing model weights, without performing\nany model inference steps. Specifically, we introduce an analytical\ndecomposition of first-layer attention in GPT-2. Our decomposition yields\ninterpretable terms that quantify the relative contributions of\nposition-related, token-related, and mixed effects. By focusing on terms in\nthis decomposition, we discover weight-based explanations of attention bias\ntoward close tokens and attention for detokenization."
                },
                "authors": [
                    {
                        "name": "Go Kamoda"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Tatsuro Inaba"
                    },
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "arxiv_comment": "22 pages, 14 figures, to appear in NAACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08193v2",
                "updated": "2025-01-28T03:28:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    28,
                    12,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T17:58:24Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining."
                },
                "authors": [
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "arxiv_comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16820v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16820v5",
                "updated": "2025-01-28T03:21:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    21,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2024-01-30T08:46:48Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    8,
                    46,
                    48,
                    1,
                    30,
                    0
                ],
                "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Robust Multi-bit Watermarking for AI-generated Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities of\ngenerating texts resembling human language. However, they can be misused by\ncriminals to create deceptive content, such as fake news and phishing emails,\nwhich raises ethical concerns. Watermarking is a key technique to address these\nconcerns, which embeds a message (e.g., a bit string) into a text generated by\nan LLM. By embedding the user ID (represented as a bit string) into generated\ntexts, we can trace generated texts to the user, known as content source\ntracing. The major limitation of existing watermarking techniques is that they\nachieve sub-optimal performance for content source tracing in real-world\nscenarios. The reason is that they cannot accurately or efficiently extract a\nlong message from a generated text. We aim to address the limitations.\n  In this work, we introduce a new watermarking method for LLM-generated text\ngrounded in pseudo-random segment assignment. We also propose multiple\ntechniques to further enhance the robustness of our watermarking algorithm. We\nconduct extensive experiments to evaluate our method. Our experimental results\nshow that our method substantially outperforms existing baselines in both\naccuracy and robustness on benchmark datasets. For instance, when embedding a\nmessage of length 20 into a 200-token generated text, our method achieves a\nmatch rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only\nachieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits\nwithin an edit distance of 17 on average for each paragraph under the same\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities of\ngenerating texts resembling human language. However, they can be misused by\ncriminals to create deceptive content, such as fake news and phishing emails,\nwhich raises ethical concerns. Watermarking is a key technique to address these\nconcerns, which embeds a message (e.g., a bit string) into a text generated by\nan LLM. By embedding the user ID (represented as a bit string) into generated\ntexts, we can trace generated texts to the user, known as content source\ntracing. The major limitation of existing watermarking techniques is that they\nachieve sub-optimal performance for content source tracing in real-world\nscenarios. The reason is that they cannot accurately or efficiently extract a\nlong message from a generated text. We aim to address the limitations.\n  In this work, we introduce a new watermarking method for LLM-generated text\ngrounded in pseudo-random segment assignment. We also propose multiple\ntechniques to further enhance the robustness of our watermarking algorithm. We\nconduct extensive experiments to evaluate our method. Our experimental results\nshow that our method substantially outperforms existing baselines in both\naccuracy and robustness on benchmark datasets. For instance, when embedding a\nmessage of length 20 into a 200-token generated text, our method achieves a\nmatch rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only\nachieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits\nwithin an edit distance of 17 on average for each paragraph under the same\nsetting."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Wengrui Zheng"
                    },
                    {
                        "name": "Tianyang Tao"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yanze Jiang"
                    },
                    {
                        "name": "Zhihua Tian"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear in Proceedings of USENIX Security '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16820v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16820v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16673v1",
                "updated": "2025-01-28T03:18:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "title": "Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting"
                },
                "summary": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research."
                },
                "authors": [
                    {
                        "name": "Li Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "arxiv_affiliation": "Atlas",
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16672v1",
                "updated": "2025-01-28T03:13:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    13,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:13:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    13,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records"
                },
                "summary": "Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks."
                },
                "authors": [
                    {
                        "name": "Philip Chung"
                    },
                    {
                        "name": "Akshay Swaminathan"
                    },
                    {
                        "name": "Alex J. Goodell"
                    },
                    {
                        "name": "Yeasul Kim"
                    },
                    {
                        "name": "S. Momsen Reincke"
                    },
                    {
                        "name": "Lichy Han"
                    },
                    {
                        "name": "Ben Deverett"
                    },
                    {
                        "name": "Mohammad Amin Sadeghi"
                    },
                    {
                        "name": "Abdel-Badih Ariss"
                    },
                    {
                        "name": "Marc Ghanem"
                    },
                    {
                        "name": "David Seong"
                    },
                    {
                        "name": "Andrew A. Lee"
                    },
                    {
                        "name": "Caitlin E. Coombes"
                    },
                    {
                        "name": "Brad Bradshaw"
                    },
                    {
                        "name": "Mahir A. Sufian"
                    },
                    {
                        "name": "Hyo Jung Hong"
                    },
                    {
                        "name": "Teresa P. Nguyen"
                    },
                    {
                        "name": "Mohammad R. Rasouli"
                    },
                    {
                        "name": "Komal Kamra"
                    },
                    {
                        "name": "Mark A. Burbridge"
                    },
                    {
                        "name": "James C. McAvoy"
                    },
                    {
                        "name": "Roya Saffary"
                    },
                    {
                        "name": "Stephen P. Ma"
                    },
                    {
                        "name": "Dev Dash"
                    },
                    {
                        "name": "James Xie"
                    },
                    {
                        "name": "Ellen Y. Wang"
                    },
                    {
                        "name": "Clifford A. Schmiesing"
                    },
                    {
                        "name": "Nigam Shah"
                    },
                    {
                        "name": "Nima Aghaeepour"
                    }
                ],
                "author_detail": {
                    "name": "Nima Aghaeepour"
                },
                "author": "Nima Aghaeepour",
                "arxiv_comment": "62 pages, 5 figures, 1 table, pre-print manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16671v1",
                "updated": "2025-01-28T03:12:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    12,
                    57,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:12:57Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    12,
                    57,
                    1,
                    28,
                    0
                ],
                "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative\n  AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative\n  AI"
                },
                "summary": "Generative AI technology has become increasingly integrated into our daily\nlives, offering powerful capabilities to enhance productivity. However, these\nsame capabilities can be exploited by adversaries for malicious purposes. While\nexisting research on adversarial applications of generative AI predominantly\nfocuses on cyberattacks, less attention has been given to attacks targeting\ndeep learning models. In this paper, we introduce the use of generative AI for\nfacilitating model-related attacks, including model extraction, membership\ninference, and model inversion. Our study reveals that adversaries can launch a\nvariety of model-related attacks against both image and text models in a\ndata-free and black-box manner, achieving comparable performance to baseline\nmethods that have access to the target models' training data and parameters in\na white-box manner. This research serves as an important early warning to the\ncommunity about the potential risks associated with generative AI-powered\nattacks on deep learning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology has become increasingly integrated into our daily\nlives, offering powerful capabilities to enhance productivity. However, these\nsame capabilities can be exploited by adversaries for malicious purposes. While\nexisting research on adversarial applications of generative AI predominantly\nfocuses on cyberattacks, less attention has been given to attacks targeting\ndeep learning models. In this paper, we introduce the use of generative AI for\nfacilitating model-related attacks, including model extraction, membership\ninference, and model inversion. Our study reveals that adversaries can launch a\nvariety of model-related attacks against both image and text models in a\ndata-free and black-box manner, achieving comparable performance to baseline\nmethods that have access to the target models' training data and parameters in\na white-box manner. This research serves as an important early warning to the\ncommunity about the potential risks associated with generative AI-powered\nattacks on deep learning models."
                },
                "authors": [
                    {
                        "name": "Dayong Ye"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Shang Wang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Wanlei Zhou"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "Accepted at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.14622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.14622v4",
                "updated": "2025-01-28T03:12:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    12,
                    13,
                    1,
                    28,
                    0
                ],
                "published": "2022-12-30T10:17:09Z",
                "published_parsed": [
                    2022,
                    12,
                    30,
                    10,
                    17,
                    9,
                    4,
                    364,
                    0
                ],
                "title": "Identifying causal effects with subjective ordinal outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying causal effects with subjective ordinal outcomes"
                },
                "summary": "Survey questions often ask respondents to select from ordered scales where\nthe meanings of the categories are subjective, leaving each individual free to\napply their own definitions in answering. This paper studies the use of these\nresponses as an outcome variable in causal inference, accounting for variation\nin interpretation of the categories across individuals. I find that when a\ncontinuous treatment variable is statistically independent of both i) potential\noutcomes; and ii) heterogeneity in reporting styles, a nonparametric regression\nof response category number on that treatment variable recovers a quantity\nproportional to an average causal effect among individuals who are on the\nmargin between successive response categories. The magnitude of a given\nregression coefficient is not meaningful on its own, but the ratio of local\nregression derivatives with respect to two such treatment variables identifies\nthe relative magnitudes of convex averages of their effects. These results can\nbe seen as limiting cases of analogous results for binary treatment variables,\nthough comparisons of magnitude involving discrete treatments are not as\nreadily interpretable outside of the limit. I obtain a partial identification\nresult for comparisons involving discrete treatments under further assumptions.\nAn empirical application illustrates the results by revisiting the effects of\nincome comparisons on subjective well-being, without assuming cardinality or\ninterpersonal comparability of responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey questions often ask respondents to select from ordered scales where\nthe meanings of the categories are subjective, leaving each individual free to\napply their own definitions in answering. This paper studies the use of these\nresponses as an outcome variable in causal inference, accounting for variation\nin interpretation of the categories across individuals. I find that when a\ncontinuous treatment variable is statistically independent of both i) potential\noutcomes; and ii) heterogeneity in reporting styles, a nonparametric regression\nof response category number on that treatment variable recovers a quantity\nproportional to an average causal effect among individuals who are on the\nmargin between successive response categories. The magnitude of a given\nregression coefficient is not meaningful on its own, but the ratio of local\nregression derivatives with respect to two such treatment variables identifies\nthe relative magnitudes of convex averages of their effects. These results can\nbe seen as limiting cases of analogous results for binary treatment variables,\nthough comparisons of magnitude involving discrete treatments are not as\nreadily interpretable outside of the limit. I obtain a partial identification\nresult for comparisons involving discrete treatments under further assumptions.\nAn empirical application illustrates the results by revisiting the effects of\nincome comparisons on subjective well-being, without assuming cardinality or\ninterpersonal comparability of responses."
                },
                "authors": [
                    {
                        "name": "Leonard Goff"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Goff"
                },
                "author": "Leonard Goff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.14622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.14622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13255v2",
                "updated": "2025-01-28T02:52:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    52,
                    10,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-22T22:23:19Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    22,
                    23,
                    19,
                    2,
                    22,
                    0
                ],
                "title": "Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in\n  Microstructure-Properties of Ceramic Aerogels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in\n  Microstructure-Properties of Ceramic Aerogels"
                },
                "summary": "Deep learning surrogate models have become pivotal in enabling model-driven\nmaterials discovery to achieve exceptional properties. However, ensuring the\naccuracy and reliability of predictions from these models, trained on limited\nand sparse material datasets remains a significant challenge. This study\nintroduces an integrated deep learning framework for predicting the synthesis,\nmicrostructure, and mechanical properties of ceramic aerogels, leveraging\nphysics-based models such as Lattice Boltzmann simulations for microstructure\nformation and stochastic finite element methods for mechanical property\ncalculations. To address the computational demands of repeated physics-based\nsimulations required for experimental calibration and material design, a linked\nsurrogate model is developed, leveraging Convolutional Neural Networks (CNNs)\nfor stochastic microstructure generation and microstructure-to-mechanical\nproperty mapping. To overcome challenges associated with limited training\ndatasets from expensive physical modeling, CNN training is formulated within a\nBayesian inference framework, enabling robust uncertainty quantification in\npredictions. Numerical results highlight the strengths and limitations of the\nlinked surrogate framework, demonstrating its effectiveness in predicting\nproperties of aerogels with pore sizes and morphologies similar to the training\ndata (in-distribution) and its ability to interpolate to new microstructural\nfeatures between training data (out-of-distribution).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning surrogate models have become pivotal in enabling model-driven\nmaterials discovery to achieve exceptional properties. However, ensuring the\naccuracy and reliability of predictions from these models, trained on limited\nand sparse material datasets remains a significant challenge. This study\nintroduces an integrated deep learning framework for predicting the synthesis,\nmicrostructure, and mechanical properties of ceramic aerogels, leveraging\nphysics-based models such as Lattice Boltzmann simulations for microstructure\nformation and stochastic finite element methods for mechanical property\ncalculations. To address the computational demands of repeated physics-based\nsimulations required for experimental calibration and material design, a linked\nsurrogate model is developed, leveraging Convolutional Neural Networks (CNNs)\nfor stochastic microstructure generation and microstructure-to-mechanical\nproperty mapping. To overcome challenges associated with limited training\ndatasets from expensive physical modeling, CNN training is formulated within a\nBayesian inference framework, enabling robust uncertainty quantification in\npredictions. Numerical results highlight the strengths and limitations of the\nlinked surrogate framework, demonstrating its effectiveness in predicting\nproperties of aerogels with pore sizes and morphologies similar to the training\ndata (in-distribution) and its ability to interpolate to new microstructural\nfeatures between training data (out-of-distribution)."
                },
                "authors": [
                    {
                        "name": "Md Azharul Islam"
                    },
                    {
                        "name": "Dwyer Deighan"
                    },
                    {
                        "name": "Shayan Bhattacharjee"
                    },
                    {
                        "name": "Daniel Tantalo"
                    },
                    {
                        "name": "Pratyush Kumar Singh"
                    },
                    {
                        "name": "David Salac"
                    },
                    {
                        "name": "Danial Faghihi"
                    }
                ],
                "author_detail": {
                    "name": "Danial Faghihi"
                },
                "author": "Danial Faghihi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16661v1",
                "updated": "2025-01-28T02:49:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    49,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:49:45Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    49,
                    45,
                    1,
                    28,
                    0
                ],
                "title": "Jupybara: Operationalizing a Design Space for Actionable Data Analysis\n  and Storytelling with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jupybara: Operationalizing a Design Space for Actionable Data Analysis\n  and Storytelling with LLMs"
                },
                "summary": "Mining and conveying actionable insights from complex data is a key challenge\nof exploratory data analysis (EDA) and storytelling. To address this challenge,\nwe present a design space for actionable EDA and storytelling. Synthesizing\ntheory and expert interviews, we highlight how semantic precision, rhetorical\npersuasion, and pragmatic relevance underpin effective EDA and storytelling. We\nalso show how this design space subsumes common challenges in actionable EDA\nand storytelling, such as identifying appropriate analytical strategies and\nleveraging relevant domain knowledge. Building on the potential of LLMs to\ngenerate coherent narratives with commonsense reasoning, we contribute\nJupybara, an AI-enabled assistant for actionable EDA and storytelling\nimplemented as a Jupyter Notebook extension. Jupybara employs two strategies --\ndesign-space-aware prompting and multi-agent architectures -- to operationalize\nour design space. An expert evaluation confirms Jupybara's usability,\nsteerability, explainability, and reparability, as well as the effectiveness of\nour strategies in operationalizing the design space framework with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining and conveying actionable insights from complex data is a key challenge\nof exploratory data analysis (EDA) and storytelling. To address this challenge,\nwe present a design space for actionable EDA and storytelling. Synthesizing\ntheory and expert interviews, we highlight how semantic precision, rhetorical\npersuasion, and pragmatic relevance underpin effective EDA and storytelling. We\nalso show how this design space subsumes common challenges in actionable EDA\nand storytelling, such as identifying appropriate analytical strategies and\nleveraging relevant domain knowledge. Building on the potential of LLMs to\ngenerate coherent narratives with commonsense reasoning, we contribute\nJupybara, an AI-enabled assistant for actionable EDA and storytelling\nimplemented as a Jupyter Notebook extension. Jupybara employs two strategies --\ndesign-space-aware prompting and multi-agent architectures -- to operationalize\nour design space. An expert evaluation confirms Jupybara's usability,\nsteerability, explainability, and reparability, as well as the effectiveness of\nour strategies in operationalizing the design space framework with LLMs."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Larry Birnbaum"
                    },
                    {
                        "name": "Vidya Setlur"
                    }
                ],
                "author_detail": {
                    "name": "Vidya Setlur"
                },
                "author": "Vidya Setlur",
                "arxiv_comment": "Conditionally Accepted at CHI '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.03146v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.03146v6",
                "updated": "2025-01-28T02:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    48,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2022-01-24T17:01:59Z",
                "published_parsed": [
                    2022,
                    1,
                    24,
                    17,
                    1,
                    59,
                    0,
                    24,
                    0
                ],
                "title": "Time-Series K-means in Causal Inference and Mechanism Clustering for\n  Financial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Series K-means in Causal Inference and Mechanism Clustering for\n  Financial Data"
                },
                "summary": "This paper investigates the application of Time Series K-means (TS-K-means)\nwithin the context of causal inference and mechanism clustering of financial\ntime series data. Traditional clustering approaches like K-means often rely on\nstatic distance metrics, such as Euclidean distance, which inadequately capture\nthe temporal dependencies intrinsic to financial returns. By incorporating\nDynamic Time Warping (DTW) as a distance metric, TS-K-means addresses this\nlimitation, improving the robustness of clustering in time-dependent financial\ndata. This study extends the Additive Noise Model Mixture Model (ANM-MM)\nframework by integrating TS-K-means, facilitating more accurate causal\ninference and mechanism clustering. The approach is validated through\nsimulations and applied to real-world financial data, demonstrating its\neffectiveness in enhancing the analysis of complex financial time series,\nparticularly in identifying causal relationships and clustering data based on\nunderlying generative mechanisms. The results show that TS-K-means outperforms\ntraditional K-means, especially with smaller datasets, while maintaining robust\ncausal direction detection as the dataset size changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the application of Time Series K-means (TS-K-means)\nwithin the context of causal inference and mechanism clustering of financial\ntime series data. Traditional clustering approaches like K-means often rely on\nstatic distance metrics, such as Euclidean distance, which inadequately capture\nthe temporal dependencies intrinsic to financial returns. By incorporating\nDynamic Time Warping (DTW) as a distance metric, TS-K-means addresses this\nlimitation, improving the robustness of clustering in time-dependent financial\ndata. This study extends the Additive Noise Model Mixture Model (ANM-MM)\nframework by integrating TS-K-means, facilitating more accurate causal\ninference and mechanism clustering. The approach is validated through\nsimulations and applied to real-world financial data, demonstrating its\neffectiveness in enhancing the analysis of complex financial time series,\nparticularly in identifying causal relationships and clustering data based on\nunderlying generative mechanisms. The results show that TS-K-means outperforms\ntraditional K-means, especially with smaller datasets, while maintaining robust\ncausal direction detection as the dataset size changes."
                },
                "authors": [
                    {
                        "name": "Shi Bo"
                    },
                    {
                        "name": "Minheng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Minheng Xiao"
                },
                "author": "Minheng Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.03146v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.03146v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16655v1",
                "updated": "2025-01-28T02:38:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    38,
                    56,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:38:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    38,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Large Language Model Critics for Execution-Free Evaluation of Code\n  Changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Critics for Execution-Free Evaluation of Code\n  Changes"
                },
                "summary": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via\nmulti-step LLM-based agentic workflows. However, existing metrics for\nevaluating such workflows, mainly build status and occasionally log analysis,\nare too sparse and limited in providing the information needed to assess the\nquality of changes made. In this work, we designed LLM-based critics to derive\nwell-structured and rigorous intermediate/step-level, execution-free evaluation\nproxies for repo-level code changes. Importantly, we assume access to the gold\ntest patch for the problem (i.e., reference-aware) to assess both semantics and\nexecutability of generated patches. With the gold test patch as a reference, we\npredict executability of all editing locations with an F1 score of 91.6%,\naggregating which, we can predict the build status in 84.8% of the instances in\nSWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%.\nMoreover, we demonstrate the usefulness of such a reference-aware framework in\ncomparing patches generated by different agentic workflows. Finally, we\nopen-source the library developed for this project, which allows further usage\nfor either other agentic workflows or other benchmarks. The source code is\navailable at https://github.com/amazon-science/code-agent-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via\nmulti-step LLM-based agentic workflows. However, existing metrics for\nevaluating such workflows, mainly build status and occasionally log analysis,\nare too sparse and limited in providing the information needed to assess the\nquality of changes made. In this work, we designed LLM-based critics to derive\nwell-structured and rigorous intermediate/step-level, execution-free evaluation\nproxies for repo-level code changes. Importantly, we assume access to the gold\ntest patch for the problem (i.e., reference-aware) to assess both semantics and\nexecutability of generated patches. With the gold test patch as a reference, we\npredict executability of all editing locations with an F1 score of 91.6%,\naggregating which, we can predict the build status in 84.8% of the instances in\nSWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%.\nMoreover, we demonstrate the usefulness of such a reference-aware framework in\ncomparing patches generated by different agentic workflows. Finally, we\nopen-source the library developed for this project, which allows further usage\nfor either other agentic workflows or other benchmarks. The source code is\navailable at https://github.com/amazon-science/code-agent-eval."
                },
                "authors": [
                    {
                        "name": "Aashish Yadavally"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Laurent Callot"
                    },
                    {
                        "name": "Gauthier Guinet"
                    }
                ],
                "author_detail": {
                    "name": "Gauthier Guinet"
                },
                "author": "Gauthier Guinet",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16650v1",
                "updated": "2025-01-28T02:32:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    32,
                    49,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:32:49Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    32,
                    49,
                    1,
                    28,
                    0
                ],
                "title": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large\n  Language Models"
                },
                "summary": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for\nquantitatively assessing the similarity between weight matrices in Large\nLanguage Models (LLMs), aiming to facilitate the analysis of their complex\narchitectures. Leveraging DOCS, our analysis uncovers intriguing patterns in\nthe latest open-source LLMs: adjacent layers frequently exhibit high weight\nsimilarity and tend to form clusters, suggesting depth-wise functional\nspecialization. Additionally, we prove that DOCS is theoretically effective in\nquantifying similarity for orthogonal matrices, a crucial aspect given the\nprevalence of orthogonal initializations in LLMs. This research contributes to\na deeper understanding of LLM architecture and behavior, offering tools with\npotential implications for developing more efficient and interpretable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for\nquantitatively assessing the similarity between weight matrices in Large\nLanguage Models (LLMs), aiming to facilitate the analysis of their complex\narchitectures. Leveraging DOCS, our analysis uncovers intriguing patterns in\nthe latest open-source LLMs: adjacent layers frequently exhibit high weight\nsimilarity and tend to form clusters, suggesting depth-wise functional\nspecialization. Additionally, we prove that DOCS is theoretically effective in\nquantifying similarity for orthogonal matrices, a crucial aspect given the\nprevalence of orthogonal initializations in LLMs. This research contributes to\na deeper understanding of LLM architecture and behavior, offering tools with\npotential implications for developing more efficient and interpretable models."
                },
                "authors": [
                    {
                        "name": "Zeping Min"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.17148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17148v1",
                "updated": "2025-01-28T18:51:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:51:24Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders"
                },
                "summary": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean."
                },
                "authors": [
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17144v1",
                "updated": "2025-01-28T18:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:45:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data"
                },
                "summary": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size."
                },
                "authors": [
                    {
                        "name": "Deren Lei"
                    },
                    {
                        "name": "Yaxi Li"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Mengya Hu"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Ken Archer"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Emily Ching"
                    },
                    {
                        "name": "Alex Deng"
                    }
                ],
                "author_detail": {
                    "name": "Alex Deng"
                },
                "author": "Alex Deng",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07272v2",
                "updated": "2025-01-28T18:40:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    40,
                    26,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-14T03:42:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "Abstract Operations Research Modeling Using Natural Language Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Operations Research Modeling Using Natural Language Inputs"
                },
                "summary": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Ryan Wickman"
                    },
                    {
                        "name": "Sahil Bhatnagar"
                    },
                    {
                        "name": "Raj Kumar Maity"
                    },
                    {
                        "name": "Arko Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Arko Mukherjee"
                },
                "author": "Arko Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17132v1",
                "updated": "2025-01-28T18:25:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:25:11Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "title": "ASTRAL: Automated Safety Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRAL: Automated Safety Testing of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors."
                },
                "authors": [
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    },
                    {
                        "name": "Aitor Arrieta"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Arrieta"
                },
                "author": "Aitor Arrieta",
                "arxiv_journal_ref": "The 6th ACM/IEEE International Conference on Automation of\n  Software Test (AST 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17131v1",
                "updated": "2025-01-28T18:23:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    23,
                    12,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:23:12Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    23,
                    12,
                    1,
                    28,
                    0
                ],
                "title": "Scenario Understanding of Traffic Scenes Through Large Visual Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario Understanding of Traffic Scenes Through Large Visual Language\n  Models"
                },
                "summary": "Deep learning models for autonomous driving, encompassing perception,\nplanning, and control, depend on vast datasets to achieve their high\nperformance. However, their generalization often suffers due to domain-specific\ndata distributions, making an effective scene-based categorization of samples\nnecessary to improve their reliability across diverse domains. Manual\ncaptioning, though valuable, is both labor-intensive and time-consuming,\ncreating a bottleneck in the data annotation process. Large Visual Language\nModels (LVLMs) present a compelling solution by automating image analysis and\ncategorization through contextual queries, often without requiring retraining\nfor new categories. In this study, we evaluate the capabilities of LVLMs,\nincluding GPT-4 and LLaVA, to understand and classify urban traffic scenes on\nboth an in-house dataset and the BDD100K. We propose a scalable captioning\npipeline that integrates state-of-the-art models, enabling a flexible\ndeployment on new datasets. Our analysis, combining quantitative metrics with\nqualitative insights, demonstrates the effectiveness of LVLMs to understand\nurban traffic scenarios and highlights their potential as an efficient tool for\ndata-driven advancements in autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models for autonomous driving, encompassing perception,\nplanning, and control, depend on vast datasets to achieve their high\nperformance. However, their generalization often suffers due to domain-specific\ndata distributions, making an effective scene-based categorization of samples\nnecessary to improve their reliability across diverse domains. Manual\ncaptioning, though valuable, is both labor-intensive and time-consuming,\ncreating a bottleneck in the data annotation process. Large Visual Language\nModels (LVLMs) present a compelling solution by automating image analysis and\ncategorization through contextual queries, often without requiring retraining\nfor new categories. In this study, we evaluate the capabilities of LVLMs,\nincluding GPT-4 and LLaVA, to understand and classify urban traffic scenes on\nboth an in-house dataset and the BDD100K. We propose a scalable captioning\npipeline that integrates state-of-the-art models, enabling a flexible\ndeployment on new datasets. Our analysis, combining quantitative metrics with\nqualitative insights, demonstrates the effectiveness of LVLMs to understand\nurban traffic scenarios and highlights their potential as an efficient tool for\ndata-driven advancements in autonomous driving."
                },
                "authors": [
                    {
                        "name": "Rivera Esteban"
                    },
                    {
                        "name": "Lübberstedt Jannik"
                    },
                    {
                        "name": "Nico Uhlemann"
                    },
                    {
                        "name": "Markus Lienkamp"
                    }
                ],
                "author_detail": {
                    "name": "Markus Lienkamp"
                },
                "author": "Markus Lienkamp",
                "arxiv_comment": "Accepted at WACV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17117v1",
                "updated": "2025-01-28T18:07:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:07:30Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Histoires Morales: A French Dataset for Assessing Moral Alignment"
                },
                "summary": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata."
                },
                "authors": [
                    {
                        "name": "Thibaud Leteno"
                    },
                    {
                        "name": "Irina Proskurina"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Julien Velcin"
                    },
                    {
                        "name": "Charlotte Laclau"
                    },
                    {
                        "name": "Guillaume Metzler"
                    },
                    {
                        "name": "Christophe Gravier"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Gravier"
                },
                "author": "Christophe Gravier",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17116v1",
                "updated": "2025-01-28T18:04:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:04:50Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Large Language Model Training Using FP4 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Model Training Using FP4 Quantization"
                },
                "summary": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training."
                },
                "authors": [
                    {
                        "name": "Ruizhe Wang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Baining Guo"
                    },
                    {
                        "name": "Zhengjun Zha"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v2",
                "updated": "2025-01-28T18:00:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    0,
                    22,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17112v1",
                "updated": "2025-01-28T17:59:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    59,
                    56,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:59:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    59,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction"
                },
                "summary": "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Carl-Leander Henneking"
                    },
                    {
                        "name": "Claas Beger"
                    }
                ],
                "author_detail": {
                    "name": "Claas Beger"
                },
                "author": "Claas Beger",
                "arxiv_comment": "8 Pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14925v2",
                "updated": "2025-01-28T17:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-24T21:31:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    21,
                    31,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "Profiling Apple Silicon Performance for ML Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Apple Silicon Performance for ML Training"
                },
                "summary": "Apple Silicon has attracted much attention for its performance and role in\nmachine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally\ndominated ML training, Apple Silicon has a significant difference in memory\narchitecture. It uses Unified Memory, which integrates CPU and GPU memory\ninstead of separate CPU memory and GPU VRAM. However, it is difficult to tell\nwhether Unified Memory means more performance benefits.\n  This paper investigates the performance differences by training several large\nlanguage model (LLM) workloads end-to-end under different memory scenarios. The\nresults show a significant performance gap between Apple Silicon and NVIDIA\nGPUs. This paper attributes this gap to system-level factors such as page\nfaults, power consumption, and kernel launch time. In addition, the performance\ndifference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and\nApple Silicon chips is analyzed to further explain the observed gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Silicon has attracted much attention for its performance and role in\nmachine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally\ndominated ML training, Apple Silicon has a significant difference in memory\narchitecture. It uses Unified Memory, which integrates CPU and GPU memory\ninstead of separate CPU memory and GPU VRAM. However, it is difficult to tell\nwhether Unified Memory means more performance benefits.\n  This paper investigates the performance differences by training several large\nlanguage model (LLM) workloads end-to-end under different memory scenarios. The\nresults show a significant performance gap between Apple Silicon and NVIDIA\nGPUs. This paper attributes this gap to system-level factors such as page\nfaults, power consumption, and kernel launch time. In addition, the performance\ndifference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and\nApple Silicon chips is analyzed to further explain the observed gap."
                },
                "authors": [
                    {
                        "name": "Dahua Feng"
                    },
                    {
                        "name": "Zhiming Xu"
                    },
                    {
                        "name": "Rongxiang Wang"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08099v3",
                "updated": "2025-01-28T17:33:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    33,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-11T04:53:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    53,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications."
                },
                "authors": [
                    {
                        "name": "Fuqiang Liu"
                    },
                    {
                        "name": "Sicong Jiang"
                    },
                    {
                        "name": "Luis Miranda-Moreno"
                    },
                    {
                        "name": "Seongjin Choi"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "arxiv_comment": "AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04430v2",
                "updated": "2025-01-28T17:32:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    32,
                    33,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-08T12:57:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    12,
                    57,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "Large Language Models for cross-language code clone detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for cross-language code clone detection"
                },
                "summary": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection."
                },
                "authors": [
                    {
                        "name": "Micheline Bénédicte Moumoula"
                    },
                    {
                        "name": "Abdoul Kader Kabore"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé Bissyande"
                },
                "author": "Tegawendé Bissyande",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17084v1",
                "updated": "2025-01-28T17:11:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:11:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on\n  Advanced Mathematical Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on\n  Advanced Mathematical Problem-Solving"
                },
                "summary": "Large language models (LLMs) excel in many natural language tasks, yet they\nstruggle with complex mathemat-ical problem-solving, particularly in symbolic\nreasoning and maintaining consistent output. This study evalu-ates 10 LLMs with\n7 to 8 billion parameters using 945 competition-level problems from the MATH\ndataset. The focus is on their ability to generate executable Python code as a\nstep in their reasoning process, involving over 9,450 code executions. The\nresearch introduces an evaluation framework using mistral-large-2411 to rate\nanswers on a 5-point scale, which helps address inconsistencies in mathematical\nnotation. It also examines the impact of regenerating output token-by-token on\nrefining results. The findings reveal a significant 34.5% per-formance gap\nbetween the top commercial model (gpt-4o-mini, scoring 83.7%) and the least\neffective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This\ndisparity is especially noticeable in complex areas like Number Theory. While\ntoken-by-token regeneration slightly improved accuracy (+0.8%) for the model\nllama3.1:8b, it also reduced code execution time by 36.7%, highlighting a\ntrade-off between efficiency and precision. The study also noted a consistent\ntrend where harder problems correlated with lower accuracy across all models.\nDespite using controlled execution environments, less than 1% of the generated\ncode was unsafe, and 3.17% of problems remained unsolved after 10 attempts,\nsuggesting that hybrid reasoning methods may be beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many natural language tasks, yet they\nstruggle with complex mathemat-ical problem-solving, particularly in symbolic\nreasoning and maintaining consistent output. This study evalu-ates 10 LLMs with\n7 to 8 billion parameters using 945 competition-level problems from the MATH\ndataset. The focus is on their ability to generate executable Python code as a\nstep in their reasoning process, involving over 9,450 code executions. The\nresearch introduces an evaluation framework using mistral-large-2411 to rate\nanswers on a 5-point scale, which helps address inconsistencies in mathematical\nnotation. It also examines the impact of regenerating output token-by-token on\nrefining results. The findings reveal a significant 34.5% per-formance gap\nbetween the top commercial model (gpt-4o-mini, scoring 83.7%) and the least\neffective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This\ndisparity is especially noticeable in complex areas like Number Theory. While\ntoken-by-token regeneration slightly improved accuracy (+0.8%) for the model\nllama3.1:8b, it also reduced code execution time by 36.7%, highlighting a\ntrade-off between efficiency and precision. The study also noted a consistent\ntrend where harder problems correlated with lower accuracy across all models.\nDespite using controlled execution environments, less than 1% of the generated\ncode was unsafe, and 3.17% of problems remained unsolved after 10 attempts,\nsuggesting that hybrid reasoning methods may be beneficial."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17083v1",
                "updated": "2025-01-28T17:10:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    10,
                    47,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T17:10:47Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    10,
                    47,
                    1,
                    28,
                    0
                ],
                "title": "Experimental Outdoor Performance Evaluation of TVWS Narrowband Data\n  Communication using SDR Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Outdoor Performance Evaluation of TVWS Narrowband Data\n  Communication using SDR Platform"
                },
                "summary": "Large-scale deployment of Internet of Things (IoT) networks in the\nindustrial, scientific, and medical (ISM) band leads to spectrum congestion and\nrequires multiple gateways to cover wide areas. This will increase cost,\ncomplexity, and energy consumption. TV White Spaces (TVWS) provides an abundant\nspectrum that is sufficient for low data rate IoT applications. This\nlow-frequency band offers coverage over larger areas due to the ability of\nwireless signals to penetrate obstacles and terrain. In this paper, we examine\nthe performance of narrowband data communications in TVWS through an outdoor\nexperiment in a suburban area with line-of-sight (LOS) and non-line-of-sight\n(NLOS) propagation scenarios. We implement a software-defined radio (SDR)\ntestbed and develop a GNU radio benchmark tool to perform outdoor experiments\nfor TVWS narrowband data communication between a gateway and wireless nodes at\nvarious locations. The results reveal that the system can achieve a throughput\nof up to 97 Kbps with a packet error rate (PER) and packet loss rate (PLR)\nunder 1% over NLOS paths, making it suitable for low-data rate applications.\nThis work offers valuable insights for designing the physical layer of\nnarrowband white space devices (WSDs). The developed benchmark tool will also\ngreatly assist other researchers in evaluating the performance of SDR-based\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale deployment of Internet of Things (IoT) networks in the\nindustrial, scientific, and medical (ISM) band leads to spectrum congestion and\nrequires multiple gateways to cover wide areas. This will increase cost,\ncomplexity, and energy consumption. TV White Spaces (TVWS) provides an abundant\nspectrum that is sufficient for low data rate IoT applications. This\nlow-frequency band offers coverage over larger areas due to the ability of\nwireless signals to penetrate obstacles and terrain. In this paper, we examine\nthe performance of narrowband data communications in TVWS through an outdoor\nexperiment in a suburban area with line-of-sight (LOS) and non-line-of-sight\n(NLOS) propagation scenarios. We implement a software-defined radio (SDR)\ntestbed and develop a GNU radio benchmark tool to perform outdoor experiments\nfor TVWS narrowband data communication between a gateway and wireless nodes at\nvarious locations. The results reveal that the system can achieve a throughput\nof up to 97 Kbps with a packet error rate (PER) and packet loss rate (PLR)\nunder 1% over NLOS paths, making it suitable for low-data rate applications.\nThis work offers valuable insights for designing the physical layer of\nnarrowband white space devices (WSDs). The developed benchmark tool will also\ngreatly assist other researchers in evaluating the performance of SDR-based\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Muneer M. AlZubi"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17070v1",
                "updated": "2025-01-28T16:55:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    55,
                    39,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:55:39Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    55,
                    39,
                    1,
                    28,
                    0
                ],
                "title": "Context is Key in Agent Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context is Key in Agent Security"
                },
                "summary": "Judging the safety of an action, whether taken by a human or a system, must\ntake into account the context in which the action takes place. Deleting an\nemail from user's mailbox may or may not be appropriate depending on email's\ncontent, user's goals, or even available space. Systems today that make these\njudgements -- providing security against harmful or inappropriate actions --\nrely on manually-crafted policies or user confirmation for each relevant\ncontext. With the upcoming deployment of systems like generalist agents, we\nargue that we must rethink security designs to adapt to the scale of contexts\nand capabilities of these systems. As a first step, this paper explores\ncontextual security in the domain of agents and proposes contextual security\nfor agents (Conseca), a framework to generate just-in-time, contextual, and\nhuman-verifiable security policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the safety of an action, whether taken by a human or a system, must\ntake into account the context in which the action takes place. Deleting an\nemail from user's mailbox may or may not be appropriate depending on email's\ncontent, user's goals, or even available space. Systems today that make these\njudgements -- providing security against harmful or inappropriate actions --\nrely on manually-crafted policies or user confirmation for each relevant\ncontext. With the upcoming deployment of systems like generalist agents, we\nargue that we must rethink security designs to adapt to the scale of contexts\nand capabilities of these systems. As a first step, this paper explores\ncontextual security in the domain of agents and proposes contextual security\nfor agents (Conseca), a framework to generate just-in-time, contextual, and\nhuman-verifiable security policies."
                },
                "authors": [
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13631v2",
                "updated": "2025-01-28T16:42:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    42,
                    59,
                    1,
                    28,
                    0
                ],
                "published": "2024-06-19T15:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    28,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "On AI-Inspired UI-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On AI-Inspired UI-Design"
                },
                "summary": "Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches."
                },
                "authors": [
                    {
                        "name": "Jialiang Wei"
                    },
                    {
                        "name": "Anne-Lise Courbis"
                    },
                    {
                        "name": "Thomas Lambolais"
                    },
                    {
                        "name": "Gérard Dray"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17062v1",
                "updated": "2025-01-28T16:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    40,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    40,
                    40,
                    1,
                    28,
                    0
                ],
                "title": "EdgeMLOps: Operationalizing ML models with Cumulocity IoT and\n  thin-edge.io for Visual quality Inspection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeMLOps: Operationalizing ML models with Cumulocity IoT and\n  thin-edge.io for Visual quality Inspection"
                },
                "summary": "This paper introduces EdgeMLOps, a framework leveraging Cumulocity IoT and\nthin-edge.io for deploying and managing machine learning models on\nresource-constrained edge devices. We address the challenges of model\noptimization, deployment, and lifecycle management in edge environments. The\nframework's efficacy is demonstrated through a visual quality inspection (VQI)\nuse case where images of assets are processed on edge devices, enabling\nreal-time condition updates within an asset management system. Furthermore, we\nevaluate the performance benefits of different quantization methods,\nspecifically static and dynamic signed-int8, on a Raspberry Pi 4, demonstrating\nsignificant inference time reductions compared to FP32 precision. Our results\nhighlight the potential of EdgeMLOps to enable efficient and scalable AI\ndeployments at the edge for industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EdgeMLOps, a framework leveraging Cumulocity IoT and\nthin-edge.io for deploying and managing machine learning models on\nresource-constrained edge devices. We address the challenges of model\noptimization, deployment, and lifecycle management in edge environments. The\nframework's efficacy is demonstrated through a visual quality inspection (VQI)\nuse case where images of assets are processed on edge devices, enabling\nreal-time condition updates within an asset management system. Furthermore, we\nevaluate the performance benefits of different quantization methods,\nspecifically static and dynamic signed-int8, on a Raspberry Pi 4, demonstrating\nsignificant inference time reductions compared to FP32 precision. Our results\nhighlight the potential of EdgeMLOps to enable efficient and scalable AI\ndeployments at the edge for industrial applications."
                },
                "authors": [
                    {
                        "name": "Kanishk Chaturvedi"
                    },
                    {
                        "name": "Johannes Gasthuber"
                    },
                    {
                        "name": "Mohamed Abdelaal"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Abdelaal"
                },
                "author": "Mohamed Abdelaal",
                "arxiv_journal_ref": "Industry Track of the 21st Conference on Database Systems for\n  Business, Technology and Web (BTW'25, Bamberg, Germany), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17127v2",
                "updated": "2025-01-28T16:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    31,
                    51,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-22T16:00:26Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    0,
                    26,
                    1,
                    296,
                    0
                ],
                "title": "PAPILLON: Privacy Preservation from Internet-based and Local Language\n  Model Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAPILLON: Privacy Preservation from Internet-based and Local Language\n  Model Ensembles"
                },
                "summary": "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON."
                },
                "authors": [
                    {
                        "name": "Li Siyan"
                    },
                    {
                        "name": "Vethavikashini Chithrra Raghuram"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17032v2",
                "updated": "2025-01-28T16:28:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    28,
                    10,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-22T14:17:12Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    14,
                    17,
                    12,
                    6,
                    357,
                    0
                ],
                "title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04477v2",
                "updated": "2025-01-28T16:24:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    24,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2024-11-19T19:05:04Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    5,
                    4,
                    1,
                    324,
                    0
                ],
                "title": "Intelligent Tutors for Adult Learners: An Analysis of Needs and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Tutors for Adult Learners: An Analysis of Needs and\n  Challenges"
                },
                "summary": "This research examines the sociotechnical factors that influence the adoption\nand usage of intelligent tutoring systems in self-directed learning contexts,\nfocusing specifically on adult learners. The study is divided into two parts.\nFirst, we present Apprentice Tutors, a novel intelligent tutoring system\ndesigned to address the unique needs of adult learners. The platform includes\nadaptive problem selection, real-time feedback, and visual dashboards to\nsupport learning in college algebra topics. Second, we investigate the specific\nneeds and experiences of adult users through a deployment study and a series of\nfocus groups. Using thematic analysis, we identify key challenges and\nopportunities for improving tutor design and adoption. Based on these findings,\nwe offer actionable design recommendations to help developers create\nintelligent tutoring systems that better align with the motivations and\nlearning preferences of adult learners. This work contributes to the broader\nunderstanding of how to enhance educational technologies to support lifelong\nlearning and professional development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research examines the sociotechnical factors that influence the adoption\nand usage of intelligent tutoring systems in self-directed learning contexts,\nfocusing specifically on adult learners. The study is divided into two parts.\nFirst, we present Apprentice Tutors, a novel intelligent tutoring system\ndesigned to address the unique needs of adult learners. The platform includes\nadaptive problem selection, real-time feedback, and visual dashboards to\nsupport learning in college algebra topics. Second, we investigate the specific\nneeds and experiences of adult users through a deployment study and a series of\nfocus groups. Using thematic analysis, we identify key challenges and\nopportunities for improving tutor design and adoption. Based on these findings,\nwe offer actionable design recommendations to help developers create\nintelligent tutoring systems that better align with the motivations and\nlearning preferences of adult learners. This work contributes to the broader\nunderstanding of how to enhance educational technologies to support lifelong\nlearning and professional development."
                },
                "authors": [
                    {
                        "name": "Adit Gupta"
                    },
                    {
                        "name": "Momin Siddiqui"
                    },
                    {
                        "name": "Glen Smith"
                    },
                    {
                        "name": "Jenn Reddig"
                    },
                    {
                        "name": "Christopher MacLellan"
                    }
                ],
                "author_detail": {
                    "name": "Christopher MacLellan"
                },
                "author": "Christopher MacLellan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17039v1",
                "updated": "2025-01-28T16:03:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    3,
                    52,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T16:03:52Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    3,
                    52,
                    1,
                    28,
                    0
                ],
                "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block\n  Representations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block\n  Representations with Large Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated exceptional\npower in various domains, including information retrieval. Most of the previous\npractices involve leveraging these models to create a single embedding for each\nquery, each passage, or each document individually, a strategy exemplified and\nused by the Retrieval-Augmented Generation (RAG) framework. While this method\nhas proven effective, we argue that it falls short in fully capturing the\nnuanced intricacies of document-level texts due to its reliance on a relatively\ncoarse-grained representation. To address this limitation, we introduce a\nnovel, fine-grained approach aimed at enhancing the accuracy of relevance\nscoring for long documents. Our methodology firstly segments a long document\ninto blocks, each of which is embedded using an LLM, for matching with the\nquery representation. When calculating the relevance score, we aggregate the\nquery-block relevance scores through a weighted sum method, yielding a\ncomprehensive score for the query with the entire document. Despite its\napparent simplicity, our experimental findings reveal that this approach\noutperforms standard representation methods and achieves a significant\nreduction in embedding generation latency. Moreover, by carefully optimizing\npairwise loss functions, superior performances have been achieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated exceptional\npower in various domains, including information retrieval. Most of the previous\npractices involve leveraging these models to create a single embedding for each\nquery, each passage, or each document individually, a strategy exemplified and\nused by the Retrieval-Augmented Generation (RAG) framework. While this method\nhas proven effective, we argue that it falls short in fully capturing the\nnuanced intricacies of document-level texts due to its reliance on a relatively\ncoarse-grained representation. To address this limitation, we introduce a\nnovel, fine-grained approach aimed at enhancing the accuracy of relevance\nscoring for long documents. Our methodology firstly segments a long document\ninto blocks, each of which is embedded using an LLM, for matching with the\nquery representation. When calculating the relevance score, we aggregate the\nquery-block relevance scores through a weighted sum method, yielding a\ncomprehensive score for the query with the entire document. Despite its\napparent simplicity, our experimental findings reveal that this approach\noutperforms standard representation methods and achieves a significant\nreduction in embedding generation latency. Moreover, by carefully optimizing\npairwise loss functions, superior performances have been achieved."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07688v2",
                "updated": "2025-01-28T16:02:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    2,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T07:54:17Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    7,
                    54,
                    17,
                    3,
                    284,
                    0
                ],
                "title": "PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for\n  Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for\n  Robotics"
                },
                "summary": "Data-driven methods have shown great potential in solving challenging\nmanipulation tasks; however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this\nlack, we propose PokeFlex, a dataset featuring real-world multimodal data that\nis paired and annotated. The modalities include 3D textured meshes, point\nclouds, RGB images, and depth maps. Such data can be leveraged for several\ndownstream tasks, such as online 3D mesh reconstruction, and it can potentially\nenable underexplored applications such as the real-world deployment of\ntraditional control methods based on mesh simulations. To deal with the\nchallenges posed by real-world 3D mesh reconstruction, we leverage a\nprofessional volumetric capture system that allows complete 360{\\deg}\nreconstruction. PokeFlex consists of 18 deformable objects with varying\nstiffness and shapes. Deformations are generated by dropping objects onto a\nflat surface or by poking the objects with a robot arm. Interaction wrenches\nand contact locations are also reported for the latter case. Using different\ndata modalities, we demonstrated a use case for our dataset training models\nthat, given the novelty of the multimodal nature of Pokeflex, constitute the\nstate-of-the-art in multi-object online template-based mesh reconstruction from\nmultimodal data, to the best of our knowledge. We refer the reader to our\nwebsite ( https://pokeflex-dataset.github.io/ ) for further demos and examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven methods have shown great potential in solving challenging\nmanipulation tasks; however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this\nlack, we propose PokeFlex, a dataset featuring real-world multimodal data that\nis paired and annotated. The modalities include 3D textured meshes, point\nclouds, RGB images, and depth maps. Such data can be leveraged for several\ndownstream tasks, such as online 3D mesh reconstruction, and it can potentially\nenable underexplored applications such as the real-world deployment of\ntraditional control methods based on mesh simulations. To deal with the\nchallenges posed by real-world 3D mesh reconstruction, we leverage a\nprofessional volumetric capture system that allows complete 360{\\deg}\nreconstruction. PokeFlex consists of 18 deformable objects with varying\nstiffness and shapes. Deformations are generated by dropping objects onto a\nflat surface or by poking the objects with a robot arm. Interaction wrenches\nand contact locations are also reported for the latter case. Using different\ndata modalities, we demonstrated a use case for our dataset training models\nthat, given the novelty of the multimodal nature of Pokeflex, constitute the\nstate-of-the-art in multi-object online template-based mesh reconstruction from\nmultimodal data, to the best of our knowledge. We refer the reader to our\nwebsite ( https://pokeflex-dataset.github.io/ ) for further demos and examples."
                },
                "authors": [
                    {
                        "name": "Jan Obrist"
                    },
                    {
                        "name": "Miguel Zamora"
                    },
                    {
                        "name": "Hehui Zheng"
                    },
                    {
                        "name": "Ronan Hinchet"
                    },
                    {
                        "name": "Firat Ozdemir"
                    },
                    {
                        "name": "Juan Zarate"
                    },
                    {
                        "name": "Robert K. Katzschmann"
                    },
                    {
                        "name": "Stelian Coros"
                    }
                ],
                "author_detail": {
                    "name": "Stelian Coros"
                },
                "author": "Stelian Coros",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17037v1",
                "updated": "2025-01-28T15:59:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    59,
                    1,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T15:59:01Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    59,
                    1,
                    1,
                    28,
                    0
                ],
                "title": "Standardised schema and taxonomy for AI incident databases in critical\n  digital infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardised schema and taxonomy for AI incident databases in critical\n  digital infrastructure"
                },
                "summary": "The rapid deployment of Artificial Intelligence (AI) in critical digital\ninfrastructure introduces significant risks, necessitating a robust framework\nfor systematically collecting AI incident data to prevent future incidents.\nExisting databases lack the granularity as well as the standardized structure\nrequired for consistent data collection and analysis, impeding effective\nincident management. This work proposes a standardized schema and taxonomy for\nAI incident databases, addressing these challenges by enabling detailed and\nstructured documentation of AI incidents across sectors. Key contributions\ninclude developing a unified schema, introducing new fields such as incident\nseverity, causes, and harms caused, and proposing a taxonomy for classifying AI\nincidents in critical digital infrastructure. The proposed solution facilitates\nmore effective incident data collection and analysis, thus supporting\nevidence-based policymaking, enhancing industry safety measures, and promoting\ntransparency. This work lays the foundation for a coordinated global response\nto AI incidents, ensuring trust, safety, and accountability in using AI across\nregions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of Artificial Intelligence (AI) in critical digital\ninfrastructure introduces significant risks, necessitating a robust framework\nfor systematically collecting AI incident data to prevent future incidents.\nExisting databases lack the granularity as well as the standardized structure\nrequired for consistent data collection and analysis, impeding effective\nincident management. This work proposes a standardized schema and taxonomy for\nAI incident databases, addressing these challenges by enabling detailed and\nstructured documentation of AI incidents across sectors. Key contributions\ninclude developing a unified schema, introducing new fields such as incident\nseverity, causes, and harms caused, and proposing a taxonomy for classifying AI\nincidents in critical digital infrastructure. The proposed solution facilitates\nmore effective incident data collection and analysis, thus supporting\nevidence-based policymaking, enhancing industry safety measures, and promoting\ntransparency. This work lays the foundation for a coordinated global response\nto AI incidents, ensuring trust, safety, and accountability in using AI across\nregions."
                },
                "authors": [
                    {
                        "name": "Avinash Agarwal"
                    },
                    {
                        "name": "Manisha J. Nene"
                    }
                ],
                "author_detail": {
                    "name": "Manisha J. Nene"
                },
                "author": "Manisha J. Nene",
                "arxiv_comment": "6 pages, 3 tables. Accepted at the 2024 IEEE Pune Section\n  International Conference (PuneCon)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17030v1",
                "updated": "2025-01-28T15:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    52,
                    51,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T15:52:51Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    52,
                    51,
                    1,
                    28,
                    0
                ],
                "title": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented."
                },
                "authors": [
                    {
                        "name": "Manojkumar Parmar"
                    },
                    {
                        "name": "Yuvaraj Govindarajulu"
                    }
                ],
                "author_detail": {
                    "name": "Yuvaraj Govindarajulu"
                },
                "author": "Yuvaraj Govindarajulu",
                "arxiv_comment": "9 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17024v1",
                "updated": "2025-01-28T15:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    41,
                    54,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T15:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    41,
                    54,
                    1,
                    28,
                    0
                ],
                "title": "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs"
                },
                "summary": "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required."
                },
                "authors": [
                    {
                        "name": "Alessandro Midolo"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Di Penta"
                },
                "author": "Massimiliano Di Penta",
                "arxiv_journal_ref": "Proceedings of the 33rd IEEE/ACM International Conference on\n  Program Comprehension (ICPC 2025), April 27-28 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v2",
                "updated": "2025-01-28T15:11:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    15,
                    11,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "arxiv_comment": "Accepted to ICLR 2025 main conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16998v1",
                "updated": "2025-01-28T14:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T14:52:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    52,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "Large Language Models for Code Generation: The Practitioners Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code Generation: The Practitioners Perspective"
                },
                "summary": "Large Language Models (LLMs) have emerged as coding assistants, capable of\ngenerating source code from natural language prompts. With the increasing\nadoption of LLMs in software development, academic research and industry based\nprojects are developing various tools, benchmarks, and metrics to evaluate the\neffectiveness of LLM-generated code. However, there is a lack of solutions\nevaluated through empirically grounded methods that incorporate practitioners\nperspectives to assess functionality, syntax, and accuracy in real world\napplications. To address this gap, we propose and develop a multi-model unified\nplatform to generate and execute code based on natural language prompts. We\nconducted a survey with 60 software practitioners from 11 countries across four\ncontinents working in diverse professional roles and domains to evaluate the\nusability, performance, strengths, and limitations of each model. The results\npresent practitioners feedback and insights into the use of LLMs in software\ndevelopment, including their strengths and weaknesses, key aspects overlooked\nby benchmarks and metrics, and a broader understanding of their practical\napplicability. These findings can help researchers and practitioners make\ninformed decisions for systematically selecting and using LLMs in software\ndevelopment projects. Future research will focus on integrating more diverse\nmodels into the proposed system, incorporating additional case studies, and\nconducting developer interviews for deeper empirical insights into LLM-driven\nsoftware development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as coding assistants, capable of\ngenerating source code from natural language prompts. With the increasing\nadoption of LLMs in software development, academic research and industry based\nprojects are developing various tools, benchmarks, and metrics to evaluate the\neffectiveness of LLM-generated code. However, there is a lack of solutions\nevaluated through empirically grounded methods that incorporate practitioners\nperspectives to assess functionality, syntax, and accuracy in real world\napplications. To address this gap, we propose and develop a multi-model unified\nplatform to generate and execute code based on natural language prompts. We\nconducted a survey with 60 software practitioners from 11 countries across four\ncontinents working in diverse professional roles and domains to evaluate the\nusability, performance, strengths, and limitations of each model. The results\npresent practitioners feedback and insights into the use of LLMs in software\ndevelopment, including their strengths and weaknesses, key aspects overlooked\nby benchmarks and metrics, and a broader understanding of their practical\napplicability. These findings can help researchers and practitioners make\ninformed decisions for systematically selecting and using LLMs in software\ndevelopment projects. Future research will focus on integrating more diverse\nmodels into the proposed system, incorporating additional case studies, and\nconducting developer interviews for deeper empirical insights into LLM-driven\nsoftware development."
                },
                "authors": [
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai Kristian Kemell"
                    },
                    {
                        "name": "Aakash Ahmad"
                    },
                    {
                        "name": "Malik Abdul Sami"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Kari Systä"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "20 pages, 4 figures, 2 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10652v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10652v5",
                "updated": "2025-01-28T14:38:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    38,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2023-11-17T17:14:32Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    17,
                    14,
                    32,
                    4,
                    321,
                    0
                ],
                "title": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems"
                },
                "summary": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems."
                },
                "authors": [
                    {
                        "name": "Vikram Mohanty"
                    },
                    {
                        "name": "Jude Lim"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "arxiv_doi": "10.1145/3706598.3713751",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713751",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10652v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10652v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM CHI 2025 Conference on Human Factors in Computing\n  Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18644v3",
                "updated": "2025-01-28T14:23:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    23,
                    17,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-24T16:06:53Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    6,
                    53,
                    1,
                    359,
                    0
                ],
                "title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation"
                },
                "summary": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    }
                ],
                "author_detail": {
                    "name": "Karishma Thakrar"
                },
                "author": "Karishma Thakrar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16975v1",
                "updated": "2025-01-28T14:15:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    15,
                    42,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T14:15:42Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    15,
                    42,
                    1,
                    28,
                    0
                ],
                "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling"
                },
                "summary": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs."
                },
                "authors": [
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Defa Zhu"
                    },
                    {
                        "name": "Banggu Wu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15602v2",
                "updated": "2025-01-28T14:14:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    14,
                    3,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-26T17:05:16Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    17,
                    5,
                    16,
                    6,
                    26,
                    0
                ],
                "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning"
                },
                "summary": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability."
                },
                "authors": [
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Yun Liao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16952v1",
                "updated": "2025-01-28T13:49:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    49,
                    39,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:49:39Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    49,
                    39,
                    1,
                    28,
                    0
                ],
                "title": "Multiple Abstraction Level Retrieve Augment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Abstraction Level Retrieve Augment Generation"
                },
                "summary": "A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers."
                },
                "authors": [
                    {
                        "name": "Zheng Zheng"
                    },
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Pengyu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Hong"
                },
                "arxiv_affiliation": "Brandeis University",
                "author": "Pengyu Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16945v1",
                "updated": "2025-01-28T13:42:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    42,
                    33,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:42:33Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    42,
                    33,
                    1,
                    28,
                    0
                ],
                "title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations"
                },
                "summary": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows."
                },
                "authors": [
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Qiuyang Wang"
                    },
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Pengyu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Hong"
                },
                "arxiv_affiliation": "Brandeis University",
                "author": "Pengyu Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16937v1",
                "updated": "2025-01-28T13:31:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    31,
                    18,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:31:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    31,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models"
                },
                "summary": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies."
                },
                "authors": [
                    {
                        "name": "Makoto Shing"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Sho Yokoi"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16929v1",
                "updated": "2025-01-28T13:18:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    18,
                    27,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:18:27Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    18,
                    27,
                    1,
                    28,
                    0
                ],
                "title": "Giving Sense to Inputs: Toward an Accessible Control Framework for\n  Shared Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giving Sense to Inputs: Toward an Accessible Control Framework for\n  Shared Autonomy"
                },
                "summary": "While shared autonomy offers significant potential for assistive robotics,\nkey questions remain about how to effectively map 2D control inputs to 6D robot\nmotions. An intuitive framework should allow users to input commands\neffortlessly, with the robot responding as expected, without users needing to\nanticipate the impact of their inputs. In this article, we propose a dynamic\ninput mapping framework that links joystick movements to motions on control\nframes defined along a trajectory encoded with canal surfaces. We evaluate our\nmethod in a user study with 20 participants, demonstrating that our input\nmapping framework reduces the workload and improves usability compared to a\nbaseline mapping with similar motion encoding. To prepare for deployment in\nassistive scenarios, we built on the development from the accessible gaming\ncommunity to select an accessible control interface. We then tested the system\nin an exploratory study, where three wheelchair users controlled the robot for\nboth daily living activities and a creative painting task, demonstrating its\nfeasibility for users closer to our target population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While shared autonomy offers significant potential for assistive robotics,\nkey questions remain about how to effectively map 2D control inputs to 6D robot\nmotions. An intuitive framework should allow users to input commands\neffortlessly, with the robot responding as expected, without users needing to\nanticipate the impact of their inputs. In this article, we propose a dynamic\ninput mapping framework that links joystick movements to motions on control\nframes defined along a trajectory encoded with canal surfaces. We evaluate our\nmethod in a user study with 20 participants, demonstrating that our input\nmapping framework reduces the workload and improves usability compared to a\nbaseline mapping with similar motion encoding. To prepare for deployment in\nassistive scenarios, we built on the development from the accessible gaming\ncommunity to select an accessible control interface. We then tested the system\nin an exploratory study, where three wheelchair users controlled the robot for\nboth daily living activities and a creative painting task, demonstrating its\nfeasibility for users closer to our target population."
                },
                "authors": [
                    {
                        "name": "Shalutha Rajapakshe"
                    },
                    {
                        "name": "Jean-Marc Odobez"
                    },
                    {
                        "name": "Emmanuel Senft"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Senft"
                },
                "author": "Emmanuel Senft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09560v2",
                "updated": "2025-01-28T13:17:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    17,
                    29,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-12T18:46:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "Foundational Large Language Models for Materials Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models for Materials Research"
                },
                "summary": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems."
                },
                "authors": [
                    {
                        "name": "Vaibhav Mishra"
                    },
                    {
                        "name": "Somaditya Singh"
                    },
                    {
                        "name": "Dhruv Ahlawat"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Vaibhav Bihani"
                    },
                    {
                        "name": "Hargun Singh Grover"
                    },
                    {
                        "name": "Biswajit Mishra"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Mausam"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16917v1",
                "updated": "2025-01-28T13:01:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    1,
                    41,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T13:01:41Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    1,
                    41,
                    1,
                    28,
                    0
                ],
                "title": "B-FPGM: Lightweight Face Detection via Bayesian-Optimized Soft FPGM\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B-FPGM: Lightweight Face Detection via Bayesian-Optimized Soft FPGM\n  Pruning"
                },
                "summary": "Face detection is a computer vision application that increasingly demands\nlightweight models to facilitate deployment on devices with limited\ncomputational resources. Neural network pruning is a promising technique that\ncan effectively reduce network size without significantly affecting\nperformance. In this work, we propose a novel face detection pruning pipeline\nthat leverages Filter Pruning via Geometric Median (FPGM) pruning, Soft Filter\nPruning (SFP) and Bayesian optimization in order to achieve a superior\ntrade-off between size and performance compared to existing approaches. FPGM\npruning is a structured pruning technique that allows pruning the least\nsignificant filters in each layer, while SFP iteratively prunes the filters and\nallows them to be updated in any subsequent training step. Bayesian\noptimization is employed in order to optimize the pruning rates of each layer,\nrather than relying on engineering expertise to determine the optimal pruning\nrates for each layer. In our experiments across all three subsets of the WIDER\nFACE dataset, our proposed approach B-FPGM consistently outperforms existing\nones in balancing model size and performance. All our experiments were applied\nto EResFD, the currently smallest (in number of parameters) well-performing\nface detector of the literature; a small ablation study with a second small\nface detector, EXTD, is also reported. The source code and trained pruned face\ndetection models can be found at: https://github.com/IDTITI/B-FPGM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face detection is a computer vision application that increasingly demands\nlightweight models to facilitate deployment on devices with limited\ncomputational resources. Neural network pruning is a promising technique that\ncan effectively reduce network size without significantly affecting\nperformance. In this work, we propose a novel face detection pruning pipeline\nthat leverages Filter Pruning via Geometric Median (FPGM) pruning, Soft Filter\nPruning (SFP) and Bayesian optimization in order to achieve a superior\ntrade-off between size and performance compared to existing approaches. FPGM\npruning is a structured pruning technique that allows pruning the least\nsignificant filters in each layer, while SFP iteratively prunes the filters and\nallows them to be updated in any subsequent training step. Bayesian\noptimization is employed in order to optimize the pruning rates of each layer,\nrather than relying on engineering expertise to determine the optimal pruning\nrates for each layer. In our experiments across all three subsets of the WIDER\nFACE dataset, our proposed approach B-FPGM consistently outperforms existing\nones in balancing model size and performance. All our experiments were applied\nto EResFD, the currently smallest (in number of parameters) well-performing\nface detector of the literature; a small ablation study with a second small\nface detector, EXTD, is also reported. The source code and trained pruned face\ndetection models can be found at: https://github.com/IDTITI/B-FPGM."
                },
                "authors": [
                    {
                        "name": "Nikolaos Kaparinos"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Mezaris"
                },
                "author": "Vasileios Mezaris",
                "arxiv_comment": "Accepted for publication, RWS Workshop @ IEEE/CVF Winter Conference\n  on Applications of Computer Vision (WACV 2025), Tucson, AZ, USA, Feb. 2025.\n  This is the authors' \"accepted version\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.14409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.14409v3",
                "updated": "2025-01-28T12:59:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    59,
                    17,
                    1,
                    28,
                    0
                ],
                "published": "2023-08-28T08:47:06Z",
                "published_parsed": [
                    2023,
                    8,
                    28,
                    8,
                    47,
                    6,
                    0,
                    240,
                    0
                ],
                "title": "Steerable Conditional Diffusion for Out-of-Distribution Adaptation in\n  Medical Image Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Conditional Diffusion for Out-of-Distribution Adaptation in\n  Medical Image Reconstruction"
                },
                "summary": "Denoising diffusion models have emerged as the go-to generative framework for\nsolving inverse problems in imaging. A critical concern regarding these models\nis their performance on out-of-distribution tasks, which remains an\nunder-explored challenge. Using a diffusion model on an out-of-distribution\ndataset, realistic reconstructions can be generated, but with hallucinating\nimage features that are uniquely present in the training dataset. To address\nthis discrepancy during train-test time and improve reconstruction accuracy, we\nintroduce a novel sampling framework called Steerable Conditional Diffusion.\nSpecifically, this framework adapts the diffusion model, concurrently with\nimage reconstruction, based solely on the information provided by the available\nmeasurement. Utilising our proposed method, we achieve substantial enhancements\nin out-of-distribution performance across diverse imaging modalities, advancing\nthe robust deployment of denoising diffusion models in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as the go-to generative framework for\nsolving inverse problems in imaging. A critical concern regarding these models\nis their performance on out-of-distribution tasks, which remains an\nunder-explored challenge. Using a diffusion model on an out-of-distribution\ndataset, realistic reconstructions can be generated, but with hallucinating\nimage features that are uniquely present in the training dataset. To address\nthis discrepancy during train-test time and improve reconstruction accuracy, we\nintroduce a novel sampling framework called Steerable Conditional Diffusion.\nSpecifically, this framework adapts the diffusion model, concurrently with\nimage reconstruction, based solely on the information provided by the available\nmeasurement. Utilising our proposed method, we achieve substantial enhancements\nin out-of-distribution performance across diverse imaging modalities, advancing\nthe robust deployment of denoising diffusion models in real-world applications."
                },
                "authors": [
                    {
                        "name": "Riccardo Barbano"
                    },
                    {
                        "name": "Alexander Denker"
                    },
                    {
                        "name": "Hyungjin Chung"
                    },
                    {
                        "name": "Tae Hoon Roh"
                    },
                    {
                        "name": "Simon Arridge"
                    },
                    {
                        "name": "Peter Maass"
                    },
                    {
                        "name": "Bangti Jin"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.14409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.14409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16902v1",
                "updated": "2025-01-28T12:40:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    40,
                    37,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    40,
                    37,
                    1,
                    28,
                    0
                ],
                "title": "Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks"
                },
                "summary": "Recent advancements in dense retrieval have introduced vision-language model\n(VLM)-based retrievers, such as DSE and ColPali, which leverage document\nscreenshots embedded as vectors to enable effective search and offer a\nsimplified pipeline over traditional text-only methods. In this study, we\npropose three pixel poisoning attack methods designed to compromise VLM-based\nretrievers and evaluate their effectiveness under various attack settings and\nparameter configurations. Our empirical results demonstrate that injecting even\na single adversarial screenshot into the retrieval corpus can significantly\ndisrupt search results, poisoning the top-10 retrieved documents for 41.9% of\nqueries in the case of DSE and 26.4% for ColPali. These vulnerability rates\nnotably exceed those observed with equivalent attacks on text-only retrievers.\nMoreover, when targeting a small set of known queries, the attack success rate\nraises, achieving complete success in certain cases. By exposing the\nvulnerabilities inherent in vision-language models, this work highlights the\npotential risks associated with their deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in dense retrieval have introduced vision-language model\n(VLM)-based retrievers, such as DSE and ColPali, which leverage document\nscreenshots embedded as vectors to enable effective search and offer a\nsimplified pipeline over traditional text-only methods. In this study, we\npropose three pixel poisoning attack methods designed to compromise VLM-based\nretrievers and evaluate their effectiveness under various attack settings and\nparameter configurations. Our empirical results demonstrate that injecting even\na single adversarial screenshot into the retrieval corpus can significantly\ndisrupt search results, poisoning the top-10 retrieved documents for 41.9% of\nqueries in the case of DSE and 26.4% for ColPali. These vulnerability rates\nnotably exceed those observed with equivalent attacks on text-only retrievers.\nMoreover, when targeting a small set of known queries, the attack success rate\nraises, achieving complete success in certain cases. By exposing the\nvulnerabilities inherent in vision-language models, this work highlights the\npotential risks associated with their deployment."
                },
                "authors": [
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Ekaterina Khramtsova"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16899v1",
                "updated": "2025-01-28T12:35:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    35,
                    6,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:35:06Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    35,
                    6,
                    1,
                    28,
                    0
                ],
                "title": "RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with\n  Enhanced Contextual Awareness in Specific Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with\n  Enhanced Contextual Awareness in Specific Domains"
                },
                "summary": "Large language models (LLMs) represent a significant advancement in\nintegrating physical robots with AI-driven systems. We showcase the\ncapabilities of our framework within the context of the real-world household\ncompetition. This research introduces a framework that utilizes RDMM (Robotics\nDecision-Making Models), which possess the capacity for decision-making within\ndomain-specific contexts, as well as an awareness of their personal knowledge\nand capabilities. The framework leverages information to enhance the autonomous\ndecision-making of the system. In contrast to other approaches, our focus is on\nreal-time, on-device solutions, successfully operating on hardware with as\nlittle as 8GB of memory. Our framework incorporates visual perception models\nequipping robots with understanding of their environment. Additionally, the\nframework has integrated real-time speech recognition capabilities, thus\nenhancing the human-robot interaction experience. Experimental results\ndemonstrate that the RDMM framework can plan with an 93\\% accuracy.\nFurthermore, we introduce a new dataset consisting of 27k planning instances,\nas well as 1.3k text-image annotated samples derived from the competition. The\nframework, benchmarks, datasets, and models developed in this work are publicly\navailable on our GitHub repository at https://github.com/shadynasrat/RDMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a significant advancement in\nintegrating physical robots with AI-driven systems. We showcase the\ncapabilities of our framework within the context of the real-world household\ncompetition. This research introduces a framework that utilizes RDMM (Robotics\nDecision-Making Models), which possess the capacity for decision-making within\ndomain-specific contexts, as well as an awareness of their personal knowledge\nand capabilities. The framework leverages information to enhance the autonomous\ndecision-making of the system. In contrast to other approaches, our focus is on\nreal-time, on-device solutions, successfully operating on hardware with as\nlittle as 8GB of memory. Our framework incorporates visual perception models\nequipping robots with understanding of their environment. Additionally, the\nframework has integrated real-time speech recognition capabilities, thus\nenhancing the human-robot interaction experience. Experimental results\ndemonstrate that the RDMM framework can plan with an 93\\% accuracy.\nFurthermore, we introduce a new dataset consisting of 27k planning instances,\nas well as 1.3k text-image annotated samples derived from the competition. The\nframework, benchmarks, datasets, and models developed in this work are publicly\navailable on our GitHub repository at https://github.com/shadynasrat/RDMM."
                },
                "authors": [
                    {
                        "name": "Shady Nasrat"
                    },
                    {
                        "name": "Myungsu Kim"
                    },
                    {
                        "name": "Seonil Lee"
                    },
                    {
                        "name": "Jiho Lee"
                    },
                    {
                        "name": "Yeoncheol Jang"
                    },
                    {
                        "name": "Seung-joon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Seung-joon Yi"
                },
                "author": "Seung-joon Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20935v2",
                "updated": "2025-01-28T12:26:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    26,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2024-05-31T15:34:13Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    34,
                    13,
                    4,
                    152,
                    0
                ],
                "title": "Effective Interplay between Sparsity and Quantization: From Theory to\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Interplay between Sparsity and Quantization: From Theory to\n  Practice"
                },
                "summary": "The increasing size of deep neural networks (DNNs) necessitates effective\nmodel compression to reduce their computational and memory footprints. Sparsity\nand quantization are two prominent compression methods that have been shown to\nreduce DNNs' computational and memory footprints significantly while preserving\nmodel accuracy. However, how these two methods interact when combined together\nremains a key question for developers, as many tacitly assume that they are\northogonal, meaning that their combined use does not introduce additional\nerrors beyond those introduced by each method independently. In this paper, we\nprovide the first mathematical proof that sparsity and quantization are\nnon-orthogonal. We corroborate these results with experiments spanning a range\nof large language models, including the OPT and LLaMA model families (with 125M\nto 8B parameters), and vision models like ViT and ResNet. We show that the\norder in which we apply these methods matters because applying quantization\nbefore sparsity may disrupt the relative importance of tensor elements, which\nmay inadvertently remove significant elements from a tensor. More importantly,\nwe show that even if applied in the correct order, the compounded errors from\nsparsity and quantization can significantly harm accuracy. Our findings extend\nto the efficient deployment of large models in resource-constrained compute\nplatforms to reduce serving cost, offering insights into best practices for\napplying these compression methods to maximize hardware resource efficiency\nwithout compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of deep neural networks (DNNs) necessitates effective\nmodel compression to reduce their computational and memory footprints. Sparsity\nand quantization are two prominent compression methods that have been shown to\nreduce DNNs' computational and memory footprints significantly while preserving\nmodel accuracy. However, how these two methods interact when combined together\nremains a key question for developers, as many tacitly assume that they are\northogonal, meaning that their combined use does not introduce additional\nerrors beyond those introduced by each method independently. In this paper, we\nprovide the first mathematical proof that sparsity and quantization are\nnon-orthogonal. We corroborate these results with experiments spanning a range\nof large language models, including the OPT and LLaMA model families (with 125M\nto 8B parameters), and vision models like ViT and ResNet. We show that the\norder in which we apply these methods matters because applying quantization\nbefore sparsity may disrupt the relative importance of tensor elements, which\nmay inadvertently remove significant elements from a tensor. More importantly,\nwe show that even if applied in the correct order, the compounded errors from\nsparsity and quantization can significantly harm accuracy. Our findings extend\nto the efficient deployment of large models in resource-constrained compute\nplatforms to reduce serving cost, offering insights into best practices for\napplying these compression methods to maximize hardware resource efficiency\nwithout compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Simla Burcu Harma"
                    },
                    {
                        "name": "Ayan Chakraborty"
                    },
                    {
                        "name": "Elizaveta Kostenok"
                    },
                    {
                        "name": "Danila Mishin"
                    },
                    {
                        "name": "Dongho Ha"
                    },
                    {
                        "name": "Babak Falsafi"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Yunho Oh"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Yazdanbakhsh"
                },
                "author": "Amir Yazdanbakhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16884v1",
                "updated": "2025-01-28T12:13:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    13,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:13:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    13,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irony Detection, Reasoning and Understanding in Zero-shot Learning"
                },
                "summary": "Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs."
                },
                "authors": [
                    {
                        "name": "Peiling Yi"
                    },
                    {
                        "name": "Yuhan Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Xia"
                },
                "author": "Yuhan Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07839v2",
                "updated": "2025-01-28T11:42:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    42,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T11:58:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Semantic Self-Consistency: Enhancing Language Model Reasoning via\n  Semantic Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Self-Consistency: Enhancing Language Model Reasoning via\n  Semantic Weighting"
                },
                "summary": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al.'s self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the semantic information detailed in the step-by-step reasoning\npaths. Our work introduces semantic self-consistency, enhancing this approach\nby incorporating and analyzing both the reasoning paths of these rationales in\naddition to their final decisions before taking a majority vote. These methods\nnot only improve the reliability of reasoning paths but also cause more robust\nperformance on complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al.'s self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the semantic information detailed in the step-by-step reasoning\npaths. Our work introduces semantic self-consistency, enhancing this approach\nby incorporating and analyzing both the reasoning paths of these rationales in\naddition to their final decisions before taking a majority vote. These methods\nnot only improve the reliability of reasoning paths but also cause more robust\nperformance on complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Tim Knappe"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Ayush Chauhan"
                    },
                    {
                        "name": "Kaylee Chhua"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted to MATH-AI at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16865v1",
                "updated": "2025-01-28T11:30:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    30,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T11:30:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    30,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience"
                },
                "summary": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com/Zzoay/JRE-L.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com/Zzoay/JRE-L."
                },
                "authors": [
                    {
                        "name": "Gongyao Jiang"
                    },
                    {
                        "name": "Xinran Shi"
                    },
                    {
                        "name": "Qiong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Qiong Luo"
                },
                "author": "Qiong Luo",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.09756",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16857v1",
                "updated": "2025-01-28T11:11:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T11:11:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    11,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "Comparing Human and LLM Generated Code: The Jury is Still Out!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Human and LLM Generated Code: The Jury is Still Out!"
                },
                "summary": "Much is promised in relation to AI-supported software development. However,\nthere has been limited evaluation effort in the research domain aimed at\nvalidating the true utility of such techniques, especially when compared to\nhuman coding outputs. We bridge this gap, where a benchmark dataset comprising\n72 distinct software engineering tasks is used to compare the effectiveness of\nlarge language models (LLMs) and human programmers in producing Python software\ncode. GPT-4 is used as a representative LLM, where for the code generated by\nhumans and this LLM, we evaluate code quality and adherence to Python coding\nstandards, code security and vulnerabilities, code complexity and functional\ncorrectness. We use various static analysis benchmarks, including Pylint,\nRadon, Bandit and test cases. Among the notable outcomes, results show that\nhuman-generated code recorded higher ratings for adhering to coding standards\nthan GPT-4. We observe security flaws in code generated by both humans and\nGPT-4, however, code generated by humans shows a greater variety of problems,\nbut GPT-4 code included more severe outliers. Our results show that although\nGPT-4 is capable of producing coding solutions, it frequently produces more\ncomplex code that may need more reworking to ensure maintainability. On the\ncontrary however, our outcomes show that a higher number of test cases passed\nfor code generated by GPT-4 across a range of tasks than code that was\ngenerated by humans. That said, GPT-4 frequently struggles with complex\nproblem-solving that involve in-depth domain knowledge. This study highlights\nthe potential utility of LLMs for supporting software development, however,\ntasks requiring comprehensive, innovative or unconventional solutions, and\ncareful debugging and error correction seem to be better developed by human\nprogrammers. We plot an agenda for the software engineering community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much is promised in relation to AI-supported software development. However,\nthere has been limited evaluation effort in the research domain aimed at\nvalidating the true utility of such techniques, especially when compared to\nhuman coding outputs. We bridge this gap, where a benchmark dataset comprising\n72 distinct software engineering tasks is used to compare the effectiveness of\nlarge language models (LLMs) and human programmers in producing Python software\ncode. GPT-4 is used as a representative LLM, where for the code generated by\nhumans and this LLM, we evaluate code quality and adherence to Python coding\nstandards, code security and vulnerabilities, code complexity and functional\ncorrectness. We use various static analysis benchmarks, including Pylint,\nRadon, Bandit and test cases. Among the notable outcomes, results show that\nhuman-generated code recorded higher ratings for adhering to coding standards\nthan GPT-4. We observe security flaws in code generated by both humans and\nGPT-4, however, code generated by humans shows a greater variety of problems,\nbut GPT-4 code included more severe outliers. Our results show that although\nGPT-4 is capable of producing coding solutions, it frequently produces more\ncomplex code that may need more reworking to ensure maintainability. On the\ncontrary however, our outcomes show that a higher number of test cases passed\nfor code generated by GPT-4 across a range of tasks than code that was\ngenerated by humans. That said, GPT-4 frequently struggles with complex\nproblem-solving that involve in-depth domain knowledge. This study highlights\nthe potential utility of LLMs for supporting software development, however,\ntasks requiring comprehensive, innovative or unconventional solutions, and\ncareful debugging and error correction seem to be better developed by human\nprogrammers. We plot an agenda for the software engineering community."
                },
                "authors": [
                    {
                        "name": "Sherlock A. Licorish"
                    },
                    {
                        "name": "Ansh Bajpai"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Fanyu Wang"
                    },
                    {
                        "name": "Kla Tantithamthavorn"
                    }
                ],
                "author_detail": {
                    "name": "Kla Tantithamthavorn"
                },
                "author": "Kla Tantithamthavorn",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; D.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v3",
                "updated": "2025-01-28T11:05:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    5,
                    18,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16842v1",
                "updated": "2025-01-28T10:33:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    33,
                    1,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T10:33:01Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    33,
                    1,
                    1,
                    28,
                    0
                ],
                "title": "Adapting Network Information to Semantics for Generalizable and\n  Plug-and-Play Multi-Scenario Network Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Network Information to Semantics for Generalizable and\n  Plug-and-Play Multi-Scenario Network Diagnosis"
                },
                "summary": "Network fault diagnosis is a core challenge in ensuring the stability and\nreliability of modern network operations. Traditional approaches, limited by\ntheir training on specific performance metrics for predefined scenarios,\nstruggle to generalize across diverse faults and anomalies in varying network\nenvironments. In recent years, large language models (LLMs) have demonstrated\nstrong generalization capabilities across various domains. Building on this\nsuccess, we propose NetSemantic, a plug-and-play intelligent network fault\ndiagnosis framework based on LLMs. NetSemantic transforms multimodal network\ninformation into unified textual representations, enabling LLMs to perform\nreasoning and generate efficient fault resolutions and health assessment\nreports. To further enhance the logical reasoning capabilities of LLMs, we\nintroduce a novel symbolic representation method that transforms logically\nstrong network information into symbols. Additionally, we propose a\nself-adaptive data updating mechanism that dynamically incorporates network\ninformation into a knowledge graph to ensure the validity and timeliness of the\nknowledge base. Experimental results demonstrate that NetSemantic excels in\nnetwork fault diagnosis across various complex scenarios, significantly\nimproving diagnostic accuracy and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network fault diagnosis is a core challenge in ensuring the stability and\nreliability of modern network operations. Traditional approaches, limited by\ntheir training on specific performance metrics for predefined scenarios,\nstruggle to generalize across diverse faults and anomalies in varying network\nenvironments. In recent years, large language models (LLMs) have demonstrated\nstrong generalization capabilities across various domains. Building on this\nsuccess, we propose NetSemantic, a plug-and-play intelligent network fault\ndiagnosis framework based on LLMs. NetSemantic transforms multimodal network\ninformation into unified textual representations, enabling LLMs to perform\nreasoning and generate efficient fault resolutions and health assessment\nreports. To further enhance the logical reasoning capabilities of LLMs, we\nintroduce a novel symbolic representation method that transforms logically\nstrong network information into symbols. Additionally, we propose a\nself-adaptive data updating mechanism that dynamically incorporates network\ninformation into a knowledge graph to ensure the validity and timeliness of the\nknowledge base. Experimental results demonstrate that NetSemantic excels in\nnetwork fault diagnosis across various complex scenarios, significantly\nimproving diagnostic accuracy and reliability."
                },
                "authors": [
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Ming Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhao"
                },
                "author": "Ming Zhao",
                "arxiv_comment": "10 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16220v2",
                "updated": "2025-01-28T10:16:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    10,
                    16,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T17:09:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "DBRouting: Routing End User Queries to Databases for Answerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBRouting: Routing End User Queries to Databases for Answerability"
                },
                "summary": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
                },
                "authors": [
                    {
                        "name": "Priyangshu Mandal"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "arxiv_comment": "Accepted at 1st Workshop on GenAI and RAG Systems for Enterprise at\n  CIKM 2024 Conference. 10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16751v2",
                "updated": "2025-01-28T09:45:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    45,
                    41,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-25T09:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL"
                },
                "summary": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set."
                },
                "authors": [
                    {
                        "name": "Hasan Alp Caferoğlu"
                    },
                    {
                        "name": "Özgür Ulusoy"
                    }
                ],
                "author_detail": {
                    "name": "Özgür Ulusoy"
                },
                "author": "Özgür Ulusoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07739v3",
                "updated": "2025-01-28T09:26:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    26,
                    27,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T09:16:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    16,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture"
                },
                "summary": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Weibo Zheng"
                    },
                    {
                        "name": "Donghong Han"
                    }
                ],
                "author_detail": {
                    "name": "Donghong Han"
                },
                "author": "Donghong Han",
                "arxiv_comment": "13 pages, 7 figures, 4 tables; Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v7",
                "updated": "2025-01-28T09:15:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    15,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17840v2",
                "updated": "2025-01-28T09:09:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    9,
                    9,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-23T13:05:46Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs"
                },
                "summary": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces."
                },
                "authors": [
                    {
                        "name": "Ferdi Kossmann"
                    },
                    {
                        "name": "Bruce Fontaine"
                    },
                    {
                        "name": "Daya Khudia"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Samuel Madden"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Madden"
                },
                "author": "Samuel Madden",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16786v1",
                "updated": "2025-01-28T08:30:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    30,
                    58,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:30:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    30,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large\n  Language Models for Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large\n  Language Models for Video Understanding"
                },
                "summary": "Applying Multimodal Large Language Models (MLLMs) to video understanding\npresents significant challenges due to the need to model temporal relations\nacross frames. Existing approaches adopt either implicit temporal modeling,\nrelying solely on the LLM decoder, or explicit temporal modeling, employing\nauxiliary temporal encoders. To investigate this debate between the two\nparadigms, we propose the Stackable Temporal Encoder (STE). STE enables\nflexible explicit temporal modeling with adjustable temporal receptive fields\nand token compression ratios. Using STE, we systematically compare implicit and\nexplicit temporal modeling across dimensions such as overall performance, token\ncompression effectiveness, and temporal-specific understanding. We also explore\nSTE's design considerations and broader impacts as a plug-in module and in\nimage modalities. Our findings emphasize the critical role of explicit temporal\nmodeling, providing actionable insights to advance video MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Multimodal Large Language Models (MLLMs) to video understanding\npresents significant challenges due to the need to model temporal relations\nacross frames. Existing approaches adopt either implicit temporal modeling,\nrelying solely on the LLM decoder, or explicit temporal modeling, employing\nauxiliary temporal encoders. To investigate this debate between the two\nparadigms, we propose the Stackable Temporal Encoder (STE). STE enables\nflexible explicit temporal modeling with adjustable temporal receptive fields\nand token compression ratios. Using STE, we systematically compare implicit and\nexplicit temporal modeling across dimensions such as overall performance, token\ncompression effectiveness, and temporal-specific understanding. We also explore\nSTE's design considerations and broader impacts as a plug-in module and in\nimage modalities. Our findings emphasize the critical role of explicit temporal\nmodeling, providing actionable insights to advance video MLLMs."
                },
                "authors": [
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yajing Kong"
                    },
                    {
                        "name": "Guangrui Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Chao Bian"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Zhenbang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenbang Sun"
                },
                "author": "Zhenbang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16784v1",
                "updated": "2025-01-28T08:13:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    13,
                    2,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:13:02Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    13,
                    2,
                    1,
                    28,
                    0
                ],
                "title": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT\n  Devices Concealed within the Tor Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT\n  Devices Concealed within the Tor Network"
                },
                "summary": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views."
                },
                "authors": [
                    {
                        "name": "Yumingzhi Pan"
                    },
                    {
                        "name": "Zhen Ling"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Hongze Wang"
                    },
                    {
                        "name": "Guangchi Liu"
                    },
                    {
                        "name": "Junzhou Luo"
                    },
                    {
                        "name": "Xinwen Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwen Fu"
                },
                "author": "Xinwen Fu",
                "arxiv_comment": "27 pages, 14 figure, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16783v1",
                "updated": "2025-01-28T08:08:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    8,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T08:08:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    8,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling\n  Severity Drift as a Critical Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling\n  Severity Drift as a Critical Process"
                },
                "summary": "This paper introduces a continuous-time stochastic dynamical framework for\nunderstanding how large language models (LLMs) may self-amplify latent biases\nor toxicity through their own chain-of-thought reasoning. The model posits an\ninstantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic\ndifferential equation (SDE) with a drift term $\\mu(x)$ and diffusion\n$\\sigma(x)$. Crucially, such a process can be consistently analyzed via the\nFokker--Planck approach if each incremental step behaves nearly Markovian in\nseverity space. The analysis investigates critical phenomena, showing that\ncertain parameter regimes create phase transitions from subcritical\n(self-correcting) to supercritical (runaway severity). The paper derives\nstationary distributions, first-passage times to harmful thresholds, and\nscaling laws near critical points. Finally, it highlights implications for\nagents and extended LLM reasoning models: in principle, these equations might\nserve as a basis for formal verification of whether a model remains stable or\npropagates bias over repeated inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a continuous-time stochastic dynamical framework for\nunderstanding how large language models (LLMs) may self-amplify latent biases\nor toxicity through their own chain-of-thought reasoning. The model posits an\ninstantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic\ndifferential equation (SDE) with a drift term $\\mu(x)$ and diffusion\n$\\sigma(x)$. Crucially, such a process can be consistently analyzed via the\nFokker--Planck approach if each incremental step behaves nearly Markovian in\nseverity space. The analysis investigates critical phenomena, showing that\ncertain parameter regimes create phase transitions from subcritical\n(self-correcting) to supercritical (runaway severity). The paper derives\nstationary distributions, first-passage times to harmful thresholds, and\nscaling laws near critical points. Finally, it highlights implications for\nagents and extended LLM reasoning models: in principle, these equations might\nserve as a basis for formal verification of whether a model remains stable or\npropagates bias over repeated inferences."
                },
                "authors": [
                    {
                        "name": "Jack David Carson"
                    }
                ],
                "author_detail": {
                    "name": "Jack David Carson"
                },
                "author": "Jack David Carson",
                "arxiv_comment": "Experimental verification and more formal argument for Markov\n  approximation of bias propagation to be released soon. Primarily pushed now\n  to establish novelty and ease of sharing. Please do not cite this work until\n  the forthcoming experimental validation and updated mathematical model are\n  provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15691v2",
                "updated": "2025-01-28T07:59:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    59,
                    26,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-26T22:38:04Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    22,
                    38,
                    4,
                    6,
                    26,
                    0
                ],
                "title": "An Empirical Study on Decision-Making Aspects in Responsible Software\n  Engineering for AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Decision-Making Aspects in Responsible Software\n  Engineering for AI"
                },
                "summary": "Incorporating responsible practices into software engineering (SE) for AI is\nessential to ensure ethical principles, societal impact, and accountability\nremain at the forefront of AI system design and deployment. This study\ninvestigates the ethical challenges and complexities inherent in responsible\nsoftware engineering (RSE) for AI, underscoring the need for\npractical,scenario-driven operational guidelines. Given the complexity of AI\nand the relative inexperience of professionals in this rapidly evolving field,\ncontinuous learning and market adaptation are crucial. Through qualitative\ninterviews with seven practitioners(conducted until saturation), quantitative\nsurveys of 51 practitioners, and static validation of results with four\nindustry experts in AI, this study explores how personal values, emerging\nroles, and awareness of AIs societal impact influence responsible\ndecision-making in RSE for AI. A key finding is the gap between the current\nstate of the art and actual practice in RSE for AI, particularly in the failure\nto operationalize ethical and responsible decision-making within the software\nengineering life cycle for AI. While ethical issues in RSE for AI largely\nmirror those found in broader SE process, the study highlights a distinct lack\nof operational frameworks and resources to guide RSE practices for AI\neffectively. The results reveal that current ethical guidelines are\ninsufficiently implemented at the operational level, reinforcing the complexity\nof embedding ethics throughout the software engineering life cycle. The study\nconcludes that interdisciplinary collaboration, H-shaped\ncompetencies(Ethical-Technical dual competence), and a strong organizational\nculture of ethics are critical for fostering RSE practices for AI, with a\nparticular focus on transparency and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating responsible practices into software engineering (SE) for AI is\nessential to ensure ethical principles, societal impact, and accountability\nremain at the forefront of AI system design and deployment. This study\ninvestigates the ethical challenges and complexities inherent in responsible\nsoftware engineering (RSE) for AI, underscoring the need for\npractical,scenario-driven operational guidelines. Given the complexity of AI\nand the relative inexperience of professionals in this rapidly evolving field,\ncontinuous learning and market adaptation are crucial. Through qualitative\ninterviews with seven practitioners(conducted until saturation), quantitative\nsurveys of 51 practitioners, and static validation of results with four\nindustry experts in AI, this study explores how personal values, emerging\nroles, and awareness of AIs societal impact influence responsible\ndecision-making in RSE for AI. A key finding is the gap between the current\nstate of the art and actual practice in RSE for AI, particularly in the failure\nto operationalize ethical and responsible decision-making within the software\nengineering life cycle for AI. While ethical issues in RSE for AI largely\nmirror those found in broader SE process, the study highlights a distinct lack\nof operational frameworks and resources to guide RSE practices for AI\neffectively. The results reveal that current ethical guidelines are\ninsufficiently implemented at the operational level, reinforcing the complexity\nof embedding ethics throughout the software engineering life cycle. The study\nconcludes that interdisciplinary collaboration, H-shaped\ncompetencies(Ethical-Technical dual competence), and a strong organizational\nculture of ethics are critical for fostering RSE practices for AI, with a\nparticular focus on transparency and accountability."
                },
                "authors": [
                    {
                        "name": "Lekshmi Murali Rani"
                    },
                    {
                        "name": "Faezeh Mohammadi"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Richard Berntsson Svensson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Berntsson Svensson"
                },
                "author": "Richard Berntsson Svensson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00989v3",
                "updated": "2025-01-28T07:45:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    45,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-02T03:25:20Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    3,
                    25,
                    20,
                    4,
                    215,
                    0
                ],
                "title": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty\n  Agents"
                },
                "summary": "Large language model-based multi-agent systems have shown great abilities\nacross various tasks due to the collaboration of expert agents, each focusing\non a specific domain. However, the impact of clumsy or even malicious agents,\ni.e., those who frequently make errors in their tasks, on the overall\nperformance of the system remains underexplored. This paper investigates: (1)\nWhat is the resilience of various system structures (e.g.,\nA$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under\nfaulty agents, on different downstream tasks? (2) How can we increase system\nresilience to defend against these agents? To simulate faulty agents, we\npropose two approaches, AutoTransform and AutoInject, which introduce mistakes\ninto the agents' responses. We select four downstream tasks, including code\ngeneration, math problems, translation, and text evaluation. Results suggest\nthat the hierarchical structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C),\nexhibits superior resilience with the lowest performance drop of $9.2\\%$,\ncompared to $26.0\\%$ and $31.2\\%$ of other two structures. Additionally, we\nimprove the system resilience with two methods, introducing a mechanism for\neach agent to challenge others' outputs, and an additional agent to review and\ncorrect messages. Our code and data are available at\nhttps://github.com/CUHK-ARISE/MAS-Resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based multi-agent systems have shown great abilities\nacross various tasks due to the collaboration of expert agents, each focusing\non a specific domain. However, the impact of clumsy or even malicious agents,\ni.e., those who frequently make errors in their tasks, on the overall\nperformance of the system remains underexplored. This paper investigates: (1)\nWhat is the resilience of various system structures (e.g.,\nA$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under\nfaulty agents, on different downstream tasks? (2) How can we increase system\nresilience to defend against these agents? To simulate faulty agents, we\npropose two approaches, AutoTransform and AutoInject, which introduce mistakes\ninto the agents' responses. We select four downstream tasks, including code\ngeneration, math problems, translation, and text evaluation. Results suggest\nthat the hierarchical structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C),\nexhibits superior resilience with the lowest performance drop of $9.2\\%$,\ncompared to $26.0\\%$ and $31.2\\%$ of other two structures. Additionally, we\nimprove the system resilience with two methods, introducing a mechanism for\neach agent to challenge others' outputs, and an additional agent to review and\ncorrect messages. Our code and data are available at\nhttps://github.com/CUHK-ARISE/MAS-Resilience."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Jiaxu Zhou"
                    },
                    {
                        "name": "Tailin Jin"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "9 pages of main text; 11 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19272v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19272v4",
                "updated": "2025-01-28T07:36:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    36,
                    15,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-28T07:13:33Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    7,
                    13,
                    33,
                    5,
                    272,
                    0
                ],
                "title": "Perception Compressor: A Training-Free Prompt Compression Framework in\n  Long Context Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Compressor: A Training-Free Prompt Compression Framework in\n  Long Context Scenarios"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information in long context scenarios. To\naddress these challenges, we present Perception Compressor, a training-free\nprompt compression framework. It includes a perception retriever that leverages\nguiding questions and instruction to retrieve the most relevant demonstrations,\na dual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information in long context scenarios. To\naddress these challenges, we present Perception Compressor, a training-free\nprompt compression framework. It includes a perception retriever that leverages\nguiding questions and instruction to retrieve the most relevant demonstrations,\na dual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Lin Hai"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "arxiv_comment": "Accepted at NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19272v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16750v1",
                "updated": "2025-01-28T07:00:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    0,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T07:00:45Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    7,
                    0,
                    45,
                    1,
                    28,
                    0
                ],
                "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content\n  and Hate Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content\n  and Hate Campaigns"
                },
                "summary": "Large Language Models (LLMs) have raised increasing concerns about their\nmisuse in generating hate speech. Among all the efforts to address this issue,\nhate speech detectors play a crucial role. However, the effectiveness of\ndifferent detectors against LLM-generated hate speech remains largely unknown.\nIn this paper, we propose HateBench, a framework for benchmarking hate speech\ndetectors on LLM-generated hate speech. We first construct a hate speech\ndataset of 7,838 samples generated by six widely-used LLMs covering 34 identity\ngroups, with meticulous annotations by three labelers. We then assess the\neffectiveness of eight representative hate speech detectors on the\nLLM-generated dataset. Our results show that while detectors are generally\neffective in identifying LLM-generated hate speech, their performance degrades\nwith newer versions of LLMs. We also reveal the potential of LLM-driven hate\ncampaigns, a new threat that LLMs bring to the field of hate speech detection.\nBy leveraging advanced techniques like adversarial attacks and model stealing\nattacks, the adversary can intentionally evade the detector and automate hate\ncampaigns online. The most potent adversarial attack achieves an attack success\nrate of 0.966, and its attack efficiency can be further improved by\n$13-21\\times$ through model stealing attacks with acceptable attack\nperformance. We hope our study can serve as a call to action for the research\ncommunity and platform moderators to fortify defenses against these emerging\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have raised increasing concerns about their\nmisuse in generating hate speech. Among all the efforts to address this issue,\nhate speech detectors play a crucial role. However, the effectiveness of\ndifferent detectors against LLM-generated hate speech remains largely unknown.\nIn this paper, we propose HateBench, a framework for benchmarking hate speech\ndetectors on LLM-generated hate speech. We first construct a hate speech\ndataset of 7,838 samples generated by six widely-used LLMs covering 34 identity\ngroups, with meticulous annotations by three labelers. We then assess the\neffectiveness of eight representative hate speech detectors on the\nLLM-generated dataset. Our results show that while detectors are generally\neffective in identifying LLM-generated hate speech, their performance degrades\nwith newer versions of LLMs. We also reveal the potential of LLM-driven hate\ncampaigns, a new threat that LLMs bring to the field of hate speech detection.\nBy leveraging advanced techniques like adversarial attacks and model stealing\nattacks, the adversary can intentionally evade the detector and automate hate\ncampaigns online. The most potent adversarial attack achieves an attack success\nrate of 0.966, and its attack efficiency can be further improved by\n$13-21\\times$ through model stealing attacks with acceptable attack\nperformance. We hope our study can serve as a call to action for the research\ncommunity and platform moderators to fortify defenses against these emerging\nthreats."
                },
                "authors": [
                    {
                        "name": "Xinyue Shen"
                    },
                    {
                        "name": "Yixin Wu"
                    },
                    {
                        "name": "Yiting Qu"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Savvas Zannettou"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16748v1",
                "updated": "2025-01-28T06:58:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:58:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03168v3",
                "updated": "2025-01-28T06:55:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    55,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-04T06:01:27Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    1,
                    27,
                    4,
                    278,
                    0
                ],
                "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?"
                },
                "summary": "Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Sheng Guan"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "28 pages, 5 figures, 11 tables Published as a conference paper at\n  ICLR 2025 Github link:\n  https://github.com/THU-BPM/Watermarked_LLM_Identification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16744v1",
                "updated": "2025-01-28T06:41:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    41,
                    37,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:41:37Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    41,
                    37,
                    1,
                    28,
                    0
                ],
                "title": "LLM Assisted Anomaly Detection Service for Site Reliability Engineers:\n  Enhancing Cloud Infrastructure Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assisted Anomaly Detection Service for Site Reliability Engineers:\n  Enhancing Cloud Infrastructure Resilience"
                },
                "summary": "This paper introduces a scalable Anomaly Detection Service with a\ngeneralizable API tailored for industrial time-series data, designed to assist\nSite Reliability Engineers (SREs) in managing cloud infrastructure. The service\nenables efficient anomaly detection in complex data streams, supporting\nproactive identification and resolution of issues. Furthermore, it presents an\ninnovative approach to anomaly modeling in cloud infrastructure by utilizing\nLarge Language Models (LLMs) to understand key components, their failure modes,\nand behaviors. A suite of algorithms for detecting anomalies is offered in\nunivariate and multivariate time series data, including regression-based,\nmixture-model-based, and semi-supervised approaches. We provide insights into\nthe usage patterns of the service, with over 500 users and 200,000 API calls in\na year. The service has been successfully applied in various industrial\nsettings, including IoT-based AI applications. We have also evaluated our\nsystem on public anomaly benchmarks to show its effectiveness. By leveraging\nit, SREs can proactively identify potential issues before they escalate,\nreducing downtime and improving response times to incidents, ultimately\nenhancing the overall customer experience. We plan to extend the system to\ninclude time series foundation models, enabling zero-shot anomaly detection\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a scalable Anomaly Detection Service with a\ngeneralizable API tailored for industrial time-series data, designed to assist\nSite Reliability Engineers (SREs) in managing cloud infrastructure. The service\nenables efficient anomaly detection in complex data streams, supporting\nproactive identification and resolution of issues. Furthermore, it presents an\ninnovative approach to anomaly modeling in cloud infrastructure by utilizing\nLarge Language Models (LLMs) to understand key components, their failure modes,\nand behaviors. A suite of algorithms for detecting anomalies is offered in\nunivariate and multivariate time series data, including regression-based,\nmixture-model-based, and semi-supervised approaches. We provide insights into\nthe usage patterns of the service, with over 500 users and 200,000 API calls in\na year. The service has been successfully applied in various industrial\nsettings, including IoT-based AI applications. We have also evaluated our\nsystem on public anomaly benchmarks to show its effectiveness. By leveraging\nit, SREs can proactively identify potential issues before they escalate,\nreducing downtime and improving response times to incidents, ultimately\nenhancing the overall customer experience. We plan to extend the system to\ninclude time series foundation models, enabling zero-shot anomaly detection\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Nimesh Jha"
                    },
                    {
                        "name": "Shuxin Lin"
                    },
                    {
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "name": "Kyle Frohling"
                    },
                    {
                        "name": "Christodoulos Constantinides"
                    },
                    {
                        "name": "Dhaval Patel"
                    }
                ],
                "author_detail": {
                    "name": "Dhaval Patel"
                },
                "author": "Dhaval Patel",
                "arxiv_comment": "Accepted at the AAAI-2025 Deployable AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15453v2",
                "updated": "2025-01-28T06:35:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    35,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-26T08:49:46Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    8,
                    49,
                    46,
                    6,
                    26,
                    0
                ],
                "title": "Data-adaptive Safety Rules for Training Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-adaptive Safety Rules for Training Reward Models"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is commonly employed to\ntailor models to human preferences, especially to improve the safety of outputs\nfrom large language models (LLMs). Traditionally, this method depends on\nselecting preferred responses from pairs. However, due to the variability in\nhuman opinions and the challenges in directly comparing two responses, there is\nan increasing trend towards fine-grained annotation approaches that evaluate\nresponses using multiple targeted metrics or rules. The challenge lies in\nefficiently choosing and applying these rules to handle the diverse range of\npreference data. In this paper, we propose a dynamic method that adaptively\nselects the most important rules for each response pair. We introduce a\nmathematical framework that utilizes the maximum discrepancy across paired\nresponses and demonstrate theoretically that this approach maximizes the mutual\ninformation between the rule-based annotations and the underlying true\npreferences. We then train an 8B reward model using this adaptively labeled\npreference dataset and assess its efficacy using RewardBench. As of January 25,\n2025, our model achieved the highest safety performance on the leaderboard,\nsurpassing various larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is commonly employed to\ntailor models to human preferences, especially to improve the safety of outputs\nfrom large language models (LLMs). Traditionally, this method depends on\nselecting preferred responses from pairs. However, due to the variability in\nhuman opinions and the challenges in directly comparing two responses, there is\nan increasing trend towards fine-grained annotation approaches that evaluate\nresponses using multiple targeted metrics or rules. The challenge lies in\nefficiently choosing and applying these rules to handle the diverse range of\npreference data. In this paper, we propose a dynamic method that adaptively\nselects the most important rules for each response pair. We introduce a\nmathematical framework that utilizes the maximum discrepancy across paired\nresponses and demonstrate theoretically that this approach maximizes the mutual\ninformation between the rule-based annotations and the underlying true\npreferences. We then train an 8B reward model using this adaptively labeled\npreference dataset and assess its efficacy using RewardBench. As of January 25,\n2025, our model achieved the highest safety performance on the leaderboard,\nsurpassing various larger models."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Weiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiyu Li"
                },
                "author": "Weiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16740v1",
                "updated": "2025-01-28T06:33:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    33,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:33:30Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    33,
                    30,
                    1,
                    28,
                    0
                ],
                "title": "Efficient Knowledge Distillation of SAM for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Knowledge Distillation of SAM for Medical Image Segmentation"
                },
                "summary": "The Segment Anything Model (SAM) has set a new standard in interactive image\nsegmentation, offering robust performance across various tasks. However, its\nsignificant computational requirements limit its deployment in real-time or\nresource-constrained environments. To address these challenges, we propose a\nnovel knowledge distillation approach, KD SAM, which incorporates both encoder\nand decoder optimization through a combination of Mean Squared Error (MSE) and\nPerceptual Loss. This dual-loss framework captures structural and semantic\nfeatures, enabling the student model to maintain high segmentation accuracy\nwhile reducing computational complexity. Based on the model evaluation on\ndatasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast\nUltrasound, we demonstrate that KD SAM achieves comparable or superior\nperformance to the baseline models, with significantly fewer parameters. KD SAM\neffectively balances segmentation accuracy and computational efficiency, making\nit well-suited for real-time medical image segmentation applications in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has set a new standard in interactive image\nsegmentation, offering robust performance across various tasks. However, its\nsignificant computational requirements limit its deployment in real-time or\nresource-constrained environments. To address these challenges, we propose a\nnovel knowledge distillation approach, KD SAM, which incorporates both encoder\nand decoder optimization through a combination of Mean Squared Error (MSE) and\nPerceptual Loss. This dual-loss framework captures structural and semantic\nfeatures, enabling the student model to maintain high segmentation accuracy\nwhile reducing computational complexity. Based on the model evaluation on\ndatasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast\nUltrasound, we demonstrate that KD SAM achieves comparable or superior\nperformance to the baseline models, with significantly fewer parameters. KD SAM\neffectively balances segmentation accuracy and computational efficiency, making\nit well-suited for real-time medical image segmentation applications in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Kunal Dasharath Patil"
                    },
                    {
                        "name": "Gowthamaan Palani"
                    },
                    {
                        "name": "Ganapathy Krishnamurthi"
                    }
                ],
                "author_detail": {
                    "name": "Ganapathy Krishnamurthi"
                },
                "author": "Ganapathy Krishnamurthi",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16734v1",
                "updated": "2025-01-28T06:19:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:19:29Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "title": "Distilling Large Language Models for Network Active Queue Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Large Language Models for Network Active Queue Management"
                },
                "summary": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems."
                },
                "authors": [
                    {
                        "name": "Deol Satish"
                    },
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Jonathan Kua"
                    },
                    {
                        "name": "Anwar Walid"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Walid"
                },
                "author": "Anwar Walid",
                "arxiv_comment": "11 pages",
                "arxiv_journal_ref": "IEEE Trans on Networking, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16727v1",
                "updated": "2025-01-28T06:07:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking"
                },
                "summary": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Shuaimin Li"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16726v1",
                "updated": "2025-01-28T06:07:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    39,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T06:07:39Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    39,
                    1,
                    28,
                    0
                ],
                "title": "Bridging Neural Networks and Wireless Systems with MIMO-OFDM Semantic\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Neural Networks and Wireless Systems with MIMO-OFDM Semantic\n  Communications"
                },
                "summary": "Semantic communications aim to enhance transmission efficiency by jointly\noptimizing source coding, channel coding, and modulation. While prior research\nhas demonstrated promising performance in simulations, real-world\nimplementations often face significant challenges, including noise variability\nand nonlinear distortions, leading to performance gaps. This article\ninvestigates these challenges in a multiple-input multiple-output (MIMO) and\northogonal frequency division multiplexing (OFDM)-based semantic communication\nsystem, focusing on the practical impacts of power amplifier (PA) nonlinearity\nand peak-to-average power ratio (PAPR) variations. Our analysis identifies\nfrequency selectivity of the actual channel as a critical factor in performance\ndegradation and demonstrates that targeted mitigation strategies can enable\nsemantic systems to approach theoretical performance. By addressing key\nlimitations in existing designs, we provide actionable insights for advancing\nsemantic communications in practical wireless environments. This work\nestablishes a foundation for bridging the gap between theoretical models and\nreal-world deployment, highlighting essential considerations for system design\nand optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communications aim to enhance transmission efficiency by jointly\noptimizing source coding, channel coding, and modulation. While prior research\nhas demonstrated promising performance in simulations, real-world\nimplementations often face significant challenges, including noise variability\nand nonlinear distortions, leading to performance gaps. This article\ninvestigates these challenges in a multiple-input multiple-output (MIMO) and\northogonal frequency division multiplexing (OFDM)-based semantic communication\nsystem, focusing on the practical impacts of power amplifier (PA) nonlinearity\nand peak-to-average power ratio (PAPR) variations. Our analysis identifies\nfrequency selectivity of the actual channel as a critical factor in performance\ndegradation and demonstrates that targeted mitigation strategies can enable\nsemantic systems to approach theoretical performance. By addressing key\nlimitations in existing designs, we provide actionable insights for advancing\nsemantic communications in practical wireless environments. This work\nestablishes a foundation for bridging the gap between theoretical models and\nreal-world deployment, highlighting essential considerations for system design\nand optimization."
                },
                "authors": [
                    {
                        "name": "Hanju Yoo"
                    },
                    {
                        "name": "Dongha Choi"
                    },
                    {
                        "name": "Yonghwi Kim"
                    },
                    {
                        "name": "Yoontae Kim"
                    },
                    {
                        "name": "Songkuk Kim"
                    },
                    {
                        "name": "Chan-Byoung Chae"
                    },
                    {
                        "name": "Robert W. Heath Jr"
                    }
                ],
                "author_detail": {
                    "name": "Robert W. Heath Jr"
                },
                "author": "Robert W. Heath Jr",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14848v2",
                "updated": "2025-01-28T06:00:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    0,
                    44,
                    1,
                    28,
                    0
                ],
                "published": "2024-06-21T03:33:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    3,
                    33,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models"
                },
                "summary": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "arxiv_comment": "Accepted by WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16711v1",
                "updated": "2025-01-28T05:19:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    5,
                    19,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T05:19:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    5,
                    19,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "Bayesian Analyses of Structural Vector Autoregressions with Sign, Zero,\n  and Narrative Restrictions Using the R Package bsvarSIGNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Analyses of Structural Vector Autoregressions with Sign, Zero,\n  and Narrative Restrictions Using the R Package bsvarSIGNs"
                },
                "summary": "The R package bsvarSIGNs implements state-of-the-art algorithms for the\nBayesian analysis of Structural Vector Autoregressions identified by sign,\nzero, and narrative restrictions. It offers fast and efficient estimation\nthanks to the deployment of frontier econometric and numerical techniques and\nalgorithms written in C++. The core model is based on a flexible Vector\nAutoregression with estimated hyper-parameters of the Minnesota prior and the\ndummy observation priors. The structural model can be identified by sign, zero,\nand narrative restrictions, including a novel solution, making it possible to\nuse the three types of restrictions at once. The package facilitates predictive\nand structural analyses using impulse responses, forecast error variance and\nhistorical decompositions, forecasting and conditional forecasting, as well as\nanalyses of structural shocks and fitted values. All this is complemented by\ncolourful plots, user-friendly summary functions, and comprehensive\ndocumentation. The package was granted the Di Cook Open-Source Statistical\nSoftware Award by the Statistical Society of Australia in 2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The R package bsvarSIGNs implements state-of-the-art algorithms for the\nBayesian analysis of Structural Vector Autoregressions identified by sign,\nzero, and narrative restrictions. It offers fast and efficient estimation\nthanks to the deployment of frontier econometric and numerical techniques and\nalgorithms written in C++. The core model is based on a flexible Vector\nAutoregression with estimated hyper-parameters of the Minnesota prior and the\ndummy observation priors. The structural model can be identified by sign, zero,\nand narrative restrictions, including a novel solution, making it possible to\nuse the three types of restrictions at once. The package facilitates predictive\nand structural analyses using impulse responses, forecast error variance and\nhistorical decompositions, forecasting and conditional forecasting, as well as\nanalyses of structural shocks and fitted values. All this is complemented by\ncolourful plots, user-friendly summary functions, and comprehensive\ndocumentation. The package was granted the Di Cook Open-Source Statistical\nSoftware Award by the Statistical Society of Australia in 2024."
                },
                "authors": [
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Tomasz Woźniak"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Woźniak"
                },
                "arxiv_affiliation": "University of Melbourne",
                "author": "Tomasz Woźniak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15747v2",
                "updated": "2025-01-28T04:56:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    56,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-27T03:19:03Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    19,
                    3,
                    0,
                    27,
                    0
                ],
                "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task\n  Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task\n  Language Understanding"
                },
                "summary": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models."
                },
                "authors": [
                    {
                        "name": "Sankalp KJ"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Laxmaan Balaji"
                    },
                    {
                        "name": "Nikunj Kotecha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    }
                ],
                "author_detail": {
                    "name": "Sreyoshi Bhaduri"
                },
                "author": "Sreyoshi Bhaduri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16698v1",
                "updated": "2025-01-28T04:31:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    19,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T04:31:19Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    19,
                    1,
                    28,
                    0
                ],
                "title": "3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow"
                },
                "summary": "3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters."
                },
                "authors": [
                    {
                        "name": "Yueen Ma"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14211v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14211v3",
                "updated": "2025-01-28T04:31:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    31,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-18T06:57:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    57,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "arxiv_comment": "Accepted by The Web Conference 2025 (WWW, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14211v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11900v2",
                "updated": "2025-01-28T04:04:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    4,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-21T05:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation"
                },
                "summary": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines."
                },
                "authors": [
                    {
                        "name": "Junhong Lian"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_doi": "10.1145/3701716.3715539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to The ACM Web Conference 2025 (WWW'25, short paper)",
                "arxiv_journal_ref": "In Companion Proceedings of the ACM Web Conference 2025 (WWW\n  Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16692v1",
                "updated": "2025-01-28T04:00:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T04:00:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation"
                },
                "summary": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization."
                },
                "authors": [
                    {
                        "name": "Manish Acharya"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Kevin Leach"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leach"
                },
                "author": "Kevin Leach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09003v3",
                "updated": "2025-01-28T03:59:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    59,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2024-11-13T20:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "title": "Refusal in LLMs is an Affine Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal in LLMs is an Affine Function"
                },
                "summary": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 ."
                },
                "authors": [
                    {
                        "name": "Thomas Marshall"
                    },
                    {
                        "name": "Adam Scherlis"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16689v1",
                "updated": "2025-01-28T03:57:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:57:22Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "title": "MACI: Multi-Agent Collaborative Intelligence for Robust Reasoning and\n  Temporal Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACI: Multi-Agent Collaborative Intelligence for Robust Reasoning and\n  Temporal Planning"
                },
                "summary": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities beyond the pattern-matching\nstrengths of LLMs. LLMs struggle with planning tasks because of their reliance\non associative reasoning, inability to self-verify, and inconsistent constraint\nawareness. We propose Multi-Agent Collaborative Intelligence (MACI), a\nframework centered on a meta-planner (MP) that orchestrates multiple agents to\ngenerate planner templates that define roles and constraints. These planners\nproduce actionable workflows of role nodes and dependency constraints, enabling\nadvanced temporal reasoning and adaptability.\n  MACI's three-tier architecture includes a meta-planning module for planner\nconstruction, common agents for general reasoning, and specialized agents for\ndomain expertise. By decoupling planning from validation, it overcomes key LLM\nlimitations. Evaluations demonstrate MACI's effective constraint satisfaction,\nconflict detection, and reasoning, positioning it as a robust solution for\ncomplex reasoning and planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities beyond the pattern-matching\nstrengths of LLMs. LLMs struggle with planning tasks because of their reliance\non associative reasoning, inability to self-verify, and inconsistent constraint\nawareness. We propose Multi-Agent Collaborative Intelligence (MACI), a\nframework centered on a meta-planner (MP) that orchestrates multiple agents to\ngenerate planner templates that define roles and constraints. These planners\nproduce actionable workflows of role nodes and dependency constraints, enabling\nadvanced temporal reasoning and adaptability.\n  MACI's three-tier architecture includes a meta-planning module for planner\nconstruction, common agents for general reasoning, and specialized agents for\ndomain expertise. By decoupling planning from validation, it overcomes key LLM\nlimitations. Evaluations demonstrate MACI's effective constraint satisfaction,\nconflict detection, and reasoning, positioning it as a robust solution for\ncomplex reasoning and planning tasks."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "22 pages, 21 tables",
                "arxiv_journal_ref": "Stanford University InfoLab Technical Report, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08193v2",
                "updated": "2025-01-28T03:28:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    28,
                    12,
                    1,
                    28,
                    0
                ],
                "published": "2024-10-10T17:58:24Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining."
                },
                "authors": [
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "arxiv_comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16820v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16820v5",
                "updated": "2025-01-28T03:21:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    21,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2024-01-30T08:46:48Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    8,
                    46,
                    48,
                    1,
                    30,
                    0
                ],
                "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Robust Multi-bit Watermarking for AI-generated Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities of\ngenerating texts resembling human language. However, they can be misused by\ncriminals to create deceptive content, such as fake news and phishing emails,\nwhich raises ethical concerns. Watermarking is a key technique to address these\nconcerns, which embeds a message (e.g., a bit string) into a text generated by\nan LLM. By embedding the user ID (represented as a bit string) into generated\ntexts, we can trace generated texts to the user, known as content source\ntracing. The major limitation of existing watermarking techniques is that they\nachieve sub-optimal performance for content source tracing in real-world\nscenarios. The reason is that they cannot accurately or efficiently extract a\nlong message from a generated text. We aim to address the limitations.\n  In this work, we introduce a new watermarking method for LLM-generated text\ngrounded in pseudo-random segment assignment. We also propose multiple\ntechniques to further enhance the robustness of our watermarking algorithm. We\nconduct extensive experiments to evaluate our method. Our experimental results\nshow that our method substantially outperforms existing baselines in both\naccuracy and robustness on benchmark datasets. For instance, when embedding a\nmessage of length 20 into a 200-token generated text, our method achieves a\nmatch rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only\nachieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits\nwithin an edit distance of 17 on average for each paragraph under the same\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities of\ngenerating texts resembling human language. However, they can be misused by\ncriminals to create deceptive content, such as fake news and phishing emails,\nwhich raises ethical concerns. Watermarking is a key technique to address these\nconcerns, which embeds a message (e.g., a bit string) into a text generated by\nan LLM. By embedding the user ID (represented as a bit string) into generated\ntexts, we can trace generated texts to the user, known as content source\ntracing. The major limitation of existing watermarking techniques is that they\nachieve sub-optimal performance for content source tracing in real-world\nscenarios. The reason is that they cannot accurately or efficiently extract a\nlong message from a generated text. We aim to address the limitations.\n  In this work, we introduce a new watermarking method for LLM-generated text\ngrounded in pseudo-random segment assignment. We also propose multiple\ntechniques to further enhance the robustness of our watermarking algorithm. We\nconduct extensive experiments to evaluate our method. Our experimental results\nshow that our method substantially outperforms existing baselines in both\naccuracy and robustness on benchmark datasets. For instance, when embedding a\nmessage of length 20 into a 200-token generated text, our method achieves a\nmatch rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only\nachieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits\nwithin an edit distance of 17 on average for each paragraph under the same\nsetting."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Wengrui Zheng"
                    },
                    {
                        "name": "Tianyang Tao"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yanze Jiang"
                    },
                    {
                        "name": "Zhihua Tian"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear in Proceedings of USENIX Security '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16820v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16820v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16673v1",
                "updated": "2025-01-28T03:18:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "title": "Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting"
                },
                "summary": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research."
                },
                "authors": [
                    {
                        "name": "Li Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "arxiv_affiliation": "Atlas",
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16672v1",
                "updated": "2025-01-28T03:13:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    13,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:13:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    13,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records"
                },
                "summary": "Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks."
                },
                "authors": [
                    {
                        "name": "Philip Chung"
                    },
                    {
                        "name": "Akshay Swaminathan"
                    },
                    {
                        "name": "Alex J. Goodell"
                    },
                    {
                        "name": "Yeasul Kim"
                    },
                    {
                        "name": "S. Momsen Reincke"
                    },
                    {
                        "name": "Lichy Han"
                    },
                    {
                        "name": "Ben Deverett"
                    },
                    {
                        "name": "Mohammad Amin Sadeghi"
                    },
                    {
                        "name": "Abdel-Badih Ariss"
                    },
                    {
                        "name": "Marc Ghanem"
                    },
                    {
                        "name": "David Seong"
                    },
                    {
                        "name": "Andrew A. Lee"
                    },
                    {
                        "name": "Caitlin E. Coombes"
                    },
                    {
                        "name": "Brad Bradshaw"
                    },
                    {
                        "name": "Mahir A. Sufian"
                    },
                    {
                        "name": "Hyo Jung Hong"
                    },
                    {
                        "name": "Teresa P. Nguyen"
                    },
                    {
                        "name": "Mohammad R. Rasouli"
                    },
                    {
                        "name": "Komal Kamra"
                    },
                    {
                        "name": "Mark A. Burbridge"
                    },
                    {
                        "name": "James C. McAvoy"
                    },
                    {
                        "name": "Roya Saffary"
                    },
                    {
                        "name": "Stephen P. Ma"
                    },
                    {
                        "name": "Dev Dash"
                    },
                    {
                        "name": "James Xie"
                    },
                    {
                        "name": "Ellen Y. Wang"
                    },
                    {
                        "name": "Clifford A. Schmiesing"
                    },
                    {
                        "name": "Nigam Shah"
                    },
                    {
                        "name": "Nima Aghaeepour"
                    }
                ],
                "author_detail": {
                    "name": "Nima Aghaeepour"
                },
                "author": "Nima Aghaeepour",
                "arxiv_comment": "62 pages, 5 figures, 1 table, pre-print manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16666v1",
                "updated": "2025-01-28T03:04:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    4,
                    47,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T03:04:47Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    4,
                    47,
                    1,
                    28,
                    0
                ],
                "title": "Federated Learning for Efficient Condition Monitoring and Anomaly\n  Detection in Industrial Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning for Efficient Condition Monitoring and Anomaly\n  Detection in Industrial Cyber-Physical Systems"
                },
                "summary": "Detecting and localizing anomalies in cyber-physical systems (CPS) has become\nincreasingly challenging as systems grow in complexity, particularly due to\nvarying sensor reliability and node failures in distributed environments. While\nfederated learning (FL) provides a foundation for distributed model training,\nexisting approaches often lack mechanisms to address these CPS-specific\nchallenges. This paper introduces an enhanced FL framework with three key\ninnovations: adaptive model aggregation based on sensor reliability, dynamic\nnode selection for resource optimization, and Weibull-based checkpointing for\nfault tolerance. The proposed framework ensures reliable condition monitoring\nwhile tackling the computational and reliability challenges of industrial CPS\ndeployments. Experiments on the NASA Bearing and Hydraulic System datasets\ndemonstrate superior performance compared to state-of-the-art FL methods,\nachieving 99.5% AUC-ROC in anomaly detection and maintaining accuracy even\nunder node failures. Statistical validation using the Mann-Whitney U test\nconfirms significant improvements, with a p-value less than 0.05, in both\ndetection accuracy and computational efficiency across various operational\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and localizing anomalies in cyber-physical systems (CPS) has become\nincreasingly challenging as systems grow in complexity, particularly due to\nvarying sensor reliability and node failures in distributed environments. While\nfederated learning (FL) provides a foundation for distributed model training,\nexisting approaches often lack mechanisms to address these CPS-specific\nchallenges. This paper introduces an enhanced FL framework with three key\ninnovations: adaptive model aggregation based on sensor reliability, dynamic\nnode selection for resource optimization, and Weibull-based checkpointing for\nfault tolerance. The proposed framework ensures reliable condition monitoring\nwhile tackling the computational and reliability challenges of industrial CPS\ndeployments. Experiments on the NASA Bearing and Hydraulic System datasets\ndemonstrate superior performance compared to state-of-the-art FL methods,\nachieving 99.5% AUC-ROC in anomaly detection and maintaining accuracy even\nunder node failures. Statistical validation using the Mann-Whitney U test\nconfirms significant improvements, with a p-value less than 0.05, in both\ndetection accuracy and computational efficiency across various operational\nscenarios."
                },
                "authors": [
                    {
                        "name": "William Marfo"
                    },
                    {
                        "name": "Deepak K. Tosh"
                    },
                    {
                        "name": "Shirley V. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Shirley V. Moore"
                },
                "author": "Shirley V. Moore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16661v1",
                "updated": "2025-01-28T02:49:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    49,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:49:45Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    49,
                    45,
                    1,
                    28,
                    0
                ],
                "title": "Jupybara: Operationalizing a Design Space for Actionable Data Analysis\n  and Storytelling with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jupybara: Operationalizing a Design Space for Actionable Data Analysis\n  and Storytelling with LLMs"
                },
                "summary": "Mining and conveying actionable insights from complex data is a key challenge\nof exploratory data analysis (EDA) and storytelling. To address this challenge,\nwe present a design space for actionable EDA and storytelling. Synthesizing\ntheory and expert interviews, we highlight how semantic precision, rhetorical\npersuasion, and pragmatic relevance underpin effective EDA and storytelling. We\nalso show how this design space subsumes common challenges in actionable EDA\nand storytelling, such as identifying appropriate analytical strategies and\nleveraging relevant domain knowledge. Building on the potential of LLMs to\ngenerate coherent narratives with commonsense reasoning, we contribute\nJupybara, an AI-enabled assistant for actionable EDA and storytelling\nimplemented as a Jupyter Notebook extension. Jupybara employs two strategies --\ndesign-space-aware prompting and multi-agent architectures -- to operationalize\nour design space. An expert evaluation confirms Jupybara's usability,\nsteerability, explainability, and reparability, as well as the effectiveness of\nour strategies in operationalizing the design space framework with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining and conveying actionable insights from complex data is a key challenge\nof exploratory data analysis (EDA) and storytelling. To address this challenge,\nwe present a design space for actionable EDA and storytelling. Synthesizing\ntheory and expert interviews, we highlight how semantic precision, rhetorical\npersuasion, and pragmatic relevance underpin effective EDA and storytelling. We\nalso show how this design space subsumes common challenges in actionable EDA\nand storytelling, such as identifying appropriate analytical strategies and\nleveraging relevant domain knowledge. Building on the potential of LLMs to\ngenerate coherent narratives with commonsense reasoning, we contribute\nJupybara, an AI-enabled assistant for actionable EDA and storytelling\nimplemented as a Jupyter Notebook extension. Jupybara employs two strategies --\ndesign-space-aware prompting and multi-agent architectures -- to operationalize\nour design space. An expert evaluation confirms Jupybara's usability,\nsteerability, explainability, and reparability, as well as the effectiveness of\nour strategies in operationalizing the design space framework with LLMs."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Larry Birnbaum"
                    },
                    {
                        "name": "Vidya Setlur"
                    }
                ],
                "author_detail": {
                    "name": "Vidya Setlur"
                },
                "author": "Vidya Setlur",
                "arxiv_comment": "Conditionally Accepted at CHI '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16655v1",
                "updated": "2025-01-28T02:38:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    38,
                    56,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:38:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    38,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Large Language Model Critics for Execution-Free Evaluation of Code\n  Changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Critics for Execution-Free Evaluation of Code\n  Changes"
                },
                "summary": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via\nmulti-step LLM-based agentic workflows. However, existing metrics for\nevaluating such workflows, mainly build status and occasionally log analysis,\nare too sparse and limited in providing the information needed to assess the\nquality of changes made. In this work, we designed LLM-based critics to derive\nwell-structured and rigorous intermediate/step-level, execution-free evaluation\nproxies for repo-level code changes. Importantly, we assume access to the gold\ntest patch for the problem (i.e., reference-aware) to assess both semantics and\nexecutability of generated patches. With the gold test patch as a reference, we\npredict executability of all editing locations with an F1 score of 91.6%,\naggregating which, we can predict the build status in 84.8% of the instances in\nSWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%.\nMoreover, we demonstrate the usefulness of such a reference-aware framework in\ncomparing patches generated by different agentic workflows. Finally, we\nopen-source the library developed for this project, which allows further usage\nfor either other agentic workflows or other benchmarks. The source code is\navailable at https://github.com/amazon-science/code-agent-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via\nmulti-step LLM-based agentic workflows. However, existing metrics for\nevaluating such workflows, mainly build status and occasionally log analysis,\nare too sparse and limited in providing the information needed to assess the\nquality of changes made. In this work, we designed LLM-based critics to derive\nwell-structured and rigorous intermediate/step-level, execution-free evaluation\nproxies for repo-level code changes. Importantly, we assume access to the gold\ntest patch for the problem (i.e., reference-aware) to assess both semantics and\nexecutability of generated patches. With the gold test patch as a reference, we\npredict executability of all editing locations with an F1 score of 91.6%,\naggregating which, we can predict the build status in 84.8% of the instances in\nSWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%.\nMoreover, we demonstrate the usefulness of such a reference-aware framework in\ncomparing patches generated by different agentic workflows. Finally, we\nopen-source the library developed for this project, which allows further usage\nfor either other agentic workflows or other benchmarks. The source code is\navailable at https://github.com/amazon-science/code-agent-eval."
                },
                "authors": [
                    {
                        "name": "Aashish Yadavally"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Laurent Callot"
                    },
                    {
                        "name": "Gauthier Guinet"
                    }
                ],
                "author_detail": {
                    "name": "Gauthier Guinet"
                },
                "author": "Gauthier Guinet",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16650v1",
                "updated": "2025-01-28T02:32:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    32,
                    49,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:32:49Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    32,
                    49,
                    1,
                    28,
                    0
                ],
                "title": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large\n  Language Models"
                },
                "summary": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for\nquantitatively assessing the similarity between weight matrices in Large\nLanguage Models (LLMs), aiming to facilitate the analysis of their complex\narchitectures. Leveraging DOCS, our analysis uncovers intriguing patterns in\nthe latest open-source LLMs: adjacent layers frequently exhibit high weight\nsimilarity and tend to form clusters, suggesting depth-wise functional\nspecialization. Additionally, we prove that DOCS is theoretically effective in\nquantifying similarity for orthogonal matrices, a crucial aspect given the\nprevalence of orthogonal initializations in LLMs. This research contributes to\na deeper understanding of LLM architecture and behavior, offering tools with\npotential implications for developing more efficient and interpretable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for\nquantitatively assessing the similarity between weight matrices in Large\nLanguage Models (LLMs), aiming to facilitate the analysis of their complex\narchitectures. Leveraging DOCS, our analysis uncovers intriguing patterns in\nthe latest open-source LLMs: adjacent layers frequently exhibit high weight\nsimilarity and tend to form clusters, suggesting depth-wise functional\nspecialization. Additionally, we prove that DOCS is theoretically effective in\nquantifying similarity for orthogonal matrices, a crucial aspect given the\nprevalence of orthogonal initializations in LLMs. This research contributes to\na deeper understanding of LLM architecture and behavior, offering tools with\npotential implications for developing more efficient and interpretable models."
                },
                "authors": [
                    {
                        "name": "Zeping Min"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16646v1",
                "updated": "2025-01-28T02:29:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    29,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:29:40Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    29,
                    40,
                    1,
                    28,
                    0
                ],
                "title": "instancespace: a Python Package for Insightful Algorithm Testing through\n  Instance Space Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "instancespace: a Python Package for Insightful Algorithm Testing through\n  Instance Space Analysis"
                },
                "summary": "Instance Space Analysis is a methodology to evaluate algorithm performance\nacross diverse problem fields. Through visualisation and exploratory data\nanalysis techniques, Instance Space Analysis offers objective, data-driven\ninsights into the diversity of test instances, algorithm behaviour, and\nalgorithm strengths and weaknesses. As such, it supports automated algorithm\nselection and synthetic test instance generation, increasing testing\nreliability in optimisation, machine learning, and scheduling fields. This\npaper introduces instancespace, a Python package that implements an automated\npipeline for Instance Space Analysis. This package supports research by\nstreamlining the testing process, providing unbiased metrics, and facilitating\nmore informed algorithmic design and deployment decisions, particularly for\ncomplex and safety-critical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance Space Analysis is a methodology to evaluate algorithm performance\nacross diverse problem fields. Through visualisation and exploratory data\nanalysis techniques, Instance Space Analysis offers objective, data-driven\ninsights into the diversity of test instances, algorithm behaviour, and\nalgorithm strengths and weaknesses. As such, it supports automated algorithm\nselection and synthetic test instance generation, increasing testing\nreliability in optimisation, machine learning, and scheduling fields. This\npaper introduces instancespace, a Python package that implements an automated\npipeline for Instance Space Analysis. This package supports research by\nstreamlining the testing process, providing unbiased metrics, and facilitating\nmore informed algorithmic design and deployment decisions, particularly for\ncomplex and safety-critical systems."
                },
                "authors": [
                    {
                        "name": "Yusuf Berdan Güzel"
                    },
                    {
                        "name": "Kushagra Khare"
                    },
                    {
                        "name": "Nathan Harvey"
                    },
                    {
                        "name": "Kian Dsouza"
                    },
                    {
                        "name": "Dong Hyeog Jang"
                    },
                    {
                        "name": "Junheng Chen"
                    },
                    {
                        "name": "Cheng Ze Lam"
                    },
                    {
                        "name": "Mario Andrés Muñoz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Andrés Muñoz"
                },
                "author": "Mario Andrés Muñoz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16643v1",
                "updated": "2025-01-28T02:27:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    27,
                    55,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:27:55Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    27,
                    55,
                    1,
                    28,
                    0
                ],
                "title": "An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party\n  Dialogue"
                },
                "summary": "Handling multi-party dialogues represents a significant step for advancing\nspoken dialogue systems, necessitating the development of tasks specific to\nmulti-party interactions. To address this challenge, we are constructing a\nmulti-modal multi-party dialogue corpus of triadic (three-participant)\ndiscussions. This paper focuses on the task of addressee recognition,\nidentifying who is being addressed to take the next turn, a critical component\nunique to multi-party dialogue systems. A subset of the corpus was annotated\nwith addressee information, revealing that explicit addressees are indicated in\napproximately 20% of conversational turns. To evaluate the task's complexity,\nwe benchmarked the performance of a large language model (GPT-4o) on addressee\nrecognition. The results showed that GPT-4o achieved an accuracy only\nmarginally above chance, underscoring the challenges of addressee recognition\nin multi-party dialogue. These findings highlight the need for further research\nto enhance the capabilities of large language models in understanding and\nnavigating the intricacies of multi-party conversational dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling multi-party dialogues represents a significant step for advancing\nspoken dialogue systems, necessitating the development of tasks specific to\nmulti-party interactions. To address this challenge, we are constructing a\nmulti-modal multi-party dialogue corpus of triadic (three-participant)\ndiscussions. This paper focuses on the task of addressee recognition,\nidentifying who is being addressed to take the next turn, a critical component\nunique to multi-party dialogue systems. A subset of the corpus was annotated\nwith addressee information, revealing that explicit addressees are indicated in\napproximately 20% of conversational turns. To evaluate the task's complexity,\nwe benchmarked the performance of a large language model (GPT-4o) on addressee\nrecognition. The results showed that GPT-4o achieved an accuracy only\nmarginally above chance, underscoring the challenges of addressee recognition\nin multi-party dialogue. These findings highlight the need for further research\nto enhance the capabilities of large language models in understanding and\nnavigating the intricacies of multi-party conversational dynamics."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Keiko Ochi"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16635v1",
                "updated": "2025-01-28T02:16:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    16,
                    18,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    16,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation"
                },
                "summary": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4's performance in\nrecognizing the majority labels of laughable contexts, achieving an F1 score of\n43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4's performance in\nrecognizing the majority labels of laughable contexts, achieving an F1 score of\n43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16629v1",
                "updated": "2025-01-28T02:05:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    5,
                    38,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T02:05:38Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    5,
                    38,
                    1,
                    28,
                    0
                ],
                "title": "CHiP: Cross-modal Hierarchical Direct Preference Optimization for\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHiP: Cross-modal Hierarchical Direct Preference Optimization for\n  Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) still struggle with hallucinations\ndespite their impressive capabilities. Recent studies have attempted to\nmitigate this by applying Direct Preference Optimization (DPO) to multimodal\nscenarios using preference pairs from text-based responses. However, our\nanalysis of representation distributions reveals that multimodal DPO struggles\nto align image and text representations and to distinguish between hallucinated\nand non-hallucinated descriptions. To address these challenges, in this work,\nwe propose a Cross-modal Hierarchical Direct Preference Optimization (CHiP) to\naddress these limitations. We introduce a visual preference optimization module\nwithin the DPO framework, enabling MLLMs to learn from both textual and visual\npreferences simultaneously. Furthermore, we propose a hierarchical textual\npreference optimization module that allows the model to capture preferences at\nmultiple granular levels, including response, segment, and token levels. We\nevaluate CHiP through both quantitative and qualitative analyses, with results\nacross multiple benchmarks demonstrating its effectiveness in reducing\nhallucinations. On the Object HalBench dataset, CHiP outperforms DPO in\nhallucination reduction, achieving improvements of 52.7% and 55.5% relative\npoints based on the base model Muffin and LLaVA models, respectively. We make\nall our datasets and code publicly available: https://github.com/LVUGAI/CHiP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) still struggle with hallucinations\ndespite their impressive capabilities. Recent studies have attempted to\nmitigate this by applying Direct Preference Optimization (DPO) to multimodal\nscenarios using preference pairs from text-based responses. However, our\nanalysis of representation distributions reveals that multimodal DPO struggles\nto align image and text representations and to distinguish between hallucinated\nand non-hallucinated descriptions. To address these challenges, in this work,\nwe propose a Cross-modal Hierarchical Direct Preference Optimization (CHiP) to\naddress these limitations. We introduce a visual preference optimization module\nwithin the DPO framework, enabling MLLMs to learn from both textual and visual\npreferences simultaneously. Furthermore, we propose a hierarchical textual\npreference optimization module that allows the model to capture preferences at\nmultiple granular levels, including response, segment, and token levels. We\nevaluate CHiP through both quantitative and qualitative analyses, with results\nacross multiple benchmarks demonstrating its effectiveness in reducing\nhallucinations. On the Object HalBench dataset, CHiP outperforms DPO in\nhallucination reduction, achieving improvements of 52.7% and 55.5% relative\npoints based on the base model Muffin and LLaVA models, respectively. We make\nall our datasets and code publicly available: https://github.com/LVUGAI/CHiP."
                },
                "authors": [
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Shenzhen Huangfu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17167v2",
                "updated": "2025-01-28T01:57:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    1,
                    57,
                    14,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-14T08:32:31Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    8,
                    32,
                    31,
                    5,
                    258,
                    0
                ],
                "title": "StressPrompt: Does Stress Impact Large Language Models and Human\n  Performance Similarly?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StressPrompt: Does Stress Impact Large Language Models and Human\n  Performance Similarly?"
                },
                "summary": "Human beings often experience stress, which can significantly influence their\nperformance. This study explores whether Large Language Models (LLMs) exhibit\nstress responses similar to those of humans and whether their performance\nfluctuates under different stress-inducing prompts. To investigate this, we\ndeveloped a novel set of prompts, termed StressPrompt, designed to induce\nvarying levels of stress. These prompts were derived from established\npsychological frameworks and carefully calibrated based on ratings from human\nparticipants. We then applied these prompts to several LLMs to assess their\nresponses across a range of tasks, including instruction-following, complex\nreasoning, and emotional intelligence. The findings suggest that LLMs, like\nhumans, perform optimally under moderate stress, consistent with the\nYerkes-Dodson law. Notably, their performance declines under both low and\nhigh-stress conditions. Our analysis further revealed that these StressPrompts\nsignificantly alter the internal states of LLMs, leading to changes in their\nneural representations that mirror human responses to stress. This research\nprovides critical insights into the operational robustness and flexibility of\nLLMs, demonstrating the importance of designing AI systems capable of\nmaintaining high performance in real-world scenarios where stress is prevalent,\nsuch as in customer service, healthcare, and emergency response contexts.\nMoreover, this study contributes to the broader AI research community by\noffering a new perspective on how LLMs handle different scenarios and their\nsimilarities to human cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human beings often experience stress, which can significantly influence their\nperformance. This study explores whether Large Language Models (LLMs) exhibit\nstress responses similar to those of humans and whether their performance\nfluctuates under different stress-inducing prompts. To investigate this, we\ndeveloped a novel set of prompts, termed StressPrompt, designed to induce\nvarying levels of stress. These prompts were derived from established\npsychological frameworks and carefully calibrated based on ratings from human\nparticipants. We then applied these prompts to several LLMs to assess their\nresponses across a range of tasks, including instruction-following, complex\nreasoning, and emotional intelligence. The findings suggest that LLMs, like\nhumans, perform optimally under moderate stress, consistent with the\nYerkes-Dodson law. Notably, their performance declines under both low and\nhigh-stress conditions. Our analysis further revealed that these StressPrompts\nsignificantly alter the internal states of LLMs, leading to changes in their\nneural representations that mirror human responses to stress. This research\nprovides critical insights into the operational robustness and flexibility of\nLLMs, demonstrating the importance of designing AI systems capable of\nmaintaining high performance in real-world scenarios where stress is prevalent,\nsuch as in customer service, healthcare, and emergency response contexts.\nMoreover, this study contributes to the broader AI research community by\noffering a new perspective on how LLMs handle different scenarios and their\nsimilarities to human cognition."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Aorigele Bao"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "11 pages, 9 figures, Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16615v1",
                "updated": "2025-01-28T01:24:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    1,
                    24,
                    16,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T01:24:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    1,
                    24,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "Sparse Autoencoders Trained on the Same Data Learn Different Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Trained on the Same Data Learn Different Features"
                },
                "summary": "Sparse autoencoders (SAEs) are a useful tool for uncovering\nhuman-interpretable features in the activations of large language models\n(LLMs). While some expect SAEs to find the true underlying features used by a\nmodel, our research shows that SAEs trained on the same model and data,\ndiffering only in the random seed used to initialize their weights, identify\ndifferent sets of features. For example, in an SAE with 131K latents trained on\na feedforward network in Llama 3 8B, only 30% of the features were shared\nacross different seeds. We observed this phenomenon across multiple layers of\nthree different LLMs, two datasets, and several SAE architectures. While ReLU\nSAEs trained with the L1 sparsity loss showed greater stability across seeds,\nSAEs using the state-of-the-art TopK activation function were more\nseed-dependent, even when controlling for the level of sparsity. Our results\nsuggest that the set of features uncovered by an SAE should be viewed as a\npragmatically useful decomposition of activation space, rather than an\nexhaustive and universal list of features \"truly used\" by the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) are a useful tool for uncovering\nhuman-interpretable features in the activations of large language models\n(LLMs). While some expect SAEs to find the true underlying features used by a\nmodel, our research shows that SAEs trained on the same model and data,\ndiffering only in the random seed used to initialize their weights, identify\ndifferent sets of features. For example, in an SAE with 131K latents trained on\na feedforward network in Llama 3 8B, only 30% of the features were shared\nacross different seeds. We observed this phenomenon across multiple layers of\nthree different LLMs, two datasets, and several SAE architectures. While ReLU\nSAEs trained with the L1 sparsity loss showed greater stability across seeds,\nSAEs using the state-of-the-art TopK activation function were more\nseed-dependent, even when controlling for the level of sparsity. Our results\nsuggest that the set of features uncovered by an SAE should be viewed as a\npragmatically useful decomposition of activation space, rather than an\nexhaustive and universal list of features \"truly used\" by the model."
                },
                "authors": [
                    {
                        "name": "Gonçalo Paulo"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16610v1",
                "updated": "2025-01-28T01:09:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    1,
                    9,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T01:09:50Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    1,
                    9,
                    50,
                    1,
                    28,
                    0
                ],
                "title": "Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture\n  for 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture\n  for 6G"
                },
                "summary": "Multiple-input multiple-output (MIMO) communication has led to immense\nenhancements in data rates and efficient spectrum management. The evolution of\nMIMO has been accompanied by increased hardware complexity and array sizes,\ncausing system power consumption to rise as a result. Despite past advances in\npower-efficient hybrid architectures, new solutions are needed to enable\nextremely large-scale MIMO deployments for 6G and beyond. In this paper, we\nintroduce a novel architecture that integrates low-power reconfigurable\nantennas with both digital and analog precoding. This \\emph{tri-hybrid}\napproach addresses key limitations in traditional and hybrid MIMO systems by\nimproving power consumption and adding new layer for signal processing. We\nprovide a comprehensive analysis of the proposed architecture and compare its\nperformance with existing solutions, including fully-digital and hybrid MIMO\nsystems. The results demonstrate significant improvements in energy efficiency,\nhighlighting the potential of the tri-hybrid system to meet the growing demands\nof future wireless networks. We also discuss several design and implementation\nchallenges, including the need for technological advancements in reconfigurable\narray hardware and tunable antenna parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-input multiple-output (MIMO) communication has led to immense\nenhancements in data rates and efficient spectrum management. The evolution of\nMIMO has been accompanied by increased hardware complexity and array sizes,\ncausing system power consumption to rise as a result. Despite past advances in\npower-efficient hybrid architectures, new solutions are needed to enable\nextremely large-scale MIMO deployments for 6G and beyond. In this paper, we\nintroduce a novel architecture that integrates low-power reconfigurable\nantennas with both digital and analog precoding. This \\emph{tri-hybrid}\napproach addresses key limitations in traditional and hybrid MIMO systems by\nimproving power consumption and adding new layer for signal processing. We\nprovide a comprehensive analysis of the proposed architecture and compare its\nperformance with existing solutions, including fully-digital and hybrid MIMO\nsystems. The results demonstrate significant improvements in energy efficiency,\nhighlighting the potential of the tri-hybrid system to meet the growing demands\nof future wireless networks. We also discuss several design and implementation\nchallenges, including the need for technological advancements in reconfigurable\narray hardware and tunable antenna parameters."
                },
                "authors": [
                    {
                        "name": "Miguel Rodrigo Castellanos"
                    },
                    {
                        "name": "Siyun Yang"
                    },
                    {
                        "name": "Chan-Byoung Chae"
                    },
                    {
                        "name": "Robert W. Heath Jr"
                    }
                ],
                "author_detail": {
                    "name": "Robert W. Heath Jr"
                },
                "author": "Robert W. Heath Jr",
                "arxiv_comment": "IEEE Transactions on Communications (invited)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v1",
                "updated": "2025-01-28T00:52:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: An Effective Framework for Text-to-SQL with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: An Effective Framework for Text-to-SQL with Monte Carlo Tree\n  Search"
                },
                "summary": "Text-to-SQL is a fundamental and longstanding problem in the NLP area, aiming\nat converting natural language queries into SQL, enabling non-expert users to\noperate databases. Recent advances in LLM have greatly improved text-to-SQL\nperformance. However, challenges persist, especially when dealing with complex\nuser queries. Current approaches (e.g., COT prompting and multi-agent\nframeworks) rely on the ability of models to plan and generate SQL\nautonomously, but controlling performance remains difficult. In addition, LLMs\nare still prone to hallucinations. To alleviate these challenges, we designed a\nnovel MCTS-SQL to guide SQL generation iteratively. The approach generates SQL\nqueries through Monte Carlo Tree Search (MCTS) and a heuristic self-refinement\nmechanism are used to enhance accuracy and reliability. Key components include\na schema selector for extracting relevant information and an MCTS-based\ngenerator for iterative query refinement. Experimental results from the SPIDER\nand BIRD benchmarks show that MCTS-SQL achieves state-of-the-art performance.\nSpecifically, on the BIRD development dataset, MCTS-SQL achieves an Execution\n(EX) accuracy of 69.40% using GPT-4o as the base model and a significant\nimprovement when dealing with challenging tasks, with an EX of 51.48%, which is\n3.41% higher than the existing method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental and longstanding problem in the NLP area, aiming\nat converting natural language queries into SQL, enabling non-expert users to\noperate databases. Recent advances in LLM have greatly improved text-to-SQL\nperformance. However, challenges persist, especially when dealing with complex\nuser queries. Current approaches (e.g., COT prompting and multi-agent\nframeworks) rely on the ability of models to plan and generate SQL\nautonomously, but controlling performance remains difficult. In addition, LLMs\nare still prone to hallucinations. To alleviate these challenges, we designed a\nnovel MCTS-SQL to guide SQL generation iteratively. The approach generates SQL\nqueries through Monte Carlo Tree Search (MCTS) and a heuristic self-refinement\nmechanism are used to enhance accuracy and reliability. Key components include\na schema selector for extracting relevant information and an MCTS-based\ngenerator for iterative query refinement. Experimental results from the SPIDER\nand BIRD benchmarks show that MCTS-SQL achieves state-of-the-art performance.\nSpecifically, on the BIRD development dataset, MCTS-SQL achieves an Execution\n(EX) accuracy of 69.40% using GPT-4o as the base model and a significant\nimprovement when dealing with challenging tasks, with an EX of 51.48%, which is\n3.41% higher than the existing method."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Liming Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Haoran Peng"
                    },
                    {
                        "name": "Wenming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Guo"
                },
                "author": "Wenming Guo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16588v1",
                "updated": "2025-01-28T00:02:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    2,
                    0,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:02:00Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    2,
                    0,
                    1,
                    28,
                    0
                ],
                "title": "Fine-Tuned Language Models as Space Systems Controllers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned Language Models as Space Systems Controllers"
                },
                "summary": "Large language models (LLMs), or foundation models (FMs), are pretrained\ntransformers that coherently complete sentences auto-regressively. In this\npaper, we show that LLMs can control simplified space systems after some\nadditional training, called fine-tuning. We look at relatively small language\nmodels, ranging between 7 and 13 billion parameters. We focus on four problems:\na three-dimensional spring toy problem, low-thrust orbit transfer, low-thrust\ncislunar control, and powered descent guidance. The fine-tuned LLMs are capable\nof controlling systems by generating sufficiently accurate outputs that are\nmulti-dimensional vectors with up to 10 significant digits. We show that for\nseveral problems the amount of data required to perform fine-tuning is smaller\nthan what is generally required of traditional deep neural networks (DNNs), and\nthat fine-tuned LLMs are good at generalizing outside of the training dataset.\nFurther, the same LLM can be fine-tuned with data from different problems, with\nonly minor performance degradation with respect to LLMs trained for a single\napplication. This work is intended as a first step towards the development of a\ngeneral space systems controller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), or foundation models (FMs), are pretrained\ntransformers that coherently complete sentences auto-regressively. In this\npaper, we show that LLMs can control simplified space systems after some\nadditional training, called fine-tuning. We look at relatively small language\nmodels, ranging between 7 and 13 billion parameters. We focus on four problems:\na three-dimensional spring toy problem, low-thrust orbit transfer, low-thrust\ncislunar control, and powered descent guidance. The fine-tuned LLMs are capable\nof controlling systems by generating sufficiently accurate outputs that are\nmulti-dimensional vectors with up to 10 significant digits. We show that for\nseveral problems the amount of data required to perform fine-tuning is smaller\nthan what is generally required of traditional deep neural networks (DNNs), and\nthat fine-tuned LLMs are good at generalizing outside of the training dataset.\nFurther, the same LLM can be fine-tuned with data from different problems, with\nonly minor performance degradation with respect to LLMs trained for a single\napplication. This work is intended as a first step towards the development of a\ngeneral space systems controller."
                },
                "authors": [
                    {
                        "name": "Enrico M. Zucchelli"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Julia Briden"
                    },
                    {
                        "name": "Christian Hofmann"
                    },
                    {
                        "name": "Victor Rodriguez-Fernandez"
                    },
                    {
                        "name": "Richard Linares"
                    }
                ],
                "author_detail": {
                    "name": "Richard Linares"
                },
                "author": "Richard Linares",
                "arxiv_journal_ref": "Proceedings of the AAS/AIAA Astrodynamics Specialist Conference,\n  paper number AAS 24-445, Broomfield, CO, August 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13662v2",
                "updated": "2025-01-27T23:35:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    23,
                    35,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-19T16:09:58Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    9,
                    58,
                    2,
                    171,
                    0
                ],
                "title": "Jailbreaking Large Language Models Through Alignment Vulnerabilities in\n  Out-of-Distribution Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models Through Alignment Vulnerabilities in\n  Out-of-Distribution Settings"
                },
                "summary": "Recently, Large Language Models (LLMs) have garnered significant attention\nfor their exceptional natural language processing capabilities. However,\nconcerns about their trustworthiness remain unresolved, particularly in\naddressing ``jailbreaking'' attacks on aligned LLMs. Previous research\npredominantly relies on scenarios involving white-box LLMs or specific, fixed\nprompt templates, which are often impractical and lack broad applicability. In\nthis paper, we introduce a straightforward and novel method called\nObscurePrompt for jailbreaking LLMs, inspired by the observed fragile\nalignments in Out-of-Distribution (OOD) data. Specifically, we first formulate\nthe decision boundary in the jailbreaking process and then explore how obscure\ntext affects LLM's ethical decision boundary. ObscurePrompt starts with\nconstructing a base prompt that integrates well-known jailbreaking techniques.\nPowerful LLMs are then utilized to obscure the original prompt through\niterative transformations, aiming to bolster the attack's robustness.\nComprehensive experiments show that our approach substantially improves upon\nprevious methods in terms of attack effectiveness, maintaining efficacy against\ntwo prevalent defense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have garnered significant attention\nfor their exceptional natural language processing capabilities. However,\nconcerns about their trustworthiness remain unresolved, particularly in\naddressing ``jailbreaking'' attacks on aligned LLMs. Previous research\npredominantly relies on scenarios involving white-box LLMs or specific, fixed\nprompt templates, which are often impractical and lack broad applicability. In\nthis paper, we introduce a straightforward and novel method called\nObscurePrompt for jailbreaking LLMs, inspired by the observed fragile\nalignments in Out-of-Distribution (OOD) data. Specifically, we first formulate\nthe decision boundary in the jailbreaking process and then explore how obscure\ntext affects LLM's ethical decision boundary. ObscurePrompt starts with\nconstructing a base prompt that integrates well-known jailbreaking techniques.\nPowerful LLMs are then utilized to obscure the original prompt through\niterative transformations, aiming to bolster the attack's robustness.\nComprehensive experiments show that our approach substantially improves upon\nprevious methods in terms of attack effectiveness, maintaining efficacy against\ntwo prevalent defense mechanisms."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Bingda Tang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16558v1",
                "updated": "2025-01-27T23:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    23,
                    1,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T23:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    23,
                    1,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Distributional Information Embedding: A Framework for Multi-bit\n  Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Information Embedding: A Framework for Multi-bit\n  Watermarking"
                },
                "summary": "This paper introduces a novel problem, distributional information embedding,\nmotivated by the practical demands of multi-bit watermarking for large language\nmodels (LLMs). Unlike traditional information embedding, which embeds\ninformation into a pre-existing host signal, LLM watermarking actively controls\nthe text generation process--adjusting the token distribution--to embed a\ndetectable signal. We develop an information-theoretic framework to analyze\nthis distributional information embedding problem, characterizing the\nfundamental trade-offs among three critical performance metrics: text quality,\ndetectability, and information rate. In the asymptotic regime, we demonstrate\nthat the maximum achievable rate with vanishing error corresponds to the\nentropy of the LLM's output distribution and increases with higher allowable\ndistortion. We also characterize the optimal watermarking scheme to achieve\nthis rate. Extending the analysis to the finite-token case, we identify schemes\nthat maximize detection probability while adhering to constraints on false\nalarm and distortion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel problem, distributional information embedding,\nmotivated by the practical demands of multi-bit watermarking for large language\nmodels (LLMs). Unlike traditional information embedding, which embeds\ninformation into a pre-existing host signal, LLM watermarking actively controls\nthe text generation process--adjusting the token distribution--to embed a\ndetectable signal. We develop an information-theoretic framework to analyze\nthis distributional information embedding problem, characterizing the\nfundamental trade-offs among three critical performance metrics: text quality,\ndetectability, and information rate. In the asymptotic regime, we demonstrate\nthat the maximum achievable rate with vanishing error corresponds to the\nentropy of the LLM's output distribution and increases with higher allowable\ndistortion. We also characterize the optimal watermarking scheme to achieve\nthis rate. Extending the analysis to the finite-token case, we identify schemes\nthat maximize detection probability while adhering to constraints on false\nalarm and distortion."
                },
                "authors": [
                    {
                        "name": "Haiyun He"
                    },
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Ziqiao Wang"
                    },
                    {
                        "name": "Yongyi Mao"
                    },
                    {
                        "name": "Yuheng Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Bu"
                },
                "author": "Yuheng Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16539v1",
                "updated": "2025-01-27T22:20:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    20,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:20:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    20,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Generalized Mission Planning for Heterogeneous Multi-Robot Teams via\n  LLM-constructed Hierarchical Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Mission Planning for Heterogeneous Multi-Robot Teams via\n  LLM-constructed Hierarchical Trees"
                },
                "summary": "We present a novel mission-planning strategy for heterogeneous multi-robot\nteams, taking into account the specific constraints and capabilities of each\nrobot. Our approach employs hierarchical trees to systematically break down\ncomplex missions into manageable sub-tasks. We develop specialized APIs and\ntools, which are utilized by Large Language Models (LLMs) to efficiently\nconstruct these hierarchical trees. Once the hierarchical tree is generated, it\nis further decomposed to create optimized schedules for each robot, ensuring\nadherence to their individual constraints and capabilities. We demonstrate the\neffectiveness of our framework through detailed examples covering a wide range\nof missions, showcasing its flexibility and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel mission-planning strategy for heterogeneous multi-robot\nteams, taking into account the specific constraints and capabilities of each\nrobot. Our approach employs hierarchical trees to systematically break down\ncomplex missions into manageable sub-tasks. We develop specialized APIs and\ntools, which are utilized by Large Language Models (LLMs) to efficiently\nconstruct these hierarchical trees. Once the hierarchical tree is generated, it\nis further decomposed to create optimized schedules for each robot, ensuring\nadherence to their individual constraints and capabilities. We demonstrate the\neffectiveness of our framework through detailed examples covering a wide range\nof missions, showcasing its flexibility and scalability."
                },
                "authors": [
                    {
                        "name": "Piyush Gupta"
                    },
                    {
                        "name": "David Isele"
                    },
                    {
                        "name": "Enna Sachdeva"
                    },
                    {
                        "name": "Pin-Hao Huang"
                    },
                    {
                        "name": "Behzad Dariush"
                    },
                    {
                        "name": "Kwonjoon Lee"
                    },
                    {
                        "name": "Sangjae Bae"
                    }
                ],
                "author_detail": {
                    "name": "Sangjae Bae"
                },
                "author": "Sangjae Bae",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16534v1",
                "updated": "2025-01-27T22:13:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    13,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:13:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    13,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs"
                },
                "summary": "Alignment in large language models (LLMs) is used to enforce guidelines such\nas safety. Yet, alignment fails in the face of jailbreak attacks that modify\ninputs to induce unsafe outputs. In this paper, we present and evaluate a\nmethod to assess the robustness of LLM alignment. We observe that alignment\nembeds a safety classifier in the target model that is responsible for deciding\nbetween refusal and compliance. We seek to extract an approximation of this\nclassifier, called a surrogate classifier, from the LLM. We develop an\nalgorithm for identifying candidate classifiers from subsets of the LLM model.\nWe evaluate the degree to which the candidate classifiers approximate the\nmodel's embedded classifier in benign (F1 score) and adversarial (using\nsurrogates in a white-box attack) settings. Our evaluation shows that the best\ncandidates achieve accurate agreement (an F1 score above 80%) using as little\nas 20% of the model architecture. Further, we find attacks mounted on the\nsurrogate models can be transferred with high accuracy. For example, a\nsurrogate using only 50% of the Llama 2 model achieved an attack success rate\n(ASR) of 70%, a substantial improvement over attacking the LLM directly, where\nwe only observed a 22% ASR. These results show that extracting surrogate\nclassifiers is a viable (and highly effective) means for modeling (and therein\naddressing) the vulnerability of aligned models to jailbreaking attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment in large language models (LLMs) is used to enforce guidelines such\nas safety. Yet, alignment fails in the face of jailbreak attacks that modify\ninputs to induce unsafe outputs. In this paper, we present and evaluate a\nmethod to assess the robustness of LLM alignment. We observe that alignment\nembeds a safety classifier in the target model that is responsible for deciding\nbetween refusal and compliance. We seek to extract an approximation of this\nclassifier, called a surrogate classifier, from the LLM. We develop an\nalgorithm for identifying candidate classifiers from subsets of the LLM model.\nWe evaluate the degree to which the candidate classifiers approximate the\nmodel's embedded classifier in benign (F1 score) and adversarial (using\nsurrogates in a white-box attack) settings. Our evaluation shows that the best\ncandidates achieve accurate agreement (an F1 score above 80%) using as little\nas 20% of the model architecture. Further, we find attacks mounted on the\nsurrogate models can be transferred with high accuracy. For example, a\nsurrogate using only 50% of the Llama 2 model achieved an attack success rate\n(ASR) of 70%, a substantial improvement over attacking the LLM directly, where\nwe only observed a 22% ASR. These results show that extracting surrogate\nclassifiers is a viable (and highly effective) means for modeling (and therein\naddressing) the vulnerability of aligned models to jailbreaking attacks."
                },
                "authors": [
                    {
                        "name": "Jean-Charles Noirot Ferrand"
                    },
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Eric Pauley"
                    },
                    {
                        "name": "Ryan Sheatsley"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "author": "Patrick McDaniel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16533v1",
                "updated": "2025-01-27T22:12:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    12,
                    9,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:12:09Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    12,
                    9,
                    0,
                    27,
                    0
                ],
                "title": "A comparison of data filtering techniques for English-Polish LLM-based\n  machine translation in the biomedical domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of data filtering techniques for English-Polish LLM-based\n  machine translation in the biomedical domain"
                },
                "summary": "Large Language Models (LLMs) have become state-of-the-art in Machine\nTranslation (MT), often trained on massive bilingual parallel corpora scraped\nfrom the web, that contain low-quality entries and redundant information,\nleading to significant computational challenges. Various data filtering methods\nexist to reduce dataset sizes, but their effectiveness largely varies based on\nspecific language pairs and domains. This paper evaluates the impact of\ncommonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on\nEnglish-Polish translation within the biomedical domain. By filtering the UFAL\nMedical Corpus, we created varying dataset sizes to fine-tune the mBART50\nmodel, which was then evaluated using the SacreBLEU metric on the Khresmoi\ndataset, having the quality of translations assessed by bilingual speakers. Our\nresults show that both LASER and MUSE can significantly reduce dataset sizes\nwhile maintaining or even enhancing performance. We recommend the use of LASER,\nas it consistently outperforms the other methods and provides the most fluent\nand natural-sounding translations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become state-of-the-art in Machine\nTranslation (MT), often trained on massive bilingual parallel corpora scraped\nfrom the web, that contain low-quality entries and redundant information,\nleading to significant computational challenges. Various data filtering methods\nexist to reduce dataset sizes, but their effectiveness largely varies based on\nspecific language pairs and domains. This paper evaluates the impact of\ncommonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on\nEnglish-Polish translation within the biomedical domain. By filtering the UFAL\nMedical Corpus, we created varying dataset sizes to fine-tune the mBART50\nmodel, which was then evaluated using the SacreBLEU metric on the Khresmoi\ndataset, having the quality of translations assessed by bilingual speakers. Our\nresults show that both LASER and MUSE can significantly reduce dataset sizes\nwhile maintaining or even enhancing performance. We recommend the use of LASER,\nas it consistently outperforms the other methods and provides the most fluent\nand natural-sounding translations."
                },
                "authors": [
                    {
                        "name": "Jorge del Pozo Lérida"
                    },
                    {
                        "name": "Kamil Kojs"
                    },
                    {
                        "name": "János Máté"
                    },
                    {
                        "name": "Mikołaj Antoni Barański"
                    },
                    {
                        "name": "Christian Hardmeier"
                    }
                ],
                "author_detail": {
                    "name": "Christian Hardmeier"
                },
                "author": "Christian Hardmeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12222v2",
                "updated": "2025-01-27T22:05:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    5,
                    29,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-16T04:36:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    4,
                    36,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation"
                },
                "summary": "Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in bad data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubrics template and use large language models (LLMs) to score the\ngeneration into quantifiable scales. We compared popular LLMs as well as the\nwidely adopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods to generation synthetic\nunfaithful data, as well as a heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation on whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non latency and cost for deploying such system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in bad data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubrics template and use large language models (LLMs) to score the\ngeneration into quantifiable scales. We compared popular LLMs as well as the\nwidely adopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods to generation synthetic\nunfaithful data, as well as a heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation on whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non latency and cost for deploying such system."
                },
                "authors": [
                    {
                        "name": "Xiaonan Jing"
                    },
                    {
                        "name": "Srinivas Billa"
                    },
                    {
                        "name": "Danny Godbout"
                    }
                ],
                "author_detail": {
                    "name": "Danny Godbout"
                },
                "author": "Danny Godbout",
                "arxiv_comment": "Accepted to NAACL 2025 Findings. 15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16524v1",
                "updated": "2025-01-27T21:48:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    21,
                    48,
                    39,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T21:48:39Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    21,
                    48,
                    39,
                    0,
                    27,
                    0
                ],
                "title": "Programming by Examples Meets Historical Linguistics: A Large Language\n  Model Based Approach to Sound Law Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming by Examples Meets Historical Linguistics: A Large Language\n  Model Based Approach to Sound Law Induction"
                },
                "summary": "Historical linguists have long written \"programs\" that convert reconstructed\nwords in an ancestor language into their attested descendants via ordered\nstring rewrite functions (called sound laws) However, writing these programs is\ntime-consuming, motivating the development of automated Sound Law Induction\n(SLI) which we formulate as Programming by Examples (PBE) with Large Language\nModels (LLMs) in this paper. While LLMs have been effective for code\ngeneration, recent work has shown that PBE is challenging but improvable by\nfine-tuning, especially with training data drawn from the same distribution as\nevaluation data. In this paper, we create a conceptual framework of what\nconstitutes a \"similar distribution\" for SLI and propose four kinds of\nsynthetic data generation methods with varying amounts of inductive bias to\ninvestigate what leads to the best performance. Based on the results we create\na SOTA open-source model for SLI as PBE (+6% pass rate with a third of the\nparameters of the second-best LLM) and also highlight exciting future\ndirections for PBE research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical linguists have long written \"programs\" that convert reconstructed\nwords in an ancestor language into their attested descendants via ordered\nstring rewrite functions (called sound laws) However, writing these programs is\ntime-consuming, motivating the development of automated Sound Law Induction\n(SLI) which we formulate as Programming by Examples (PBE) with Large Language\nModels (LLMs) in this paper. While LLMs have been effective for code\ngeneration, recent work has shown that PBE is challenging but improvable by\nfine-tuning, especially with training data drawn from the same distribution as\nevaluation data. In this paper, we create a conceptual framework of what\nconstitutes a \"similar distribution\" for SLI and propose four kinds of\nsynthetic data generation methods with varying amounts of inductive bias to\ninvestigate what leads to the best performance. Based on the results we create\na SOTA open-source model for SLI as PBE (+6% pass rate with a third of the\nparameters of the second-best LLM) and also highlight exciting future\ndirections for PBE research."
                },
                "authors": [
                    {
                        "name": "Atharva Naik"
                    },
                    {
                        "name": "Darsh Agrawal"
                    },
                    {
                        "name": "Hong Sng"
                    },
                    {
                        "name": "Clayton Marr"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Nathaniel R Robinson"
                    },
                    {
                        "name": "Kalvin Chang"
                    },
                    {
                        "name": "Rebecca Byrnes"
                    },
                    {
                        "name": "Aravind Mysore"
                    },
                    {
                        "name": "Carolyn Rose"
                    },
                    {
                        "name": "David R Mortensen"
                    }
                ],
                "author_detail": {
                    "name": "David R Mortensen"
                },
                "author": "David R Mortensen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]