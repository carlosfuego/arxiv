[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v1",
                "updated": "2025-03-07T03:27:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gathe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gathe"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.10351v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v4",
                "updated": "2025-03-07T18:59:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    59,
                    21,
                    4,
                    66,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in evaluating\nsocial biases that may be present in the code produced by LLMs. To solve this\nissue, we propose a novel fairness framework, i.e., Solar, to assess and\nmitigate the social biases of LLM-generated code. Specifically, Solar can\nautomatically generate test cases for quantitatively uncovering social biases\nof the auto-generated code by LLMs. To quantify the severity of social biases\nin generated code, we develop a dataset that covers a diverse set of social\nproblems. We applied Solar and the crafted dataset to four state-of-the-art\nLLMs for code generation. Our evaluation reveals severe bias in the\nLLM-generated code from all the subject LLMs. Furthermore, we explore several\nprompting strategies for mitigating bias, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and dialogue with\nSolar. Our experiments show that dialogue with Solar can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Last, we make the code and data\npublicly available is highly extensible to evaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in evaluating\nsocial biases that may be present in the code produced by LLMs. To solve this\nissue, we propose a novel fairness framework, i.e., Solar, to assess and\nmitigate the social biases of LLM-generated code. Specifically, Solar can\nautomatically generate test cases for quantitatively uncovering social biases\nof the auto-generated code by LLMs. To quantify the severity of social biases\nin generated code, we develop a dataset that covers a diverse set of social\nproblems. We applied Solar and the crafted dataset to four state-of-the-art\nLLMs for code generation. Our evaluation reveals severe bias in the\nLLM-generated code from all the subject LLMs. Furthermore, we explore several\nprompting strategies for mitigating bias, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and dialogue with\nSolar. Our experiments show that dialogue with Solar can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Last, we make the code and data\npublicly available is highly extensible to evaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "accepted for publication in the Association for the Advancement of\n  Artificial Intelligence (AAAI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05691v1",
                "updated": "2025-03-07T18:57:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:57:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Reionization and the Hubble Constant: Correlations in the Cosmic\n  Microwave Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reionization and the Hubble Constant: Correlations in the Cosmic\n  Microwave Background"
                },
                "summary": "Recently, the James Webb Space Telescope (JWST) has found early galaxies\nproducing photons from more efficient ionization than previously assumed. This\nmay suggest a reionization process with a larger reionization optical depth,\n$\\tau_{\\rm reio}$, in some mild disagreement with that inferred from\nmeasurements of cosmic microwave background (CMB). Intriguingly, the CMB would\nprefer larger values of $\\tau_{\\rm reio}$, more consistent with the recent JWST\nhint, if the large-scale measurements (i.e. $\\ell <30$) of E-mode polarization\nare removed. In addition, $\\tau_{\\rm reio}$ has an indirect correlation with\ntoday's Hubble constant $H_0$ in $\\Lambda$CDM. Motivated by these interesting\nobservations, we investigate and reveal the underlying mechanism for this\ncorrelation, using the CMB dataset without the low-$\\ell$ polarization data as\na proxy for a potential cosmology with a larger $\\tau_{\\rm reio}$. We further\nexplore how this correlation may impact the Hubble tension between early and\nlate universe measurements of $H_0$, in $\\Lambda$CDM as well as two proposals\nto alleviate the Hubble tension: the dark radiation (DR) and early dark energy\n(EDE) models. We find that the Hubble tension gets further reduced mildly for\nalmost all cases due to the larger $\\tau_{\\rm reio}$ and its positive\ncorrelation with $H_0$, with either the Baryon Acoustic Oscillations (BAO) data\nbefore those from the Dark Energy Spectroscopic Instrument (DESI) or the DESI\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the James Webb Space Telescope (JWST) has found early galaxies\nproducing photons from more efficient ionization than previously assumed. This\nmay suggest a reionization process with a larger reionization optical depth,\n$\\tau_{\\rm reio}$, in some mild disagreement with that inferred from\nmeasurements of cosmic microwave background (CMB). Intriguingly, the CMB would\nprefer larger values of $\\tau_{\\rm reio}$, more consistent with the recent JWST\nhint, if the large-scale measurements (i.e. $\\ell <30$) of E-mode polarization\nare removed. In addition, $\\tau_{\\rm reio}$ has an indirect correlation with\ntoday's Hubble constant $H_0$ in $\\Lambda$CDM. Motivated by these interesting\nobservations, we investigate and reveal the underlying mechanism for this\ncorrelation, using the CMB dataset without the low-$\\ell$ polarization data as\na proxy for a potential cosmology with a larger $\\tau_{\\rm reio}$. We further\nexplore how this correlation may impact the Hubble tension between early and\nlate universe measurements of $H_0$, in $\\Lambda$CDM as well as two proposals\nto alleviate the Hubble tension: the dark radiation (DR) and early dark energy\n(EDE) models. We find that the Hubble tension gets further reduced mildly for\nalmost all cases due to the larger $\\tau_{\\rm reio}$ and its positive\ncorrelation with $H_0$, with either the Baryon Acoustic Oscillations (BAO) data\nbefore those from the Dark Energy Spectroscopic Instrument (DESI) or the DESI\ndata."
                },
                "authors": [
                    {
                        "name": "Itamar J. Allali"
                    },
                    {
                        "name": "Praniti Singh"
                    },
                    {
                        "name": "JiJi Fan"
                    },
                    {
                        "name": "Lingfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Li"
                },
                "author": "Lingfeng Li",
                "arxiv_comment": "15 pages, 5 figures, 4 tables, plus appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00226v2",
                "updated": "2025-03-07T18:48:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    48,
                    54,
                    4,
                    66,
                    0
                ],
                "published": "2024-05-31T23:05:04Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    23,
                    5,
                    4,
                    4,
                    152,
                    0
                ],
                "title": "Entangled Relations: Leveraging NLI and Meta-analysis to Enhance\n  Biomedical Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entangled Relations: Leveraging NLI and Meta-analysis to Enhance\n  Biomedical Relation Extraction"
                },
                "summary": "Recent research efforts have explored the potential of leveraging natural\nlanguage inference (NLI) techniques to enhance relation extraction (RE). In\nthis vein, we introduce MetaEntailRE, a novel adaptation method that harnesses\nNLI principles to enhance RE performance. Our approach follows past works by\nverbalizing relation classes into class-indicative hypotheses, aligning a\ntraditionally multi-class classification task to one of textual entailment. We\nintroduce three key enhancements: (1) Meta-class analysis which, instead of\nlabeling non-entailed premise-hypothesis pairs with the less informative\n\"neutral\" entailment label, provides additional context by analyzing\noverarching meta-relationships between classes; (2) Feasible hypothesis\nfiltering, which removes unlikely hypotheses from consideration based on domain\nknowledge derived from data; and (3) Group-based prediction selection, which\nfurther improves performance by selecting highly confident predictions.\nMetaEntailRE is conceptually simple and empirically powerful, yielding\nsignificant improvements over conventional relation extraction techniques and\nother NLI formulations. We observe surprisingly large F1 gains of 17.6 points\non BioRED and 13.4 points on ReTACRED compared to conventional methods,\nunderscoring the versatility of MetaEntailRE across both biomedical and general\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research efforts have explored the potential of leveraging natural\nlanguage inference (NLI) techniques to enhance relation extraction (RE). In\nthis vein, we introduce MetaEntailRE, a novel adaptation method that harnesses\nNLI principles to enhance RE performance. Our approach follows past works by\nverbalizing relation classes into class-indicative hypotheses, aligning a\ntraditionally multi-class classification task to one of textual entailment. We\nintroduce three key enhancements: (1) Meta-class analysis which, instead of\nlabeling non-entailed premise-hypothesis pairs with the less informative\n\"neutral\" entailment label, provides additional context by analyzing\noverarching meta-relationships between classes; (2) Feasible hypothesis\nfiltering, which removes unlikely hypotheses from consideration based on domain\nknowledge derived from data; and (3) Group-based prediction selection, which\nfurther improves performance by selecting highly confident predictions.\nMetaEntailRE is conceptually simple and empirically powerful, yielding\nsignificant improvements over conventional relation extraction techniques and\nother NLI formulations. We observe surprisingly large F1 gains of 17.6 points\non BioRED and 13.4 points on ReTACRED compared to conventional methods,\nunderscoring the versatility of MetaEntailRE across both biomedical and general\ndomains."
                },
                "authors": [
                    {
                        "name": "William Hogan"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "arxiv_comment": "17 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05683v1",
                "updated": "2025-03-07T18:45:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    45,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:45:42Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    45,
                    42,
                    4,
                    66,
                    0
                ],
                "title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Limits of Lifelong Knowledge Editing in LLMs"
                },
                "summary": "Keeping large language models factually up-to-date is crucial for deployment,\nyet costly retraining remains a challenge. Knowledge editing offers a promising\nalternative, but methods are only tested on small-scale or synthetic edit\nbenchmarks. In this work, we aim to bridge research into lifelong knowledge\nediting to real-world edits at practically relevant scale. We first introduce\nWikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to\nautomatically extend lifelong for future-proof benchmarking. In its first\ninstance, it includes over 500K question-answer pairs for knowledge editing\nalongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to\nstudy existing knowledge editing techniques' ability to incorporate large\nvolumes of real-world facts and contrast their capabilities to generic\nmodification techniques such as retrieval augmentation and continual finetuning\nto acquire a complete picture of the practical extent of current lifelong\nknowledge editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping large language models factually up-to-date is crucial for deployment,\nyet costly retraining remains a challenge. Knowledge editing offers a promising\nalternative, but methods are only tested on small-scale or synthetic edit\nbenchmarks. In this work, we aim to bridge research into lifelong knowledge\nediting to real-world edits at practically relevant scale. We first introduce\nWikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to\nautomatically extend lifelong for future-proof benchmarking. In its first\ninstance, it includes over 500K question-answer pairs for knowledge editing\nalongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to\nstudy existing knowledge editing techniques' ability to incorporate large\nvolumes of real-world facts and contrast their capabilities to generic\nmodification techniques such as retrieval augmentation and continual finetuning\nto acquire a complete picture of the practical extent of current lifelong\nknowledge editing."
                },
                "authors": [
                    {
                        "name": "Lukas Thede"
                    },
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10297v2",
                "updated": "2025-03-07T18:31:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    31,
                    55,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-14T16:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    59,
                    5,
                    4,
                    45,
                    0
                ],
                "title": "DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders"
                },
                "summary": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers."
                },
                "authors": [
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    },
                    {
                        "name": "Riccardo Grazzi"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Grazzi"
                },
                "author": "Riccardo Grazzi",
                "arxiv_comment": "Accepted at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05665v1",
                "updated": "2025-03-07T18:26:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    26,
                    48,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:26:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    26,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning\n  Biased Models with Contextual Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning\n  Biased Models with Contextual Synthetic Data"
                },
                "summary": "Recent advances in generative models have sparked research on improving model\nfairness with AI-generated data. However, existing methods often face\nlimitations in the diversity and quality of synthetic data, leading to\ncompromised fairness and overall model accuracy. Moreover, many approaches rely\non the availability of demographic group labels, which are often costly to\nannotate. This paper proposes AIM-Fair, aiming to overcome these limitations\nand harness the potential of cutting-edge generative models in promoting\nalgorithmic fairness. We investigate a fine-tuning paradigm starting from a\nbiased model initially trained on real-world data without demographic\nannotations. This model is then fine-tuned using unbiased synthetic data\ngenerated by a state-of-the-art diffusion model to improve its fairness. Two\nkey challenges are identified in this fine-tuning paradigm, 1) the low quality\nof synthetic data, which can still happen even with advanced generative models,\nand 2) the domain and bias gap between real and synthetic data. To address the\nlimitation of synthetic data quality, we propose Contextual Synthetic Data\nGeneration (CSDG) to generate data using a text-to-image diffusion model (T2I)\nwith prompts generated by a context-aware LLM, ensuring both data diversity and\ncontrol of bias in synthetic data. To resolve domain and bias shifts, we\nintroduce a novel selective fine-tuning scheme in which only model parameters\nmore sensitive to bias and less sensitive to domain shift are updated.\nExperiments on CelebA and UTKFace datasets show that our AIM-Fair improves\nmodel fairness while maintaining utility, outperforming both fully and\npartially fine-tuned approaches to model fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative models have sparked research on improving model\nfairness with AI-generated data. However, existing methods often face\nlimitations in the diversity and quality of synthetic data, leading to\ncompromised fairness and overall model accuracy. Moreover, many approaches rely\non the availability of demographic group labels, which are often costly to\nannotate. This paper proposes AIM-Fair, aiming to overcome these limitations\nand harness the potential of cutting-edge generative models in promoting\nalgorithmic fairness. We investigate a fine-tuning paradigm starting from a\nbiased model initially trained on real-world data without demographic\nannotations. This model is then fine-tuned using unbiased synthetic data\ngenerated by a state-of-the-art diffusion model to improve its fairness. Two\nkey challenges are identified in this fine-tuning paradigm, 1) the low quality\nof synthetic data, which can still happen even with advanced generative models,\nand 2) the domain and bias gap between real and synthetic data. To address the\nlimitation of synthetic data quality, we propose Contextual Synthetic Data\nGeneration (CSDG) to generate data using a text-to-image diffusion model (T2I)\nwith prompts generated by a context-aware LLM, ensuring both data diversity and\ncontrol of bias in synthetic data. To resolve domain and bias shifts, we\nintroduce a novel selective fine-tuning scheme in which only model parameters\nmore sensitive to bias and less sensitive to domain shift are updated.\nExperiments on CelebA and UTKFace datasets show that our AIM-Fair improves\nmodel fairness while maintaining utility, outperforming both fully and\npartially fine-tuned approaches to model fairness."
                },
                "authors": [
                    {
                        "name": "Zengqun Zhao"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted at CVPR 2025. Github:\n  https://github.com/zengqunzhao/AIM-Fair. Project page:\n  https://zengqunzhao.github.io/AIMFair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05659v1",
                "updated": "2025-03-07T18:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval"
                },
                "summary": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, search and recommendation systems (collectively referred\nto as information retrieval systems) have evolved significantly to address\nthese challenges. Recent advances in large language models (LLMs) have\ndemonstrated capabilities that surpass human performance in various\nlanguage-related tasks and exhibit general understanding, reasoning, and\ndecision-making abilities. This paper explores the transformative potential of\nlarge language model agents in enhancing search and recommendation systems. We\ndiscuss the motivations and roles of LLM agents, and establish a classification\nframework to elaborate on the existing research. We highlight the immense\npotential of LLM agents in addressing current challenges in search and\nrecommendation, providing insights into future research directions. This paper\nis the first to systematically review and classify the research on LLM agents\nin these domains, offering a novel perspective on leveraging this advanced AI\ntechnology for information retrieval. To help understand the existing works, we\nlist the existing papers on agent-based simulation with large language models\nat this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, search and recommendation systems (collectively referred\nto as information retrieval systems) have evolved significantly to address\nthese challenges. Recent advances in large language models (LLMs) have\ndemonstrated capabilities that surpass human performance in various\nlanguage-related tasks and exhibit general understanding, reasoning, and\ndecision-making abilities. This paper explores the transformative potential of\nlarge language model agents in enhancing search and recommendation systems. We\ndiscuss the motivations and roles of LLM agents, and establish a classification\nframework to elaborate on the existing research. We highlight the immense\npotential of LLM agents in addressing current challenges in search and\nrecommendation, providing insights into future research directions. This paper\nis the first to systematically review and classify the research on LLM agents\nin these domains, offering a novel perspective on leveraging this advanced AI\ntechnology for information retrieval. To help understand the existing works, we\nlist the existing papers on agent-based simulation with large language models\nat this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Tzu-Heng Lin"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04625v2",
                "updated": "2025-03-07T18:13:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    13,
                    22,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T17:11:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "START: Self-taught Reasoner with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "START: Self-taught Reasoner with Tools"
                },
                "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview."
                },
                "authors": [
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dayiheng Liu"
                },
                "author": "Dayiheng Liu",
                "arxiv_comment": "38 pages, 5 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05641v1",
                "updated": "2025-03-07T18:03:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    3,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:03:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    3,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning"
                },
                "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "The first three authors contributed equally. Project Page:\n  https://symbolic_moe.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05634v1",
                "updated": "2025-03-07T17:56:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    56,
                    44,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:56:44Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    56,
                    44,
                    4,
                    66,
                    0
                ],
                "title": "Integration of aggregated data in causally interpretable meta-analysis\n  by inverse weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of aggregated data in causally interpretable meta-analysis\n  by inverse weighting"
                },
                "summary": "Obtaining causally interpretable meta-analysis results is challenging when\nthere are differences in the distribution of effect modifiers between eligible\ntrials. To overcome this, recent work on transportability methods has\nconsidered standardizing results of individual studies over the case-mix of a\ntarget population, prior to pooling them as in a classical random-effect\nmeta-analysis. One practical challenge, however, is that case-mix\nstandardization often requires individual participant data (IPD) on outcome,\ntreatments and case-mix characteristics to be fully accessible in every\neligible study, along with IPD case-mix characteristics for a random sample\nfrom the target population. In this paper, we aim to develop novel strategies\nto integrate aggregated-level data from eligible trials with non-accessible IPD\ninto a causal meta-analysis, by extending moment-based methods frequently used\nfor population-adjusted indirect comparison in health technology assessment.\nSince valid inference for these moment-based methods by M-estimation theory\nrequires additional aggregated data that are often unavailable in practice,\ncomputational methods to address this concern are also developed. We assess the\nfinite-sample performance of the proposed approaches by simulated data, and\nthen apply these on real-world clinical data to investigate the effectiveness\nof risankizumab versus ustekinumab among patients with moderate to severe\npsoriasis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining causally interpretable meta-analysis results is challenging when\nthere are differences in the distribution of effect modifiers between eligible\ntrials. To overcome this, recent work on transportability methods has\nconsidered standardizing results of individual studies over the case-mix of a\ntarget population, prior to pooling them as in a classical random-effect\nmeta-analysis. One practical challenge, however, is that case-mix\nstandardization often requires individual participant data (IPD) on outcome,\ntreatments and case-mix characteristics to be fully accessible in every\neligible study, along with IPD case-mix characteristics for a random sample\nfrom the target population. In this paper, we aim to develop novel strategies\nto integrate aggregated-level data from eligible trials with non-accessible IPD\ninto a causal meta-analysis, by extending moment-based methods frequently used\nfor population-adjusted indirect comparison in health technology assessment.\nSince valid inference for these moment-based methods by M-estimation theory\nrequires additional aggregated data that are often unavailable in practice,\ncomputational methods to address this concern are also developed. We assess the\nfinite-sample performance of the proposed approaches by simulated data, and\nthen apply these on real-world clinical data to investigate the effectiveness\nof risankizumab versus ustekinumab among patients with moderate to severe\npsoriasis."
                },
                "authors": [
                    {
                        "name": "Tat-Thang Vo"
                    },
                    {
                        "name": "Tran Trong Khoi Le"
                    },
                    {
                        "name": "Sivem Afach"
                    },
                    {
                        "name": "Stijn Vansteelandt"
                    }
                ],
                "author_detail": {
                    "name": "Stijn Vansteelandt"
                },
                "author": "Stijn Vansteelandt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09952v2",
                "updated": "2025-03-07T17:48:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    48,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2024-04-15T17:25:14Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    25,
                    14,
                    0,
                    106,
                    0
                ],
                "title": "LLMorpheus: Mutation Testing using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMorpheus: Mutation Testing using Large Language Models"
                },
                "summary": "In mutation testing, the quality of a test suite is evaluated by introducing\nfaults into a program and determining whether the program's tests detect them.\nMost existing approaches for mutation testing involve the application of a\nfixed set of mutation operators, e.g., replacing a \"+\" with a \"-\", or removing\na function's body. However, certain types of real-world bugs cannot easily be\nsimulated by such approaches, limiting their effectiveness. This paper presents\na technique for mutation testing where placeholders are introduced at\ndesignated locations in a program's source code and where a Large Language\nModel (LLM) is prompted to ask what they could be replaced with. The technique\nis implemented in LLMorpheus, a mutation testing tool for JavaScript, and\nevaluated on 13 subject packages, considering several variations on the\nprompting strategy, and using several LLMs. We find LLMorpheus to be capable of\nproducing mutants that resemble existing bugs that cannot be produced by\nStrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the\nrunning time, cost, and number of mutants produced by LLMorpheus, demonstrating\nits practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mutation testing, the quality of a test suite is evaluated by introducing\nfaults into a program and determining whether the program's tests detect them.\nMost existing approaches for mutation testing involve the application of a\nfixed set of mutation operators, e.g., replacing a \"+\" with a \"-\", or removing\na function's body. However, certain types of real-world bugs cannot easily be\nsimulated by such approaches, limiting their effectiveness. This paper presents\na technique for mutation testing where placeholders are introduced at\ndesignated locations in a program's source code and where a Large Language\nModel (LLM) is prompted to ask what they could be replaced with. The technique\nis implemented in LLMorpheus, a mutation testing tool for JavaScript, and\nevaluated on 13 subject packages, considering several variations on the\nprompting strategy, and using several LLMs. We find LLMorpheus to be capable of\nproducing mutants that resemble existing bugs that cannot be produced by\nStrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the\nrunning time, cost, and number of mutants produced by LLMorpheus, demonstrating\nits practicality."
                },
                "authors": [
                    {
                        "name": "Frank Tip"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Max Schaefer"
                    }
                ],
                "author_detail": {
                    "name": "Max Schaefer"
                },
                "author": "Max Schaefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05620v1",
                "updated": "2025-03-07T17:46:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    46,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:46:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    46,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for\n  Utterance-level Understandings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for\n  Utterance-level Understandings"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc."
                },
                "authors": [
                    {
                        "name": "Xuanqing Liu"
                    },
                    {
                        "name": "Luyang Kong"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Afshin Khashei"
                    },
                    {
                        "name": "Belinda Zeng"
                    },
                    {
                        "name": "Steve Johnson"
                    },
                    {
                        "name": "Jon Jay"
                    },
                    {
                        "name": "Davor Golac"
                    },
                    {
                        "name": "Matt Pope"
                    }
                ],
                "author_detail": {
                    "name": "Matt Pope"
                },
                "author": "Matt Pope",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05618v1",
                "updated": "2025-03-07T17:42:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    42,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:42:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    42,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "Conformal Prediction for Image Segmentation Using Morphological\n  Prediction Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction for Image Segmentation Using Morphological\n  Prediction Sets"
                },
                "summary": "Image segmentation is a challenging task influenced by multiple sources of\nuncertainty, such as the data labeling process or the sampling of training\ndata. In this paper we focus on binary segmentation and address these\nchallenges using conformal prediction, a family of model- and data-agnostic\nmethods for uncertainty quantification that provide finite-sample theoretical\nguarantees and applicable to any pretrained predictor. Our approach involves\ncomputing nonconformity scores, a type of prediction residual, on held-out\ncalibration data not used during training. We use dilation, one of the\nfundamental operations in mathematical morphology, to construct a margin added\nto the borders of predicted segmentation masks. At inference, the predicted set\nformed by the mask and its margin contains the ground-truth mask with high\nprobability, at a confidence level specified by the user. The size of the\nmargin serves as an indicator of predictive uncertainty for a given model and\ndataset. We work in a regime of minimal information as we do not require any\nfeedback from the predictor: only the predicted masks are needed for computing\nthe prediction sets. Hence, our method is applicable to any segmentation model,\nincluding those based on deep learning; we evaluate our approach on several\nmedical imaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image segmentation is a challenging task influenced by multiple sources of\nuncertainty, such as the data labeling process or the sampling of training\ndata. In this paper we focus on binary segmentation and address these\nchallenges using conformal prediction, a family of model- and data-agnostic\nmethods for uncertainty quantification that provide finite-sample theoretical\nguarantees and applicable to any pretrained predictor. Our approach involves\ncomputing nonconformity scores, a type of prediction residual, on held-out\ncalibration data not used during training. We use dilation, one of the\nfundamental operations in mathematical morphology, to construct a margin added\nto the borders of predicted segmentation masks. At inference, the predicted set\nformed by the mask and its margin contains the ground-truth mask with high\nprobability, at a confidence level specified by the user. The size of the\nmargin serves as an indicator of predictive uncertainty for a given model and\ndataset. We work in a regime of minimal information as we do not require any\nfeedback from the predictor: only the predicted masks are needed for computing\nthe prediction sets. Hence, our method is applicable to any segmentation model,\nincluding those based on deep learning; we evaluate our approach on several\nmedical imaging applications."
                },
                "authors": [
                    {
                        "name": "Luca Mossina"
                    },
                    {
                        "name": "Corentin Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Corentin Friedrich"
                },
                "author": "Corentin Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05613v1",
                "updated": "2025-03-07T17:38:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:38:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05612v1",
                "updated": "2025-03-07T17:36:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    36,
                    31,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:36:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    36,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "The shape of FIREbox galaxies and a potential tension with low-mass\n  disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shape of FIREbox galaxies and a potential tension with low-mass\n  disks"
                },
                "summary": "We study the intrinsic and observable shapes of approximately 700\nstar-forming galaxies with stellar masses $10^8 - 10^{11}$ M$_\\odot$ from the\nFIREbox simulation at $z=0$. We calculate intrinsic axis ratios using inertia\ntensors weighted by: All Stars, Young Stars, and Luminosity-weighted Stars.\nYoung Stars, in particular, are arranged in systematically different 3D\nconfigurations as a function of galaxy stellar mass, with spheroidal,\nelongated, and disky shapes dominant at stellar masses of $10^{8.5}$ M$_\\odot$,\n$10^{9.5}$ M$_\\odot$, and $10^{10.5}$ M$_\\odot$, respectively. We construct\nmock images for each galaxy and show that projected short-to-long axis ratios,\n$q$, inferred from 2D S\\'ersic fits are most closely related to\nLuminosity-weighted tensor shapes and least resemble the All Stars shapes. This\nsuggests observed 2D shape distributions should not be compared to predictions\nbased on 3D stellar mass shapes. We construct a sample of mock images projected\nin random orientations and compare them to observed axis ratio distributions\nfrom the GAMA survey. For galaxies with stellar masses $10^{10} - 10^{11}$\nM$_\\odot$, we reproduce axis ratios comparable to the thinnest observed in real\ngalaxies ($q \\sim 0.1$), suggesting this model is capable of making thin disk\ngalaxies at Milky Way scale. However, at masses below $10^{10}$ M$_\\odot$, we\nproduce an insufficient population of galaxies with observed $q<0.4$ and none\nwith $q<0.2$, suggesting that FIREbox does not produce enough low-mass disk\ngalaxies. Future observational and theoretical programs aimed at understanding\nlow-mass disk fractions will provide crucial tests of galaxy formation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the intrinsic and observable shapes of approximately 700\nstar-forming galaxies with stellar masses $10^8 - 10^{11}$ M$_\\odot$ from the\nFIREbox simulation at $z=0$. We calculate intrinsic axis ratios using inertia\ntensors weighted by: All Stars, Young Stars, and Luminosity-weighted Stars.\nYoung Stars, in particular, are arranged in systematically different 3D\nconfigurations as a function of galaxy stellar mass, with spheroidal,\nelongated, and disky shapes dominant at stellar masses of $10^{8.5}$ M$_\\odot$,\n$10^{9.5}$ M$_\\odot$, and $10^{10.5}$ M$_\\odot$, respectively. We construct\nmock images for each galaxy and show that projected short-to-long axis ratios,\n$q$, inferred from 2D S\\'ersic fits are most closely related to\nLuminosity-weighted tensor shapes and least resemble the All Stars shapes. This\nsuggests observed 2D shape distributions should not be compared to predictions\nbased on 3D stellar mass shapes. We construct a sample of mock images projected\nin random orientations and compare them to observed axis ratio distributions\nfrom the GAMA survey. For galaxies with stellar masses $10^{10} - 10^{11}$\nM$_\\odot$, we reproduce axis ratios comparable to the thinnest observed in real\ngalaxies ($q \\sim 0.1$), suggesting this model is capable of making thin disk\ngalaxies at Milky Way scale. However, at masses below $10^{10}$ M$_\\odot$, we\nproduce an insufficient population of galaxies with observed $q<0.4$ and none\nwith $q<0.2$, suggesting that FIREbox does not produce enough low-mass disk\ngalaxies. Future observational and theoretical programs aimed at understanding\nlow-mass disk fractions will provide crucial tests of galaxy formation models."
                },
                "authors": [
                    {
                        "name": "Courtney Klein"
                    },
                    {
                        "name": "James S. Bullock"
                    },
                    {
                        "name": "Luke Xia"
                    },
                    {
                        "name": "Jorge Moreno"
                    },
                    {
                        "name": "Robert Feldmann"
                    },
                    {
                        "name": "Francisco J. Mercado"
                    },
                    {
                        "name": "Claude-André Faucher-Giguère"
                    },
                    {
                        "name": "Jonathan Stern"
                    },
                    {
                        "name": "N. Nicole Sanchez"
                    },
                    {
                        "name": "Abdelaziz Hussein"
                    }
                ],
                "author_detail": {
                    "name": "Abdelaziz Hussein"
                },
                "author": "Abdelaziz Hussein",
                "arxiv_comment": "16 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09433v3",
                "updated": "2025-03-07T17:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    33,
                    50,
                    4,
                    66,
                    0
                ],
                "published": "2023-04-19T06:00:26Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    6,
                    0,
                    26,
                    2,
                    109,
                    0
                ],
                "title": "Language Models Enable Simple Systems for Generating Structured Views of\n  Heterogeneous Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Enable Simple Systems for Generating Structured Views of\n  Heterogeneous Data Lakes"
                },
                "summary": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Brandon Yang"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Avanika Narayan"
                    },
                    {
                        "name": "Andrew Hojel"
                    },
                    {
                        "name": "Immanuel Trummer"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06825v2",
                "updated": "2025-03-07T17:33:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    33,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-09T23:16:43Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    23,
                    16,
                    43,
                    4,
                    40,
                    0
                ],
                "title": "Improved Generalizability of CNN Based Lane Detection in Challenging\n  Weather Using Adaptive Preprocessing Parameter Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Generalizability of CNN Based Lane Detection in Challenging\n  Weather Using Adaptive Preprocessing Parameter Tuning"
                },
                "summary": "Ensuring the robustness of lane detection systems is essential for the\nreliability of autonomous vehicles, particularly in the face of diverse weather\nconditions. While numerous algorithms have been proposed, addressing challenges\nposed by varying weather remains an ongoing issue. Geometric-based lane\ndetection methods, rooted in the inherent properties of road geometry, provide\nenhanced generalizability. However, these methods often require manual\nparameter tuning to accommodate it fluctuating illumination and weather\nconditions. Conversely, learning-based approaches, trained on pre-labeled\ndatasets, excel in localizing intricate and curved lane configurations but\ngrapple with the absence of diverse weather datasets. This paper introduces a\npromising hybrid approach that merges the strengths of both methodologies. A\nnovel adaptive preprocessing method is proposed in this work. Utilizing a fuzzy\ninference system (FIS), the algorithm dynamically adjusts parameters in\ngeometric-based image processing functions and enhances adaptability to diverse\nweather conditions. Notably, this preprocessing algorithm is designed to\nseamlessly integrate with all learning-based lane detection models. When\nimplemented in conjunction with CNN-based models, the hybrid approach\ndemonstrates commendable generalizability across weather conditions and\nadaptability to complex lane configurations. Rigorous testing on datasets\nfeaturing challenging weather conditions showcases the proposed method's\nsignificant improvements over existing models, underscoring its efficacy in\naddressing the persistent challenges associated with lane detection in adverse\nweather scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of lane detection systems is essential for the\nreliability of autonomous vehicles, particularly in the face of diverse weather\nconditions. While numerous algorithms have been proposed, addressing challenges\nposed by varying weather remains an ongoing issue. Geometric-based lane\ndetection methods, rooted in the inherent properties of road geometry, provide\nenhanced generalizability. However, these methods often require manual\nparameter tuning to accommodate it fluctuating illumination and weather\nconditions. Conversely, learning-based approaches, trained on pre-labeled\ndatasets, excel in localizing intricate and curved lane configurations but\ngrapple with the absence of diverse weather datasets. This paper introduces a\npromising hybrid approach that merges the strengths of both methodologies. A\nnovel adaptive preprocessing method is proposed in this work. Utilizing a fuzzy\ninference system (FIS), the algorithm dynamically adjusts parameters in\ngeometric-based image processing functions and enhances adaptability to diverse\nweather conditions. Notably, this preprocessing algorithm is designed to\nseamlessly integrate with all learning-based lane detection models. When\nimplemented in conjunction with CNN-based models, the hybrid approach\ndemonstrates commendable generalizability across weather conditions and\nadaptability to complex lane configurations. Rigorous testing on datasets\nfeaturing challenging weather conditions showcases the proposed method's\nsignificant improvements over existing models, underscoring its efficacy in\naddressing the persistent challenges associated with lane detection in adverse\nweather scenarios."
                },
                "authors": [
                    {
                        "name": "I-Chen Sang"
                    },
                    {
                        "name": "William R. Norris"
                    }
                ],
                "author_detail": {
                    "name": "William R. Norris"
                },
                "author": "William R. Norris",
                "arxiv_doi": "10.1016/j.eswa.2025.127055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2025.127055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.06825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated journal reference for paper acceptance | 26 pages, 6 figures",
                "arxiv_journal_ref": "Expert Systems with Applications, Volume 275, 25 May 2025, 127055",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05598v1",
                "updated": "2025-03-07T17:25:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    25,
                    25,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:25:25Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    25,
                    25,
                    4,
                    66,
                    0
                ],
                "title": "From Theory to Application: A Practical Introduction to Neural Operators\n  in Scientific Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Theory to Application: A Practical Introduction to Neural Operators\n  in Scientific Computing"
                },
                "summary": "This focused review explores a range of neural operator architectures for\napproximating solutions to parametric partial differential equations (PDEs),\nemphasizing high-level concepts and practical implementation strategies. The\nstudy covers foundational models such as Deep Operator Networks (DeepONet),\nPrincipal Component Analysis-based Neural Networks (PCANet), and Fourier Neural\nOperators (FNO), providing comparative insights into their core methodologies\nand performance. These architectures are demonstrated on two classical linear\nparametric PDEs: the Poisson equation and linear elastic deformation. Beyond\nforward problem-solving, the review delves into applying neural operators as\nsurrogates in Bayesian inference problems, showcasing their effectiveness in\naccelerating posterior inference while maintaining accuracy. The paper\nconcludes by discussing current challenges, particularly in controlling\nprediction accuracy and generalization. It outlines emerging strategies to\naddress these issues, such as residual-based error correction and multi-level\ntraining. This review can be seen as a comprehensive guide to implementing\nneural operators and integrating them into scientific computing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This focused review explores a range of neural operator architectures for\napproximating solutions to parametric partial differential equations (PDEs),\nemphasizing high-level concepts and practical implementation strategies. The\nstudy covers foundational models such as Deep Operator Networks (DeepONet),\nPrincipal Component Analysis-based Neural Networks (PCANet), and Fourier Neural\nOperators (FNO), providing comparative insights into their core methodologies\nand performance. These architectures are demonstrated on two classical linear\nparametric PDEs: the Poisson equation and linear elastic deformation. Beyond\nforward problem-solving, the review delves into applying neural operators as\nsurrogates in Bayesian inference problems, showcasing their effectiveness in\naccelerating posterior inference while maintaining accuracy. The paper\nconcludes by discussing current challenges, particularly in controlling\nprediction accuracy and generalization. It outlines emerging strategies to\naddress these issues, such as residual-based error correction and multi-level\ntraining. This review can be seen as a comprehensive guide to implementing\nneural operators and integrating them into scientific computing workflows."
                },
                "authors": [
                    {
                        "name": "Prashant K. Jha"
                    }
                ],
                "author_detail": {
                    "name": "Prashant K. Jha"
                },
                "author": "Prashant K. Jha",
                "arxiv_comment": "53 pages, 17 figures, Github repository:\n  https://github.com/CEADpx/neural_operators",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M45, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16976v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16976v3",
                "updated": "2025-03-07T17:24:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    24,
                    35,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-23T06:22:49Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    6,
                    22,
                    49,
                    6,
                    175,
                    0
                ],
                "title": "Efficient Evolutionary Search Over Chemical Space with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Evolutionary Search Over Chemical Space with Large Language\n  Models"
                },
                "summary": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO"
                },
                "authors": [
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Cher-Tian Ser"
                    },
                    {
                        "name": "Wenhao Gao"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Felix Strieth-Kalthoff"
                    },
                    {
                        "name": "Chenru Duan"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Kirill Neklyudov"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16976v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16976v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07991v2",
                "updated": "2025-03-07T17:15:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    15,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-11T22:18:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    22,
                    18,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "Exact Simulation of Longitudinal Data from Marginal Structural Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact Simulation of Longitudinal Data from Marginal Structural Models"
                },
                "summary": "Simulating longitudinal data from specified marginal structural models is a\ncrucial but challenging task for evaluating causal inference methods and\nclinical trial design. While data generation typically proceeds in a fully\nconditional manner using structural equations according to a temporal ordering,\nit is difficult to ensure alignment between conditional distributions and the\ntarget marginal causal effects. This misalignment presents a fundamental\nchallenge in simulating data that adheres to marginal structural model\nspecifications. To address this, we propose a flexible and efficient algorithm\nfor simulating longitudinal data that adheres exactly to a specified marginal\nstructural model. Recognizing the importance of time-to-event outcomes in\nclinical research, we extend our approach to accommodate survival models.\nCompared to existing approaches, our method offers several advantages: it\nenables exact simulation from a known causal model rather than relying on\napproximations; avoids restrictive assumptions about the data-generating\nprocess; and remains computationally efficient by requiring only the evaluation\nof analytic functions. This last benefit contrasts with methods that use\ncomputationally intensive techniques such as Monte Carlo approximations or\nnumerical integration. Through simulation studies replicating realistic\nscenarios, we validate the method's accuracy and utility. Our method will\nfacilitate researchers in effectively simulating data with target causal\nstructures for their specific scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating longitudinal data from specified marginal structural models is a\ncrucial but challenging task for evaluating causal inference methods and\nclinical trial design. While data generation typically proceeds in a fully\nconditional manner using structural equations according to a temporal ordering,\nit is difficult to ensure alignment between conditional distributions and the\ntarget marginal causal effects. This misalignment presents a fundamental\nchallenge in simulating data that adheres to marginal structural model\nspecifications. To address this, we propose a flexible and efficient algorithm\nfor simulating longitudinal data that adheres exactly to a specified marginal\nstructural model. Recognizing the importance of time-to-event outcomes in\nclinical research, we extend our approach to accommodate survival models.\nCompared to existing approaches, our method offers several advantages: it\nenables exact simulation from a known causal model rather than relying on\napproximations; avoids restrictive assumptions about the data-generating\nprocess; and remains computationally efficient by requiring only the evaluation\nof analytic functions. This last benefit contrasts with methods that use\ncomputationally intensive techniques such as Monte Carlo approximations or\nnumerical integration. Through simulation studies replicating realistic\nscenarios, we validate the method's accuracy and utility. Our method will\nfacilitate researchers in effectively simulating data with target causal\nstructures for their specific scenarios."
                },
                "authors": [
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Daniel de Vassimon Manela"
                    },
                    {
                        "name": "Chase Mathis"
                    },
                    {
                        "name": "Jens Magelund Tarp"
                    },
                    {
                        "name": "Robin J. Evans"
                    }
                ],
                "author_detail": {
                    "name": "Robin J. Evans"
                },
                "author": "Robin J. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05592v1",
                "updated": "2025-03-07T17:14:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    14,
                    44,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:14:44Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    14,
                    44,
                    4,
                    66,
                    0
                ],
                "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05587v1",
                "updated": "2025-03-07T17:11:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    11,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:11:34Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    11,
                    34,
                    4,
                    66,
                    0
                ],
                "title": "Quantifying the Robustness of Retrieval-Augmented Language Models\n  Against Spurious Features in Grounding Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Robustness of Retrieval-Augmented Language Models\n  Against Spurious Features in Grounding Data"
                },
                "summary": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$."
                },
                "authors": [
                    {
                        "name": "Shiping Yang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Wenbiao Ding"
                    },
                    {
                        "name": "Ning Wu"
                    },
                    {
                        "name": "Shining Liang"
                    },
                    {
                        "name": "Ming Gong"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19492v2",
                "updated": "2025-03-07T17:08:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    8,
                    45,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-25T11:49:40Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    49,
                    40,
                    4,
                    299,
                    0
                ],
                "title": "TRADE: Transfer of Distributions between External Conditions with\n  Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRADE: Transfer of Distributions between External Conditions with\n  Normalizing Flows"
                },
                "summary": "Modeling distributions that depend on external control parameters is a common\nscenario in diverse applications like molecular simulations, where system\nproperties like temperature affect molecular configurations. Despite the\nrelevance of these applications, existing solutions are unsatisfactory as they\nrequire severely restricted model architectures or rely on energy-based\ntraining, which is prone to instability. We introduce TRADE, which overcomes\nthese limitations by formulating the learning process as a boundary value\nproblem. By initially training the model for a specific condition using either\ni.i.d.~samples or backward KL training, we establish a boundary distribution.\nWe then propagate this information across other conditions using the gradient\nof the unnormalized density with respect to the external parameter. This\nformulation, akin to the principles of physics-informed neural networks, allows\nus to efficiently learn parameter-dependent distributions without restrictive\nassumptions. Experimentally, we demonstrate that TRADE achieves excellent\nresults in a wide range of applications, ranging from Bayesian inference and\nmolecular simulations to physical lattice models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling distributions that depend on external control parameters is a common\nscenario in diverse applications like molecular simulations, where system\nproperties like temperature affect molecular configurations. Despite the\nrelevance of these applications, existing solutions are unsatisfactory as they\nrequire severely restricted model architectures or rely on energy-based\ntraining, which is prone to instability. We introduce TRADE, which overcomes\nthese limitations by formulating the learning process as a boundary value\nproblem. By initially training the model for a specific condition using either\ni.i.d.~samples or backward KL training, we establish a boundary distribution.\nWe then propagate this information across other conditions using the gradient\nof the unnormalized density with respect to the external parameter. This\nformulation, akin to the principles of physics-informed neural networks, allows\nus to efficiently learn parameter-dependent distributions without restrictive\nassumptions. Experimentally, we demonstrate that TRADE achieves excellent\nresults in a wide range of applications, ranging from Bayesian inference and\nmolecular simulations to physical lattice models."
                },
                "authors": [
                    {
                        "name": "Stefan Wahl"
                    },
                    {
                        "name": "Armand Rousselot"
                    },
                    {
                        "name": "Felix Draxler"
                    },
                    {
                        "name": "Henrik Schopmans"
                    },
                    {
                        "name": "Ullrich Köthe"
                    }
                ],
                "author_detail": {
                    "name": "Ullrich Köthe"
                },
                "author": "Ullrich Köthe",
                "arxiv_comment": "Accepted as Poster at AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12740v3",
                "updated": "2025-03-07T17:08:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    8,
                    25,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-16T16:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    59,
                    56,
                    2,
                    290,
                    0
                ],
                "title": "Just Ramp-up: Unleash the Potential of Regression-based Estimator for\n  A/B Tests under Network Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Ramp-up: Unleash the Potential of Regression-based Estimator for\n  A/B Tests under Network Interference"
                },
                "summary": "Recent research in causal inference under network interference has explored\nvarious experimental designs and estimation techniques to address this issue.\nHowever, existing methods, which typically rely on single experiments, often\nreach a performance bottleneck and face limitations in handling diverse\ninterference structures. In contrast, we propose leveraging multiple\nexperiments to overcome these limitations. In industry, the use of sequential\nexperiments, often known as the ramp-up process, where traffic to the treatment\ngradually increases, is common due to operational needs like risk management\nand cost control. Our approach shifts the focus from operational aspects to the\nstatistical advantages of merging data from multiple experiments. By combining\ndata from sequentially conducted experiments, we aim to estimate the global\naverage treatment effect more effectively. In this paper, we begin by analyzing\nthe bias and variance of the linear regression estimator for GATE under general\nlinear network interference. We demonstrate that bias plays a dominant role in\nthe bias-variance tradeoff and highlight the intrinsic bias reduction achieved\nby merging data from experiments with strictly different treatment proportions.\nHerein the improvement introduced by merging two steps of experimental data is\nessential. In addition, we show that merging more steps of experimental data is\nunnecessary under general linear interference, while it can become beneficial\nwhen nonlinear interference occurs. Furthermore, we look into a more advanced\nestimator based on graph neural networks. Through extensive simulation studies,\nwe show that the regression-based estimator benefits remarkably from training\non merged experiment data, achieving outstanding statistical performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in causal inference under network interference has explored\nvarious experimental designs and estimation techniques to address this issue.\nHowever, existing methods, which typically rely on single experiments, often\nreach a performance bottleneck and face limitations in handling diverse\ninterference structures. In contrast, we propose leveraging multiple\nexperiments to overcome these limitations. In industry, the use of sequential\nexperiments, often known as the ramp-up process, where traffic to the treatment\ngradually increases, is common due to operational needs like risk management\nand cost control. Our approach shifts the focus from operational aspects to the\nstatistical advantages of merging data from multiple experiments. By combining\ndata from sequentially conducted experiments, we aim to estimate the global\naverage treatment effect more effectively. In this paper, we begin by analyzing\nthe bias and variance of the linear regression estimator for GATE under general\nlinear network interference. We demonstrate that bias plays a dominant role in\nthe bias-variance tradeoff and highlight the intrinsic bias reduction achieved\nby merging data from experiments with strictly different treatment proportions.\nHerein the improvement introduced by merging two steps of experimental data is\nessential. In addition, we show that merging more steps of experimental data is\nunnecessary under general linear interference, while it can become beneficial\nwhen nonlinear interference occurs. Furthermore, we look into a more advanced\nestimator based on graph neural networks. Through extensive simulation studies,\nwe show that the regression-based estimator benefits remarkably from training\non merged experiment data, achieving outstanding statistical performance."
                },
                "authors": [
                    {
                        "name": "Qianyi Chen"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02355v3",
                "updated": "2025-03-07T17:06:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    6,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-03T10:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    6,
                    27,
                    3,
                    277,
                    0
                ],
                "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Shi Jie"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_journal_ref": "13th International Conference on Learning Representations (ICLR\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.05421v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.05421v4",
                "updated": "2025-03-07T16:58:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    58,
                    50,
                    4,
                    66,
                    0
                ],
                "published": "2021-06-09T22:27:11Z",
                "published_parsed": [
                    2021,
                    6,
                    9,
                    22,
                    27,
                    11,
                    2,
                    160,
                    0
                ],
                "title": "Data-Driven Invariant Learning for Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Invariant Learning for Probabilistic Programs"
                },
                "summary": "Morgan and McIver's weakest pre-expectation framework is one of the most\nwell-established methods for deductive verification of probabilistic programs.\nRoughly, the idea is to generalize binary state assertions to real-valued\nexpectations, which can measure expected values of probabilistic program\nquantities. While loop-free programs can be analyzed by mechanically\ntransforming expectations, verifying loops usually requires finding an\ninvariant expectation, a difficult task. We propose a new view of invariant\nexpectation synthesis as a regression problem: given an input state, predict\nthe average value of the post-expectation in the output distribution. Guided by\nthis perspective, we develop the first data-driven invariant synthesis method\nfor probabilistic programs. Unlike prior work on probabilistic invariant\ninference, our approach can learn piecewise continuous invariants without\nrelying on template expectations, and also works with black-box access to the\nprogram. We also develop a data-driven approach to learn sub-invariants from\ndata, which can be used to upper- or lower-bound expected values. We implement\nour approaches and demonstrate their effectiveness on a variety of benchmarks\nfrom the probabilistic programming literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morgan and McIver's weakest pre-expectation framework is one of the most\nwell-established methods for deductive verification of probabilistic programs.\nRoughly, the idea is to generalize binary state assertions to real-valued\nexpectations, which can measure expected values of probabilistic program\nquantities. While loop-free programs can be analyzed by mechanically\ntransforming expectations, verifying loops usually requires finding an\ninvariant expectation, a difficult task. We propose a new view of invariant\nexpectation synthesis as a regression problem: given an input state, predict\nthe average value of the post-expectation in the output distribution. Guided by\nthis perspective, we develop the first data-driven invariant synthesis method\nfor probabilistic programs. Unlike prior work on probabilistic invariant\ninference, our approach can learn piecewise continuous invariants without\nrelying on template expectations, and also works with black-box access to the\nprogram. We also develop a data-driven approach to learn sub-invariants from\ndata, which can be used to upper- or lower-bound expected values. We implement\nour approaches and demonstrate their effectiveness on a variety of benchmarks\nfrom the probabilistic programming literature."
                },
                "authors": [
                    {
                        "name": "Jialu Bao"
                    },
                    {
                        "name": "Nitesh Trivedi"
                    },
                    {
                        "name": "Drashti Pathak"
                    },
                    {
                        "name": "Justin Hsu"
                    },
                    {
                        "name": "Subhajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Roy"
                },
                "author": "Subhajit Roy",
                "arxiv_doi": "10.1007/s10703-024-00466-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10703-024-00466-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2106.05421v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.05421v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "37 pages",
                "arxiv_journal_ref": "Formal Methods in System Design 2024 (CAV Collection)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05570v1",
                "updated": "2025-03-07T16:53:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    53,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:53:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    53,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "Search for primordial black holes from gravitational wave populations\n  using deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for primordial black holes from gravitational wave populations\n  using deep learning"
                },
                "summary": "Gravitational waves (GWs) signals detected by the LIGO/Virgo/KAGRA\ncollaboration might be sourced (partly) by the merges of primordial black holes\n(PBHs). The conventional hierarchical Bayesian inference methods can allow us\nto study population properties of GW events to search for the hints for PBHs.\nHowever, hierarchical Bayesian analysis require an analytic population model,\nand becomes increasingly computationally expensive as the number of sources\ngrows. In this paper, we present a novel population analysis method based on\ndeep learning, which enables the direct and efficient estimation of PBH\npopulation hyperparameters, such as the PBH fraction in dark matter, $f_{\\rm\nPBH}$. Our approach leverages neural posterior estimation combined with\nconditional normalizing flows and two embedding networks. Our results\ndemonstrate that inference can be performed within seconds, highlighting the\npromise of deep learning as a powerful tool for population inference with an\nincreasing number of GW signals for next-generation detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) signals detected by the LIGO/Virgo/KAGRA\ncollaboration might be sourced (partly) by the merges of primordial black holes\n(PBHs). The conventional hierarchical Bayesian inference methods can allow us\nto study population properties of GW events to search for the hints for PBHs.\nHowever, hierarchical Bayesian analysis require an analytic population model,\nand becomes increasingly computationally expensive as the number of sources\ngrows. In this paper, we present a novel population analysis method based on\ndeep learning, which enables the direct and efficient estimation of PBH\npopulation hyperparameters, such as the PBH fraction in dark matter, $f_{\\rm\nPBH}$. Our approach leverages neural posterior estimation combined with\nconditional normalizing flows and two embedding networks. Our results\ndemonstrate that inference can be performed within seconds, highlighting the\npromise of deep learning as a powerful tool for population inference with an\nincreasing number of GW signals for next-generation detectors."
                },
                "authors": [
                    {
                        "name": "Hai-Long Huang"
                    },
                    {
                        "name": "Jun-Qian Jiang"
                    },
                    {
                        "name": "Jibin He"
                    },
                    {
                        "name": "Yu-Tong Wang"
                    },
                    {
                        "name": "Yun-Song Piao"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Song Piao"
                },
                "author": "Yun-Song Piao",
                "arxiv_comment": "34 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05565v1",
                "updated": "2025-03-07T16:45:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    45,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:45:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    45,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Evaluating open-source Large Language Models for automated fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating open-source Large Language Models for automated fact-checking"
                },
                "summary": "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers."
                },
                "authors": [
                    {
                        "name": "Nicolo' Fontana"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Enrico Zuccolotto"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "arxiv_comment": "Main: 10 pages, 13 figures. Supplementary Materials: 7 pages, 29\n  figures, 1 table ### This work has been submitted to the IEEE for possible\n  publication. ###",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17975v3",
                "updated": "2025-03-07T16:30:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    30,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-25T23:12:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    23,
                    12,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)"
                },
                "summary": "Whether LLMs memorize their training data and what this means, from measuring\nprivacy leakage to detecting copyright violations, has become a rapidly growing\narea of research. In the last few months, more than 10 new methods have been\nproposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary\nto traditional MIAs which rely on fixed-but randomized-records or models, these\nmethods are mostly trained and tested on datasets collected post-hoc. Sets of\nmembers and non-members, used to evaluate the MIA, are constructed using\ninformed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In\nthis work, we first extensively review the literature on MIAs against LLMs and\nshow that, while most work focuses on sequence-level MIAs evaluated in post-hoc\nsetups, a range of target models, motivations and units of interest are\nconsidered. We then quantify distribution shifts present in 6 datasets used in\nthe literature using a model-less bag of word classifier and show that all\ndatasets constructed post-hoc suffer from strong distribution shifts. These\nshifts invalidate the claims of LLMs memorizing strongly in real-world\nscenarios and, potentially, also the methodological contributions of the recent\npapers based on these datasets. Yet, all hope might not be lost. We introduce\nimportant considerations to properly evaluate MIAs against LLMs and discuss, in\nturn, potential ways forwards: randomized test splits, injections of randomized\n(unique) sequences, randomized fine-tuning, and several post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide MIA development and\nstudy LLM memorization. We conclude with an overview of recommended approaches\nto benchmark sequence-level and document-level MIAs against LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether LLMs memorize their training data and what this means, from measuring\nprivacy leakage to detecting copyright violations, has become a rapidly growing\narea of research. In the last few months, more than 10 new methods have been\nproposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary\nto traditional MIAs which rely on fixed-but randomized-records or models, these\nmethods are mostly trained and tested on datasets collected post-hoc. Sets of\nmembers and non-members, used to evaluate the MIA, are constructed using\ninformed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In\nthis work, we first extensively review the literature on MIAs against LLMs and\nshow that, while most work focuses on sequence-level MIAs evaluated in post-hoc\nsetups, a range of target models, motivations and units of interest are\nconsidered. We then quantify distribution shifts present in 6 datasets used in\nthe literature using a model-less bag of word classifier and show that all\ndatasets constructed post-hoc suffer from strong distribution shifts. These\nshifts invalidate the claims of LLMs memorizing strongly in real-world\nscenarios and, potentially, also the methodological contributions of the recent\npapers based on these datasets. Yet, all hope might not be lost. We introduce\nimportant considerations to properly evaluate MIAs against LLMs and discuss, in\nturn, potential ways forwards: randomized test splits, injections of randomized\n(unique) sequences, randomized fine-tuning, and several post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide MIA development and\nstudy LLM memorization. We conclude with an overview of recommended approaches\nto benchmark sequence-level and document-level MIAs against LLMs."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Shubham Jain"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "arxiv_comment": "IEEE Conference on Secure and Trustworthy Machine Learning (SaTML\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05551v1",
                "updated": "2025-03-07T16:25:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    25,
                    9,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:25:09Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    25,
                    9,
                    4,
                    66,
                    0
                ],
                "title": "Revitalizing Saturated Benchmarks: A Weighted Metric Approach for\n  Differentiating Large Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revitalizing Saturated Benchmarks: A Weighted Metric Approach for\n  Differentiating Large Language Model Performance"
                },
                "summary": "Existing benchmarks are becoming saturated and struggle to separate model\nperformances due to factors like data contamination and advancing LLM\ncapabilities. This paper introduces EMDM (Enhanced Model Differentiation\nMetric), a novel weighted metric that revitalizes benchmarks by enhancing model\nseparation. EMDM integrates final answer and Chain-of-Thought (CoT) reasoning\ncorrectness, assigning weights based on the complexity and reasoning depth\nrequired to solve a given sample in the evaluation data. Using a baseline LLM\nin two setups-Unguided, where the model has no prior exposure to test samples,\nand Guided, where the model has prior knowledge of the desired answer-EMDM\ndistinguishes instances of varying difficulty. The CoT and answer correctness\nfrom these setups inform an optimization objective for weight assignment,\nresulting in a more nuanced evaluation of model performance. Compared to the\nexact match (EM) metric, which achieves 17% separation on ARC-Challenge, EMDM\nachieves 46%, demonstrating its effectiveness in differentiating models based\non reasoning and knowledge requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks are becoming saturated and struggle to separate model\nperformances due to factors like data contamination and advancing LLM\ncapabilities. This paper introduces EMDM (Enhanced Model Differentiation\nMetric), a novel weighted metric that revitalizes benchmarks by enhancing model\nseparation. EMDM integrates final answer and Chain-of-Thought (CoT) reasoning\ncorrectness, assigning weights based on the complexity and reasoning depth\nrequired to solve a given sample in the evaluation data. Using a baseline LLM\nin two setups-Unguided, where the model has no prior exposure to test samples,\nand Guided, where the model has prior knowledge of the desired answer-EMDM\ndistinguishes instances of varying difficulty. The CoT and answer correctness\nfrom these setups inform an optimization objective for weight assignment,\nresulting in a more nuanced evaluation of model performance. Compared to the\nexact match (EM) metric, which achieves 17% separation on ARC-Challenge, EMDM\nachieves 46%, demonstrating its effectiveness in differentiating models based\non reasoning and knowledge requirements."
                },
                "authors": [
                    {
                        "name": "Bryan Etzine"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Nishanth Madhusudhan"
                    },
                    {
                        "name": "Sagar Davasam"
                    },
                    {
                        "name": "Roshnee Sharma"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Vikas Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Yadav"
                },
                "author": "Vikas Yadav",
                "arxiv_comment": "conference NAACL, TrustNLP Workshop",
                "arxiv_journal_ref": "TrustNLP workshop NAACL, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05543v1",
                "updated": "2025-03-07T16:15:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    15,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:15:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    15,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information"
                },
                "summary": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance."
                },
                "authors": [
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Jiayu Sun"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01565v2",
                "updated": "2025-03-07T16:08:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    8,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-03T14:09:36Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    9,
                    36,
                    0,
                    62,
                    0
                ],
                "title": "AutoLUT: LUT-Based Image Super-Resolution with Automatic Sampling and\n  Adaptive Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLUT: LUT-Based Image Super-Resolution with Automatic Sampling and\n  Adaptive Residual Learning"
                },
                "summary": "In recent years, the increasing popularity of Hi-DPI screens has driven a\nrising demand for high-resolution images. However, the limited computational\npower of edge devices poses a challenge in deploying complex super-resolution\nneural networks, highlighting the need for efficient methods. While prior works\nhave made significant progress, they have not fully exploited pixel-level\ninformation. Moreover, their reliance on fixed sampling patterns limits both\naccuracy and the ability to capture fine details in low-resolution images. To\naddress these challenges, we introduce two plug-and-play modules designed to\ncapture and leverage pixel information effectively in Look-Up Table (LUT) based\nsuper-resolution networks. Our method introduces Automatic Sampling\n(AutoSample), a flexible LUT sampling approach where sampling weights are\nautomatically learned during training to adapt to pixel variations and expand\nthe receptive field without added inference cost. We also incorporate Adaptive\nResidual Learning (AdaRL) to enhance inter-layer connections, enabling detailed\ninformation flow and improving the network's ability to reconstruct fine\ndetails. Our method achieves significant performance improvements on both MuLUT\nand SPF-LUT while maintaining similar storage sizes. Specifically, for MuLUT,\nwe achieve a PSNR improvement of approximately +0.20 dB improvement on average\nacross five datasets. For SPF-LUT, with more than a 50% reduction in storage\nspace and about a 2/3 reduction in inference time, our method still maintains\nperformance comparable to the original. The code is available at\nhttps://github.com/SuperKenVery/AutoLUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the increasing popularity of Hi-DPI screens has driven a\nrising demand for high-resolution images. However, the limited computational\npower of edge devices poses a challenge in deploying complex super-resolution\nneural networks, highlighting the need for efficient methods. While prior works\nhave made significant progress, they have not fully exploited pixel-level\ninformation. Moreover, their reliance on fixed sampling patterns limits both\naccuracy and the ability to capture fine details in low-resolution images. To\naddress these challenges, we introduce two plug-and-play modules designed to\ncapture and leverage pixel information effectively in Look-Up Table (LUT) based\nsuper-resolution networks. Our method introduces Automatic Sampling\n(AutoSample), a flexible LUT sampling approach where sampling weights are\nautomatically learned during training to adapt to pixel variations and expand\nthe receptive field without added inference cost. We also incorporate Adaptive\nResidual Learning (AdaRL) to enhance inter-layer connections, enabling detailed\ninformation flow and improving the network's ability to reconstruct fine\ndetails. Our method achieves significant performance improvements on both MuLUT\nand SPF-LUT while maintaining similar storage sizes. Specifically, for MuLUT,\nwe achieve a PSNR improvement of approximately +0.20 dB improvement on average\nacross five datasets. For SPF-LUT, with more than a 50% reduction in storage\nspace and about a 2/3 reduction in inference time, our method still maintains\nperformance comparable to the original. The code is available at\nhttps://github.com/SuperKenVery/AutoLUT."
                },
                "authors": [
                    {
                        "name": "Yuheng Xu"
                    },
                    {
                        "name": "Shijie Yang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Gangshan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Gangshan Wu"
                },
                "author": "Gangshan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03554v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03554v3",
                "updated": "2025-03-07T16:05:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    5,
                    19,
                    4,
                    66,
                    0
                ],
                "published": "2024-11-05T23:26:10Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    26,
                    10,
                    1,
                    310,
                    0
                ],
                "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial\n  Identity Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vision Language Model Unlearning via Fictitious Facial\n  Identity Dataset"
                },
                "summary": "Machine unlearning has emerged as an effective strategy for forgetting\nspecific information in the training data. However, with the increasing\nintegration of visual data, privacy concerns in Vision Language Models (VLMs)\nremain underexplored. To address this, we introduce Facial Identity Unlearning\nBenchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly\nevaluate the effectiveness of unlearning algorithms under the Right to be\nForgotten setting. Specifically, we formulate the VLM unlearning task via\nconstructing the Fictitious Facial Identity VQA dataset and apply a two-stage\nevaluation pipeline that is designed to precisely control the sources of\ninformation and their exposure levels. In terms of evaluation, since VLM\nsupports various forms of ways to ask questions with the same semantic meaning,\nwe also provide robust evaluation metrics including membership inference\nattacks and carefully designed adversarial privacy attacks to evaluate the\nperformance of algorithms. Through the evaluation of four baseline VLM\nunlearning algorithms within FIUBench, we find that all methods remain limited\nin their unlearning performance, with significant trade-offs between model\nutility and forget quality. Furthermore, our findings also highlight the\nimportance of privacy attacks for robust evaluations. We hope FIUBench will\ndrive progress in developing more effective VLM unlearning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning has emerged as an effective strategy for forgetting\nspecific information in the training data. However, with the increasing\nintegration of visual data, privacy concerns in Vision Language Models (VLMs)\nremain underexplored. To address this, we introduce Facial Identity Unlearning\nBenchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly\nevaluate the effectiveness of unlearning algorithms under the Right to be\nForgotten setting. Specifically, we formulate the VLM unlearning task via\nconstructing the Fictitious Facial Identity VQA dataset and apply a two-stage\nevaluation pipeline that is designed to precisely control the sources of\ninformation and their exposure levels. In terms of evaluation, since VLM\nsupports various forms of ways to ask questions with the same semantic meaning,\nwe also provide robust evaluation metrics including membership inference\nattacks and carefully designed adversarial privacy attacks to evaluate the\nperformance of algorithms. Through the evaluation of four baseline VLM\nunlearning algorithms within FIUBench, we find that all methods remain limited\nin their unlearning performance, with significant trade-offs between model\nutility and forget quality. Furthermore, our findings also highlight the\nimportance of privacy attacks for robust evaluations. We hope FIUBench will\ndrive progress in developing more effective VLM unlearning algorithms."
                },
                "authors": [
                    {
                        "name": "Yingzi Ma"
                    },
                    {
                        "name": "Jiongxiao Wang"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Siyuan Ma"
                    },
                    {
                        "name": "Jiazhao Li"
                    },
                    {
                        "name": "Jinsheng Pan"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03554v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03554v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05529v1",
                "updated": "2025-03-07T15:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    49,
                    56,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    49,
                    56,
                    4,
                    66,
                    0
                ],
                "title": "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs"
                },
                "summary": "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods."
                },
                "authors": [
                    {
                        "name": "Roberto Cerina"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Cerina"
                },
                "author": "Roberto Cerina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16305v2",
                "updated": "2025-03-07T15:48:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    48,
                    5,
                    4,
                    66,
                    0
                ],
                "published": "2023-12-26T19:24:47Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    19,
                    24,
                    47,
                    1,
                    360,
                    0
                ],
                "title": "Bayesian framework to infer the Hubble constant from the\n  cross-correlation of individual gravitational wave events with galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian framework to infer the Hubble constant from the\n  cross-correlation of individual gravitational wave events with galaxies"
                },
                "summary": "Gravitational waves (GWs) from the inspiral of binary compact objects offer a\none-step measurement of the luminosity distance to the event, which is\nessential for the measurement of the Hubble constant, $H_0$, which\ncharacterizes the expansion rate of the Universe. However, unlike binary\nneutron stars, the inspiral of binary black holes is not expected to be\naccompanied by electromagnetic radiation and a subsequent determination of its\nredshift. Consequently, independent redshift measurements of such GW events are\nnecessary to measure $H_0$. In this study, we present a novel Bayesian approach\nto infer $H_0$ by measuring the overdensity of galaxies around individual\nbinary black hole merger events in configuration space. We model the measured\noverdensity using the $3$D cross-correlation between galaxies and GW events,\nexplicitly accounting for the GW event localization uncertainty. We demonstrate\nthe efficacy of our method with $250$ simulated GW events distributed within\n$1$ Gpc in colored Gaussian noise of Advanced LIGO and Advanced Virgo detectors\noperating at O4 sensitivity. We show that such measurements can constrain the\nHubble constant with a precision of $\\lesssim 8 \\%$ ($90\\%$ highest density\ninterval). We highlight the potential improvements that need to be accounted\nfor in further studies before the method can be applied to real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) from the inspiral of binary compact objects offer a\none-step measurement of the luminosity distance to the event, which is\nessential for the measurement of the Hubble constant, $H_0$, which\ncharacterizes the expansion rate of the Universe. However, unlike binary\nneutron stars, the inspiral of binary black holes is not expected to be\naccompanied by electromagnetic radiation and a subsequent determination of its\nredshift. Consequently, independent redshift measurements of such GW events are\nnecessary to measure $H_0$. In this study, we present a novel Bayesian approach\nto infer $H_0$ by measuring the overdensity of galaxies around individual\nbinary black hole merger events in configuration space. We model the measured\noverdensity using the $3$D cross-correlation between galaxies and GW events,\nexplicitly accounting for the GW event localization uncertainty. We demonstrate\nthe efficacy of our method with $250$ simulated GW events distributed within\n$1$ Gpc in colored Gaussian noise of Advanced LIGO and Advanced Virgo detectors\noperating at O4 sensitivity. We show that such measurements can constrain the\nHubble constant with a precision of $\\lesssim 8 \\%$ ($90\\%$ highest density\ninterval). We highlight the potential improvements that need to be accounted\nfor in further studies before the method can be applied to real data."
                },
                "authors": [
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "Surhud More"
                    },
                    {
                        "name": "Sayantani Bera"
                    },
                    {
                        "name": "Sukanta Bose"
                    }
                ],
                "author_detail": {
                    "name": "Sukanta Bose"
                },
                "author": "Sukanta Bose",
                "arxiv_doi": "10.1103/PhysRevD.111.063513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.063513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.16305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 6 figures, 1 table; Published in PRD",
                "arxiv_journal_ref": "Phys.Rev.D 111, 063513 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05516v1",
                "updated": "2025-03-07T15:35:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    35,
                    37,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:35:37Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    35,
                    37,
                    4,
                    66,
                    0
                ],
                "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Bias Detection Using Advanced Prompt Engineering"
                },
                "summary": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making."
                },
                "authors": [
                    {
                        "name": "Frederic Lemieux"
                    },
                    {
                        "name": "Aisha Behr"
                    },
                    {
                        "name": "Clara Kellermann-Bryant"
                    },
                    {
                        "name": "Zaki Mohammed"
                    }
                ],
                "author_detail": {
                    "name": "Zaki Mohammed"
                },
                "author": "Zaki Mohammed",
                "arxiv_comment": "17 pages. 6 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09760v2",
                "updated": "2025-03-07T15:26:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    26,
                    3,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-14T06:57:18Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    6,
                    57,
                    18,
                    4,
                    166,
                    0
                ],
                "title": "Bootstrapping Language Models with DPO Implicit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Language Models with DPO Implicit Rewards"
                },
                "summary": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM to construct a preference dataset, which is then\nused in subsequent DPO rounds. We incorporate two refinements to further\nimprove our approach: 1) length-regularized reward shaping to make the\npreference dataset length-unbiased; 2) experience replay to enhance the quality\nof the preference dataset. Our approach, named self-alignment with DPO ImpliCit\nrEwards (DICE), shows great improvements in alignment. It achieves an increase\nof more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the\ndifferent base models that we tried, without relying on external feedback. Our\ncode is available at https://github.com/sail-sg/dice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM to construct a preference dataset, which is then\nused in subsequent DPO rounds. We incorporate two refinements to further\nimprove our approach: 1) length-regularized reward shaping to make the\npreference dataset length-unbiased; 2) experience replay to enhance the quality\nof the preference dataset. Our approach, named self-alignment with DPO ImpliCit\nrEwards (DICE), shows great improvements in alignment. It achieves an increase\nof more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the\ndifferent base models that we tried, without relying on external feedback. Our\ncode is available at https://github.com/sail-sg/dice."
                },
                "authors": [
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "Accepted in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05507v1",
                "updated": "2025-03-07T15:23:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    23,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:23:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    23,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?"
                },
                "summary": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Yueyi Xiao"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yingfei Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yingfei Xiong"
                },
                "author": "Yingfei Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05505v1",
                "updated": "2025-03-07T15:22:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:22:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "Statistical Guarantees of Correctness Coverage for Medical\n  Multiple-Choice Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Guarantees of Correctness Coverage for Medical\n  Multiple-Choice Question Answering"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in real-world\nquestion-answering (QA) applications. However, LLMs have been proven to\ngenerate hallucinations and nonfactual information, undermining their\ntrustworthiness in high-stakes medical tasks. Conformal prediction (CP) is\nwell-known to be model-agnostic and distribution-free, which creates\nstatistically rigorous prediction sets in classification tasks. In this work,\nwe for the first time adapt the CP framework to medical multiple-choice\nquestion-answering (MCQA) tasks, by correlating the nonconformity score with\nthe frequency score of correct options grounded in self-consistency theory,\nassuming no access to internal model information. Considering that the adapted\nCP framework can only control the (mis)coverage rate, we employ a risk control\nframework, which can manage task-specific metrics by devising a monotonically\ndecreasing loss function. We evaluate our framework on 3 popular medical MCQA\ndatasets utilizing 4 ``off-the-shelf'' LLMs. Empirical results demonstrate that\nwe achieve user-specified average (or marginal) error rates on the test set.\nFurthermore, we observe that the average prediction set size (APSS) on the test\nset decreases as the risk level increases, which concludes a promising\nevaluation metric for the uncertainty of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in real-world\nquestion-answering (QA) applications. However, LLMs have been proven to\ngenerate hallucinations and nonfactual information, undermining their\ntrustworthiness in high-stakes medical tasks. Conformal prediction (CP) is\nwell-known to be model-agnostic and distribution-free, which creates\nstatistically rigorous prediction sets in classification tasks. In this work,\nwe for the first time adapt the CP framework to medical multiple-choice\nquestion-answering (MCQA) tasks, by correlating the nonconformity score with\nthe frequency score of correct options grounded in self-consistency theory,\nassuming no access to internal model information. Considering that the adapted\nCP framework can only control the (mis)coverage rate, we employ a risk control\nframework, which can manage task-specific metrics by devising a monotonically\ndecreasing loss function. We evaluate our framework on 3 popular medical MCQA\ndatasets utilizing 4 ``off-the-shelf'' LLMs. Empirical results demonstrate that\nwe achieve user-specified average (or marginal) error rates on the test set.\nFurthermore, we observe that the average prediction set size (APSS) on the test\nset decreases as the risk level increases, which concludes a promising\nevaluation metric for the uncertainty of LLMs."
                },
                "authors": [
                    {
                        "name": "Yusong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Yusong Ke"
                },
                "author": "Yusong Ke",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08080v2",
                "updated": "2025-03-07T15:17:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    17,
                    43,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-12T02:54:12Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    2,
                    54,
                    12,
                    2,
                    43,
                    0
                ],
                "title": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals"
                },
                "summary": "Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts."
                },
                "authors": [
                    {
                        "name": "Neha Srikanth"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05239v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05239v4",
                "updated": "2025-03-07T15:13:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    13,
                    45,
                    4,
                    66,
                    0
                ],
                "published": "2023-08-09T21:54:34Z",
                "published_parsed": [
                    2023,
                    8,
                    9,
                    21,
                    54,
                    34,
                    2,
                    221,
                    0
                ],
                "title": "Enhancing Architecture Frameworks by Including Modern Stakeholders and\n  their Views/Viewpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Architecture Frameworks by Including Modern Stakeholders and\n  their Views/Viewpoints"
                },
                "summary": "Various architecture frameworks for software, systems, and enterprises have\nbeen proposed in the literature. They identified several stakeholders and\ndefined modeling perspectives, architecture viewpoints, and views to frame and\naddress stakeholder concerns. However, the stakeholders with data science and\nMachine Learning (ML) related concerns, such as data scientists and data\nengineers, are yet to be included in existing architecture frameworks. Only\nthis way can we envision a holistic system architecture description of an\nML-enabled system. Note that the ML component behavior and functionalities are\nspecial and should be distinguished from traditional software system behavior\nand functionalities. The main reason is that the actual functionality should be\ninferred from data instead of being specified at design time. Additionally, the\nstructural models of ML components, such as ML model architectures, are\ntypically specified using different notations and formalisms from what the\nSoftware Engineering (SE) community uses for software structural models. Yet,\nthese two aspects, namely ML and non-ML, are becoming so intertwined that it\nnecessitates an extension of software architecture frameworks and modeling\npractices toward supporting ML-enabled system architectures. In this paper, we\naddress this gap through an empirical study using an online survey instrument.\nWe surveyed 61 subject matter experts from over 25 organizations in 10\ncountries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various architecture frameworks for software, systems, and enterprises have\nbeen proposed in the literature. They identified several stakeholders and\ndefined modeling perspectives, architecture viewpoints, and views to frame and\naddress stakeholder concerns. However, the stakeholders with data science and\nMachine Learning (ML) related concerns, such as data scientists and data\nengineers, are yet to be included in existing architecture frameworks. Only\nthis way can we envision a holistic system architecture description of an\nML-enabled system. Note that the ML component behavior and functionalities are\nspecial and should be distinguished from traditional software system behavior\nand functionalities. The main reason is that the actual functionality should be\ninferred from data instead of being specified at design time. Additionally, the\nstructural models of ML components, such as ML model architectures, are\ntypically specified using different notations and formalisms from what the\nSoftware Engineering (SE) community uses for software structural models. Yet,\nthese two aspects, namely ML and non-ML, are becoming so intertwined that it\nnecessitates an extension of software architecture frameworks and modeling\npractices toward supporting ML-enabled system architectures. In this paper, we\naddress this gap through an empirical study using an online survey instrument.\nWe surveyed 61 subject matter experts from over 25 organizations in 10\ncountries."
                },
                "authors": [
                    {
                        "name": "Armin Moin"
                    },
                    {
                        "name": "Atta Badii"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Moharram Challenger"
                    }
                ],
                "author_detail": {
                    "name": "Moharram Challenger"
                },
                "author": "Moharram Challenger",
                "arxiv_comment": "ICICT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05239v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05239v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04346v2",
                "updated": "2025-03-07T15:13:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    13,
                    32,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T11:42:03Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    42,
                    3,
                    3,
                    65,
                    0
                ],
                "title": "Adding Alignment Control to Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adding Alignment Control to Language Models"
                },
                "summary": "Post-training alignment has increasingly become a crucial factor in enhancing\nthe usability of language models (LMs). However, the strength of alignment\nvaries depending on individual preferences. This paper proposes a method to\nincorporate alignment control into a single model, referred to as CLM. This\napproach adds one identity layer preceding the initial layers and performs\npreference learning only on this layer to map unaligned input token embeddings\ninto the aligned space. Experimental results demonstrate that this efficient\nfine-tuning method performs comparable to full fine-tuning. During inference,\nthe input embeddings are processed through the aligned and unaligned layers,\nwhich are then merged through the interpolation coefficient. By controlling\nthis parameter, the alignment exhibits a clear interpolation and extrapolation\nphenomenon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training alignment has increasingly become a crucial factor in enhancing\nthe usability of language models (LMs). However, the strength of alignment\nvaries depending on individual preferences. This paper proposes a method to\nincorporate alignment control into a single model, referred to as CLM. This\napproach adds one identity layer preceding the initial layers and performs\npreference learning only on this layer to map unaligned input token embeddings\ninto the aligned space. Experimental results demonstrate that this efficient\nfine-tuning method performs comparable to full fine-tuning. During inference,\nthe input embeddings are processed through the aligned and unaligned layers,\nwhich are then merged through the interpolation coefficient. By controlling\nthis parameter, the alignment exhibits a clear interpolation and extrapolation\nphenomenon."
                },
                "authors": [
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04704v2",
                "updated": "2025-03-07T15:12:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    12,
                    57,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T18:54:32Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size"
                },
                "summary": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\nconsistent improvements in the quality-compression trade-off regardless of\nmodel scale or architectural design. A surprising finding of EWQ is its ability\nto reduce perplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\nconsistent improvements in the quality-compression trade-off regardless of\nmodel scale or architectural design. A surprising finding of EWQ is its ability\nto reduce perplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Alireza Behtash"
                    },
                    {
                        "name": "Marijan Fofonjka"
                    },
                    {
                        "name": "Ethan Baird"
                    },
                    {
                        "name": "Tyler Mauer"
                    },
                    {
                        "name": "Hossein Moghimifam"
                    },
                    {
                        "name": "David Stout"
                    },
                    {
                        "name": "Joel Dennison"
                    }
                ],
                "author_detail": {
                    "name": "Joel Dennison"
                },
                "author": "Joel Dennison",
                "arxiv_comment": "29 pages, 7 figures, 14 tables; Fixed some types, added some\n  clarifications and improvements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10386v2",
                "updated": "2025-03-07T15:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    11,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2024-04-16T08:37:36Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    8,
                    37,
                    36,
                    1,
                    107,
                    0
                ],
                "title": "I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey"
                },
                "summary": "Growing interest in Artificial Intelligence (AI) has resulted in a surge in\ndemand for faster methods of Machine Learning (ML) model training and\ninference. This demand for speed has prompted the use of high performance\ncomputing (HPC) systems that excel in managing distributed workloads. Because\ndata is the main fuel for AI applications, the performance of the storage and\nI/O subsystem of HPC systems is critical. In the past, HPC applications\naccessed large portions of data written by simulations or experiments or\ningested data for visualizations or analysis tasks. ML workloads perform small\nreads spread across a large number of random files. This shift of I/O access\npatterns poses several challenges to modern parallel storage systems. In this\npaper, we survey I/O in ML applications on HPC systems, and target literature\nwithin a 6-year time window from 2019 to 2024. We define the scope of the\nsurvey, provide an overview of the common phases of ML, review available\nprofilers and benchmarks, examine the I/O patterns encountered during offline\ndata preparation, training, and inference, and explore I/O optimizations\nutilized in modern ML frameworks and proposed in recent literature. Lastly, we\nseek to expose research gaps that could spawn further R&D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing interest in Artificial Intelligence (AI) has resulted in a surge in\ndemand for faster methods of Machine Learning (ML) model training and\ninference. This demand for speed has prompted the use of high performance\ncomputing (HPC) systems that excel in managing distributed workloads. Because\ndata is the main fuel for AI applications, the performance of the storage and\nI/O subsystem of HPC systems is critical. In the past, HPC applications\naccessed large portions of data written by simulations or experiments or\ningested data for visualizations or analysis tasks. ML workloads perform small\nreads spread across a large number of random files. This shift of I/O access\npatterns poses several challenges to modern parallel storage systems. In this\npaper, we survey I/O in ML applications on HPC systems, and target literature\nwithin a 6-year time window from 2019 to 2024. We define the scope of the\nsurvey, provide an overview of the common phases of ML, review available\nprofilers and benchmarks, examine the I/O patterns encountered during offline\ndata preparation, training, and inference, and explore I/O optimizations\nutilized in modern ML frameworks and proposed in recent literature. Lastly, we\nseek to expose research gaps that could spawn further R&D."
                },
                "authors": [
                    {
                        "name": "Noah Lewis"
                    },
                    {
                        "name": "Jean Luca Bez"
                    },
                    {
                        "name": "Surendra Byna"
                    }
                ],
                "author_detail": {
                    "name": "Surendra Byna"
                },
                "author": "Surendra Byna",
                "arxiv_doi": "10.1145/3722215",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3722215",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ACM Computing Surveys (CSUR), Vol. 1, No. 1, Article 1, January\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2; H.3.4; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05493v1",
                "updated": "2025-03-07T15:05:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:05:23Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "title": "Benchmarking LLMs in Recommendation Tasks: A Comparative Evaluation with\n  Conventional Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Recommendation Tasks: A Comparative Evaluation with\n  Conventional Recommenders"
                },
                "summary": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05481v1",
                "updated": "2025-03-07T14:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Maximum Hallucination Standards for Domain-Specific Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Hallucination Standards for Domain-Specific Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation."
                },
                "authors": [
                    {
                        "name": "Tingmingke Lu"
                    }
                ],
                "author_detail": {
                    "name": "Tingmingke Lu"
                },
                "author": "Tingmingke Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05468v1",
                "updated": "2025-03-07T14:39:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    39,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:39:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    39,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Asymptotic expansions of solutions to Markov renewal equations and their\n  application to general branching processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic expansions of solutions to Markov renewal equations and their\n  application to general branching processes"
                },
                "summary": "We consider the Markov renewal equation $F(t) = f(t) + \\boldsymbol{\\mu}*F(t)$\nfor vector-valued functions $f,F: \\mathbb{R} \\to \\mathbb{R}^{p}$ and a $p\n\\times p$ matrix $\\boldsymbol{\\mu}$ of locally finite measures $\\mu^{i,j}$ on\n$[0,\\infty)$, $i,j=1,\\ldots,p$. Sgibnev [Semimultiplicative estimates for the\nsolution of the multidimensional renewal equation. {\\em Izv.\\ Ross.\\ Akad.\\\nNauk Ser.\\ Mat.}, 66(3):159--174, 2002] derived an asymptotic expansion for the\nsolution $F$ to the above equation. We give a new, more elementary proof of\nSgibnev's result, which also covers the reducible case. As a corollary, we\ninfer an asymptotic expansion for the mean of a multi-type general branching\nprocess with finite type space counted with random characteristic.\n  Finally, some examples are discussed that illustrate phenomena of multi-type\nbranching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the Markov renewal equation $F(t) = f(t) + \\boldsymbol{\\mu}*F(t)$\nfor vector-valued functions $f,F: \\mathbb{R} \\to \\mathbb{R}^{p}$ and a $p\n\\times p$ matrix $\\boldsymbol{\\mu}$ of locally finite measures $\\mu^{i,j}$ on\n$[0,\\infty)$, $i,j=1,\\ldots,p$. Sgibnev [Semimultiplicative estimates for the\nsolution of the multidimensional renewal equation. {\\em Izv.\\ Ross.\\ Akad.\\\nNauk Ser.\\ Mat.}, 66(3):159--174, 2002] derived an asymptotic expansion for the\nsolution $F$ to the above equation. We give a new, more elementary proof of\nSgibnev's result, which also covers the reducible case. As a corollary, we\ninfer an asymptotic expansion for the mean of a multi-type general branching\nprocess with finite type space counted with random characteristic.\n  Finally, some examples are discussed that illustrate phenomena of multi-type\nbranching."
                },
                "authors": [
                    {
                        "name": "Konrad Kolesko"
                    },
                    {
                        "name": "Matthias Meiners"
                    },
                    {
                        "name": "Ivana Tomic"
                    }
                ],
                "author_detail": {
                    "name": "Ivana Tomic"
                },
                "author": "Ivana Tomic",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "45M05, 60K15, 60J80",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00435v2",
                "updated": "2025-03-07T14:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    33,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-01T10:24:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    10,
                    24,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error\n  Fixing for Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error\n  Fixing for Tabular Question Answering"
                },
                "summary": "In this paper, we present our submission to SemEval-2025 Task 8: Question\nAnswering over Tabular Data. This task, evaluated on the DataBench dataset,\nassesses Large Language Models' (LLMs) ability to answer natural language\nquestions over structured data while addressing topic diversity and table size\nlimitations in previous benchmarks. We propose a system that employs effective\nLLM prompting to translate natural language queries into executable code,\nenabling accurate responses, error correction, and interpretability. Our\napproach ranks first in both subtasks of the competition in the proprietary\nmodel category, significantly outperforming the organizer's baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present our submission to SemEval-2025 Task 8: Question\nAnswering over Tabular Data. This task, evaluated on the DataBench dataset,\nassesses Large Language Models' (LLMs) ability to answer natural language\nquestions over structured data while addressing topic diversity and table size\nlimitations in previous benchmarks. We propose a system that employs effective\nLLM prompting to translate natural language queries into executable code,\nenabling accurate responses, error correction, and interpretability. Our\napproach ranks first in both subtasks of the competition in the proprietary\nmodel category, significantly outperforming the organizer's baseline."
                },
                "authors": [
                    {
                        "name": "Andreas Evangelatos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Athanasios Voulodimos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05460v1",
                "updated": "2025-03-07T14:32:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    32,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:32:42Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    32,
                    42,
                    4,
                    66,
                    0
                ],
                "title": "Determining the Polarisation of a Coronal Standing Kink Oscillation\n  Using Spectral Imaging Techniques with the Coronal Multi-channel Polarimeter\n  (CoMP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the Polarisation of a Coronal Standing Kink Oscillation\n  Using Spectral Imaging Techniques with the Coronal Multi-channel Polarimeter\n  (CoMP)"
                },
                "summary": "Coronal oscillations offer insight into energy transport and driving in the\nsolar atmosphere. Knowing its polarisation state helps constrain a wave's\ndisplacement and velocity amplitude, improving estimates of wave energy flux\nand deposition rate. We demonstrate a method to combine imaging and spectral\ndata to infer the polarisation of a coronal loop's standing kink wave, without\nthe need for multiple instruments or multiple lines of sight. We use the unique\ncapabilities of the Coronal Multi-channel Polarimeter (CoMP) to observe the\nstanding kink mode of an off-limb coronal loop perturbed by an eruption. The\nfull off-disk corona is observed using the 1074 nm Fe XIII spectral line,\nproviding Doppler velocity, intensity and line width. By tracking the\noscillatory motion of a loop apex in a time-distance map, we extract the\nline-of-sight (Doppler) velocity of the inhomogeneity as it sways and compare\nit with the derivative of its plane-of-sky displacement. This analysis provides\nthe loop's velocity in two perpendicular planes as it oscillates with a period\nof $8.9^{+0.5}_{-0.5}$ minutes. Through detailed analysis of the phase relation\nbetween the transverse velocities we infer the kink oscillation to be\nhorizontally polarised, oscillating in a plane tilted $-13.6^{+2.9}_{-3.0}$\ndegrees away from the plane of sky. The line widths show a periodic enhancement\nduring the kink oscillation, exhibiting both the kink period and its double.\nThis study is the first to combine direct imaging and spectral data to infer\nthe polarisation of a coronal loop oscillation from a single viewpoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal oscillations offer insight into energy transport and driving in the\nsolar atmosphere. Knowing its polarisation state helps constrain a wave's\ndisplacement and velocity amplitude, improving estimates of wave energy flux\nand deposition rate. We demonstrate a method to combine imaging and spectral\ndata to infer the polarisation of a coronal loop's standing kink wave, without\nthe need for multiple instruments or multiple lines of sight. We use the unique\ncapabilities of the Coronal Multi-channel Polarimeter (CoMP) to observe the\nstanding kink mode of an off-limb coronal loop perturbed by an eruption. The\nfull off-disk corona is observed using the 1074 nm Fe XIII spectral line,\nproviding Doppler velocity, intensity and line width. By tracking the\noscillatory motion of a loop apex in a time-distance map, we extract the\nline-of-sight (Doppler) velocity of the inhomogeneity as it sways and compare\nit with the derivative of its plane-of-sky displacement. This analysis provides\nthe loop's velocity in two perpendicular planes as it oscillates with a period\nof $8.9^{+0.5}_{-0.5}$ minutes. Through detailed analysis of the phase relation\nbetween the transverse velocities we infer the kink oscillation to be\nhorizontally polarised, oscillating in a plane tilted $-13.6^{+2.9}_{-3.0}$\ndegrees away from the plane of sky. The line widths show a periodic enhancement\nduring the kink oscillation, exhibiting both the kink period and its double.\nThis study is the first to combine direct imaging and spectral data to infer\nthe polarisation of a coronal loop oscillation from a single viewpoint."
                },
                "authors": [
                    {
                        "name": "T. J. Duckenfield"
                    },
                    {
                        "name": "D. B. Jess"
                    },
                    {
                        "name": "R. J. Morton"
                    },
                    {
                        "name": "S. Jafarzadeh"
                    }
                ],
                "author_detail": {
                    "name": "S. Jafarzadeh"
                },
                "author": "S. Jafarzadeh",
                "arxiv_doi": "10.3847/1538-4357/adb8d6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adb8d6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures, accepted for publication in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01161v2",
                "updated": "2025-03-07T14:24:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    24,
                    6,
                    4,
                    66,
                    0
                ],
                "published": "2024-07-01T10:29:56Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    10,
                    29,
                    56,
                    0,
                    183,
                    0
                ],
                "title": "GazeNoter: Co-Piloted AR Note-Taking via Gaze Selection of LLM\n  Suggestions to Match Users' Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GazeNoter: Co-Piloted AR Note-Taking via Gaze Selection of LLM\n  Suggestions to Match Users' Intentions"
                },
                "summary": "Note-taking is critical during speeches and discussions, serving not only for\nlater summarization and organization but also for real-time question and\nopinion reminding in question-and-answer sessions or timely contributions in\ndiscussions. Manually typing on smartphones for note-taking could be\ndistracting and increase cognitive load for users. While large language models\n(LLMs) are used to automatically generate summaries and highlights, the content\ngenerated by artificial intelligence (AI) may not match users' intentions\nwithout user input or interaction. Therefore, we propose an AI-copiloted\naugmented reality (AR) system, GazeNoter, to allow users to swiftly select\ndiverse LLM-generated suggestions via gaze on an AR headset for real-time\nnote-taking. GazeNoter leverages an AR headset as a medium for users to swiftly\nadjust the LLM output to match their intentions, forming a user-in-the-loop AI\nsystem for both within-context and beyond-context notes. We conducted two user\nstudies to verify the usability of GazeNoter in attending speeches in a static\nsitting condition and walking meetings and discussions in a mobile walking\ncondition, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Note-taking is critical during speeches and discussions, serving not only for\nlater summarization and organization but also for real-time question and\nopinion reminding in question-and-answer sessions or timely contributions in\ndiscussions. Manually typing on smartphones for note-taking could be\ndistracting and increase cognitive load for users. While large language models\n(LLMs) are used to automatically generate summaries and highlights, the content\ngenerated by artificial intelligence (AI) may not match users' intentions\nwithout user input or interaction. Therefore, we propose an AI-copiloted\naugmented reality (AR) system, GazeNoter, to allow users to swiftly select\ndiverse LLM-generated suggestions via gaze on an AR headset for real-time\nnote-taking. GazeNoter leverages an AR headset as a medium for users to swiftly\nadjust the LLM output to match their intentions, forming a user-in-the-loop AI\nsystem for both within-context and beyond-context notes. We conducted two user\nstudies to verify the usability of GazeNoter in attending speeches in a static\nsitting condition and walking meetings and discussions in a mobile walking\ncondition, respectively."
                },
                "authors": [
                    {
                        "name": "Hsin-Ruey Tsai"
                    },
                    {
                        "name": "Shih-Kang Chiu"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "arxiv_doi": "10.1145/3706598.3714294",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714294",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.01161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19798v2",
                "updated": "2025-03-07T14:20:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    20,
                    23,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-29T21:49:32Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    21,
                    49,
                    32,
                    6,
                    273,
                    0
                ],
                "title": "Membership Inference Attacks Cannot Prove that a Model Was Trained On\n  Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks Cannot Prove that a Model Was Trained On\n  Your Data"
                },
                "summary": "We consider the problem of a training data proof, where a data creator or\nowner wants to demonstrate to a third party that some machine learning model\nwas trained on their data. Training data proofs play a key role in recent\nlawsuits against foundation models trained on web-scale data. Many prior works\nsuggest to instantiate training data proofs using membership inference attacks.\nWe argue that this approach is fundamentally unsound: to provide convincing\nevidence, the data creator needs to demonstrate that their attack has a low\nfalse positive rate, i.e., that the attack's output is unlikely under the null\nhypothesis that the model was not trained on the target data. Yet, sampling\nfrom this null hypothesis is impossible, as we do not know the exact contents\nof the training set, nor can we (efficiently) retrain a large foundation model.\nWe conclude by offering two paths forward, by showing that data extraction\nattacks and membership inference on special canary data can be used to create\nsound training data proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of a training data proof, where a data creator or\nowner wants to demonstrate to a third party that some machine learning model\nwas trained on their data. Training data proofs play a key role in recent\nlawsuits against foundation models trained on web-scale data. Many prior works\nsuggest to instantiate training data proofs using membership inference attacks.\nWe argue that this approach is fundamentally unsound: to provide convincing\nevidence, the data creator needs to demonstrate that their attack has a low\nfalse positive rate, i.e., that the attack's output is unlikely under the null\nhypothesis that the model was not trained on the target data. Yet, sampling\nfrom this null hypothesis is impossible, as we do not know the exact contents\nof the training set, nor can we (efficiently) retrain a large foundation model.\nWe conclude by offering two paths forward, by showing that data extraction\nattacks and membership inference on special canary data can be used to create\nsound training data proofs."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Debeshee Das"
                    },
                    {
                        "name": "Gautam Kamath"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "position paper at IEEE SaTML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05449v1",
                "updated": "2025-03-07T14:19:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    19,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:19:17Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    19,
                    17,
                    4,
                    66,
                    0
                ],
                "title": "LLM-based Iterative Approach to Metamodeling in Automotive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Iterative Approach to Metamodeling in Automotive"
                },
                "summary": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required."
                },
                "authors": [
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14644v2",
                "updated": "2025-03-07T14:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    18,
                    56,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-20T15:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning"
                },
                "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research."
                },
                "authors": [
                    {
                        "name": "Yansheng Mao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Xiyuan Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.13626",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05448v1",
                "updated": "2025-03-07T14:18:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    18,
                    21,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    18,
                    21,
                    4,
                    66,
                    0
                ],
                "title": "Joint graphical model estimation using Stein-type shrinkage for fast\n  large scale network inference in scRNAseq data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint graphical model estimation using Stein-type shrinkage for fast\n  large scale network inference in scRNAseq data"
                },
                "summary": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data."
                },
                "authors": [
                    {
                        "name": "Duong H. T. Vo"
                    },
                    {
                        "name": "Nelofer Syed"
                    },
                    {
                        "name": "Thomas Thorne"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Thorne"
                },
                "author": "Thomas Thorne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05445v1",
                "updated": "2025-03-07T14:16:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection\n  via Backdoor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection\n  via Backdoor Attacks"
                },
                "summary": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats."
                },
                "authors": [
                    {
                        "name": "Meiyu Lin"
                    },
                    {
                        "name": "Haichuan Zhang"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Renyuan Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05439v1",
                "updated": "2025-03-07T14:10:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:10:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning"
                },
                "summary": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00140v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00140v4",
                "updated": "2025-03-07T14:09:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    9,
                    36,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-29T21:43:34Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    21,
                    43,
                    34,
                    3,
                    60,
                    0
                ],
                "title": "Bootstrap inference for linear regression between variables that are\n  never jointly observed: application in in vivo experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap inference for linear regression between variables that are\n  never jointly observed: application in in vivo experiments"
                },
                "summary": "In modern experimental science, there is a common problem of estimating the\ncoefficients of a linear regression in a context where the variables of\ninterest cannot be observed simultaneously. When there is a categorical\nvariable that is observed on all statistical units, we consider two estimators\nof linear regression that take this additional information into account: an\nestimator based on moments and an estimator based on optimal transport theory.\nThese estimators are shown to be consistent and asymptotically Gaussian under\nweak hypotheses. The asymptotic variance has no explicit expression, except in\nsome special cases, for which reason a stratified bootstrap approach is\ndeveloped to construct confidence intervals for the estimated parameters, whose\nconsistency is also shown. A simulation study evaluating and comparing the\nfinite sample performance of these estimators demonstrates the advantages of\nthe bootstrap approach in several realistic scenarios. An application to in\nvivo experiments, conducted in the context of studying radio-induced adverse\neffects in mice, revealed important relationships between the biomarkers of\ninterest that could not be identified with the considered naive approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern experimental science, there is a common problem of estimating the\ncoefficients of a linear regression in a context where the variables of\ninterest cannot be observed simultaneously. When there is a categorical\nvariable that is observed on all statistical units, we consider two estimators\nof linear regression that take this additional information into account: an\nestimator based on moments and an estimator based on optimal transport theory.\nThese estimators are shown to be consistent and asymptotically Gaussian under\nweak hypotheses. The asymptotic variance has no explicit expression, except in\nsome special cases, for which reason a stratified bootstrap approach is\ndeveloped to construct confidence intervals for the estimated parameters, whose\nconsistency is also shown. A simulation study evaluating and comparing the\nfinite sample performance of these estimators demonstrates the advantages of\nthe bootstrap approach in several realistic scenarios. An application to in\nvivo experiments, conducted in the context of studying radio-induced adverse\neffects in mice, revealed important relationships between the biomarkers of\ninterest that could not be identified with the considered naive approach."
                },
                "authors": [
                    {
                        "name": "Polina Arsenteva"
                    },
                    {
                        "name": "Mohamed Amine Benadjaoud"
                    },
                    {
                        "name": "Hervé Cardot"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Cardot"
                },
                "author": "Hervé Cardot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00140v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00140v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05429v1",
                "updated": "2025-03-07T13:58:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    58,
                    58,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:58:58Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    58,
                    58,
                    4,
                    66,
                    0
                ],
                "title": "Wi-Fi 6 Cross-Technology Interference Detection and Mitigation by OFDMA:\n  an Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi 6 Cross-Technology Interference Detection and Mitigation by OFDMA:\n  an Experimental Study"
                },
                "summary": "Cross-Technology Interference (CTI) poses challenges for the performance and\nrobustness of wireless networks. There are opportunities for better cooperation\nif the spectral occupation and technology of the interference can be detected.\nNamely, this information can help the Orthogonal Frequency Division Multiple\nAccess (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate\nresources to multiple users inthe frequency domain. This work shows that a\nsingle Channel State Information (CSI) snapshot, which is used for packet\ndemodulation in the receiver, is enough to detect and classify the type of CTI\non low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small\nConvolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR)\nand Signal-to-Interference Ratio (SIR) with simulated data, as well as using a\nwired and over-the-air test with a professional wireless connectivity tester,\nwhile running the inference on the low-cost device. Furthermore, we use\nopenwifi, a full-stack Wi-Fi transceiver running on software-defined radio\n(SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a\nCTI-aware multi-user OFDMA scheduler when the clients send CTI detection\nfeedback to the AP. We show experimentally that it can fully mitigate the 35%\nthroughput loss caused by CTI when the AP applies the appropriate scheduling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Technology Interference (CTI) poses challenges for the performance and\nrobustness of wireless networks. There are opportunities for better cooperation\nif the spectral occupation and technology of the interference can be detected.\nNamely, this information can help the Orthogonal Frequency Division Multiple\nAccess (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate\nresources to multiple users inthe frequency domain. This work shows that a\nsingle Channel State Information (CSI) snapshot, which is used for packet\ndemodulation in the receiver, is enough to detect and classify the type of CTI\non low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small\nConvolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR)\nand Signal-to-Interference Ratio (SIR) with simulated data, as well as using a\nwired and over-the-air test with a professional wireless connectivity tester,\nwhile running the inference on the low-cost device. Furthermore, we use\nopenwifi, a full-stack Wi-Fi transceiver running on software-defined radio\n(SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a\nCTI-aware multi-user OFDMA scheduler when the clients send CTI detection\nfeedback to the AP. We show experimentally that it can fully mitigate the 35%\nthroughput loss caused by CTI when the AP applies the appropriate scheduling."
                },
                "authors": [
                    {
                        "name": "Thijs Havinga"
                    },
                    {
                        "name": "Xianjun Jiao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Baiheng Chen"
                    },
                    {
                        "name": "Adnan Shahid"
                    },
                    {
                        "name": "Ingrid Moerman"
                    }
                ],
                "author_detail": {
                    "name": "Ingrid Moerman"
                },
                "author": "Ingrid Moerman",
                "arxiv_comment": "6 pages, 6 figures. Submitted to EuCNC & 6G Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20576v2",
                "updated": "2025-03-07T13:35:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    35,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-27T22:35:31Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    22,
                    35,
                    31,
                    3,
                    58,
                    0
                ],
                "title": "ECCOS: Efficient Capability and Cost Coordinated Scheduling for\n  Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECCOS: Efficient Capability and Cost Coordinated Scheduling for\n  Multi-LLM Serving"
                },
                "summary": "As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. This paper\naddresses this challenge by proposing a capability-cost coordinated scheduling\nframework, ECCOS, for multi-LLM serving, which explicitly constrains response\nquality and workload to optimize LLM inference cost. Specifically, it\nintroduces the two-stage scheduling by designing a multi-objective predictor\nand a constrained optimizer. The predictor estimates both model capabilities\nand computational costs through training-based and retrieval-based approaches,\nwhile the optimizer determines cost-optimal assignments under quality and\nworkload constraints. It also introduces QAServe, a dataset collected for\nsample-wise response quality and costs by zero-shot prompting different LLMs on\nknowledge QA and mathematical reasoning. Extensive experiments demonstrate that\nECCOS improves success rates by 6.30% while reducing costs by 10.15% compared\nto existing methods, consuming less than 0.5% of LLM response time. The code is\navailable at: https://github.com/agiresearch/ECCOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. This paper\naddresses this challenge by proposing a capability-cost coordinated scheduling\nframework, ECCOS, for multi-LLM serving, which explicitly constrains response\nquality and workload to optimize LLM inference cost. Specifically, it\nintroduces the two-stage scheduling by designing a multi-objective predictor\nand a constrained optimizer. The predictor estimates both model capabilities\nand computational costs through training-based and retrieval-based approaches,\nwhile the optimizer determines cost-optimal assignments under quality and\nworkload constraints. It also introduces QAServe, a dataset collected for\nsample-wise response quality and costs by zero-shot prompting different LLMs on\nknowledge QA and mathematical reasoning. Extensive experiments demonstrate that\nECCOS improves success rates by 6.30% while reducing costs by 10.15% compared\nto existing methods, consuming less than 0.5% of LLM response time. The code is\navailable at: https://github.com/agiresearch/ECCOS."
                },
                "authors": [
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05394v1",
                "updated": "2025-03-07T13:09:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    9,
                    37,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:09:37Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    9,
                    37,
                    4,
                    66,
                    0
                ],
                "title": "Static Program Analysis Guided LLM Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Program Analysis Guided LLM Based Unit Test Generation"
                },
                "summary": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project."
                },
                "authors": [
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Sourav Mazumdar"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "Srinivasan Bajji Sugumaran"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05388v1",
                "updated": "2025-03-07T13:03:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    3,
                    28,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:03:28Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    3,
                    28,
                    4,
                    66,
                    0
                ],
                "title": "Ontology Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Generation using Large Language Models"
                },
                "summary": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research."
                },
                "authors": [
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Robin Keskisärkkä"
                    },
                    {
                        "name": "Sara Zuppiroli"
                    },
                    {
                        "name": "Miguel Ceriani"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Eva Blomqvist"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "2 figures and 3 tables. 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05053v2",
                "updated": "2025-03-07T12:46:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    46,
                    14,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-07T16:22:51Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    22,
                    51,
                    4,
                    159,
                    0
                ],
                "title": "Hints-In-Browser: Benchmarking Language Models for Programming Feedback\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hints-In-Browser: Benchmarking Language Models for Programming Feedback\n  Generation"
                },
                "summary": "Generative AI and large language models hold great promise in enhancing\nprogramming education by generating individualized feedback and hints for\nlearners. Recent works have primarily focused on improving the quality of\ngenerated feedback to achieve human tutors' quality. While quality is an\nimportant performance criterion, it is not the only criterion to optimize for\nreal-world educational deployments. In this paper, we benchmark language models\nfor programming feedback generation across several performance criteria,\nincluding quality, cost, time, and data privacy. The key idea is to leverage\nrecent advances in the new paradigm of in-browser inference that allow running\nthese models directly in the browser, thereby providing direct benefits across\ncost and data privacy. To boost the feedback quality of small models compatible\nwith in-browser inference engines, we develop a fine-tuning pipeline based on\nGPT-4 generated synthetic data. We showcase the efficacy of fine-tuned\nLlama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser\ninference engine on three different Python programming datasets. We will\nrelease the full implementation along with a web app and datasets to facilitate\nfurther research on in-browser language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and large language models hold great promise in enhancing\nprogramming education by generating individualized feedback and hints for\nlearners. Recent works have primarily focused on improving the quality of\ngenerated feedback to achieve human tutors' quality. While quality is an\nimportant performance criterion, it is not the only criterion to optimize for\nreal-world educational deployments. In this paper, we benchmark language models\nfor programming feedback generation across several performance criteria,\nincluding quality, cost, time, and data privacy. The key idea is to leverage\nrecent advances in the new paradigm of in-browser inference that allow running\nthese models directly in the browser, thereby providing direct benefits across\ncost and data privacy. To boost the feedback quality of small models compatible\nwith in-browser inference engines, we develop a fine-tuning pipeline based on\nGPT-4 generated synthetic data. We showcase the efficacy of fine-tuned\nLlama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser\ninference engine on three different Python programming datasets. We will\nrelease the full implementation along with a web app and datasets to facilitate\nfurther research on in-browser language models."
                },
                "authors": [
                    {
                        "name": "Nachiket Kotalwar"
                    },
                    {
                        "name": "Alkis Gotovos"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16011v2",
                "updated": "2025-03-07T12:41:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    41,
                    55,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-24T12:11:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    11,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized\n  Variational AutoEncoder for Dense Crowd Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized\n  Variational AutoEncoder for Dense Crowd Navigation"
                },
                "summary": "Navigation amongst densely packed crowds remains a challenge for mobile\nrobots. The complexity increases further if the environment layout changes,\nmaking the prior computed global plan infeasible. In this paper, we show that\nit is possible to dramatically enhance crowd navigation by just improving the\nlocal planner. Our approach combines generative modelling with inference time\noptimization to generate sophisticated long-horizon local plans at interactive\nrates. More specifically, we train a Vector Quantized Variational AutoEncoder\nto learn a prior over the expert trajectory distribution conditioned on the\nperception input. At run-time, this is used as an initialization for a\nsampling-based optimizer for further refinement. Our approach does not require\nany sophisticated prediction of dynamic obstacles and yet provides\nstate-of-the-art performance. In particular, we compare against the recent\nDRL-VO approach and show a 40% improvement in success rate and a 6% improvement\nin travel time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigation amongst densely packed crowds remains a challenge for mobile\nrobots. The complexity increases further if the environment layout changes,\nmaking the prior computed global plan infeasible. In this paper, we show that\nit is possible to dramatically enhance crowd navigation by just improving the\nlocal planner. Our approach combines generative modelling with inference time\noptimization to generate sophisticated long-horizon local plans at interactive\nrates. More specifically, we train a Vector Quantized Variational AutoEncoder\nto learn a prior over the expert trajectory distribution conditioned on the\nperception input. At run-time, this is used as an initialization for a\nsampling-based optimizer for further refinement. Our approach does not require\nany sophisticated prediction of dynamic obstacles and yet provides\nstate-of-the-art performance. In particular, we compare against the recent\nDRL-VO approach and show a 40% improvement in success rate and a 6% improvement\nin travel time."
                },
                "authors": [
                    {
                        "name": "Naman Kumar"
                    },
                    {
                        "name": "Antareep Singha"
                    },
                    {
                        "name": "Laksh Nanwani"
                    },
                    {
                        "name": "Dhruv Potdar"
                    },
                    {
                        "name": "Tarun R"
                    },
                    {
                        "name": "Fatemeh Rastgar"
                    },
                    {
                        "name": "Simon Idoko"
                    },
                    {
                        "name": "Arun Kumar Singh"
                    },
                    {
                        "name": "K. Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "K. Madhava Krishna"
                },
                "author": "K. Madhava Krishna",
                "arxiv_comment": "Accepted at IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05375v1",
                "updated": "2025-03-07T12:33:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    33,
                    9,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:33:09Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    33,
                    9,
                    4,
                    66,
                    0
                ],
                "title": "Tailoring the breathing-mode distortions in nickelate-ferroelectric\n  heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailoring the breathing-mode distortions in nickelate-ferroelectric\n  heterostructures"
                },
                "summary": "In transition metal oxides electron-electron interaction and lattice degree\nof freedom are basic ingredients of emergent phenomena, such as\nmetal-to-insulator transition (MIT) and superconductivity. Perovskite\nrare-earth nickelates are largely studied for their temperature-driven MIT\nwhich is accompanied by a breathing mode distortion, and associated to a\nbond-disproportionation of the expanded (3d8L0) and compressed (3d8L2) NiO6\noctahedra. Steric effects control the onset temperature of the MIT, the latter\nbeing concomitant or not with a complex antiferromagnetic spin arrangement\ndepending upon the choice of the rare earth ion (TMIT>TNeel). Interface\nengineering of oxygen octahedra tilting, as imposed by the symmetry and\norientation of the substrate, has resulted in an efficient pathway to modify\nboth TMIT and TNeel, hence, suggesting a key role of the electron-phonon\ncoupling for both transport and magnetic properties in nickelate thin films.\nHere, via a combination of resonant elastic X-ray scattering and transport\nexperiments, we show a control over both TMIT and TNeel in heteroepitaxial\nPZT(d)/NNO(7 nm)//STO heterostructures, which are characterized by different\nstrains and polarization states of the PZT layer grown at different thicknesses\nd. We found the expected NNO bulk behaviour, for a fully relaxed PZT layer\nshowing a monodomain polarization state. On the other side, an almost 30 K\ndifference, is found for a fully strained PZT characterized by a multidomain\ntexture of the polarization state. We discuss our results in terms of an\naltered breathing distortion pattern of the underlying nickelate layer as\nsupported by X-ray absorption spectroscopy measurements. We infer that locally\ndifferent polar distortions controlled by a combination of polarization\ndirection and strength of the strain state play the main role in the observed\nTMIT and TNeel variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In transition metal oxides electron-electron interaction and lattice degree\nof freedom are basic ingredients of emergent phenomena, such as\nmetal-to-insulator transition (MIT) and superconductivity. Perovskite\nrare-earth nickelates are largely studied for their temperature-driven MIT\nwhich is accompanied by a breathing mode distortion, and associated to a\nbond-disproportionation of the expanded (3d8L0) and compressed (3d8L2) NiO6\noctahedra. Steric effects control the onset temperature of the MIT, the latter\nbeing concomitant or not with a complex antiferromagnetic spin arrangement\ndepending upon the choice of the rare earth ion (TMIT>TNeel). Interface\nengineering of oxygen octahedra tilting, as imposed by the symmetry and\norientation of the substrate, has resulted in an efficient pathway to modify\nboth TMIT and TNeel, hence, suggesting a key role of the electron-phonon\ncoupling for both transport and magnetic properties in nickelate thin films.\nHere, via a combination of resonant elastic X-ray scattering and transport\nexperiments, we show a control over both TMIT and TNeel in heteroepitaxial\nPZT(d)/NNO(7 nm)//STO heterostructures, which are characterized by different\nstrains and polarization states of the PZT layer grown at different thicknesses\nd. We found the expected NNO bulk behaviour, for a fully relaxed PZT layer\nshowing a monodomain polarization state. On the other side, an almost 30 K\ndifference, is found for a fully strained PZT characterized by a multidomain\ntexture of the polarization state. We discuss our results in terms of an\naltered breathing distortion pattern of the underlying nickelate layer as\nsupported by X-ray absorption spectroscopy measurements. We infer that locally\ndifferent polar distortions controlled by a combination of polarization\ndirection and strength of the strain state play the main role in the observed\nTMIT and TNeel variations."
                },
                "authors": [
                    {
                        "name": "Guillaume Krieger"
                    },
                    {
                        "name": "Chia-Ping Su"
                    },
                    {
                        "name": "Hoshang Sahib"
                    },
                    {
                        "name": "Raymond Fan"
                    },
                    {
                        "name": "Paul Steadman"
                    },
                    {
                        "name": "Alexandre Gloter"
                    },
                    {
                        "name": "Nathalie Viart"
                    },
                    {
                        "name": "Daniele Preziosi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Preziosi"
                },
                "author": "Daniele Preziosi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.01196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.01196v3",
                "updated": "2025-03-07T12:31:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    31,
                    27,
                    4,
                    66,
                    0
                ],
                "published": "2023-07-27T22:57:55Z",
                "published_parsed": [
                    2023,
                    7,
                    27,
                    22,
                    57,
                    55,
                    3,
                    208,
                    0
                ],
                "title": "Sustainable transparency in Recommender Systems: Bayesian Ranking of\n  Images for Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable transparency in Recommender Systems: Bayesian Ranking of\n  Images for Explainability"
                },
                "summary": "Recommender Systems have become crucial in the modern world, commonly guiding\nusers towards relevant content or products, and having a large influence over\nthe decisions of users and citizens. However, ensuring transparency and user\ntrust in these systems remains a challenge; personalized explanations have\nemerged as a solution, offering justifications for recommendations. Among the\nexisting approaches for generating personalized explanations, using existing\nvisual content created by users is a promising option to maximize transparency\nand user trust. State-of-the-art models that follow this approach, despite\nleveraging highly optimized architectures, employ surrogate learning tasks that\ndo not efficiently model the objective of ranking images as explanations for a\ngiven recommendation; this leads to a suboptimal training process with high\ncomputational costs that may not be reduced without affecting model\nperformance. This work presents BRIE, a novel model where we leverage Bayesian\nPairwise Ranking to enhance the training process, allowing us to consistently\noutperform state-of-the-art models in six real-world datasets while reducing\nits model size by up to 64 times and its CO2 emissions by up to 75% in training\nand inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender Systems have become crucial in the modern world, commonly guiding\nusers towards relevant content or products, and having a large influence over\nthe decisions of users and citizens. However, ensuring transparency and user\ntrust in these systems remains a challenge; personalized explanations have\nemerged as a solution, offering justifications for recommendations. Among the\nexisting approaches for generating personalized explanations, using existing\nvisual content created by users is a promising option to maximize transparency\nand user trust. State-of-the-art models that follow this approach, despite\nleveraging highly optimized architectures, employ surrogate learning tasks that\ndo not efficiently model the objective of ranking images as explanations for a\ngiven recommendation; this leads to a suboptimal training process with high\ncomputational costs that may not be reduced without affecting model\nperformance. This work presents BRIE, a novel model where we leverage Bayesian\nPairwise Ranking to enhance the training process, allowing us to consistently\noutperform state-of-the-art models in six real-world datasets while reducing\nits model size by up to 64 times and its CO2 emissions by up to 75% in training\nand inference."
                },
                "authors": [
                    {
                        "name": "Jorge Paz-Ruza"
                    },
                    {
                        "name": "Amparo Alonso-Betanzos"
                    },
                    {
                        "name": "Berta Guijarro-Berdiñas"
                    },
                    {
                        "name": "Brais Cancela"
                    },
                    {
                        "name": "Carlos Eiras-Franco"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Eiras-Franco"
                },
                "author": "Carlos Eiras-Franco",
                "arxiv_doi": "10.1016/j.inffus.2024.102497",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2024.102497",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.01196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.01196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14195v2",
                "updated": "2025-03-07T12:30:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    30,
                    18,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-20T02:00:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    2,
                    0,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "Bridging Text and Vision: A Multi-View Text-Vision Registration Approach\n  for Cross-Modal Place Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Text and Vision: A Multi-View Text-Vision Registration Approach\n  for Cross-Modal Place Recognition"
                },
                "summary": "Mobile robots necessitate advanced natural language understanding\ncapabilities to accurately identify locations and perform tasks such as package\ndelivery. However, traditional visual place recognition (VPR) methods rely\nsolely on single-view visual information and cannot interpret human language\ndescriptions. To overcome this challenge, we bridge text and vision by\nproposing a multiview (360{\\deg} views of the surroundings) text-vision\nregistration approach called Text4VPR for place recognition task, which is the\nfirst method that exclusively utilizes textual descriptions to match a database\nof images. Text4VPR employs the frozen T5 language model to extract global\ntextual embeddings. Additionally, it utilizes the Sinkhorn algorithm with\ntemperature coefficient to assign local tokens to their respective clusters,\nthereby aggregating visual descriptors from images. During the training stage,\nText4VPR emphasizes the alignment between individual text-image pairs for\nprecise textual description. In the inference stage, Text4VPR uses the Cascaded\nCross-Attention Cosine Alignment (CCCA) to address the internal mismatch\nbetween text and image groups. Subsequently, Text4VPR performs precisely place\nmatch based on the descriptions of text-image groups. On Street360Loc, the\nfirst text to image VPR dataset we created, Text4VPR builds a robust baseline,\nachieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92%\nwithin a 5-meter radius on the test set, which indicates that localization from\ntextual descriptions to images is not only feasible but also holds significant\npotential for further advancement, as shown in Figure 1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile robots necessitate advanced natural language understanding\ncapabilities to accurately identify locations and perform tasks such as package\ndelivery. However, traditional visual place recognition (VPR) methods rely\nsolely on single-view visual information and cannot interpret human language\ndescriptions. To overcome this challenge, we bridge text and vision by\nproposing a multiview (360{\\deg} views of the surroundings) text-vision\nregistration approach called Text4VPR for place recognition task, which is the\nfirst method that exclusively utilizes textual descriptions to match a database\nof images. Text4VPR employs the frozen T5 language model to extract global\ntextual embeddings. Additionally, it utilizes the Sinkhorn algorithm with\ntemperature coefficient to assign local tokens to their respective clusters,\nthereby aggregating visual descriptors from images. During the training stage,\nText4VPR emphasizes the alignment between individual text-image pairs for\nprecise textual description. In the inference stage, Text4VPR uses the Cascaded\nCross-Attention Cosine Alignment (CCCA) to address the internal mismatch\nbetween text and image groups. Subsequently, Text4VPR performs precisely place\nmatch based on the descriptions of text-image groups. On Street360Loc, the\nfirst text to image VPR dataset we created, Text4VPR builds a robust baseline,\nachieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92%\nwithin a 5-meter radius on the test set, which indicates that localization from\ntextual descriptions to images is not only feasible but also holds significant\npotential for further advancement, as shown in Figure 1."
                },
                "authors": [
                    {
                        "name": "Tianyi Shang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Pengjie Xu"
                    },
                    {
                        "name": "Jinwei Qiao"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Zihan Ruan"
                    },
                    {
                        "name": "Weijun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Weijun Hu"
                },
                "author": "Weijun Hu",
                "arxiv_comment": "8 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05371v1",
                "updated": "2025-03-07T12:25:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:25:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias\n  Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias\n  Mitigation in LLMs"
                },
                "summary": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05365v1",
                "updated": "2025-03-07T12:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    14,
                    51,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:14:51Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    14,
                    51,
                    4,
                    66,
                    0
                ],
                "title": "Multi-Grained Feature Pruning for Video-Based Human Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Grained Feature Pruning for Video-Based Human Pose Estimation"
                },
                "summary": "Human pose estimation, with its broad applications in action recognition and\nmotion capture, has experienced significant advancements. However, current\nTransformer-based methods for video pose estimation often face challenges in\nmanaging redundant temporal information and achieving fine-grained perception\nbecause they only focus on processing low-resolution features. To address these\nchallenges, we propose a novel multi-scale resolution framework that encodes\nspatio-temporal representations at varying granularities and executes\nfine-grained perception compensation. Furthermore, we employ a density peaks\nclustering method to dynamically identify and prioritize tokens that offer\nimportant semantic information. This strategy effectively prunes redundant\nfeature tokens, especially those arising from multi-frame features, thereby\noptimizing computational efficiency without sacrificing semantic richness.\nEmpirically, it sets new benchmarks for both performance and efficiency on\nthree large-scale datasets. Our method achieves a 93.8% improvement in\ninference speed compared to the baseline, while also enhancing pose estimation\naccuracy, reaching 87.4 mAP on the PoseTrack2017 dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human pose estimation, with its broad applications in action recognition and\nmotion capture, has experienced significant advancements. However, current\nTransformer-based methods for video pose estimation often face challenges in\nmanaging redundant temporal information and achieving fine-grained perception\nbecause they only focus on processing low-resolution features. To address these\nchallenges, we propose a novel multi-scale resolution framework that encodes\nspatio-temporal representations at varying granularities and executes\nfine-grained perception compensation. Furthermore, we employ a density peaks\nclustering method to dynamically identify and prioritize tokens that offer\nimportant semantic information. This strategy effectively prunes redundant\nfeature tokens, especially those arising from multi-frame features, thereby\noptimizing computational efficiency without sacrificing semantic richness.\nEmpirically, it sets new benchmarks for both performance and efficiency on\nthree large-scale datasets. Our method achieves a 93.8% improvement in\ninference speed compared to the baseline, while also enhancing pose estimation\naccuracy, reaching 87.4 mAP on the PoseTrack2017 dataset."
                },
                "authors": [
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Shaojing Fan"
                    },
                    {
                        "name": "Zhenguang Liu"
                    },
                    {
                        "name": "Zheqi Wu"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Yingying Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yingying Jiao"
                },
                "author": "Yingying Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05364v1",
                "updated": "2025-03-07T12:11:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    11,
                    12,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:11:12Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    11,
                    12,
                    4,
                    66,
                    0
                ],
                "title": "Proof-theoretic Semantics for Classical Propositional Logic with\n  Assertion and Denial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof-theoretic Semantics for Classical Propositional Logic with\n  Assertion and Denial"
                },
                "summary": "The field of proof-theoretic semantics (P-tS) offers an alternative approach\nto meaning in logic that is based on inference and argument (rather than truth\nin a model). It has been successfully developed for various logics; in\nparticular, Sandqvist has developed such semantics for both classical and\nintuitionistic logic. In the case of classical logic, P-tS provides a\nconception of consequence that avoids an \\emph{a priori} commitment to the\nprinciple of bivalence, addressing what Dummett identified as a significant\nfoundational challenge in logic. In this paper, we propose an alternative P-tS\nfor classical logic, which essentially extends the P-tS for intuitionistic\nlogic by operating over literals rather than atomic propositions. Importantly,\nliterals are atomic and not defined by negation but are defined by inferential\nrelationships. This semantics illustrates the perspective that classical logic\ncan be understood as intuitionistic logic supplemented by a principle of\nduality, offering fresh insights into the relationship between these two\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of proof-theoretic semantics (P-tS) offers an alternative approach\nto meaning in logic that is based on inference and argument (rather than truth\nin a model). It has been successfully developed for various logics; in\nparticular, Sandqvist has developed such semantics for both classical and\nintuitionistic logic. In the case of classical logic, P-tS provides a\nconception of consequence that avoids an \\emph{a priori} commitment to the\nprinciple of bivalence, addressing what Dummett identified as a significant\nfoundational challenge in logic. In this paper, we propose an alternative P-tS\nfor classical logic, which essentially extends the P-tS for intuitionistic\nlogic by operating over literals rather than atomic propositions. Importantly,\nliterals are atomic and not defined by negation but are defined by inferential\nrelationships. This semantics illustrates the perspective that classical logic\ncan be understood as intuitionistic logic supplemented by a principle of\nduality, offering fresh insights into the relationship between these two\nsystems."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    },
                    {
                        "name": "Yll Buzoku"
                    }
                ],
                "author_detail": {
                    "name": "Yll Buzoku"
                },
                "author": "Yll Buzoku",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05362v1",
                "updated": "2025-03-07T12:07:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    7,
                    59,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:07:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    7,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter"
                },
                "summary": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Xinyang Han"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Qianyun Du"
                    },
                    {
                        "name": "Shijin Wang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "19 pages, 9 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05360v1",
                "updated": "2025-03-07T12:03:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    3,
                    43,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:03:43Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    3,
                    43,
                    4,
                    66,
                    0
                ],
                "title": "On an Inferential Semantics for Intuitionistic Sentential Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On an Inferential Semantics for Intuitionistic Sentential Logic"
                },
                "summary": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Gheorghiu"
                },
                "author": "Alexander V. Gheorghiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05347v1",
                "updated": "2025-03-07T11:42:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    42,
                    22,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:42:22Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    42,
                    22,
                    4,
                    66,
                    0
                ],
                "title": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation"
                },
                "summary": "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https://github.com/Zhenxuan-Zhang/GEMA_score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https://github.com/Zhenxuan-Zhang/GEMA_score."
                },
                "authors": [
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Kinhei Lee"
                    },
                    {
                        "name": "Weihang Deng"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Zihao Jin"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Zhifan Gao"
                    },
                    {
                        "name": "Dominic C Marshall"
                    },
                    {
                        "name": "Yingying Fang"
                    },
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04398v2",
                "updated": "2025-03-07T11:41:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    41,
                    53,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T12:52:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling"
                },
                "summary": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Zewei Xu"
                    },
                    {
                        "name": "Yuanhao Lai"
                    },
                    {
                        "name": "Yunfei Du"
                    },
                    {
                        "name": "Zhengang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengang Wang"
                },
                "author": "Zhengang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05346v1",
                "updated": "2025-03-07T11:40:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    40,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    40,
                    52,
                    4,
                    66,
                    0
                ],
                "title": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT\n  Applications"
                },
                "summary": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines."
                },
                "authors": [
                    {
                        "name": "Leming Shen"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Yuanqing Zheng"
                    },
                    {
                        "name": "Mo Li"
                    }
                ],
                "author_detail": {
                    "name": "Mo Li"
                },
                "author": "Mo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05345v1",
                "updated": "2025-03-07T11:39:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    39,
                    1,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:39:01Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    39,
                    1,
                    4,
                    66,
                    0
                ],
                "title": "Characterising planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model"
                },
                "summary": "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet."
                },
                "authors": [
                    {
                        "name": "Mairi W. O'Brien"
                    },
                    {
                        "name": "Pier-Emmanuel Tremblay"
                    },
                    {
                        "name": "Beth L. Klein"
                    },
                    {
                        "name": "Carl Melis"
                    },
                    {
                        "name": "Detlev Koester"
                    },
                    {
                        "name": "Andrew M. Buchan"
                    },
                    {
                        "name": "Dimitri Veras"
                    },
                    {
                        "name": "Alexandra E. Doyle"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra E. Doyle"
                },
                "author": "Alexandra E. Doyle",
                "arxiv_comment": "Accepted to MNRAS 2025 March 07",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10444v2",
                "updated": "2025-03-07T11:32:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    32,
                    55,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-16T16:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "title": "LLM-as-BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning"
                },
                "summary": "Robotic assembly tasks remain an open challenge due to their long horizon\nnature and complex part relations. Behavior trees (BTs) are increasingly used\nin robot task planning for their modularity and flexibility, but creating them\nmanually can be effort-intensive. Large language models (LLMs) have recently\nbeen applied to robotic task planning for generating action sequences, yet\ntheir ability to generate BTs has not been fully investigated. To this end, we\npropose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT\ngeneration in robotic assembly task planning. Four in-context learning methods\nare introduced to utilize the natural language processing and inference\ncapabilities of LLMs for producing task plans in BT format, reducing manual\neffort while ensuring robustness and comprehensibility. Additionally, we\nevaluate the performance of fine-tuned smaller LLMs on the same tasks.\nExperiments in both simulated and real-world settings demonstrate that our\nframework enhances LLMs' ability to generate BTs, improving success rate\nthrough in-context learning and supervised fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic assembly tasks remain an open challenge due to their long horizon\nnature and complex part relations. Behavior trees (BTs) are increasingly used\nin robot task planning for their modularity and flexibility, but creating them\nmanually can be effort-intensive. Large language models (LLMs) have recently\nbeen applied to robotic task planning for generating action sequences, yet\ntheir ability to generate BTs has not been fully investigated. To this end, we\npropose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT\ngeneration in robotic assembly task planning. Four in-context learning methods\nare introduced to utilize the natural language processing and inference\ncapabilities of LLMs for producing task plans in BT format, reducing manual\neffort while ensuring robustness and comprehensibility. Additionally, we\nevaluate the performance of fine-tuned smaller LLMs on the same tasks.\nExperiments in both simulated and real-world settings demonstrate that our\nframework enhances LLMs' ability to generate BTs, improving success rate\nthrough in-context learning and supervised fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jicong Ao"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "7 pages. Accepted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v2",
                "updated": "2025-03-07T11:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    26,
                    20,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose into the viable pricing of model-risk and of\nnon-model risk separately such that the former has a unique and intuitive\nrisk-neutral equivalent formulation with convenient properties. Its parameter,\na dynamically conserved constant of model-risk inference, allows an integrated\nrepresentation of ex-ante risk-pricing and bias, such that their ex-post\nprice-effects can be disentangled, through well-known price anomalies such as\nMoment and Low-Risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose into the viable pricing of model-risk and of\nnon-model risk separately such that the former has a unique and intuitive\nrisk-neutral equivalent formulation with convenient properties. Its parameter,\na dynamically conserved constant of model-risk inference, allows an integrated\nrepresentation of ex-ante risk-pricing and bias, such that their ex-post\nprice-effects can be disentangled, through well-known price anomalies such as\nMoment and Low-Risk."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "25 pages of main text, 13 pages of Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.14352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.14352v2",
                "updated": "2025-03-07T11:16:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    16,
                    40,
                    4,
                    66,
                    0
                ],
                "published": "2023-08-28T06:56:08Z",
                "published_parsed": [
                    2023,
                    8,
                    28,
                    6,
                    56,
                    8,
                    0,
                    240,
                    0
                ],
                "title": "EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices"
                },
                "summary": "Large language models (LLMs) such as GPTs and Mixtral-8x7B have\nrevolutionized machine intelligence due to their exceptional abilities in\ngeneric ML tasks. Transiting LLMs from datacenters to edge devices brings\nbenefits like better privacy and availability, but is challenged by their\nmassive parameter size and thus unbearable runtime costs. To this end, we\npresent EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs\n-- a popular form of sparse LLM that scales its parameter size with almost\nconstant computing complexity. EdgeMoE achieves both memory- and\ncompute-efficiency by partitioning the model into the storage hierarchy:\nnon-expert weights are held in device memory; while expert weights are held on\nexternal storage and fetched to memory only when activated. This design is\nmotivated by a key observation that expert weights are bulky but infrequently\nused due to sparse activation. To further reduce the expert I/O swapping\noverhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth\nadaptation that reduces the expert sizes with tolerable accuracy loss; (2)\nexpert preloading that predicts the activated experts ahead of time and\npreloads it with the compute-I/O pipeline. On popular MoE LLMs and edge\ndevices, EdgeMoE showcase significant memory savings and speedup over\ncompetitive baselines. The code is available at\nhttps://github.com/UbiquitousLearning/mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as GPTs and Mixtral-8x7B have\nrevolutionized machine intelligence due to their exceptional abilities in\ngeneric ML tasks. Transiting LLMs from datacenters to edge devices brings\nbenefits like better privacy and availability, but is challenged by their\nmassive parameter size and thus unbearable runtime costs. To this end, we\npresent EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs\n-- a popular form of sparse LLM that scales its parameter size with almost\nconstant computing complexity. EdgeMoE achieves both memory- and\ncompute-efficiency by partitioning the model into the storage hierarchy:\nnon-expert weights are held in device memory; while expert weights are held on\nexternal storage and fetched to memory only when activated. This design is\nmotivated by a key observation that expert weights are bulky but infrequently\nused due to sparse activation. To further reduce the expert I/O swapping\noverhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth\nadaptation that reduces the expert sizes with tolerable accuracy loss; (2)\nexpert preloading that predicts the activated experts ahead of time and\npreloads it with the compute-I/O pipeline. On popular MoE LLMs and edge\ndevices, EdgeMoE showcase significant memory savings and speedup over\ncompetitive baselines. The code is available at\nhttps://github.com/UbiquitousLearning/mllm."
                },
                "authors": [
                    {
                        "name": "Rongjie Yi"
                    },
                    {
                        "name": "Liwei Guo"
                    },
                    {
                        "name": "Shiyun Wei"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Shangguang Wang"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.14352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.14352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05330v1",
                "updated": "2025-03-07T11:15:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    15,
                    36,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:15:36Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    15,
                    36,
                    4,
                    66,
                    0
                ],
                "title": "Speculative Decoding for Multi-Sample Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding for Multi-Sample Inference"
                },
                "summary": "We propose a novel speculative decoding method tailored for multi-sample\nreasoning scenarios, such as self-consistency and Best-of-N sampling. Our\nmethod exploits the intrinsic consensus of parallel generation paths to\nsynthesize high-quality draft tokens without requiring auxiliary models or\nexternal databases. By dynamically analyzing structural patterns across\nparallel reasoning paths through a probabilistic aggregation mechanism, it\nidentifies consensus token sequences that align with the decoding distribution.\nEvaluations on mathematical reasoning benchmarks demonstrate a substantial\nimprovement in draft acceptance rates over baselines, while reducing the\nlatency in draft token construction. This work establishes a paradigm shift for\nefficient multi-sample inference, enabling seamless integration of speculative\ndecoding with sampling-based reasoning techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel speculative decoding method tailored for multi-sample\nreasoning scenarios, such as self-consistency and Best-of-N sampling. Our\nmethod exploits the intrinsic consensus of parallel generation paths to\nsynthesize high-quality draft tokens without requiring auxiliary models or\nexternal databases. By dynamically analyzing structural patterns across\nparallel reasoning paths through a probabilistic aggregation mechanism, it\nidentifies consensus token sequences that align with the decoding distribution.\nEvaluations on mathematical reasoning benchmarks demonstrate a substantial\nimprovement in draft acceptance rates over baselines, while reducing the\nlatency in draft token construction. This work establishes a paradigm shift for\nefficient multi-sample inference, enabling seamless integration of speculative\ndecoding with sampling-based reasoning techniques."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05328v1",
                "updated": "2025-03-07T11:13:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models"
                },
                "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems."
                },
                "authors": [
                    {
                        "name": "Anar Yeginbergen"
                    },
                    {
                        "name": "Maite Oronoz"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19103v2",
                "updated": "2025-03-07T11:05:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    5,
                    1,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-26T12:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval."
                },
                "authors": [
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Rishi Ravikumar"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Xiaoyong Wei"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05324v1",
                "updated": "2025-03-07T11:02:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    2,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:02:17Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    2,
                    17,
                    4,
                    66,
                    0
                ],
                "title": "Routing for Large ML Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing for Large ML Models"
                },
                "summary": "Training large language models (LLMs), and other large machine learning\nmodels, involves repeated communication of large volumes of data across a data\ncenter network. The communication patterns induced by these training process\nexhibit high regularity and persistence, giving rise to significant\nopportunities for optimizing the manner in which flows are routed across the\nnetwork. We present an algorithmic framework for \\textit{quantifying}\nnetwork-wide efficiency in the context of training LLMs (and other large-scale\nML models), and for periodically \\textit{optimizing} routing with respect to\nthis global metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs), and other large machine learning\nmodels, involves repeated communication of large volumes of data across a data\ncenter network. The communication patterns induced by these training process\nexhibit high regularity and persistence, giving rise to significant\nopportunities for optimizing the manner in which flows are routed across the\nnetwork. We present an algorithmic framework for \\textit{quantifying}\nnetwork-wide efficiency in the context of training LLMs (and other large-scale\nML models), and for periodically \\textit{optimizing} routing with respect to\nthis global metric."
                },
                "authors": [
                    {
                        "name": "Ofir Cohen"
                    },
                    {
                        "name": "Jose Yallouz Michael Schapira"
                    },
                    {
                        "name": "Shahar Belkar"
                    },
                    {
                        "name": "Tal Mizrahi"
                    }
                ],
                "author_detail": {
                    "name": "Tal Mizrahi"
                },
                "author": "Tal Mizrahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05322v1",
                "updated": "2025-03-07T11:01:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    1,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:01:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    1,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "Attenuation artifact detection and severity classification in\n  intracoronary OCT using mixed image representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attenuation artifact detection and severity classification in\n  intracoronary OCT using mixed image representations"
                },
                "summary": "In intracoronary optical coherence tomography (OCT), blood residues and gas\nbubbles cause attenuation artifacts that can obscure critical vessel\nstructures. The presence and severity of these artifacts may warrant\nre-acquisition, prolonging procedure time and increasing use of contrast agent.\nAccurate detection of these artifacts can guide targeted re-acquisition,\nreducing the amount of repeated scans needed to achieve diagnostically viable\nimages. However, the highly heterogeneous appearance of these artifacts poses a\nchallenge for the automated detection of the affected image regions. To enable\nautomatic detection of the attenuation artifacts caused by blood residues and\ngas bubbles based on their severity, we propose a convolutional neural network\nthat performs classification of the attenuation lines (A-lines) into three\nclasses: no artifact, mild artifact and severe artifact. Our model extracts and\nmerges features from OCT images in both Cartesian and polar coordinates, where\neach column of the image represents an A-line. Our method detects the presence\nof attenuation artifacts in OCT frames reaching F-scores of 0.77 and 0.94 for\nmild and severe artifacts, respectively. The inference time over a full OCT\nscan is approximately 6 seconds. Our experiments show that analysis of images\nrepresented in both Cartesian and polar coordinate systems outperforms the\nanalysis in polar coordinates only, suggesting that these representations\ncontain complementary features. This work lays the foundation for automated\nartifact assessment and image acquisition guidance in intracoronary OCT\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In intracoronary optical coherence tomography (OCT), blood residues and gas\nbubbles cause attenuation artifacts that can obscure critical vessel\nstructures. The presence and severity of these artifacts may warrant\nre-acquisition, prolonging procedure time and increasing use of contrast agent.\nAccurate detection of these artifacts can guide targeted re-acquisition,\nreducing the amount of repeated scans needed to achieve diagnostically viable\nimages. However, the highly heterogeneous appearance of these artifacts poses a\nchallenge for the automated detection of the affected image regions. To enable\nautomatic detection of the attenuation artifacts caused by blood residues and\ngas bubbles based on their severity, we propose a convolutional neural network\nthat performs classification of the attenuation lines (A-lines) into three\nclasses: no artifact, mild artifact and severe artifact. Our model extracts and\nmerges features from OCT images in both Cartesian and polar coordinates, where\neach column of the image represents an A-line. Our method detects the presence\nof attenuation artifacts in OCT frames reaching F-scores of 0.77 and 0.94 for\nmild and severe artifacts, respectively. The inference time over a full OCT\nscan is approximately 6 seconds. Our experiments show that analysis of images\nrepresented in both Cartesian and polar coordinate systems outperforms the\nanalysis in polar coordinates only, suggesting that these representations\ncontain complementary features. This work lays the foundation for automated\nartifact assessment and image acquisition guidance in intracoronary OCT\nimaging."
                },
                "authors": [
                    {
                        "name": "Pierandrea Cancian"
                    },
                    {
                        "name": "Simone Saitta"
                    },
                    {
                        "name": "Xiaojin Gu"
                    },
                    {
                        "name": "Rudolf L. M. van Herten"
                    },
                    {
                        "name": "Thijs J. Luttikholt"
                    },
                    {
                        "name": "Jos Thannhauser"
                    },
                    {
                        "name": "Rick H. J. A. Volleberg"
                    },
                    {
                        "name": "Ruben G. A. van der Waerden"
                    },
                    {
                        "name": "Joske L. van der Zande"
                    },
                    {
                        "name": "Clarisa I. Sánchez"
                    },
                    {
                        "name": "Bram van Ginneken"
                    },
                    {
                        "name": "Niels van Royen"
                    },
                    {
                        "name": "Ivana Išgum"
                    }
                ],
                "author_detail": {
                    "name": "Ivana Išgum"
                },
                "author": "Ivana Išgum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05321v1",
                "updated": "2025-03-07T11:00:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    0,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:00:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    0,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Riemannian Metric Learning: Closer to You than You Imagine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riemannian Metric Learning: Closer to You than You Imagine"
                },
                "summary": "Riemannian metric learning is an emerging field in machine learning,\nunlocking new ways to encode complex data structures beyond traditional\ndistance metric learning. While classical approaches rely on global distances\nin Euclidean space, they often fall short in capturing intrinsic data geometry.\nEnter Riemannian metric learning: a powerful generalization that leverages\ndifferential geometry to model the data according to their underlying\nRiemannian manifold. This approach has demonstrated remarkable success across\ndiverse domains, from causal inference and optimal transport to generative\nmodeling and representation learning. In this review, we bridge the gap between\nclassical metric learning and Riemannian geometry, providing a structured and\naccessible overview of key methods, applications, and recent advances. We argue\nthat Riemannian metric learning is not merely a technical refinement but a\nfundamental shift in how we think about data representations. Thus, this review\nshould serve as a valuable resource for researchers and practitioners\ninterested in exploring Riemannian metric learning and convince them that it is\ncloser to them than they might imagine-both in theory and in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riemannian metric learning is an emerging field in machine learning,\nunlocking new ways to encode complex data structures beyond traditional\ndistance metric learning. While classical approaches rely on global distances\nin Euclidean space, they often fall short in capturing intrinsic data geometry.\nEnter Riemannian metric learning: a powerful generalization that leverages\ndifferential geometry to model the data according to their underlying\nRiemannian manifold. This approach has demonstrated remarkable success across\ndiverse domains, from causal inference and optimal transport to generative\nmodeling and representation learning. In this review, we bridge the gap between\nclassical metric learning and Riemannian geometry, providing a structured and\naccessible overview of key methods, applications, and recent advances. We argue\nthat Riemannian metric learning is not merely a technical refinement but a\nfundamental shift in how we think about data representations. Thus, this review\nshould serve as a valuable resource for researchers and practitioners\ninterested in exploring Riemannian metric learning and convince them that it is\ncloser to them than they might imagine-both in theory and in practice."
                },
                "authors": [
                    {
                        "name": "Samuel Gruffaz"
                    },
                    {
                        "name": "Josua Sassen"
                    }
                ],
                "author_detail": {
                    "name": "Josua Sassen"
                },
                "author": "Josua Sassen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05 (Primary), 58D17 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05305v1",
                "updated": "2025-03-07T10:34:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    34,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T10:34:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    34,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency Autoregressive Image Generation with Continuous Tokens"
                },
                "summary": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02667v3",
                "updated": "2025-03-07T10:33:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    33,
                    24,
                    4,
                    66,
                    0
                ],
                "published": "2024-08-05T17:58:44Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    17,
                    58,
                    44,
                    0,
                    218,
                    0
                ],
                "title": "Evaluating and Utilizing Surrogate Outcomes in Covariate-Adjusted\n  Response-Adaptive Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Utilizing Surrogate Outcomes in Covariate-Adjusted\n  Response-Adaptive Designs"
                },
                "summary": "Surrogate outcomes have long been studied as substitutes for long-term\nprimary outcomes. However, current surrogate evaluation methods do not directly\naccount for their benefits in updating treatment randomization probabilities in\nadaptive experiments that aim to learn and respond to treatment effect\nheterogeneity. In this context, surrogate outcomes can expedite updates to\nrandomization probabilities and thus improve expected outcomes of\nnewly-enrolled participants by enabling earlier detection of heterogeneous\ntreatment effects. We introduce a novel approach for evaluating candidate\nsurrogate outcomes that quantifies both of these benefits in sequential\nadaptive experiments. We also propose a new Covariate-Adjusted\nResponse-Adaptive design that uses an Online-Superlearner to evaluate and\nadaptively select surrogate outcomes for updating treatment randomization\nprobabilities during the trial. We further introduce a Targeted Maximum\nLikelihood Estimation method that addresses dependence in adaptively collected\ndata and achieves asymptotic normality without parametric assumptions. Our\ndesign and estimation methods show robust performance in simulations, including\nthose using real trial data. Overall, this framework not only provides a\ncomprehensive way to quantify benefits and select among candidate surrogate\noutcomes, but also offers a general tool for evaluating various adaptive\ndesigns with inferences, providing insights into opportunities and costs of\nalternative designs that could have been implemented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate outcomes have long been studied as substitutes for long-term\nprimary outcomes. However, current surrogate evaluation methods do not directly\naccount for their benefits in updating treatment randomization probabilities in\nadaptive experiments that aim to learn and respond to treatment effect\nheterogeneity. In this context, surrogate outcomes can expedite updates to\nrandomization probabilities and thus improve expected outcomes of\nnewly-enrolled participants by enabling earlier detection of heterogeneous\ntreatment effects. We introduce a novel approach for evaluating candidate\nsurrogate outcomes that quantifies both of these benefits in sequential\nadaptive experiments. We also propose a new Covariate-Adjusted\nResponse-Adaptive design that uses an Online-Superlearner to evaluate and\nadaptively select surrogate outcomes for updating treatment randomization\nprobabilities during the trial. We further introduce a Targeted Maximum\nLikelihood Estimation method that addresses dependence in adaptively collected\ndata and achieves asymptotic normality without parametric assumptions. Our\ndesign and estimation methods show robust performance in simulations, including\nthose using real trial data. Overall, this framework not only provides a\ncomprehensive way to quantify benefits and select among candidate surrogate\noutcomes, but also offers a general tool for evaluating various adaptive\ndesigns with inferences, providing insights into opportunities and costs of\nalternative designs that could have been implemented."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Aaron Hudson"
                    },
                    {
                        "name": "Maya Petersen"
                    },
                    {
                        "name": "Mark van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark van der Laan"
                },
                "author": "Mark van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05297v1",
                "updated": "2025-03-07T10:23:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    23,
                    12,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T10:23:12Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    23,
                    12,
                    4,
                    66,
                    0
                ],
                "title": "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy"
                },
                "summary": "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided."
                },
                "authors": [
                    {
                        "name": "Pierre Alquier"
                    },
                    {
                        "name": "Mathieu Gerber"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Gerber"
                },
                "author": "Mathieu Gerber",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14677v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v3",
                "updated": "2025-03-07T10:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    17,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld. The code is available at\nhttps://github.com/Advacheck-OU/ai-dataset-analysing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld. The code is available at\nhttps://github.com/Advacheck-OU/ai-dataset-analysing."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "arxiv_comment": "Presented at Preventing and Detecting LLM Misinformation (PDLM) at\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04280v2",
                "updated": "2025-03-07T10:06:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    6,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T10:08:44Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    8,
                    44,
                    3,
                    65,
                    0
                ],
                "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup."
                },
                "authors": [
                    {
                        "name": "Niccolò Turcato"
                    },
                    {
                        "name": "Matteo Iovino"
                    },
                    {
                        "name": "Aris Synodinos"
                    },
                    {
                        "name": "Alberto Dalla Libera"
                    },
                    {
                        "name": "Ruggero Carli"
                    },
                    {
                        "name": "Pietro Falco"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Falco"
                },
                "author": "Pietro Falco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03315v2",
                "updated": "2025-03-07T10:05:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-08T11:01:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    1,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "Learning Force Distribution Estimation for the GelSight Mini Optical\n  Tactile Sensor Based on Finite Element Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Force Distribution Estimation for the GelSight Mini Optical\n  Tactile Sensor Based on Finite Element Analysis"
                },
                "summary": "Contact-rich manipulation remains a major challenge in robotics. Optical\ntactile sensors like GelSight Mini offer a low-cost solution for contact\nsensing by capturing soft-body deformations of the silicone gel. However,\naccurately inferring shear and normal force distributions from these gel\ndeformations has yet to be fully addressed. In this work, we propose a machine\nlearning approach using a U-net architecture to predict force distributions\ndirectly from the sensor's raw images. Our model, trained on force\ndistributions inferred from Finite Element Analysis (FEA), demonstrates\npromising accuracy in predicting normal and shear force distributions for the\ncommercially available GelSight Mini sensor. It also shows potential for\ngeneralization across indenters, sensors of the same type, and for enabling\nreal-time application. The codebase, dataset and models are open-sourced and\navailable at https://feats-ai.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contact-rich manipulation remains a major challenge in robotics. Optical\ntactile sensors like GelSight Mini offer a low-cost solution for contact\nsensing by capturing soft-body deformations of the silicone gel. However,\naccurately inferring shear and normal force distributions from these gel\ndeformations has yet to be fully addressed. In this work, we propose a machine\nlearning approach using a U-net architecture to predict force distributions\ndirectly from the sensor's raw images. Our model, trained on force\ndistributions inferred from Finite Element Analysis (FEA), demonstrates\npromising accuracy in predicting normal and shear force distributions for the\ncommercially available GelSight Mini sensor. It also shows potential for\ngeneralization across indenters, sensors of the same type, and for enabling\nreal-time application. The codebase, dataset and models are open-sourced and\navailable at https://feats-ai.github.io ."
                },
                "authors": [
                    {
                        "name": "Erik Helmut"
                    },
                    {
                        "name": "Luca Dziarski"
                    },
                    {
                        "name": "Niklas Funk"
                    },
                    {
                        "name": "Boris Belousov"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08662v2",
                "updated": "2025-03-07T09:55:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    55,
                    19,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-10T09:34:15Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    34,
                    15,
                    0,
                    41,
                    0
                ],
                "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs"
                },
                "summary": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to a mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to a mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Soyoung Yoon"
                    },
                    {
                        "name": "Dongha Ahn"
                    },
                    {
                        "name": "Youngwon Lee"
                    },
                    {
                        "name": "Minkyu Jung"
                    },
                    {
                        "name": "HyungJoo Jang"
                    },
                    {
                        "name": "Seung-won Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seung-won Hwang"
                },
                "author": "Seung-won Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05281v1",
                "updated": "2025-03-07T09:51:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    51,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:51:07Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    51,
                    7,
                    4,
                    66,
                    0
                ],
                "title": "Similarity-Based Domain Adaptation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Based Domain Adaptation with LLMs"
                },
                "summary": "Unsupervised domain adaptation leverages abundant labeled data from various\nsource domains to generalize onto unlabeled target data. Prior research has\nprimarily focused on learning domain-invariant features across the source and\ntarget domains. However, these methods often require training a model using\nsource domain data, which is time-consuming and can limit model usage for\napplications with different source data. This paper introduces a simple\nframework that utilizes the impressive generalization capabilities of Large\nLanguage Models (LLMs) for target data annotation without the need of source\nmodel training, followed by a novel similarity-based knowledge distillation\nloss. Our extensive experiments on cross-domain text classification reveal that\nour framework achieves impressive performance, specifically, 2.44\\% accuracy\nimprovement when compared to the SOTA method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised domain adaptation leverages abundant labeled data from various\nsource domains to generalize onto unlabeled target data. Prior research has\nprimarily focused on learning domain-invariant features across the source and\ntarget domains. However, these methods often require training a model using\nsource domain data, which is time-consuming and can limit model usage for\napplications with different source data. This paper introduces a simple\nframework that utilizes the impressive generalization capabilities of Large\nLanguage Models (LLMs) for target data annotation without the need of source\nmodel training, followed by a novel similarity-based knowledge distillation\nloss. Our extensive experiments on cross-domain text classification reveal that\nour framework achieves impressive performance, specifically, 2.44\\% accuracy\nimprovement when compared to the SOTA method."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05280v1",
                "updated": "2025-03-07T09:49:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    49,
                    31,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:49:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    49,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with\n  Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with\n  Natural Language Processing"
                },
                "summary": "The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https://github.com/causalNLP/censorship",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https://github.com/causalNLP/censorship"
                },
                "authors": [
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Francesco Ortu"
                    },
                    {
                        "name": "Roya Ensafi"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05274v1",
                "updated": "2025-03-07T09:46:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    46,
                    21,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:46:21Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    46,
                    21,
                    4,
                    66,
                    0
                ],
                "title": "Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction"
                },
                "summary": "Accurate trajectory prediction is crucial for autonomous driving, yet\nuncertainty in agent behavior and perception noise makes it inherently\nchallenging. While multi-modal trajectory prediction models generate multiple\nplausible future paths with associated probabilities, effectively quantifying\nuncertainty remains an open problem. In this work, we propose a novel\nmulti-modal trajectory prediction approach based on evidential deep learning\nthat estimates both positional and mode probability uncertainty in real time.\nOur approach leverages a Normal Inverse Gamma distribution for positional\nuncertainty and a Dirichlet distribution for mode uncertainty. Unlike\nsampling-based methods, it infers both types of uncertainty in a single forward\npass, significantly improving efficiency. Additionally, we experimented with\nuncertainty-driven importance sampling to improve training efficiency by\nprioritizing underrepresented high-uncertainty samples over redundant ones. We\nperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2\ndatasets, demonstrating that it provides reliable uncertainty estimates while\nmaintaining high trajectory prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate trajectory prediction is crucial for autonomous driving, yet\nuncertainty in agent behavior and perception noise makes it inherently\nchallenging. While multi-modal trajectory prediction models generate multiple\nplausible future paths with associated probabilities, effectively quantifying\nuncertainty remains an open problem. In this work, we propose a novel\nmulti-modal trajectory prediction approach based on evidential deep learning\nthat estimates both positional and mode probability uncertainty in real time.\nOur approach leverages a Normal Inverse Gamma distribution for positional\nuncertainty and a Dirichlet distribution for mode uncertainty. Unlike\nsampling-based methods, it infers both types of uncertainty in a single forward\npass, significantly improving efficiency. Additionally, we experimented with\nuncertainty-driven importance sampling to improve training efficiency by\nprioritizing underrepresented high-uncertainty samples over redundant ones. We\nperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2\ndatasets, demonstrating that it provides reliable uncertainty estimates while\nmaintaining high trajectory prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Sajad Marvi"
                    },
                    {
                        "name": "Christoph Rist"
                    },
                    {
                        "name": "Julian Schmidt"
                    },
                    {
                        "name": "Julian Jordan"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04150v2",
                "updated": "2025-03-07T09:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    37,
                    53,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T06:59:09Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    59,
                    9,
                    3,
                    65,
                    0
                ],
                "title": "Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression"
                },
                "summary": "Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal."
                },
                "authors": [
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "Yitong Wang"
                    },
                    {
                        "name": "Wenchun Gao"
                    },
                    {
                        "name": "Lianlian Zhang"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Lijun Mei"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05268v1",
                "updated": "2025-03-07T09:33:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    33,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    33,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "ZOGRASCOPE: A New Benchmark for Property Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZOGRASCOPE: A New Benchmark for Property Graphs"
                },
                "summary": "Natural language interfaces to knowledge graphs have become increasingly\nimportant in recent years, enabling easy and efficient access to structured\ndata. In particular property graphs have seen growing adoption. However, these\nkind of graphs remain relatively underrepresented in research, which has\nfocused in large part on RDF-style graphs. As a matter of fact there is a lack\nof resources for evaluating systems on property graphs, with many existing\ndatasets featuring relatively simple queries. To address this gap, we introduce\nZOGRASCOPE, a benchmark designed specifically for the cypher query language.\nThe benchmark includes a diverse set of manually annotated queries of varying\ncomplexity. We complement this paper with a set of experiments that test the\nperformance of out-of-the-box LLMs of different sizes. Our experiments show\nthat semantic parsing over graphs is still a challenging open problem that can\nnot be solved by prompting LLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language interfaces to knowledge graphs have become increasingly\nimportant in recent years, enabling easy and efficient access to structured\ndata. In particular property graphs have seen growing adoption. However, these\nkind of graphs remain relatively underrepresented in research, which has\nfocused in large part on RDF-style graphs. As a matter of fact there is a lack\nof resources for evaluating systems on property graphs, with many existing\ndatasets featuring relatively simple queries. To address this gap, we introduce\nZOGRASCOPE, a benchmark designed specifically for the cypher query language.\nThe benchmark includes a diverse set of manually annotated queries of varying\ncomplexity. We complement this paper with a set of experiments that test the\nperformance of out-of-the-box LLMs of different sizes. Our experiments show\nthat semantic parsing over graphs is still a challenging open problem that can\nnot be solved by prompting LLMs alone."
                },
                "authors": [
                    {
                        "name": "Francesco Cazzaro"
                    },
                    {
                        "name": "Justin Kleindienst"
                    },
                    {
                        "name": "Sofia Marquez"
                    },
                    {
                        "name": "Ariadna Quattoni"
                    }
                ],
                "author_detail": {
                    "name": "Ariadna Quattoni"
                },
                "author": "Ariadna Quattoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.10351v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v4",
                "updated": "2025-03-07T18:59:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    59,
                    21,
                    4,
                    66,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in evaluating\nsocial biases that may be present in the code produced by LLMs. To solve this\nissue, we propose a novel fairness framework, i.e., Solar, to assess and\nmitigate the social biases of LLM-generated code. Specifically, Solar can\nautomatically generate test cases for quantitatively uncovering social biases\nof the auto-generated code by LLMs. To quantify the severity of social biases\nin generated code, we develop a dataset that covers a diverse set of social\nproblems. We applied Solar and the crafted dataset to four state-of-the-art\nLLMs for code generation. Our evaluation reveals severe bias in the\nLLM-generated code from all the subject LLMs. Furthermore, we explore several\nprompting strategies for mitigating bias, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and dialogue with\nSolar. Our experiments show that dialogue with Solar can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Last, we make the code and data\npublicly available is highly extensible to evaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in evaluating\nsocial biases that may be present in the code produced by LLMs. To solve this\nissue, we propose a novel fairness framework, i.e., Solar, to assess and\nmitigate the social biases of LLM-generated code. Specifically, Solar can\nautomatically generate test cases for quantitatively uncovering social biases\nof the auto-generated code by LLMs. To quantify the severity of social biases\nin generated code, we develop a dataset that covers a diverse set of social\nproblems. We applied Solar and the crafted dataset to four state-of-the-art\nLLMs for code generation. Our evaluation reveals severe bias in the\nLLM-generated code from all the subject LLMs. Furthermore, we explore several\nprompting strategies for mitigating bias, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and dialogue with\nSolar. Our experiments show that dialogue with Solar can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Last, we make the code and data\npublicly available is highly extensible to evaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "accepted for publication in the Association for the Advancement of\n  Artificial Intelligence (AAAI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05683v1",
                "updated": "2025-03-07T18:45:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    45,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:45:42Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    45,
                    42,
                    4,
                    66,
                    0
                ],
                "title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Limits of Lifelong Knowledge Editing in LLMs"
                },
                "summary": "Keeping large language models factually up-to-date is crucial for deployment,\nyet costly retraining remains a challenge. Knowledge editing offers a promising\nalternative, but methods are only tested on small-scale or synthetic edit\nbenchmarks. In this work, we aim to bridge research into lifelong knowledge\nediting to real-world edits at practically relevant scale. We first introduce\nWikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to\nautomatically extend lifelong for future-proof benchmarking. In its first\ninstance, it includes over 500K question-answer pairs for knowledge editing\nalongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to\nstudy existing knowledge editing techniques' ability to incorporate large\nvolumes of real-world facts and contrast their capabilities to generic\nmodification techniques such as retrieval augmentation and continual finetuning\nto acquire a complete picture of the practical extent of current lifelong\nknowledge editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping large language models factually up-to-date is crucial for deployment,\nyet costly retraining remains a challenge. Knowledge editing offers a promising\nalternative, but methods are only tested on small-scale or synthetic edit\nbenchmarks. In this work, we aim to bridge research into lifelong knowledge\nediting to real-world edits at practically relevant scale. We first introduce\nWikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to\nautomatically extend lifelong for future-proof benchmarking. In its first\ninstance, it includes over 500K question-answer pairs for knowledge editing\nalongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to\nstudy existing knowledge editing techniques' ability to incorporate large\nvolumes of real-world facts and contrast their capabilities to generic\nmodification techniques such as retrieval augmentation and continual finetuning\nto acquire a complete picture of the practical extent of current lifelong\nknowledge editing."
                },
                "authors": [
                    {
                        "name": "Lukas Thede"
                    },
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05665v1",
                "updated": "2025-03-07T18:26:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    26,
                    48,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:26:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    26,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning\n  Biased Models with Contextual Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning\n  Biased Models with Contextual Synthetic Data"
                },
                "summary": "Recent advances in generative models have sparked research on improving model\nfairness with AI-generated data. However, existing methods often face\nlimitations in the diversity and quality of synthetic data, leading to\ncompromised fairness and overall model accuracy. Moreover, many approaches rely\non the availability of demographic group labels, which are often costly to\nannotate. This paper proposes AIM-Fair, aiming to overcome these limitations\nand harness the potential of cutting-edge generative models in promoting\nalgorithmic fairness. We investigate a fine-tuning paradigm starting from a\nbiased model initially trained on real-world data without demographic\nannotations. This model is then fine-tuned using unbiased synthetic data\ngenerated by a state-of-the-art diffusion model to improve its fairness. Two\nkey challenges are identified in this fine-tuning paradigm, 1) the low quality\nof synthetic data, which can still happen even with advanced generative models,\nand 2) the domain and bias gap between real and synthetic data. To address the\nlimitation of synthetic data quality, we propose Contextual Synthetic Data\nGeneration (CSDG) to generate data using a text-to-image diffusion model (T2I)\nwith prompts generated by a context-aware LLM, ensuring both data diversity and\ncontrol of bias in synthetic data. To resolve domain and bias shifts, we\nintroduce a novel selective fine-tuning scheme in which only model parameters\nmore sensitive to bias and less sensitive to domain shift are updated.\nExperiments on CelebA and UTKFace datasets show that our AIM-Fair improves\nmodel fairness while maintaining utility, outperforming both fully and\npartially fine-tuned approaches to model fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative models have sparked research on improving model\nfairness with AI-generated data. However, existing methods often face\nlimitations in the diversity and quality of synthetic data, leading to\ncompromised fairness and overall model accuracy. Moreover, many approaches rely\non the availability of demographic group labels, which are often costly to\nannotate. This paper proposes AIM-Fair, aiming to overcome these limitations\nand harness the potential of cutting-edge generative models in promoting\nalgorithmic fairness. We investigate a fine-tuning paradigm starting from a\nbiased model initially trained on real-world data without demographic\nannotations. This model is then fine-tuned using unbiased synthetic data\ngenerated by a state-of-the-art diffusion model to improve its fairness. Two\nkey challenges are identified in this fine-tuning paradigm, 1) the low quality\nof synthetic data, which can still happen even with advanced generative models,\nand 2) the domain and bias gap between real and synthetic data. To address the\nlimitation of synthetic data quality, we propose Contextual Synthetic Data\nGeneration (CSDG) to generate data using a text-to-image diffusion model (T2I)\nwith prompts generated by a context-aware LLM, ensuring both data diversity and\ncontrol of bias in synthetic data. To resolve domain and bias shifts, we\nintroduce a novel selective fine-tuning scheme in which only model parameters\nmore sensitive to bias and less sensitive to domain shift are updated.\nExperiments on CelebA and UTKFace datasets show that our AIM-Fair improves\nmodel fairness while maintaining utility, outperforming both fully and\npartially fine-tuned approaches to model fairness."
                },
                "authors": [
                    {
                        "name": "Zengqun Zhao"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted at CVPR 2025. Github:\n  https://github.com/zengqunzhao/AIM-Fair. Project page:\n  https://zengqunzhao.github.io/AIMFair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05659v1",
                "updated": "2025-03-07T18:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval"
                },
                "summary": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, search and recommendation systems (collectively referred\nto as information retrieval systems) have evolved significantly to address\nthese challenges. Recent advances in large language models (LLMs) have\ndemonstrated capabilities that surpass human performance in various\nlanguage-related tasks and exhibit general understanding, reasoning, and\ndecision-making abilities. This paper explores the transformative potential of\nlarge language model agents in enhancing search and recommendation systems. We\ndiscuss the motivations and roles of LLM agents, and establish a classification\nframework to elaborate on the existing research. We highlight the immense\npotential of LLM agents in addressing current challenges in search and\nrecommendation, providing insights into future research directions. This paper\nis the first to systematically review and classify the research on LLM agents\nin these domains, offering a novel perspective on leveraging this advanced AI\ntechnology for information retrieval. To help understand the existing works, we\nlist the existing papers on agent-based simulation with large language models\nat this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, search and recommendation systems (collectively referred\nto as information retrieval systems) have evolved significantly to address\nthese challenges. Recent advances in large language models (LLMs) have\ndemonstrated capabilities that surpass human performance in various\nlanguage-related tasks and exhibit general understanding, reasoning, and\ndecision-making abilities. This paper explores the transformative potential of\nlarge language model agents in enhancing search and recommendation systems. We\ndiscuss the motivations and roles of LLM agents, and establish a classification\nframework to elaborate on the existing research. We highlight the immense\npotential of LLM agents in addressing current challenges in search and\nrecommendation, providing insights into future research directions. This paper\nis the first to systematically review and classify the research on LLM agents\nin these domains, offering a novel perspective on leveraging this advanced AI\ntechnology for information retrieval. To help understand the existing works, we\nlist the existing papers on agent-based simulation with large language models\nat this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Tzu-Heng Lin"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04625v2",
                "updated": "2025-03-07T18:13:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    13,
                    22,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T17:11:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "START: Self-taught Reasoner with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "START: Self-taught Reasoner with Tools"
                },
                "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview."
                },
                "authors": [
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dayiheng Liu"
                },
                "author": "Dayiheng Liu",
                "arxiv_comment": "38 pages, 5 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05641v1",
                "updated": "2025-03-07T18:03:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    3,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T18:03:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    3,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning"
                },
                "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "The first three authors contributed equally. Project Page:\n  https://symbolic_moe.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09952v2",
                "updated": "2025-03-07T17:48:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    48,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2024-04-15T17:25:14Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    25,
                    14,
                    0,
                    106,
                    0
                ],
                "title": "LLMorpheus: Mutation Testing using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMorpheus: Mutation Testing using Large Language Models"
                },
                "summary": "In mutation testing, the quality of a test suite is evaluated by introducing\nfaults into a program and determining whether the program's tests detect them.\nMost existing approaches for mutation testing involve the application of a\nfixed set of mutation operators, e.g., replacing a \"+\" with a \"-\", or removing\na function's body. However, certain types of real-world bugs cannot easily be\nsimulated by such approaches, limiting their effectiveness. This paper presents\na technique for mutation testing where placeholders are introduced at\ndesignated locations in a program's source code and where a Large Language\nModel (LLM) is prompted to ask what they could be replaced with. The technique\nis implemented in LLMorpheus, a mutation testing tool for JavaScript, and\nevaluated on 13 subject packages, considering several variations on the\nprompting strategy, and using several LLMs. We find LLMorpheus to be capable of\nproducing mutants that resemble existing bugs that cannot be produced by\nStrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the\nrunning time, cost, and number of mutants produced by LLMorpheus, demonstrating\nits practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mutation testing, the quality of a test suite is evaluated by introducing\nfaults into a program and determining whether the program's tests detect them.\nMost existing approaches for mutation testing involve the application of a\nfixed set of mutation operators, e.g., replacing a \"+\" with a \"-\", or removing\na function's body. However, certain types of real-world bugs cannot easily be\nsimulated by such approaches, limiting their effectiveness. This paper presents\na technique for mutation testing where placeholders are introduced at\ndesignated locations in a program's source code and where a Large Language\nModel (LLM) is prompted to ask what they could be replaced with. The technique\nis implemented in LLMorpheus, a mutation testing tool for JavaScript, and\nevaluated on 13 subject packages, considering several variations on the\nprompting strategy, and using several LLMs. We find LLMorpheus to be capable of\nproducing mutants that resemble existing bugs that cannot be produced by\nStrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the\nrunning time, cost, and number of mutants produced by LLMorpheus, demonstrating\nits practicality."
                },
                "authors": [
                    {
                        "name": "Frank Tip"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Max Schaefer"
                    }
                ],
                "author_detail": {
                    "name": "Max Schaefer"
                },
                "author": "Max Schaefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05620v1",
                "updated": "2025-03-07T17:46:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    46,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:46:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    46,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for\n  Utterance-level Understandings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for\n  Utterance-level Understandings"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc."
                },
                "authors": [
                    {
                        "name": "Xuanqing Liu"
                    },
                    {
                        "name": "Luyang Kong"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Afshin Khashei"
                    },
                    {
                        "name": "Belinda Zeng"
                    },
                    {
                        "name": "Steve Johnson"
                    },
                    {
                        "name": "Jon Jay"
                    },
                    {
                        "name": "Davor Golac"
                    },
                    {
                        "name": "Matt Pope"
                    }
                ],
                "author_detail": {
                    "name": "Matt Pope"
                },
                "author": "Matt Pope",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05613v1",
                "updated": "2025-03-07T17:38:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:38:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09433v3",
                "updated": "2025-03-07T17:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    33,
                    50,
                    4,
                    66,
                    0
                ],
                "published": "2023-04-19T06:00:26Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    6,
                    0,
                    26,
                    2,
                    109,
                    0
                ],
                "title": "Language Models Enable Simple Systems for Generating Structured Views of\n  Heterogeneous Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Enable Simple Systems for Generating Structured Views of\n  Heterogeneous Data Lakes"
                },
                "summary": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Brandon Yang"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Avanika Narayan"
                    },
                    {
                        "name": "Andrew Hojel"
                    },
                    {
                        "name": "Immanuel Trummer"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16976v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16976v3",
                "updated": "2025-03-07T17:24:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    24,
                    35,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-23T06:22:49Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    6,
                    22,
                    49,
                    6,
                    175,
                    0
                ],
                "title": "Efficient Evolutionary Search Over Chemical Space with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Evolutionary Search Over Chemical Space with Large Language\n  Models"
                },
                "summary": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO"
                },
                "authors": [
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Cher-Tian Ser"
                    },
                    {
                        "name": "Wenhao Gao"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Felix Strieth-Kalthoff"
                    },
                    {
                        "name": "Chenru Duan"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Kirill Neklyudov"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16976v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16976v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05592v1",
                "updated": "2025-03-07T17:14:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    14,
                    44,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:14:44Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    14,
                    44,
                    4,
                    66,
                    0
                ],
                "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05587v1",
                "updated": "2025-03-07T17:11:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    11,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T17:11:34Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    11,
                    34,
                    4,
                    66,
                    0
                ],
                "title": "Quantifying the Robustness of Retrieval-Augmented Language Models\n  Against Spurious Features in Grounding Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Robustness of Retrieval-Augmented Language Models\n  Against Spurious Features in Grounding Data"
                },
                "summary": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$."
                },
                "authors": [
                    {
                        "name": "Shiping Yang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Wenbiao Ding"
                    },
                    {
                        "name": "Ning Wu"
                    },
                    {
                        "name": "Shining Liang"
                    },
                    {
                        "name": "Ming Gong"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02355v3",
                "updated": "2025-03-07T17:06:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    6,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-03T10:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    6,
                    27,
                    3,
                    277,
                    0
                ],
                "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Shi Jie"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_journal_ref": "13th International Conference on Learning Representations (ICLR\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05571v1",
                "updated": "2025-03-07T16:53:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    53,
                    36,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:53:36Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    53,
                    36,
                    4,
                    66,
                    0
                ],
                "title": "Compliance of AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compliance of AI Systems"
                },
                "summary": "The increasing integration of artificial intelligence (AI) systems in various\nfields requires solid concepts to ensure compliance with upcoming legislation.\nThis paper systematically examines the compliance of AI systems with relevant\nlegislation, focusing on the EU's AI Act and the compliance of data sets. The\nanalysis highlighted many challenges associated with edge devices, which are\nincreasingly being used to deploy AI applications closer and closer to the data\nsources. Such devices often face unique issues due to their decentralized\nnature and limited computing resources for implementing sophisticated\ncompliance mechanisms. By analyzing AI implementations, the paper identifies\nchallenges and proposes the first best practices for legal compliance when\ndeveloping, deploying, and running AI. The importance of data set compliance is\nhighlighted as a cornerstone for ensuring the trustworthiness, transparency,\nand explainability of AI systems, which must be aligned with ethical standards\nset forth in regulatory frameworks such as the AI Act. The insights gained\nshould contribute to the ongoing discourse on the responsible development and\ndeployment of embedded AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of artificial intelligence (AI) systems in various\nfields requires solid concepts to ensure compliance with upcoming legislation.\nThis paper systematically examines the compliance of AI systems with relevant\nlegislation, focusing on the EU's AI Act and the compliance of data sets. The\nanalysis highlighted many challenges associated with edge devices, which are\nincreasingly being used to deploy AI applications closer and closer to the data\nsources. Such devices often face unique issues due to their decentralized\nnature and limited computing resources for implementing sophisticated\ncompliance mechanisms. By analyzing AI implementations, the paper identifies\nchallenges and proposes the first best practices for legal compliance when\ndeveloping, deploying, and running AI. The importance of data set compliance is\nhighlighted as a cornerstone for ensuring the trustworthiness, transparency,\nand explainability of AI systems, which must be aligned with ethical standards\nset forth in regulatory frameworks such as the AI Act. The insights gained\nshould contribute to the ongoing discourse on the responsible development and\ndeployment of embedded AI systems."
                },
                "authors": [
                    {
                        "name": "Julius Schöning"
                    },
                    {
                        "name": "Niklas Kruse"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Kruse"
                },
                "author": "Niklas Kruse",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05565v1",
                "updated": "2025-03-07T16:45:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    45,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:45:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    45,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Evaluating open-source Large Language Models for automated fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating open-source Large Language Models for automated fact-checking"
                },
                "summary": "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers."
                },
                "authors": [
                    {
                        "name": "Nicolo' Fontana"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Enrico Zuccolotto"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "arxiv_comment": "Main: 10 pages, 13 figures. Supplementary Materials: 7 pages, 29\n  figures, 1 table ### This work has been submitted to the IEEE for possible\n  publication. ###",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17975v3",
                "updated": "2025-03-07T16:30:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    30,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-25T23:12:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    23,
                    12,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)"
                },
                "summary": "Whether LLMs memorize their training data and what this means, from measuring\nprivacy leakage to detecting copyright violations, has become a rapidly growing\narea of research. In the last few months, more than 10 new methods have been\nproposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary\nto traditional MIAs which rely on fixed-but randomized-records or models, these\nmethods are mostly trained and tested on datasets collected post-hoc. Sets of\nmembers and non-members, used to evaluate the MIA, are constructed using\ninformed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In\nthis work, we first extensively review the literature on MIAs against LLMs and\nshow that, while most work focuses on sequence-level MIAs evaluated in post-hoc\nsetups, a range of target models, motivations and units of interest are\nconsidered. We then quantify distribution shifts present in 6 datasets used in\nthe literature using a model-less bag of word classifier and show that all\ndatasets constructed post-hoc suffer from strong distribution shifts. These\nshifts invalidate the claims of LLMs memorizing strongly in real-world\nscenarios and, potentially, also the methodological contributions of the recent\npapers based on these datasets. Yet, all hope might not be lost. We introduce\nimportant considerations to properly evaluate MIAs against LLMs and discuss, in\nturn, potential ways forwards: randomized test splits, injections of randomized\n(unique) sequences, randomized fine-tuning, and several post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide MIA development and\nstudy LLM memorization. We conclude with an overview of recommended approaches\nto benchmark sequence-level and document-level MIAs against LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether LLMs memorize their training data and what this means, from measuring\nprivacy leakage to detecting copyright violations, has become a rapidly growing\narea of research. In the last few months, more than 10 new methods have been\nproposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary\nto traditional MIAs which rely on fixed-but randomized-records or models, these\nmethods are mostly trained and tested on datasets collected post-hoc. Sets of\nmembers and non-members, used to evaluate the MIA, are constructed using\ninformed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In\nthis work, we first extensively review the literature on MIAs against LLMs and\nshow that, while most work focuses on sequence-level MIAs evaluated in post-hoc\nsetups, a range of target models, motivations and units of interest are\nconsidered. We then quantify distribution shifts present in 6 datasets used in\nthe literature using a model-less bag of word classifier and show that all\ndatasets constructed post-hoc suffer from strong distribution shifts. These\nshifts invalidate the claims of LLMs memorizing strongly in real-world\nscenarios and, potentially, also the methodological contributions of the recent\npapers based on these datasets. Yet, all hope might not be lost. We introduce\nimportant considerations to properly evaluate MIAs against LLMs and discuss, in\nturn, potential ways forwards: randomized test splits, injections of randomized\n(unique) sequences, randomized fine-tuning, and several post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide MIA development and\nstudy LLM memorization. We conclude with an overview of recommended approaches\nto benchmark sequence-level and document-level MIAs against LLMs."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Shubham Jain"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "arxiv_comment": "IEEE Conference on Secure and Trustworthy Machine Learning (SaTML\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05551v1",
                "updated": "2025-03-07T16:25:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    25,
                    9,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:25:09Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    25,
                    9,
                    4,
                    66,
                    0
                ],
                "title": "Revitalizing Saturated Benchmarks: A Weighted Metric Approach for\n  Differentiating Large Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revitalizing Saturated Benchmarks: A Weighted Metric Approach for\n  Differentiating Large Language Model Performance"
                },
                "summary": "Existing benchmarks are becoming saturated and struggle to separate model\nperformances due to factors like data contamination and advancing LLM\ncapabilities. This paper introduces EMDM (Enhanced Model Differentiation\nMetric), a novel weighted metric that revitalizes benchmarks by enhancing model\nseparation. EMDM integrates final answer and Chain-of-Thought (CoT) reasoning\ncorrectness, assigning weights based on the complexity and reasoning depth\nrequired to solve a given sample in the evaluation data. Using a baseline LLM\nin two setups-Unguided, where the model has no prior exposure to test samples,\nand Guided, where the model has prior knowledge of the desired answer-EMDM\ndistinguishes instances of varying difficulty. The CoT and answer correctness\nfrom these setups inform an optimization objective for weight assignment,\nresulting in a more nuanced evaluation of model performance. Compared to the\nexact match (EM) metric, which achieves 17% separation on ARC-Challenge, EMDM\nachieves 46%, demonstrating its effectiveness in differentiating models based\non reasoning and knowledge requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks are becoming saturated and struggle to separate model\nperformances due to factors like data contamination and advancing LLM\ncapabilities. This paper introduces EMDM (Enhanced Model Differentiation\nMetric), a novel weighted metric that revitalizes benchmarks by enhancing model\nseparation. EMDM integrates final answer and Chain-of-Thought (CoT) reasoning\ncorrectness, assigning weights based on the complexity and reasoning depth\nrequired to solve a given sample in the evaluation data. Using a baseline LLM\nin two setups-Unguided, where the model has no prior exposure to test samples,\nand Guided, where the model has prior knowledge of the desired answer-EMDM\ndistinguishes instances of varying difficulty. The CoT and answer correctness\nfrom these setups inform an optimization objective for weight assignment,\nresulting in a more nuanced evaluation of model performance. Compared to the\nexact match (EM) metric, which achieves 17% separation on ARC-Challenge, EMDM\nachieves 46%, demonstrating its effectiveness in differentiating models based\non reasoning and knowledge requirements."
                },
                "authors": [
                    {
                        "name": "Bryan Etzine"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Nishanth Madhusudhan"
                    },
                    {
                        "name": "Sagar Davasam"
                    },
                    {
                        "name": "Roshnee Sharma"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Vikas Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Yadav"
                },
                "author": "Vikas Yadav",
                "arxiv_comment": "conference NAACL, TrustNLP Workshop",
                "arxiv_journal_ref": "TrustNLP workshop NAACL, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05543v1",
                "updated": "2025-03-07T16:15:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    15,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T16:15:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    16,
                    15,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information"
                },
                "summary": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance."
                },
                "authors": [
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Jiayu Sun"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05531v1",
                "updated": "2025-03-07T15:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    58,
                    36,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:58:36Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    58,
                    36,
                    4,
                    66,
                    0
                ],
                "title": "State-of-the-Art Stroke Lesion Segmentation at 1/1000th of Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-Art Stroke Lesion Segmentation at 1/1000th of Parameters"
                },
                "summary": "Efficient and accurate whole-brain lesion segmentation remains a challenge in\nmedical image analysis. In this work, we revisit MeshNet, a parameter-efficient\nsegmentation model, and introduce a novel multi-scale dilation pattern with an\nencoder-decoder structure. This innovation enables capturing broad contextual\ninformation and fine-grained details without traditional downsampling,\nupsampling, or skip-connections. Unlike previous approaches processing\nsubvolumes or slices, we operate directly on whole-brain $256^3$ MRI volumes.\nEvaluations on the Aphasia Recovery Cohort (ARC) dataset demonstrate that\nMeshNet achieves superior or comparable DICE scores to state-of-the-art\narchitectures such as MedNeXt and U-MAMBA at 1/1000th of parameters. Our\nresults validate MeshNet's strong balance of efficiency and performance, making\nit particularly suitable for resource-limited environments such as web-based\napplications and opening new possibilities for the widespread deployment of\nadvanced medical image analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and accurate whole-brain lesion segmentation remains a challenge in\nmedical image analysis. In this work, we revisit MeshNet, a parameter-efficient\nsegmentation model, and introduce a novel multi-scale dilation pattern with an\nencoder-decoder structure. This innovation enables capturing broad contextual\ninformation and fine-grained details without traditional downsampling,\nupsampling, or skip-connections. Unlike previous approaches processing\nsubvolumes or slices, we operate directly on whole-brain $256^3$ MRI volumes.\nEvaluations on the Aphasia Recovery Cohort (ARC) dataset demonstrate that\nMeshNet achieves superior or comparable DICE scores to state-of-the-art\narchitectures such as MedNeXt and U-MAMBA at 1/1000th of parameters. Our\nresults validate MeshNet's strong balance of efficiency and performance, making\nit particularly suitable for resource-limited environments such as web-based\napplications and opening new possibilities for the widespread deployment of\nadvanced medical image analysis tools."
                },
                "authors": [
                    {
                        "name": "Alex Fedorov"
                    },
                    {
                        "name": "Yutong Bu"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Chris Rorden"
                    },
                    {
                        "name": "Sergey Plis"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Plis"
                },
                "author": "Sergey Plis",
                "arxiv_comment": "International Symposium on Biomedical Imaging, April 14-17, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05529v1",
                "updated": "2025-03-07T15:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    49,
                    56,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    49,
                    56,
                    4,
                    66,
                    0
                ],
                "title": "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs"
                },
                "summary": "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods."
                },
                "authors": [
                    {
                        "name": "Roberto Cerina"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Cerina"
                },
                "author": "Roberto Cerina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05516v1",
                "updated": "2025-03-07T15:35:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    35,
                    37,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:35:37Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    35,
                    37,
                    4,
                    66,
                    0
                ],
                "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Bias Detection Using Advanced Prompt Engineering"
                },
                "summary": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making."
                },
                "authors": [
                    {
                        "name": "Frederic Lemieux"
                    },
                    {
                        "name": "Aisha Behr"
                    },
                    {
                        "name": "Clara Kellermann-Bryant"
                    },
                    {
                        "name": "Zaki Mohammed"
                    }
                ],
                "author_detail": {
                    "name": "Zaki Mohammed"
                },
                "author": "Zaki Mohammed",
                "arxiv_comment": "17 pages. 6 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09760v2",
                "updated": "2025-03-07T15:26:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    26,
                    3,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-14T06:57:18Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    6,
                    57,
                    18,
                    4,
                    166,
                    0
                ],
                "title": "Bootstrapping Language Models with DPO Implicit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Language Models with DPO Implicit Rewards"
                },
                "summary": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM to construct a preference dataset, which is then\nused in subsequent DPO rounds. We incorporate two refinements to further\nimprove our approach: 1) length-regularized reward shaping to make the\npreference dataset length-unbiased; 2) experience replay to enhance the quality\nof the preference dataset. Our approach, named self-alignment with DPO ImpliCit\nrEwards (DICE), shows great improvements in alignment. It achieves an increase\nof more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the\ndifferent base models that we tried, without relying on external feedback. Our\ncode is available at https://github.com/sail-sg/dice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM to construct a preference dataset, which is then\nused in subsequent DPO rounds. We incorporate two refinements to further\nimprove our approach: 1) length-regularized reward shaping to make the\npreference dataset length-unbiased; 2) experience replay to enhance the quality\nof the preference dataset. Our approach, named self-alignment with DPO ImpliCit\nrEwards (DICE), shows great improvements in alignment. It achieves an increase\nof more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the\ndifferent base models that we tried, without relying on external feedback. Our\ncode is available at https://github.com/sail-sg/dice."
                },
                "authors": [
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "Accepted in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05507v1",
                "updated": "2025-03-07T15:23:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    23,
                    13,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:23:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    23,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?"
                },
                "summary": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Yueyi Xiao"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yingfei Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yingfei Xiong"
                },
                "author": "Yingfei Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05505v1",
                "updated": "2025-03-07T15:22:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:22:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "Statistical Guarantees of Correctness Coverage for Medical\n  Multiple-Choice Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Guarantees of Correctness Coverage for Medical\n  Multiple-Choice Question Answering"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in real-world\nquestion-answering (QA) applications. However, LLMs have been proven to\ngenerate hallucinations and nonfactual information, undermining their\ntrustworthiness in high-stakes medical tasks. Conformal prediction (CP) is\nwell-known to be model-agnostic and distribution-free, which creates\nstatistically rigorous prediction sets in classification tasks. In this work,\nwe for the first time adapt the CP framework to medical multiple-choice\nquestion-answering (MCQA) tasks, by correlating the nonconformity score with\nthe frequency score of correct options grounded in self-consistency theory,\nassuming no access to internal model information. Considering that the adapted\nCP framework can only control the (mis)coverage rate, we employ a risk control\nframework, which can manage task-specific metrics by devising a monotonically\ndecreasing loss function. We evaluate our framework on 3 popular medical MCQA\ndatasets utilizing 4 ``off-the-shelf'' LLMs. Empirical results demonstrate that\nwe achieve user-specified average (or marginal) error rates on the test set.\nFurthermore, we observe that the average prediction set size (APSS) on the test\nset decreases as the risk level increases, which concludes a promising\nevaluation metric for the uncertainty of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in real-world\nquestion-answering (QA) applications. However, LLMs have been proven to\ngenerate hallucinations and nonfactual information, undermining their\ntrustworthiness in high-stakes medical tasks. Conformal prediction (CP) is\nwell-known to be model-agnostic and distribution-free, which creates\nstatistically rigorous prediction sets in classification tasks. In this work,\nwe for the first time adapt the CP framework to medical multiple-choice\nquestion-answering (MCQA) tasks, by correlating the nonconformity score with\nthe frequency score of correct options grounded in self-consistency theory,\nassuming no access to internal model information. Considering that the adapted\nCP framework can only control the (mis)coverage rate, we employ a risk control\nframework, which can manage task-specific metrics by devising a monotonically\ndecreasing loss function. We evaluate our framework on 3 popular medical MCQA\ndatasets utilizing 4 ``off-the-shelf'' LLMs. Empirical results demonstrate that\nwe achieve user-specified average (or marginal) error rates on the test set.\nFurthermore, we observe that the average prediction set size (APSS) on the test\nset decreases as the risk level increases, which concludes a promising\nevaluation metric for the uncertainty of LLMs."
                },
                "authors": [
                    {
                        "name": "Yusong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Yusong Ke"
                },
                "author": "Yusong Ke",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08080v2",
                "updated": "2025-03-07T15:17:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    17,
                    43,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-12T02:54:12Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    2,
                    54,
                    12,
                    2,
                    43,
                    0
                ],
                "title": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals"
                },
                "summary": "Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts."
                },
                "authors": [
                    {
                        "name": "Neha Srikanth"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00758v2",
                "updated": "2025-03-07T15:12:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    12,
                    57,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-01T14:52:14Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    14,
                    52,
                    14,
                    1,
                    275,
                    0
                ],
                "title": "Under Pressure: Altimeter-Aided ICP for 3D Maps Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under Pressure: Altimeter-Aided ICP for 3D Maps Consistency"
                },
                "summary": "We propose a novel method to enhance the accuracy of the Iterative Closest\nPoint (ICP) algorithm by integrating altitude constraints from a barometric\npressure sensor. While ICP is widely used in mobile robotics for Simultaneous\nLocalization and Mapping ( SLAM ), it is susceptible to drift, especially in\nunderconstrained environments such as vertical shafts. To address this issue,\nwe propose to augment ICP with altimeter measurements, reliably constraining\ndrifts along the gravity vector. To demonstrate the potential of altimetry in\nSLAM , we offer an analysis of calibration procedures and noise sensitivity of\nvarious pressure sensors, improving measurements to centimeter-level accuracy.\nLeveraging this accuracy, we propose a novel ICP formulation that integrates\naltitude measurements along the gravity vector, thus simplifying the\noptimization problem to 3-Degree Of Freedom (DOF). Experimental results from\nreal-world deployments demonstrate that our method reduces vertical drift by\n84% and improves overall localization accuracy compared to state-of-the-art\nmethods in non-planar environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel method to enhance the accuracy of the Iterative Closest\nPoint (ICP) algorithm by integrating altitude constraints from a barometric\npressure sensor. While ICP is widely used in mobile robotics for Simultaneous\nLocalization and Mapping ( SLAM ), it is susceptible to drift, especially in\nunderconstrained environments such as vertical shafts. To address this issue,\nwe propose to augment ICP with altimeter measurements, reliably constraining\ndrifts along the gravity vector. To demonstrate the potential of altimetry in\nSLAM , we offer an analysis of calibration procedures and noise sensitivity of\nvarious pressure sensors, improving measurements to centimeter-level accuracy.\nLeveraging this accuracy, we propose a novel ICP formulation that integrates\naltitude measurements along the gravity vector, thus simplifying the\noptimization problem to 3-Degree Of Freedom (DOF). Experimental results from\nreal-world deployments demonstrate that our method reduces vertical drift by\n84% and improves overall localization accuracy compared to state-of-the-art\nmethods in non-planar environments."
                },
                "authors": [
                    {
                        "name": "William Dubois"
                    },
                    {
                        "name": "Nicolas Samson"
                    },
                    {
                        "name": "Effie Daum"
                    },
                    {
                        "name": "Johann Laconte"
                    },
                    {
                        "name": "François Pomerleau"
                    }
                ],
                "author_detail": {
                    "name": "François Pomerleau"
                },
                "author": "François Pomerleau",
                "arxiv_comment": "6 pages + references, 5 figures, final version accepted for ICRA25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04704v2",
                "updated": "2025-03-07T15:12:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    12,
                    57,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T18:54:32Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size"
                },
                "summary": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\nconsistent improvements in the quality-compression trade-off regardless of\nmodel scale or architectural design. A surprising finding of EWQ is its ability\nto reduce perplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\nconsistent improvements in the quality-compression trade-off regardless of\nmodel scale or architectural design. A surprising finding of EWQ is its ability\nto reduce perplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Alireza Behtash"
                    },
                    {
                        "name": "Marijan Fofonjka"
                    },
                    {
                        "name": "Ethan Baird"
                    },
                    {
                        "name": "Tyler Mauer"
                    },
                    {
                        "name": "Hossein Moghimifam"
                    },
                    {
                        "name": "David Stout"
                    },
                    {
                        "name": "Joel Dennison"
                    }
                ],
                "author_detail": {
                    "name": "Joel Dennison"
                },
                "author": "Joel Dennison",
                "arxiv_comment": "29 pages, 7 figures, 14 tables; Fixed some types, added some\n  clarifications and improvements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05495v1",
                "updated": "2025-03-07T15:05:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    56,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:05:56Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    56,
                    4,
                    66,
                    0
                ],
                "title": "Umbilical Choir: Automated Live Testing for Edge-To-Cloud FaaS\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Umbilical Choir: Automated Live Testing for Edge-To-Cloud FaaS\n  Applications"
                },
                "summary": "Application users react negatively to performance regressions or availability\nissues across software releases. To address this, modern cloud-based\napplications with their multiple daily releases rely on live testing techniques\nsuch as A/B testing or canary releases. In edge-to-cloud applications, however,\nwhich have similar problems, developers currently still have to hard-code\ncustom live testing tooling as there is no general framework for edge-to-cloud\nlive testing.\n  With Umbilical Choir, we partially close this gap for serverless\nedge-to-cloud applications. Umbilical Choir is compatible with all\nFunction-as-a-Service platforms and (extensively) supports various live testing\ntechniques, including canary releases with various geo-aware strategies, A/B\ntesting, and gradual roll-outs. We evaluate Umbilical Choir through a complex\nrelease scenario showcasing various live testing techniques in a mixed\nedge-cloud deployments and discuss different geo-aware strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application users react negatively to performance regressions or availability\nissues across software releases. To address this, modern cloud-based\napplications with their multiple daily releases rely on live testing techniques\nsuch as A/B testing or canary releases. In edge-to-cloud applications, however,\nwhich have similar problems, developers currently still have to hard-code\ncustom live testing tooling as there is no general framework for edge-to-cloud\nlive testing.\n  With Umbilical Choir, we partially close this gap for serverless\nedge-to-cloud applications. Umbilical Choir is compatible with all\nFunction-as-a-Service platforms and (extensively) supports various live testing\ntechniques, including canary releases with various geo-aware strategies, A/B\ntesting, and gradual roll-outs. We evaluate Umbilical Choir through a complex\nrelease scenario showcasing various live testing techniques in a mixed\nedge-cloud deployments and discuss different geo-aware strategies."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Malekabbasi"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_comment": "Accepted for publication in 9th IEEE International Conference on Fog\n  and Edge Computing (ICFEC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05493v1",
                "updated": "2025-03-07T15:05:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:05:23Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "title": "Benchmarking LLMs in Recommendation Tasks: A Comparative Evaluation with\n  Conventional Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Recommendation Tasks: A Comparative Evaluation with\n  Conventional Recommenders"
                },
                "summary": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05481v1",
                "updated": "2025-03-07T14:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Maximum Hallucination Standards for Domain-Specific Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Hallucination Standards for Domain-Specific Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation."
                },
                "authors": [
                    {
                        "name": "Tingmingke Lu"
                    }
                ],
                "author_detail": {
                    "name": "Tingmingke Lu"
                },
                "author": "Tingmingke Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05480v1",
                "updated": "2025-03-07T14:51:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    8,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:51:08Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    51,
                    8,
                    4,
                    66,
                    0
                ],
                "title": "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks"
                },
                "summary": "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization."
                },
                "authors": [
                    {
                        "name": "Guillermo Encinas-Lago"
                    },
                    {
                        "name": "Vincenzo Sciancalepore"
                    },
                    {
                        "name": "Henk Wymeersch"
                    },
                    {
                        "name": "Marco Di Renzo"
                    },
                    {
                        "name": "Xavier Costa-Perez"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Costa-Perez"
                },
                "author": "Xavier Costa-Perez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05479v1",
                "updated": "2025-03-07T14:50:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    50,
                    2,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:50:02Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    50,
                    2,
                    4,
                    66,
                    0
                ],
                "title": "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects"
                },
                "summary": "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules."
                },
                "authors": [
                    {
                        "name": "Sergio Cobos"
                    },
                    {
                        "name": "Javier Luis Cánovas Izquierdo"
                    }
                ],
                "author_detail": {
                    "name": "Javier Luis Cánovas Izquierdo"
                },
                "author": "Javier Luis Cánovas Izquierdo",
                "arxiv_comment": "Accepted at the 2025 IEEE/ACM 47th International Conference on\n  Software Engineering: Software Engineering in Society (ICSE-SEIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12481v2",
                "updated": "2025-03-07T14:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    46,
                    22,
                    4,
                    66,
                    0
                ],
                "published": "2024-08-22T15:17:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    17,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "Self-Learning for Personalized Keyword Spotting on Ultra-Low-Power Audio\n  Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Learning for Personalized Keyword Spotting on Ultra-Low-Power Audio\n  Sensors"
                },
                "summary": "This paper proposes a self-learning method to incrementally train (fine-tune)\na personalized Keyword Spotting (KWS) model after the deployment on ultra-low\npower smart audio sensors. We address the fundamental problem of the absence of\nlabeled training data by assigning pseudo-labels to the new recorded audio\nframes based on a similarity score with respect to few user recordings. By\nexperimenting with multiple KWS models with a number of parameters up to 0.5M\non two public datasets, we show an accuracy improvement of up to +19.2% and\n+16.0% vs. the initial models pretrained on a large set of generic keywords.\nThe labeling task is demonstrated on a sensor system composed of a low-power\nmicrophone and an energy-efficient Microcontroller (MCU). By efficiently\nexploiting the heterogeneous processing engines of the MCU, the always-on\nlabeling task runs in real-time with an average power cost of up to 8.2 mW. On\nthe same platform, we estimate an energy cost for on-device training 10x lower\nthan the labeling energy if sampling a new utterance every 6.1 s or 18.8 s with\na DS-CNN-S or a DS-CNN-M model. Our empirical result paves the way to\nself-adaptive personalized KWS sensors at the extreme edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a self-learning method to incrementally train (fine-tune)\na personalized Keyword Spotting (KWS) model after the deployment on ultra-low\npower smart audio sensors. We address the fundamental problem of the absence of\nlabeled training data by assigning pseudo-labels to the new recorded audio\nframes based on a similarity score with respect to few user recordings. By\nexperimenting with multiple KWS models with a number of parameters up to 0.5M\non two public datasets, we show an accuracy improvement of up to +19.2% and\n+16.0% vs. the initial models pretrained on a large set of generic keywords.\nThe labeling task is demonstrated on a sensor system composed of a low-power\nmicrophone and an energy-efficient Microcontroller (MCU). By efficiently\nexploiting the heterogeneous processing engines of the MCU, the always-on\nlabeling task runs in real-time with an average power cost of up to 8.2 mW. On\nthe same platform, we estimate an energy cost for on-device training 10x lower\nthan the labeling energy if sampling a new utterance every 6.1 s or 18.8 s with\na DS-CNN-S or a DS-CNN-M model. Our empirical result paves the way to\nself-adaptive personalized KWS sensors at the extreme edge."
                },
                "authors": [
                    {
                        "name": "Manuele Rusci"
                    },
                    {
                        "name": "Francesco Paci"
                    },
                    {
                        "name": "Marco Fariselli"
                    },
                    {
                        "name": "Eric Flamand"
                    },
                    {
                        "name": "Tinne Tuytelaars"
                    }
                ],
                "author_detail": {
                    "name": "Tinne Tuytelaars"
                },
                "author": "Tinne Tuytelaars",
                "arxiv_doi": "10.1109/JIOT.2024.3515143",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2024.3515143",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on IEEE IoT Journal",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00435v2",
                "updated": "2025-03-07T14:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    33,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-01T10:24:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    10,
                    24,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error\n  Fixing for Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error\n  Fixing for Tabular Question Answering"
                },
                "summary": "In this paper, we present our submission to SemEval-2025 Task 8: Question\nAnswering over Tabular Data. This task, evaluated on the DataBench dataset,\nassesses Large Language Models' (LLMs) ability to answer natural language\nquestions over structured data while addressing topic diversity and table size\nlimitations in previous benchmarks. We propose a system that employs effective\nLLM prompting to translate natural language queries into executable code,\nenabling accurate responses, error correction, and interpretability. Our\napproach ranks first in both subtasks of the competition in the proprietary\nmodel category, significantly outperforming the organizer's baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present our submission to SemEval-2025 Task 8: Question\nAnswering over Tabular Data. This task, evaluated on the DataBench dataset,\nassesses Large Language Models' (LLMs) ability to answer natural language\nquestions over structured data while addressing topic diversity and table size\nlimitations in previous benchmarks. We propose a system that employs effective\nLLM prompting to translate natural language queries into executable code,\nenabling accurate responses, error correction, and interpretability. Our\napproach ranks first in both subtasks of the competition in the proprietary\nmodel category, significantly outperforming the organizer's baseline."
                },
                "authors": [
                    {
                        "name": "Andreas Evangelatos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Athanasios Voulodimos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01161v2",
                "updated": "2025-03-07T14:24:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    24,
                    6,
                    4,
                    66,
                    0
                ],
                "published": "2024-07-01T10:29:56Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    10,
                    29,
                    56,
                    0,
                    183,
                    0
                ],
                "title": "GazeNoter: Co-Piloted AR Note-Taking via Gaze Selection of LLM\n  Suggestions to Match Users' Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GazeNoter: Co-Piloted AR Note-Taking via Gaze Selection of LLM\n  Suggestions to Match Users' Intentions"
                },
                "summary": "Note-taking is critical during speeches and discussions, serving not only for\nlater summarization and organization but also for real-time question and\nopinion reminding in question-and-answer sessions or timely contributions in\ndiscussions. Manually typing on smartphones for note-taking could be\ndistracting and increase cognitive load for users. While large language models\n(LLMs) are used to automatically generate summaries and highlights, the content\ngenerated by artificial intelligence (AI) may not match users' intentions\nwithout user input or interaction. Therefore, we propose an AI-copiloted\naugmented reality (AR) system, GazeNoter, to allow users to swiftly select\ndiverse LLM-generated suggestions via gaze on an AR headset for real-time\nnote-taking. GazeNoter leverages an AR headset as a medium for users to swiftly\nadjust the LLM output to match their intentions, forming a user-in-the-loop AI\nsystem for both within-context and beyond-context notes. We conducted two user\nstudies to verify the usability of GazeNoter in attending speeches in a static\nsitting condition and walking meetings and discussions in a mobile walking\ncondition, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Note-taking is critical during speeches and discussions, serving not only for\nlater summarization and organization but also for real-time question and\nopinion reminding in question-and-answer sessions or timely contributions in\ndiscussions. Manually typing on smartphones for note-taking could be\ndistracting and increase cognitive load for users. While large language models\n(LLMs) are used to automatically generate summaries and highlights, the content\ngenerated by artificial intelligence (AI) may not match users' intentions\nwithout user input or interaction. Therefore, we propose an AI-copiloted\naugmented reality (AR) system, GazeNoter, to allow users to swiftly select\ndiverse LLM-generated suggestions via gaze on an AR headset for real-time\nnote-taking. GazeNoter leverages an AR headset as a medium for users to swiftly\nadjust the LLM output to match their intentions, forming a user-in-the-loop AI\nsystem for both within-context and beyond-context notes. We conducted two user\nstudies to verify the usability of GazeNoter in attending speeches in a static\nsitting condition and walking meetings and discussions in a mobile walking\ncondition, respectively."
                },
                "authors": [
                    {
                        "name": "Hsin-Ruey Tsai"
                    },
                    {
                        "name": "Shih-Kang Chiu"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "arxiv_doi": "10.1145/3706598.3714294",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714294",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.01161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05449v1",
                "updated": "2025-03-07T14:19:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    19,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:19:17Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    19,
                    17,
                    4,
                    66,
                    0
                ],
                "title": "LLM-based Iterative Approach to Metamodeling in Automotive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Iterative Approach to Metamodeling in Automotive"
                },
                "summary": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required."
                },
                "authors": [
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14644v2",
                "updated": "2025-03-07T14:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    18,
                    56,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-20T15:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning"
                },
                "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research."
                },
                "authors": [
                    {
                        "name": "Yansheng Mao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Xiyuan Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.13626",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05445v1",
                "updated": "2025-03-07T14:16:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection\n  via Backdoor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection\n  via Backdoor Attacks"
                },
                "summary": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats."
                },
                "authors": [
                    {
                        "name": "Meiyu Lin"
                    },
                    {
                        "name": "Haichuan Zhang"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Renyuan Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05439v1",
                "updated": "2025-03-07T14:10:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T14:10:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning"
                },
                "summary": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20576v2",
                "updated": "2025-03-07T13:35:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    35,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-27T22:35:31Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    22,
                    35,
                    31,
                    3,
                    58,
                    0
                ],
                "title": "ECCOS: Efficient Capability and Cost Coordinated Scheduling for\n  Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECCOS: Efficient Capability and Cost Coordinated Scheduling for\n  Multi-LLM Serving"
                },
                "summary": "As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. This paper\naddresses this challenge by proposing a capability-cost coordinated scheduling\nframework, ECCOS, for multi-LLM serving, which explicitly constrains response\nquality and workload to optimize LLM inference cost. Specifically, it\nintroduces the two-stage scheduling by designing a multi-objective predictor\nand a constrained optimizer. The predictor estimates both model capabilities\nand computational costs through training-based and retrieval-based approaches,\nwhile the optimizer determines cost-optimal assignments under quality and\nworkload constraints. It also introduces QAServe, a dataset collected for\nsample-wise response quality and costs by zero-shot prompting different LLMs on\nknowledge QA and mathematical reasoning. Extensive experiments demonstrate that\nECCOS improves success rates by 6.30% while reducing costs by 10.15% compared\nto existing methods, consuming less than 0.5% of LLM response time. The code is\navailable at: https://github.com/agiresearch/ECCOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. This paper\naddresses this challenge by proposing a capability-cost coordinated scheduling\nframework, ECCOS, for multi-LLM serving, which explicitly constrains response\nquality and workload to optimize LLM inference cost. Specifically, it\nintroduces the two-stage scheduling by designing a multi-objective predictor\nand a constrained optimizer. The predictor estimates both model capabilities\nand computational costs through training-based and retrieval-based approaches,\nwhile the optimizer determines cost-optimal assignments under quality and\nworkload constraints. It also introduces QAServe, a dataset collected for\nsample-wise response quality and costs by zero-shot prompting different LLMs on\nknowledge QA and mathematical reasoning. Extensive experiments demonstrate that\nECCOS improves success rates by 6.30% while reducing costs by 10.15% compared\nto existing methods, consuming less than 0.5% of LLM response time. The code is\navailable at: https://github.com/agiresearch/ECCOS."
                },
                "authors": [
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05397v1",
                "updated": "2025-03-07T13:20:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    20,
                    12,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:20:12Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    20,
                    12,
                    4,
                    66,
                    0
                ],
                "title": "Multi Agent based Medical Assistant for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi Agent based Medical Assistant for Edge Devices"
                },
                "summary": "Large Action Models (LAMs) have revolutionized intelligent automation, but\ntheir application in healthcare faces challenges due to privacy concerns,\nlatency, and dependency on internet access. This report introduces an ondevice,\nmulti-agent healthcare assistant that overcomes these limitations. The system\nutilizes smaller, task-specific agents to optimize resources, ensure\nscalability and high performance. Our proposed system acts as a one-stop\nsolution for health care needs with features like appointment booking, health\nmonitoring, medication reminders, and daily health reporting. Powered by the\nQwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an\naverage RougeL score of 85.5 for planning and 96.5 for calling for our tasks\nwhile being lightweight for on-device deployment. This innovative approach\ncombines the benefits of ondevice systems with multi-agent architectures,\npaving the way for user-centric healthcare solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Action Models (LAMs) have revolutionized intelligent automation, but\ntheir application in healthcare faces challenges due to privacy concerns,\nlatency, and dependency on internet access. This report introduces an ondevice,\nmulti-agent healthcare assistant that overcomes these limitations. The system\nutilizes smaller, task-specific agents to optimize resources, ensure\nscalability and high performance. Our proposed system acts as a one-stop\nsolution for health care needs with features like appointment booking, health\nmonitoring, medication reminders, and daily health reporting. Powered by the\nQwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an\naverage RougeL score of 85.5 for planning and 96.5 for calling for our tasks\nwhile being lightweight for on-device deployment. This innovative approach\ncombines the benefits of ondevice systems with multi-agent architectures,\npaving the way for user-centric healthcare solutions."
                },
                "authors": [
                    {
                        "name": "Sakharam Gawade"
                    },
                    {
                        "name": "Shivam Akhouri"
                    },
                    {
                        "name": "Chinmay Kulkarni"
                    },
                    {
                        "name": "Jagdish Samant"
                    },
                    {
                        "name": "Pragya Sahu"
                    },
                    {
                        "name": "Aastik"
                    },
                    {
                        "name": "Jai Pahal"
                    },
                    {
                        "name": "Saswat Meher"
                    }
                ],
                "author_detail": {
                    "name": "Saswat Meher"
                },
                "author": "Saswat Meher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05394v1",
                "updated": "2025-03-07T13:09:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    9,
                    37,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:09:37Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    9,
                    37,
                    4,
                    66,
                    0
                ],
                "title": "Static Program Analysis Guided LLM Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Program Analysis Guided LLM Based Unit Test Generation"
                },
                "summary": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project."
                },
                "authors": [
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Sourav Mazumdar"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "Srinivasan Bajji Sugumaran"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05388v1",
                "updated": "2025-03-07T13:03:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    3,
                    28,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T13:03:28Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    13,
                    3,
                    28,
                    4,
                    66,
                    0
                ],
                "title": "Ontology Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Generation using Large Language Models"
                },
                "summary": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research."
                },
                "authors": [
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Robin Keskisärkkä"
                    },
                    {
                        "name": "Sara Zuppiroli"
                    },
                    {
                        "name": "Miguel Ceriani"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Eva Blomqvist"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "2 figures and 3 tables. 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05053v2",
                "updated": "2025-03-07T12:46:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    46,
                    14,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-07T16:22:51Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    22,
                    51,
                    4,
                    159,
                    0
                ],
                "title": "Hints-In-Browser: Benchmarking Language Models for Programming Feedback\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hints-In-Browser: Benchmarking Language Models for Programming Feedback\n  Generation"
                },
                "summary": "Generative AI and large language models hold great promise in enhancing\nprogramming education by generating individualized feedback and hints for\nlearners. Recent works have primarily focused on improving the quality of\ngenerated feedback to achieve human tutors' quality. While quality is an\nimportant performance criterion, it is not the only criterion to optimize for\nreal-world educational deployments. In this paper, we benchmark language models\nfor programming feedback generation across several performance criteria,\nincluding quality, cost, time, and data privacy. The key idea is to leverage\nrecent advances in the new paradigm of in-browser inference that allow running\nthese models directly in the browser, thereby providing direct benefits across\ncost and data privacy. To boost the feedback quality of small models compatible\nwith in-browser inference engines, we develop a fine-tuning pipeline based on\nGPT-4 generated synthetic data. We showcase the efficacy of fine-tuned\nLlama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser\ninference engine on three different Python programming datasets. We will\nrelease the full implementation along with a web app and datasets to facilitate\nfurther research on in-browser language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and large language models hold great promise in enhancing\nprogramming education by generating individualized feedback and hints for\nlearners. Recent works have primarily focused on improving the quality of\ngenerated feedback to achieve human tutors' quality. While quality is an\nimportant performance criterion, it is not the only criterion to optimize for\nreal-world educational deployments. In this paper, we benchmark language models\nfor programming feedback generation across several performance criteria,\nincluding quality, cost, time, and data privacy. The key idea is to leverage\nrecent advances in the new paradigm of in-browser inference that allow running\nthese models directly in the browser, thereby providing direct benefits across\ncost and data privacy. To boost the feedback quality of small models compatible\nwith in-browser inference engines, we develop a fine-tuning pipeline based on\nGPT-4 generated synthetic data. We showcase the efficacy of fine-tuned\nLlama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser\ninference engine on three different Python programming datasets. We will\nrelease the full implementation along with a web app and datasets to facilitate\nfurther research on in-browser language models."
                },
                "authors": [
                    {
                        "name": "Nachiket Kotalwar"
                    },
                    {
                        "name": "Alkis Gotovos"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05376v1",
                "updated": "2025-03-07T12:39:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    39,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:39:07Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    39,
                    7,
                    4,
                    66,
                    0
                ],
                "title": "Femur: A Flexible Framework for Fast and Secure Querying from Public\n  Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Femur: A Flexible Framework for Fast and Secure Querying from Public\n  Key-Value Store"
                },
                "summary": "With increasing demands for privacy, it becomes necessary to protect\nsensitive user query data when accessing public key-value databases. Existing\nPrivate Information Retrieval (PIR) schemes provide full security but suffer\nfrom poor scalability, limiting their applicability in large-scale deployment.\nWe argue that in many real-world scenarios, a more practical solution should\nallow users to flexibly determine the privacy levels of their queries in a\ntheoretically guided way, balancing security and performance based on specific\nneeds. To formally provide provable guarantees, we introduce a novel concept of\ndistance-based indistinguishability, which can facilitate users to comfortably\nrelax their security requirements. We then design Femur, an efficient framework\nto securely query public key-value stores with flexible security and\nperformance trade-offs. It uses a space-efficient learned index to convert\nquery keys into storage locations, obfuscates these locations with extra noise\nprovably derived by the distance-based indistinguishability theory, and sends\nthe expanded range to the server. The server then adaptively utilizes the best\nscheme to retrieve data. We also propose a novel variable-range PIR scheme\noptimized for bandwidth-constrained environments. Experiments show that Femur\noutperforms the state-of-the-art designs even when ensuring the same full\nsecurity level. When users are willing to relax their privacy requirements,\nFemur can further improve the performance gains to up to 163.9X, demonstrating\nan effective trade-off between security and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With increasing demands for privacy, it becomes necessary to protect\nsensitive user query data when accessing public key-value databases. Existing\nPrivate Information Retrieval (PIR) schemes provide full security but suffer\nfrom poor scalability, limiting their applicability in large-scale deployment.\nWe argue that in many real-world scenarios, a more practical solution should\nallow users to flexibly determine the privacy levels of their queries in a\ntheoretically guided way, balancing security and performance based on specific\nneeds. To formally provide provable guarantees, we introduce a novel concept of\ndistance-based indistinguishability, which can facilitate users to comfortably\nrelax their security requirements. We then design Femur, an efficient framework\nto securely query public key-value stores with flexible security and\nperformance trade-offs. It uses a space-efficient learned index to convert\nquery keys into storage locations, obfuscates these locations with extra noise\nprovably derived by the distance-based indistinguishability theory, and sends\nthe expanded range to the server. The server then adaptively utilizes the best\nscheme to retrieve data. We also propose a novel variable-range PIR scheme\noptimized for bandwidth-constrained environments. Experiments show that Femur\noutperforms the state-of-the-art designs even when ensuring the same full\nsecurity level. When users are willing to relax their privacy requirements,\nFemur can further improve the performance gains to up to 163.9X, demonstrating\nan effective trade-off between security and performance."
                },
                "authors": [
                    {
                        "name": "Jiaoyi Zhang"
                    },
                    {
                        "name": "Liqiang Peng"
                    },
                    {
                        "name": "Mo Sha"
                    },
                    {
                        "name": "Weiran Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "arxiv_comment": "17 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05371v1",
                "updated": "2025-03-07T12:25:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:25:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias\n  Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias\n  Mitigation in LLMs"
                },
                "summary": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05362v1",
                "updated": "2025-03-07T12:07:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    7,
                    59,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T12:07:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    7,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter"
                },
                "summary": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Xinyang Han"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Qianyun Du"
                    },
                    {
                        "name": "Shijin Wang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "19 pages, 9 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05347v1",
                "updated": "2025-03-07T11:42:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    42,
                    22,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:42:22Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    42,
                    22,
                    4,
                    66,
                    0
                ],
                "title": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation"
                },
                "summary": "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https://github.com/Zhenxuan-Zhang/GEMA_score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https://github.com/Zhenxuan-Zhang/GEMA_score."
                },
                "authors": [
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Kinhei Lee"
                    },
                    {
                        "name": "Weihang Deng"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Zihao Jin"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Zhifan Gao"
                    },
                    {
                        "name": "Dominic C Marshall"
                    },
                    {
                        "name": "Yingying Fang"
                    },
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04398v2",
                "updated": "2025-03-07T11:41:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    41,
                    53,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T12:52:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling"
                },
                "summary": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Zewei Xu"
                    },
                    {
                        "name": "Yuanhao Lai"
                    },
                    {
                        "name": "Yunfei Du"
                    },
                    {
                        "name": "Zhengang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengang Wang"
                },
                "author": "Zhengang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05346v1",
                "updated": "2025-03-07T11:40:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    40,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    40,
                    52,
                    4,
                    66,
                    0
                ],
                "title": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT\n  Applications"
                },
                "summary": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines."
                },
                "authors": [
                    {
                        "name": "Leming Shen"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Yuanqing Zheng"
                    },
                    {
                        "name": "Mo Li"
                    }
                ],
                "author_detail": {
                    "name": "Mo Li"
                },
                "author": "Mo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10444v2",
                "updated": "2025-03-07T11:32:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    32,
                    55,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-16T16:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "title": "LLM-as-BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning"
                },
                "summary": "Robotic assembly tasks remain an open challenge due to their long horizon\nnature and complex part relations. Behavior trees (BTs) are increasingly used\nin robot task planning for their modularity and flexibility, but creating them\nmanually can be effort-intensive. Large language models (LLMs) have recently\nbeen applied to robotic task planning for generating action sequences, yet\ntheir ability to generate BTs has not been fully investigated. To this end, we\npropose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT\ngeneration in robotic assembly task planning. Four in-context learning methods\nare introduced to utilize the natural language processing and inference\ncapabilities of LLMs for producing task plans in BT format, reducing manual\neffort while ensuring robustness and comprehensibility. Additionally, we\nevaluate the performance of fine-tuned smaller LLMs on the same tasks.\nExperiments in both simulated and real-world settings demonstrate that our\nframework enhances LLMs' ability to generate BTs, improving success rate\nthrough in-context learning and supervised fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic assembly tasks remain an open challenge due to their long horizon\nnature and complex part relations. Behavior trees (BTs) are increasingly used\nin robot task planning for their modularity and flexibility, but creating them\nmanually can be effort-intensive. Large language models (LLMs) have recently\nbeen applied to robotic task planning for generating action sequences, yet\ntheir ability to generate BTs has not been fully investigated. To this end, we\npropose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT\ngeneration in robotic assembly task planning. Four in-context learning methods\nare introduced to utilize the natural language processing and inference\ncapabilities of LLMs for producing task plans in BT format, reducing manual\neffort while ensuring robustness and comprehensibility. Additionally, we\nevaluate the performance of fine-tuned smaller LLMs on the same tasks.\nExperiments in both simulated and real-world settings demonstrate that our\nframework enhances LLMs' ability to generate BTs, improving success rate\nthrough in-context learning and supervised fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jicong Ao"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "7 pages. Accepted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05336v1",
                "updated": "2025-03-07T11:23:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    23,
                    48,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:23:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    23,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "Toward an Evaluation Science for Generative AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward an Evaluation Science for Generative AI Systems"
                },
                "summary": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Laura Weidinger"
                    },
                    {
                        "name": "Deb Raji"
                    },
                    {
                        "name": "Hanna Wallach"
                    },
                    {
                        "name": "Margaret Mitchell"
                    },
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Olawale Salaudeen"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Deep Ganguli"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "William Isaac"
                    }
                ],
                "author_detail": {
                    "name": "William Isaac"
                },
                "author": "William Isaac",
                "arxiv_comment": "First two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.14352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.14352v2",
                "updated": "2025-03-07T11:16:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    16,
                    40,
                    4,
                    66,
                    0
                ],
                "published": "2023-08-28T06:56:08Z",
                "published_parsed": [
                    2023,
                    8,
                    28,
                    6,
                    56,
                    8,
                    0,
                    240,
                    0
                ],
                "title": "EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices"
                },
                "summary": "Large language models (LLMs) such as GPTs and Mixtral-8x7B have\nrevolutionized machine intelligence due to their exceptional abilities in\ngeneric ML tasks. Transiting LLMs from datacenters to edge devices brings\nbenefits like better privacy and availability, but is challenged by their\nmassive parameter size and thus unbearable runtime costs. To this end, we\npresent EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs\n-- a popular form of sparse LLM that scales its parameter size with almost\nconstant computing complexity. EdgeMoE achieves both memory- and\ncompute-efficiency by partitioning the model into the storage hierarchy:\nnon-expert weights are held in device memory; while expert weights are held on\nexternal storage and fetched to memory only when activated. This design is\nmotivated by a key observation that expert weights are bulky but infrequently\nused due to sparse activation. To further reduce the expert I/O swapping\noverhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth\nadaptation that reduces the expert sizes with tolerable accuracy loss; (2)\nexpert preloading that predicts the activated experts ahead of time and\npreloads it with the compute-I/O pipeline. On popular MoE LLMs and edge\ndevices, EdgeMoE showcase significant memory savings and speedup over\ncompetitive baselines. The code is available at\nhttps://github.com/UbiquitousLearning/mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as GPTs and Mixtral-8x7B have\nrevolutionized machine intelligence due to their exceptional abilities in\ngeneric ML tasks. Transiting LLMs from datacenters to edge devices brings\nbenefits like better privacy and availability, but is challenged by their\nmassive parameter size and thus unbearable runtime costs. To this end, we\npresent EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs\n-- a popular form of sparse LLM that scales its parameter size with almost\nconstant computing complexity. EdgeMoE achieves both memory- and\ncompute-efficiency by partitioning the model into the storage hierarchy:\nnon-expert weights are held in device memory; while expert weights are held on\nexternal storage and fetched to memory only when activated. This design is\nmotivated by a key observation that expert weights are bulky but infrequently\nused due to sparse activation. To further reduce the expert I/O swapping\noverhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth\nadaptation that reduces the expert sizes with tolerable accuracy loss; (2)\nexpert preloading that predicts the activated experts ahead of time and\npreloads it with the compute-I/O pipeline. On popular MoE LLMs and edge\ndevices, EdgeMoE showcase significant memory savings and speedup over\ncompetitive baselines. The code is available at\nhttps://github.com/UbiquitousLearning/mllm."
                },
                "authors": [
                    {
                        "name": "Rongjie Yi"
                    },
                    {
                        "name": "Liwei Guo"
                    },
                    {
                        "name": "Shiyun Wei"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Shangguang Wang"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.14352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.14352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05328v1",
                "updated": "2025-03-07T11:13:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models"
                },
                "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems."
                },
                "authors": [
                    {
                        "name": "Anar Yeginbergen"
                    },
                    {
                        "name": "Maite Oronoz"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19103v2",
                "updated": "2025-03-07T11:05:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    5,
                    1,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-26T12:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval."
                },
                "authors": [
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Rishi Ravikumar"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Xiaoyong Wei"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05324v1",
                "updated": "2025-03-07T11:02:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    2,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T11:02:17Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    2,
                    17,
                    4,
                    66,
                    0
                ],
                "title": "Routing for Large ML Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing for Large ML Models"
                },
                "summary": "Training large language models (LLMs), and other large machine learning\nmodels, involves repeated communication of large volumes of data across a data\ncenter network. The communication patterns induced by these training process\nexhibit high regularity and persistence, giving rise to significant\nopportunities for optimizing the manner in which flows are routed across the\nnetwork. We present an algorithmic framework for \\textit{quantifying}\nnetwork-wide efficiency in the context of training LLMs (and other large-scale\nML models), and for periodically \\textit{optimizing} routing with respect to\nthis global metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs), and other large machine learning\nmodels, involves repeated communication of large volumes of data across a data\ncenter network. The communication patterns induced by these training process\nexhibit high regularity and persistence, giving rise to significant\nopportunities for optimizing the manner in which flows are routed across the\nnetwork. We present an algorithmic framework for \\textit{quantifying}\nnetwork-wide efficiency in the context of training LLMs (and other large-scale\nML models), and for periodically \\textit{optimizing} routing with respect to\nthis global metric."
                },
                "authors": [
                    {
                        "name": "Ofir Cohen"
                    },
                    {
                        "name": "Jose Yallouz Michael Schapira"
                    },
                    {
                        "name": "Shahar Belkar"
                    },
                    {
                        "name": "Tal Mizrahi"
                    }
                ],
                "author_detail": {
                    "name": "Tal Mizrahi"
                },
                "author": "Tal Mizrahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05316v1",
                "updated": "2025-03-07T10:50:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    50,
                    58,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T10:50:58Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    50,
                    58,
                    4,
                    66,
                    0
                ],
                "title": "CoinRobot: Generalized End-to-end Robotic Learning for Physical\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoinRobot: Generalized End-to-end Robotic Learning for Physical\n  Intelligence"
                },
                "summary": "Physical intelligence holds immense promise for advancing embodied\nintelligence, enabling robots to acquire complex behaviors from demonstrations.\nHowever, achieving generalization and transfer across diverse robotic platforms\nand environments requires careful design of model architectures, training\nstrategies, and data diversity. Meanwhile existing systems often struggle with\nscalability, adaptability to heterogeneous hardware, and objective evaluation\nin real-world settings. We present a generalized end-to-end robotic learning\nframework designed to bridge this gap. Our framework introduces a unified\narchitecture that supports cross-platform adaptability, enabling seamless\ndeployment across industrial-grade robots, collaborative arms, and novel\nembodiments without task-specific modifications. By integrating multi-task\nlearning with streamlined network designs, it achieves more robust performance\nthan conventional approaches, while maintaining compatibility with varying\nsensor configurations and action spaces. We validate our framework through\nextensive experiments on seven manipulation tasks. Notably, Diffusion-based\nmodels trained in our framework demonstrated superior performance and\ngeneralizability compared to the LeRobot framework, achieving performance\nimprovements across diverse robotic platforms and environmental conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical intelligence holds immense promise for advancing embodied\nintelligence, enabling robots to acquire complex behaviors from demonstrations.\nHowever, achieving generalization and transfer across diverse robotic platforms\nand environments requires careful design of model architectures, training\nstrategies, and data diversity. Meanwhile existing systems often struggle with\nscalability, adaptability to heterogeneous hardware, and objective evaluation\nin real-world settings. We present a generalized end-to-end robotic learning\nframework designed to bridge this gap. Our framework introduces a unified\narchitecture that supports cross-platform adaptability, enabling seamless\ndeployment across industrial-grade robots, collaborative arms, and novel\nembodiments without task-specific modifications. By integrating multi-task\nlearning with streamlined network designs, it achieves more robust performance\nthan conventional approaches, while maintaining compatibility with varying\nsensor configurations and action spaces. We validate our framework through\nextensive experiments on seven manipulation tasks. Notably, Diffusion-based\nmodels trained in our framework demonstrated superior performance and\ngeneralizability compared to the LeRobot framework, achieving performance\nimprovements across diverse robotic platforms and environmental conditions."
                },
                "authors": [
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Huxian Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jiankai Sun"
                    },
                    {
                        "name": "Jiahuan Yan"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14677v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v3",
                "updated": "2025-03-07T10:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    17,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld. The code is available at\nhttps://github.com/Advacheck-OU/ai-dataset-analysing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld. The code is available at\nhttps://github.com/Advacheck-OU/ai-dataset-analysing."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "arxiv_comment": "Presented at Preventing and Detecting LLM Misinformation (PDLM) at\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04280v2",
                "updated": "2025-03-07T10:06:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    6,
                    29,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T10:08:44Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    8,
                    44,
                    3,
                    65,
                    0
                ],
                "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup."
                },
                "authors": [
                    {
                        "name": "Niccolò Turcato"
                    },
                    {
                        "name": "Matteo Iovino"
                    },
                    {
                        "name": "Aris Synodinos"
                    },
                    {
                        "name": "Alberto Dalla Libera"
                    },
                    {
                        "name": "Ruggero Carli"
                    },
                    {
                        "name": "Pietro Falco"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Falco"
                },
                "author": "Pietro Falco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05281v1",
                "updated": "2025-03-07T09:51:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    51,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:51:07Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    51,
                    7,
                    4,
                    66,
                    0
                ],
                "title": "Similarity-Based Domain Adaptation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Based Domain Adaptation with LLMs"
                },
                "summary": "Unsupervised domain adaptation leverages abundant labeled data from various\nsource domains to generalize onto unlabeled target data. Prior research has\nprimarily focused on learning domain-invariant features across the source and\ntarget domains. However, these methods often require training a model using\nsource domain data, which is time-consuming and can limit model usage for\napplications with different source data. This paper introduces a simple\nframework that utilizes the impressive generalization capabilities of Large\nLanguage Models (LLMs) for target data annotation without the need of source\nmodel training, followed by a novel similarity-based knowledge distillation\nloss. Our extensive experiments on cross-domain text classification reveal that\nour framework achieves impressive performance, specifically, 2.44\\% accuracy\nimprovement when compared to the SOTA method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised domain adaptation leverages abundant labeled data from various\nsource domains to generalize onto unlabeled target data. Prior research has\nprimarily focused on learning domain-invariant features across the source and\ntarget domains. However, these methods often require training a model using\nsource domain data, which is time-consuming and can limit model usage for\napplications with different source data. This paper introduces a simple\nframework that utilizes the impressive generalization capabilities of Large\nLanguage Models (LLMs) for target data annotation without the need of source\nmodel training, followed by a novel similarity-based knowledge distillation\nloss. Our extensive experiments on cross-domain text classification reveal that\nour framework achieves impressive performance, specifically, 2.44\\% accuracy\nimprovement when compared to the SOTA method."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01439v2",
                "updated": "2025-03-07T09:50:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    50,
                    58,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-03T11:44:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    11,
                    44,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization"
                },
                "summary": "Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision."
                },
                "authors": [
                    {
                        "name": "Yushan Liu"
                    },
                    {
                        "name": "Shilong Mu"
                    },
                    {
                        "name": "Xintao Chao"
                    },
                    {
                        "name": "Zizhen Li"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Shoujie Li"
                    },
                    {
                        "name": "Chuqiao Lyu"
                    },
                    {
                        "name": "Xiao-ping Zhang"
                    },
                    {
                        "name": "Wenbo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Ding"
                },
                "author": "Wenbo Ding",
                "arxiv_comment": "Our experimental data have shown some issues, and the conclusions\n  need further verification. To ensure the accuracy and reliability of our\n  findings, we are withdrawing this version of the manuscript. We will conduct\n  more comprehensive experiments and analyses and plan to resubmit the revised\n  version once these concerns have been fully addressed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05280v1",
                "updated": "2025-03-07T09:49:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    49,
                    31,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:49:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    49,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with\n  Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with\n  Natural Language Processing"
                },
                "summary": "The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https://github.com/causalNLP/censorship",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https://github.com/causalNLP/censorship"
                },
                "authors": [
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Francesco Ortu"
                    },
                    {
                        "name": "Roya Ensafi"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04150v2",
                "updated": "2025-03-07T09:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    37,
                    53,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T06:59:09Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    59,
                    9,
                    3,
                    65,
                    0
                ],
                "title": "Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression"
                },
                "summary": "Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal."
                },
                "authors": [
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "Yitong Wang"
                    },
                    {
                        "name": "Wenchun Gao"
                    },
                    {
                        "name": "Lianlian Zhang"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Lijun Mei"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05268v1",
                "updated": "2025-03-07T09:33:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    33,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    33,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "ZOGRASCOPE: A New Benchmark for Property Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZOGRASCOPE: A New Benchmark for Property Graphs"
                },
                "summary": "Natural language interfaces to knowledge graphs have become increasingly\nimportant in recent years, enabling easy and efficient access to structured\ndata. In particular property graphs have seen growing adoption. However, these\nkind of graphs remain relatively underrepresented in research, which has\nfocused in large part on RDF-style graphs. As a matter of fact there is a lack\nof resources for evaluating systems on property graphs, with many existing\ndatasets featuring relatively simple queries. To address this gap, we introduce\nZOGRASCOPE, a benchmark designed specifically for the cypher query language.\nThe benchmark includes a diverse set of manually annotated queries of varying\ncomplexity. We complement this paper with a set of experiments that test the\nperformance of out-of-the-box LLMs of different sizes. Our experiments show\nthat semantic parsing over graphs is still a challenging open problem that can\nnot be solved by prompting LLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language interfaces to knowledge graphs have become increasingly\nimportant in recent years, enabling easy and efficient access to structured\ndata. In particular property graphs have seen growing adoption. However, these\nkind of graphs remain relatively underrepresented in research, which has\nfocused in large part on RDF-style graphs. As a matter of fact there is a lack\nof resources for evaluating systems on property graphs, with many existing\ndatasets featuring relatively simple queries. To address this gap, we introduce\nZOGRASCOPE, a benchmark designed specifically for the cypher query language.\nThe benchmark includes a diverse set of manually annotated queries of varying\ncomplexity. We complement this paper with a set of experiments that test the\nperformance of out-of-the-box LLMs of different sizes. Our experiments show\nthat semantic parsing over graphs is still a challenging open problem that can\nnot be solved by prompting LLMs alone."
                },
                "authors": [
                    {
                        "name": "Francesco Cazzaro"
                    },
                    {
                        "name": "Justin Kleindienst"
                    },
                    {
                        "name": "Sofia Marquez"
                    },
                    {
                        "name": "Ariadna Quattoni"
                    }
                ],
                "author_detail": {
                    "name": "Ariadna Quattoni"
                },
                "author": "Ariadna Quattoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02972v3",
                "updated": "2025-03-07T09:31:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    31,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-04T19:57:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    57,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation"
                },
                "summary": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models."
                },
                "authors": [
                    {
                        "name": "Jude Khouja"
                    },
                    {
                        "name": "Karolina Korgul"
                    },
                    {
                        "name": "Simi Hellsten"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Vlad Neacsu"
                    },
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Ryan Kearns"
                    },
                    {
                        "name": "Andrew Bean"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12319v2",
                "updated": "2025-03-07T09:30:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    30,
                    16,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-18T21:17:27Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    21,
                    17,
                    27,
                    2,
                    262,
                    0
                ],
                "title": "Large Language Models are Strong Audio-Visual Speech Recognition\n  Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Strong Audio-Visual Speech Recognition\n  Learners"
                },
                "summary": "Multimodal large language models (MLLMs) have recently become a focal point\nof research due to their formidable multimodal understanding capabilities. For\nexample, in the audio and speech domains, an LLM can be equipped with\n(automatic) speech recognition (ASR) abilities by just concatenating the audio\ntokens, computed with an audio encoder, and the text tokens to achieve\nstate-of-the-art results. On the contrary, tasks like visual and audio-visual\nspeech recognition (VSR/AVSR), which also exploit noise-invariant lip movement\ninformation, have received little or no attention. To bridge this gap, we\npropose Llama-AVSR, a new MLLM with strong audio-visual speech recognition\ncapabilities. It leverages pre-trained audio and video encoders to produce\nmodality-specific tokens which, together with the text tokens, are processed by\na pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an\nauto-regressive fashion. Llama-AVSR requires a small number of trainable\nparameters as only modality-specific projectors and LoRA modules are trained\nwhereas the multi-modal encoders and LLM are kept frozen. We evaluate our\nproposed approach on LRS3, the largest public AVSR benchmark, and we achieve\nnew state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79%\nand 0.77%, respectively. To bolster our results, we investigate the key factors\nthat underpin the effectiveness of Llama-AVSR: the choice of the pre-trained\nencoders and LLM, the efficient integration of LoRA modules, and the optimal\nperformance-efficiency trade-off obtained via modality-aware compression rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have recently become a focal point\nof research due to their formidable multimodal understanding capabilities. For\nexample, in the audio and speech domains, an LLM can be equipped with\n(automatic) speech recognition (ASR) abilities by just concatenating the audio\ntokens, computed with an audio encoder, and the text tokens to achieve\nstate-of-the-art results. On the contrary, tasks like visual and audio-visual\nspeech recognition (VSR/AVSR), which also exploit noise-invariant lip movement\ninformation, have received little or no attention. To bridge this gap, we\npropose Llama-AVSR, a new MLLM with strong audio-visual speech recognition\ncapabilities. It leverages pre-trained audio and video encoders to produce\nmodality-specific tokens which, together with the text tokens, are processed by\na pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an\nauto-regressive fashion. Llama-AVSR requires a small number of trainable\nparameters as only modality-specific projectors and LoRA modules are trained\nwhereas the multi-modal encoders and LLM are kept frozen. We evaluate our\nproposed approach on LRS3, the largest public AVSR benchmark, and we achieve\nnew state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79%\nand 0.77%, respectively. To bolster our results, we investigate the key factors\nthat underpin the effectiveness of Llama-AVSR: the choice of the pre-trained\nencoders and LLM, the efficient integration of LoRA modules, and the optimal\nperformance-efficiency trade-off obtained via modality-aware compression rates."
                },
                "authors": [
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Honglie Chen"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Stavros Petridis"
                    },
                    {
                        "name": "Daniele Falavigna"
                    },
                    {
                        "name": "Alessio Brutti"
                    },
                    {
                        "name": "Maja Pantic"
                    }
                ],
                "author_detail": {
                    "name": "Maja Pantic"
                },
                "author": "Maja Pantic",
                "arxiv_comment": "Accepted for publication at ICASSP 2025. The code and checkpoints are\n  available here: https://github.com/umbertocappellazzo/Llama-AVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23746v2",
                "updated": "2025-03-07T09:06:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    6,
                    3,
                    4,
                    66,
                    0
                ],
                "published": "2024-10-31T09:01:25Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    1,
                    25,
                    3,
                    305,
                    0
                ],
                "title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios"
                },
                "summary": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL."
                },
                "authors": [
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Yulin Yuan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13983v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13983v4",
                "updated": "2025-03-07T09:02:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    2,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2025-01-23T06:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    57,
                    24,
                    3,
                    23,
                    0
                ],
                "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess."
                },
                "authors": [
                    {
                        "name": "Yang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yang Fan"
                },
                "author": "Yang Fan",
                "arxiv_comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13983v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13983v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05248v1",
                "updated": "2025-03-07T09:02:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    2,
                    21,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T09:02:21Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    2,
                    21,
                    4,
                    66,
                    0
                ],
                "title": "Optimizing LLM Inference Throughput via Memory-aware and SLA-constrained\n  Dynamic Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference Throughput via Memory-aware and SLA-constrained\n  Dynamic Batching"
                },
                "summary": "The increasing adoption of large language models (LLMs) necessitates\ninference serving systems that can deliver both high throughput and low\nlatency. Deploying LLMs with hundreds of billions of parameters on\nmemory-constrained GPUs exposes significant limitations in static batching\nmethods. Current inference serving systems often treat batch sizes as fixed\nhyper-parameters, hindering real-time adaptation to varying system conditions.\nIn this paper, we propose a dynamic batching method that continuously monitors\nmemory utilization and adheres to service-level agreements (SLAs) to enable\nreal-time batch size configuration adjustment. The method comprises two core\ncomponents: a memory-aware batch scheduler that dynamically allocates GPU\nresources and a latency feedback mechanism that optimizes decoding processes\nunder SLA constraints. The numerical experiments demonstrate throughput gains\nof 8% to 28% and capacity improvements of 22% compared to traditional static\nbatching methods, while maintaining full compatibility with existing inference\ninfrastructure. These results highlight the effectiveness of dynamic batching\nin balancing computational efficiency and quality-of-service requirements for\ncontemporary LLM deployment scenarios. The source code of this work is publicly\navailable at https://github.com/KevinLee1110/dynamic-batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) necessitates\ninference serving systems that can deliver both high throughput and low\nlatency. Deploying LLMs with hundreds of billions of parameters on\nmemory-constrained GPUs exposes significant limitations in static batching\nmethods. Current inference serving systems often treat batch sizes as fixed\nhyper-parameters, hindering real-time adaptation to varying system conditions.\nIn this paper, we propose a dynamic batching method that continuously monitors\nmemory utilization and adheres to service-level agreements (SLAs) to enable\nreal-time batch size configuration adjustment. The method comprises two core\ncomponents: a memory-aware batch scheduler that dynamically allocates GPU\nresources and a latency feedback mechanism that optimizes decoding processes\nunder SLA constraints. The numerical experiments demonstrate throughput gains\nof 8% to 28% and capacity improvements of 22% compared to traditional static\nbatching methods, while maintaining full compatibility with existing inference\ninfrastructure. These results highlight the effectiveness of dynamic batching\nin balancing computational efficiency and quality-of-service requirements for\ncontemporary LLM deployment scenarios. The source code of this work is publicly\navailable at https://github.com/KevinLee1110/dynamic-batching."
                },
                "authors": [
                    {
                        "name": "Bowen Pang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Feifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feifan Wang"
                },
                "author": "Feifan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16771v2",
                "updated": "2025-03-07T09:00:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    0,
                    43,
                    4,
                    66,
                    0
                ],
                "published": "2024-11-25T06:17:23Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    17,
                    23,
                    0,
                    330,
                    0
                ],
                "title": "VidHal: Benchmarking Temporal Hallucinations in Vision LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidHal: Benchmarking Temporal Hallucinations in Vision LLMs"
                },
                "summary": "Vision Large Language Models (VLLMs) are widely acknowledged to be prone to\nhallucinations. Existing research addressing this problem has primarily been\nconfined to image inputs, with limited exploration of video-based\nhallucinations. Furthermore, current evaluation methods fail to capture nuanced\nerrors in generated responses, which are often exacerbated by the rich\nspatiotemporal dynamics of videos. To address this, we introduce VidHal, a\nbenchmark specially designed to evaluate video-based hallucinations in VLLMs.\nVidHal is constructed by bootstrapping video instances across a wide range of\ncommon temporal aspects. A defining feature of our benchmark lies in the\ncareful creation of captions which represent varying levels of hallucination\nassociated with each video. To enable fine-grained evaluation, we propose a\nnovel caption ordering task requiring VLLMs to rank captions by hallucinatory\nextent. We conduct extensive experiments on VidHal and comprehensively evaluate\na broad selection of models. Our results uncover significant limitations in\nexisting VLLMs regarding hallucination generation. Through our benchmark, we\naim to inspire further research on 1) holistic understanding of VLLM\ncapabilities, particularly regarding hallucination, and 2) extensive\ndevelopment of advanced VLLMs to alleviate this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Large Language Models (VLLMs) are widely acknowledged to be prone to\nhallucinations. Existing research addressing this problem has primarily been\nconfined to image inputs, with limited exploration of video-based\nhallucinations. Furthermore, current evaluation methods fail to capture nuanced\nerrors in generated responses, which are often exacerbated by the rich\nspatiotemporal dynamics of videos. To address this, we introduce VidHal, a\nbenchmark specially designed to evaluate video-based hallucinations in VLLMs.\nVidHal is constructed by bootstrapping video instances across a wide range of\ncommon temporal aspects. A defining feature of our benchmark lies in the\ncareful creation of captions which represent varying levels of hallucination\nassociated with each video. To enable fine-grained evaluation, we propose a\nnovel caption ordering task requiring VLLMs to rank captions by hallucinatory\nextent. We conduct extensive experiments on VidHal and comprehensively evaluate\na broad selection of models. Our results uncover significant limitations in\nexisting VLLMs regarding hallucination generation. Through our benchmark, we\naim to inspire further research on 1) holistic understanding of VLLM\ncapabilities, particularly regarding hallucination, and 2) extensive\ndevelopment of advanced VLLMs to alleviate this problem."
                },
                "authors": [
                    {
                        "name": "Wey Yeh Choong"
                    },
                    {
                        "name": "Yangyang Guo"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli",
                "arxiv_comment": "9 pages, 10 figures. Code available at\n  https://github.com/Lookuz/VidHal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05244v1",
                "updated": "2025-03-07T08:56:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    56,
                    20,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:56:20Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    56,
                    20,
                    4,
                    66,
                    0
                ],
                "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WritingBench: A Comprehensive Benchmark for Generative Writing"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing."
                },
                "authors": [
                    {
                        "name": "Yuning Wu"
                    },
                    {
                        "name": "Jiahao Mei"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "SHaopeng Lai"
                    },
                    {
                        "name": "Yuran Ren"
                    },
                    {
                        "name": "Zijia Wang"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05242v1",
                "updated": "2025-03-07T08:53:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    53,
                    10,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:53:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    53,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "MM-StoryAgent: Immersive Narrated Storybook Video Generation with a\n  Multi-Agent Paradigm across Text, Image and Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-StoryAgent: Immersive Narrated Storybook Video Generation with a\n  Multi-Agent Paradigm across Text, Image and Audio"
                },
                "summary": "The rapid advancement of large language models (LLMs) and artificial\nintelligence-generated content (AIGC) has accelerated AI-native applications,\nsuch as AI-based storybooks that automate engaging story production for\nchildren. However, challenges remain in improving story attractiveness,\nenriching storytelling expressiveness, and developing open-source evaluation\nbenchmarks and frameworks. Therefore, we propose and opensource MM-StoryAgent,\nwhich creates immersive narrated video storybooks with refined plots,\nrole-consistent images, and multi-channel audio. MM-StoryAgent designs a\nmulti-agent framework that employs LLMs and diverse expert tools (generative\nmodels and APIs) across several modalities to produce expressive storytelling\nvideos. The framework enhances story attractiveness through a multi-stage\nwriting pipeline. In addition, it improves the immersive storytelling\nexperience by integrating sound effects with visual, music and narrative\nassets. MM-StoryAgent offers a flexible, open-source platform for further\ndevelopment, where generative modules can be substituted. Both objective and\nsubjective evaluation regarding textual story quality and alignment between\nmodalities validate the effectiveness of our proposed MM-StoryAgent system. The\ndemo and source code are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and artificial\nintelligence-generated content (AIGC) has accelerated AI-native applications,\nsuch as AI-based storybooks that automate engaging story production for\nchildren. However, challenges remain in improving story attractiveness,\nenriching storytelling expressiveness, and developing open-source evaluation\nbenchmarks and frameworks. Therefore, we propose and opensource MM-StoryAgent,\nwhich creates immersive narrated video storybooks with refined plots,\nrole-consistent images, and multi-channel audio. MM-StoryAgent designs a\nmulti-agent framework that employs LLMs and diverse expert tools (generative\nmodels and APIs) across several modalities to produce expressive storytelling\nvideos. The framework enhances story attractiveness through a multi-stage\nwriting pipeline. In addition, it improves the immersive storytelling\nexperience by integrating sound effects with visual, music and narrative\nassets. MM-StoryAgent offers a flexible, open-source platform for further\ndevelopment, where generative modules can be substituted. Both objective and\nsubjective evaluation regarding textual story quality and alignment between\nmodalities validate the effectiveness of our proposed MM-StoryAgent system. The\ndemo and source code are available."
                },
                "authors": [
                    {
                        "name": "Xuenan Xu"
                    },
                    {
                        "name": "Jiahao Mei"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Yuning Wu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Shaopeng Lai"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Mengyue Wu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Wu"
                },
                "author": "Mengyue Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05238v1",
                "updated": "2025-03-07T08:40:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    40,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:40:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    40,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Guaranteeing Out-Of-Distribution Detection in Deep RL via Transition\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteeing Out-Of-Distribution Detection in Deep RL via Transition\n  Estimation"
                },
                "summary": "An issue concerning the use of deep reinforcement learning (RL) agents is\nwhether they can be trusted to perform reliably when deployed, as training\nenvironments may not reflect real-life environments. Anticipating instances\noutside their training scope, learning-enabled systems are often equipped with\nout-of-distribution (OOD) detectors that alert when a trained system encounters\na state it does not recognize or in which it exhibits uncertainty. There exists\nlimited work conducted on the problem of OOD detection within RL, with prior\nstudies being unable to achieve a consensus on the definition of OOD execution\nwithin the context of RL. By framing our problem using a Markov Decision\nProcess, we assume there is a transition distribution mapping each state-action\npair to another state with some probability. Based on this, we consider the\nfollowing definition of OOD execution within RL: A transition is OOD if its\nprobability during real-life deployment differs from the transition\ndistribution encountered during training. As such, we utilize conditional\nvariational autoencoders (CVAE) to approximate the transition dynamics of the\ntraining environment and implement a conformity-based detector using\nreconstruction loss that is able to guarantee OOD detection with a\npre-determined confidence level. We evaluate our detector by adapting existing\nbenchmarks and compare it with existing OOD detection models for RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An issue concerning the use of deep reinforcement learning (RL) agents is\nwhether they can be trusted to perform reliably when deployed, as training\nenvironments may not reflect real-life environments. Anticipating instances\noutside their training scope, learning-enabled systems are often equipped with\nout-of-distribution (OOD) detectors that alert when a trained system encounters\na state it does not recognize or in which it exhibits uncertainty. There exists\nlimited work conducted on the problem of OOD detection within RL, with prior\nstudies being unable to achieve a consensus on the definition of OOD execution\nwithin the context of RL. By framing our problem using a Markov Decision\nProcess, we assume there is a transition distribution mapping each state-action\npair to another state with some probability. Based on this, we consider the\nfollowing definition of OOD execution within RL: A transition is OOD if its\nprobability during real-life deployment differs from the transition\ndistribution encountered during training. As such, we utilize conditional\nvariational autoencoders (CVAE) to approximate the transition dynamics of the\ntraining environment and implement a conformity-based detector using\nreconstruction loss that is able to guarantee OOD detection with a\npre-determined confidence level. We evaluate our detector by adapting existing\nbenchmarks and compare it with existing OOD detection models for RL."
                },
                "authors": [
                    {
                        "name": "Mohit Prashant"
                    },
                    {
                        "name": "Arvind Easwaran"
                    },
                    {
                        "name": "Suman Das"
                    },
                    {
                        "name": "Michael Yuhas"
                    }
                ],
                "author_detail": {
                    "name": "Michael Yuhas"
                },
                "author": "Michael Yuhas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05605v2",
                "updated": "2025-03-07T08:35:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    35,
                    0,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-08T15:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    21,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative\n  Preference Optimization"
                },
                "summary": "A truly intelligent Large Language Model (LLM) should be capable of\ncorrecting errors in its responses through external interactions. However, even\nthe most advanced models often face challenges in improving their outputs. In\nthis paper, we explore how to cultivate LLMs with the self-refinement\ncapability through iterative preference training, and how this ability can be\nleveraged to improve model performance during inference. To this end, we\nintroduce a novel post-training and inference framework, called ARIES: Adaptive\nRefinement and Iterative Enhancement Structure. This method iteratively\nperforms preference training and self-refinement-based data collection. During\ntraining, ARIES strengthen the model's direct question-answering capability\nwhile simultaneously unlocking its self-refinement potential. During inference,\nARIES harnesses this self-refinement capability to generate a series of\nprogressively refined responses, which are then filtered using either the\nReward Model Scoring or a simple yet effective Rule-Based Selection mechanism,\nspecifically tailored to our approach, to construct a dataset for the next\nround of preference training. Experimental results demonstrate the remarkable\nperformance of ARIES. When applied to the Llama-3.1-8B model and under the\nself-refinement setting, ARIES surpasses powerful models such as GPT-4o,\nachieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval\n2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a\n50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore,\nARIES consistently enhances performance on mathematical reasoning tasks like\nGSM8K and MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A truly intelligent Large Language Model (LLM) should be capable of\ncorrecting errors in its responses through external interactions. However, even\nthe most advanced models often face challenges in improving their outputs. In\nthis paper, we explore how to cultivate LLMs with the self-refinement\ncapability through iterative preference training, and how this ability can be\nleveraged to improve model performance during inference. To this end, we\nintroduce a novel post-training and inference framework, called ARIES: Adaptive\nRefinement and Iterative Enhancement Structure. This method iteratively\nperforms preference training and self-refinement-based data collection. During\ntraining, ARIES strengthen the model's direct question-answering capability\nwhile simultaneously unlocking its self-refinement potential. During inference,\nARIES harnesses this self-refinement capability to generate a series of\nprogressively refined responses, which are then filtered using either the\nReward Model Scoring or a simple yet effective Rule-Based Selection mechanism,\nspecifically tailored to our approach, to construct a dataset for the next\nround of preference training. Experimental results demonstrate the remarkable\nperformance of ARIES. When applied to the Llama-3.1-8B model and under the\nself-refinement setting, ARIES surpasses powerful models such as GPT-4o,\nachieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval\n2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a\n50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore,\nARIES consistently enhances performance on mathematical reasoning tasks like\nGSM8K and MATH."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinyu Cui"
                    },
                    {
                        "name": "Xuanfa Jin"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Quan He"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05234v1",
                "updated": "2025-03-07T08:33:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    33,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:33:34Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    33,
                    34,
                    4,
                    66,
                    0
                ],
                "title": "Unveiling Biases in AI: ChatGPT's Political Economy Perspectives and\n  Human Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Biases in AI: ChatGPT's Political Economy Perspectives and\n  Human Comparisons"
                },
                "summary": "We explore the political and ideological positioning of ChatGPT, a leading\nlarge language model (LLM), by comparing its responses to political economy\nquestions from the European Social Survey (ESS). The questions concern\nenvironmental sustainability, civil rights, income inequality, and government\nsize. ChatGPT's self-assessed placement on a left-right political spectrum is\ncompared to the ideological stances of individuals providing similar answers in\nthe ESS dataset. Results highlight a significant left-oriented bias in\nChatGPT's answers, particularly on environmental and civil rights topics,\ndiverging from its same self-declared center-left stance. These findings\nunderscore the need for transparency in AI systems to prevent potential\nideological influences on users. We conclude by discussing the implications for\nAI governance, debiasing strategies, and educational use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the political and ideological positioning of ChatGPT, a leading\nlarge language model (LLM), by comparing its responses to political economy\nquestions from the European Social Survey (ESS). The questions concern\nenvironmental sustainability, civil rights, income inequality, and government\nsize. ChatGPT's self-assessed placement on a left-right political spectrum is\ncompared to the ideological stances of individuals providing similar answers in\nthe ESS dataset. Results highlight a significant left-oriented bias in\nChatGPT's answers, particularly on environmental and civil rights topics,\ndiverging from its same self-declared center-left stance. These findings\nunderscore the need for transparency in AI systems to prevent potential\nideological influences on users. We conclude by discussing the implications for\nAI governance, debiasing strategies, and educational use."
                },
                "authors": [
                    {
                        "name": "Leonardo Becchetti"
                    },
                    {
                        "name": "Nazaria Solferino"
                    }
                ],
                "author_detail": {
                    "name": "Nazaria Solferino"
                },
                "arxiv_affiliation": "Universitas Mercatorum",
                "author": "Nazaria Solferino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00965v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00965v5",
                "updated": "2025-03-07T08:27:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    27,
                    32,
                    4,
                    66,
                    0
                ],
                "published": "2024-06-03T03:38:56Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    3,
                    38,
                    56,
                    0,
                    155,
                    0
                ],
                "title": "HBTP: Heuristic Behavior Tree Planning with Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBTP: Heuristic Behavior Tree Planning with Large Language Model\n  Reasoning"
                },
                "summary": "Behavior Trees (BTs) are increasingly becoming a popular control structure in\nrobotics due to their modularity, reactivity, and robustness. In terms of BT\ngeneration methods, BT planning shows promise for generating reliable BTs.\nHowever, the scalability of BT planning is often constrained by prolonged\nplanning times in complex scenarios, largely due to a lack of domain knowledge.\nIn contrast, pre-trained Large Language Models (LLMs) have demonstrated task\nreasoning capabilities across various domains, though the correctness and\nsafety of their planning remain uncertain. This paper proposes integrating BT\nplanning with LLM reasoning, introducing Heuristic Behavior Tree Planning\n(HBTP)-a reliable and efficient framework for BT generation. The key idea in\nHBTP is to leverage LLMs for task-specific reasoning to generate a heuristic\npath, which BT planning can then follow to expand efficiently. We first\nintroduce the heuristic BT expansion process, along with two heuristic variants\ndesigned for optimal planning and satisficing planning, respectively. Then, we\npropose methods to address the inaccuracies of LLM reasoning, including action\nspace pruning and reflective feedback, to further enhance both reasoning\naccuracy and planning efficiency. Experiments demonstrate the theoretical\nbounds of HBTP, and results from four datasets confirm its practical\neffectiveness in everyday service robot applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Trees (BTs) are increasingly becoming a popular control structure in\nrobotics due to their modularity, reactivity, and robustness. In terms of BT\ngeneration methods, BT planning shows promise for generating reliable BTs.\nHowever, the scalability of BT planning is often constrained by prolonged\nplanning times in complex scenarios, largely due to a lack of domain knowledge.\nIn contrast, pre-trained Large Language Models (LLMs) have demonstrated task\nreasoning capabilities across various domains, though the correctness and\nsafety of their planning remain uncertain. This paper proposes integrating BT\nplanning with LLM reasoning, introducing Heuristic Behavior Tree Planning\n(HBTP)-a reliable and efficient framework for BT generation. The key idea in\nHBTP is to leverage LLMs for task-specific reasoning to generate a heuristic\npath, which BT planning can then follow to expand efficiently. We first\nintroduce the heuristic BT expansion process, along with two heuristic variants\ndesigned for optimal planning and satisficing planning, respectively. Then, we\npropose methods to address the inaccuracies of LLM reasoning, including action\nspace pruning and reflective feedback, to further enhance both reasoning\naccuracy and planning efficiency. Experiments demonstrate the theoretical\nbounds of HBTP, and results from four datasets confirm its practical\neffectiveness in everyday service robot applications."
                },
                "authors": [
                    {
                        "name": "Yishuai Cai"
                    },
                    {
                        "name": "Xinglin Chen"
                    },
                    {
                        "name": "Yunxin Mao"
                    },
                    {
                        "name": "Minglong Li"
                    },
                    {
                        "name": "Shaowu Yang"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Ji Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wang"
                },
                "author": "Ji Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00965v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00965v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05221v1",
                "updated": "2025-03-07T08:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    19,
                    51,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    19,
                    51,
                    4,
                    66,
                    0
                ],
                "title": "The MACIV multiscale seismic experiments in the French Massif Central\n  (2023-2027): deployment, data quality and availability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MACIV multiscale seismic experiments in the French Massif Central\n  (2023-2027): deployment, data quality and availability"
                },
                "summary": "In the framework of the MACIV project, a consortium of French laboratories\nhas deployed a temporary seismic network of 100 broadband stations in the\nFrench Massif Central (FMC) for 3-4 years (2023-2027). The project aims at\nimaging the crust and upper mantle of the FMC to better assess the sources of\nvolcanism, and the impacts of the Variscan inheritance or the Cenozoic rift\nsystem on volcanic systems. A large-scale array of 35 broadband stations covers\nthe entire FMC and complements the permanent networks to reach a homogeneous\ncoverage with ~35 km spacing. This network, with XP code, is the French\ncontribution to AdriaArray. The XP array is complemented with 3 quasi-linear\nnorth-south, east-west and northwest-southeast profiles with inter-station\nspacing of 5-20 km, making up the XF network of 65 stations. The profiles cross\nvolcanic areas and the main Variscan structures. We describe the experimental\nsetup designed to optimize the performance/cost ratio and minimize the number\nof field visits, the deployment, the state-of-health monitoring, the data\nmanagement and the data quality control strategies, outcomes of our 15-years'\nexperience with major temporary seismic experiments in France and neighboring\ncountries, including AlpArray. We also show some preliminary results including\nhypocenter locations and receiver function analysis. The 2 broadband arrays\nwill be supplemented in 2025 by a month-long deployment of 3 large-N dense\narrays of 625 3-C short-period nodes. These dense arrays will complete our\nmulti-scale seismic experiment and illuminate active faults and possible\nplumbing systems of the youngest volcanoes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the framework of the MACIV project, a consortium of French laboratories\nhas deployed a temporary seismic network of 100 broadband stations in the\nFrench Massif Central (FMC) for 3-4 years (2023-2027). The project aims at\nimaging the crust and upper mantle of the FMC to better assess the sources of\nvolcanism, and the impacts of the Variscan inheritance or the Cenozoic rift\nsystem on volcanic systems. A large-scale array of 35 broadband stations covers\nthe entire FMC and complements the permanent networks to reach a homogeneous\ncoverage with ~35 km spacing. This network, with XP code, is the French\ncontribution to AdriaArray. The XP array is complemented with 3 quasi-linear\nnorth-south, east-west and northwest-southeast profiles with inter-station\nspacing of 5-20 km, making up the XF network of 65 stations. The profiles cross\nvolcanic areas and the main Variscan structures. We describe the experimental\nsetup designed to optimize the performance/cost ratio and minimize the number\nof field visits, the deployment, the state-of-health monitoring, the data\nmanagement and the data quality control strategies, outcomes of our 15-years'\nexperience with major temporary seismic experiments in France and neighboring\ncountries, including AlpArray. We also show some preliminary results including\nhypocenter locations and receiver function analysis. The 2 broadband arrays\nwill be supplemented in 2025 by a month-long deployment of 3 large-N dense\narrays of 625 3-C short-period nodes. These dense arrays will complete our\nmulti-scale seismic experiment and illuminate active faults and possible\nplumbing systems of the youngest volcanoes."
                },
                "authors": [
                    {
                        "name": "Coralie Aubert"
                    },
                    {
                        "name": "Guilhem Scheiblin"
                    },
                    {
                        "name": "Anne Paul"
                    },
                    {
                        "name": "Hélène Pauchet"
                    },
                    {
                        "name": "Aurélien Mordret"
                    },
                    {
                        "name": "Vincent Baudot"
                    },
                    {
                        "name": "Sébastien Chevrot"
                    },
                    {
                        "name": "Nicolas Cluzel"
                    },
                    {
                        "name": "Isabelle Douste-Bacqué"
                    },
                    {
                        "name": "Franck Grimaud"
                    },
                    {
                        "name": "Axel Jung"
                    },
                    {
                        "name": "Stéphane Mercier"
                    },
                    {
                        "name": "Piel Pawlowski"
                    },
                    {
                        "name": "Sandrine Roussel"
                    },
                    {
                        "name": "Thierry Souriot"
                    },
                    {
                        "name": "Nikolai M. Shapiro"
                    },
                    {
                        "name": "Matthieu Sylvander"
                    },
                    {
                        "name": "Benjamin Vial"
                    },
                    {
                        "name": "David Wolyniec"
                    }
                ],
                "author_detail": {
                    "name": "David Wolyniec"
                },
                "arxiv_affiliation": "ISTerre",
                "author": "David Wolyniec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04685v2",
                "updated": "2025-03-07T08:19:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    19,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T18:27:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    27,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module"
                },
                "summary": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples."
                },
                "authors": [
                    {
                        "name": "Krish Sharma"
                    },
                    {
                        "name": "Niyar R Barman"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05220v1",
                "updated": "2025-03-07T08:16:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    16,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:16:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    16,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "ARbiter: Generating Dialogue Options and Communication Support in\n  Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARbiter: Generating Dialogue Options and Communication Support in\n  Augmented Reality"
                },
                "summary": "In this position paper, we propose researching the combination of Augmented\nReality (AR) and Artificial Intelligence (AI) to support conversations,\ninspired by the interfaces of dialogue systems commonly found in videogames.\nAR-capable devices are becoming more powerful and conventional in looks, as\nseen in head-mounted displays (HMDs) like the Snapchat Spectacles, the XREAL\nglasses, or the recently presented Meta Orion. This development reduces\npossible ergonomic, appearance, and runtime concerns, thus allowing a more\nstraightforward integration and extended use of AR in our everyday lives, both\nin private and at work. At the same time, we can observe an immense surge in AI\ndevelopment (also at CHI). Recently notorious Large Language Models (LLMs) like\nOpenAI's o3-mini or DeepSeek-R1 soar over their precursors in their ability to\nsustain conversations, provide suggestions, and handle complex topics in\n(almost) real time. In combination with natural language recognition systems,\nwhich are nowadays a standard component of smartphones and similar devices\n(including modern AR-HMDs), it is easy to imagine a combined system that\nintegrates into daily conversations and provides various types of assistance.\nSuch a system would enable many opportunities for research in AR+AI, which, as\nstated by Hirzle et al., remains scarce. In the following, we describe how the\ndesign of a conversational AR+AI system can learn from videogame dialogue\nsystems, and we propose use cases and research questions that can be\ninvestigated thanks to this AR+AI combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we propose researching the combination of Augmented\nReality (AR) and Artificial Intelligence (AI) to support conversations,\ninspired by the interfaces of dialogue systems commonly found in videogames.\nAR-capable devices are becoming more powerful and conventional in looks, as\nseen in head-mounted displays (HMDs) like the Snapchat Spectacles, the XREAL\nglasses, or the recently presented Meta Orion. This development reduces\npossible ergonomic, appearance, and runtime concerns, thus allowing a more\nstraightforward integration and extended use of AR in our everyday lives, both\nin private and at work. At the same time, we can observe an immense surge in AI\ndevelopment (also at CHI). Recently notorious Large Language Models (LLMs) like\nOpenAI's o3-mini or DeepSeek-R1 soar over their precursors in their ability to\nsustain conversations, provide suggestions, and handle complex topics in\n(almost) real time. In combination with natural language recognition systems,\nwhich are nowadays a standard component of smartphones and similar devices\n(including modern AR-HMDs), it is easy to imagine a combined system that\nintegrates into daily conversations and provides various types of assistance.\nSuch a system would enable many opportunities for research in AR+AI, which, as\nstated by Hirzle et al., remains scarce. In the following, we describe how the\ndesign of a conversational AR+AI system can learn from videogame dialogue\nsystems, and we propose use cases and research questions that can be\ninvestigated thanks to this AR+AI combination."
                },
                "authors": [
                    {
                        "name": "Julián Méndez"
                    },
                    {
                        "name": "Marc Satkowski"
                    }
                ],
                "author_detail": {
                    "name": "Marc Satkowski"
                },
                "author": "Marc Satkowski",
                "arxiv_comment": "This work has been accepted for the ACM CHI 2025 Workshop \"Everyday\n  AR through AI-in-the-Loop\" (see https://xr-and-ai.github.io/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19346v2",
                "updated": "2025-03-07T08:08:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    8,
                    18,
                    4,
                    66,
                    0
                ],
                "published": "2024-11-28T19:48:54Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    48,
                    54,
                    3,
                    333,
                    0
                ],
                "title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections"
                },
                "summary": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA."
                },
                "authors": [
                    {
                        "name": "Mohamed Fazli Imam"
                    },
                    {
                        "name": "Rufael Fedaku Marew"
                    },
                    {
                        "name": "Jameel Hassan"
                    },
                    {
                        "name": "Mustansar Fiaz"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05213v1",
                "updated": "2025-03-07T08:07:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    7,
                    15,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:07:15Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    7,
                    15,
                    4,
                    66,
                    0
                ],
                "title": "Personalized Text Generation with Contrastive Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Text Generation with Contrastive Activation Steering"
                },
                "summary": "Personalized text generation aims to infer users' writing style preferences\nfrom their historical texts and generate outputs that faithfully reflect these\nstylistic characteristics. Existing solutions primarily adopt two paradigms:\nretrieval-augmented generation (RAG) and parameter-efficient fine-tuning\n(PEFT). While these approaches have advanced the field, they suffer from two\ncritical limitations: (1) the entanglement of content semantics and stylistic\npatterns in historical texts impedes accurate modeling of user-specific writing\npreferences; and (2) scalability challenges arising from both RAG's inference\nlatency by retrieval operations and PEFT's parameter storage requirements for\nper user model. To overcome these limitations, we propose StyleVector, a\ntraining-free framework that disentangles and represents personalized writing\nstyle as a vector in LLM's activation space, enabling style-steered generation\nduring inference without requiring costly retrieval or parameter storage.\nComprehensive experiments demonstrate that our framework achieves a significant\n8% relative improvement in personalized generation while reducing storage\nrequirements by 1700 times over PEFT method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text generation aims to infer users' writing style preferences\nfrom their historical texts and generate outputs that faithfully reflect these\nstylistic characteristics. Existing solutions primarily adopt two paradigms:\nretrieval-augmented generation (RAG) and parameter-efficient fine-tuning\n(PEFT). While these approaches have advanced the field, they suffer from two\ncritical limitations: (1) the entanglement of content semantics and stylistic\npatterns in historical texts impedes accurate modeling of user-specific writing\npreferences; and (2) scalability challenges arising from both RAG's inference\nlatency by retrieval operations and PEFT's parameter storage requirements for\nper user model. To overcome these limitations, we propose StyleVector, a\ntraining-free framework that disentangles and represents personalized writing\nstyle as a vector in LLM's activation space, enabling style-steered generation\nduring inference without requiring costly retrieval or parameter storage.\nComprehensive experiments demonstrate that our framework achieves a significant\n8% relative improvement in personalized generation while reducing storage\nrequirements by 1700 times over PEFT method."
                },
                "authors": [
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03796v2",
                "updated": "2025-03-07T08:06:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    6,
                    15,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-05T14:33:18Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    33,
                    18,
                    2,
                    64,
                    0
                ],
                "title": "Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent\n  Reinforcement Learning in USV Swarm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent\n  Reinforcement Learning in USV Swarm"
                },
                "summary": "Multi-Agent Reinforcement Learning (MARL) has shown promise in solving\ncomplex problems involving cooperation and competition among agents, such as an\nUnmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,\nand vessel protection. However, aligning system behavior with user preferences\nis challenging due to the difficulty of encoding expert intuition into reward\nfunctions. To address the issue, we propose a Reinforcement Learning with Human\nFeedback (RLHF) approach for MARL that resolves credit-assignment challenges\nthrough an Agent-Level Feedback system categorizing feedback into intra-agent,\ninter-agent, and intra-team types. To overcome the challenges of direct human\nfeedback, we employ a Large Language Model (LLM) evaluator to validate our\napproach using feedback scenarios such as region constraints, collision\navoidance, and task allocation. Our method effectively refines USV swarm\npolicies, addressing key challenges in multi-agent systems while maintaining\nfairness and performance consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning (MARL) has shown promise in solving\ncomplex problems involving cooperation and competition among agents, such as an\nUnmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,\nand vessel protection. However, aligning system behavior with user preferences\nis challenging due to the difficulty of encoding expert intuition into reward\nfunctions. To address the issue, we propose a Reinforcement Learning with Human\nFeedback (RLHF) approach for MARL that resolves credit-assignment challenges\nthrough an Agent-Level Feedback system categorizing feedback into intra-agent,\ninter-agent, and intra-team types. To overcome the challenges of direct human\nfeedback, we employ a Large Language Model (LLM) evaluator to validate our\napproach using feedback scenarios such as region constraints, collision\navoidance, and task allocation. Our method effectively refines USV swarm\npolicies, addressing key challenges in multi-agent systems while maintaining\nfairness and performance consistency."
                },
                "authors": [
                    {
                        "name": "Hyeonjun Kim"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Junho Park"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20242v2",
                "updated": "2025-03-07T08:04:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    4,
                    54,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-27T16:27:42Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    27,
                    42,
                    3,
                    58,
                    0
                ],
                "title": "GreenDFL: a Framework for Assessing the Sustainability of Decentralized\n  Federated Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenDFL: a Framework for Assessing the Sustainability of Decentralized\n  Federated Learning Systems"
                },
                "summary": "Decentralized Federated Learning (DFL) is an emerging paradigm that enables\ncollaborative model training without centralized data and model aggregation,\nenhancing privacy and resilience. However, its sustainability remains\nunderexplored, as energy consumption and carbon emissions vary across different\nsystem configurations. Understanding the environmental impact of DFL is crucial\nfor optimizing its design and deployment. This work aims to develop a\ncomprehensive and operational framework for assessing the sustainability of DFL\nsystems. To address it, this work provides a systematic method for quantifying\nenergy consumption and carbon emissions, offering insights into improving the\nsustainability of DFL. This work proposes GreenDFL, a fully implementable\nframework that has been integrated into a real-world DFL platform. GreenDFL\nsystematically analyzes the impact of various factors, including hardware\naccelerators, model architecture, communication medium, data distribution,\nnetwork topology, and federation size, on the sustainability of DFL systems.\nBesides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node\nselection algorithm (GreenDFL-SN) are developed to optimize energy efficiency\nand reduce carbon emissions in DFL training. Empirical experiments are\nconducted on multiple datasets, measuring energy consumption and carbon\nemissions at different phases of the DFL lifecycle. The proposed GreenDFL\nprovides a comprehensive and practical approach for assessing the\nsustainability of DFL systems. Furthermore, it offers best practices for\nimproving environmental efficiency in DFL, making sustainability considerations\nmore actionable in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning (DFL) is an emerging paradigm that enables\ncollaborative model training without centralized data and model aggregation,\nenhancing privacy and resilience. However, its sustainability remains\nunderexplored, as energy consumption and carbon emissions vary across different\nsystem configurations. Understanding the environmental impact of DFL is crucial\nfor optimizing its design and deployment. This work aims to develop a\ncomprehensive and operational framework for assessing the sustainability of DFL\nsystems. To address it, this work provides a systematic method for quantifying\nenergy consumption and carbon emissions, offering insights into improving the\nsustainability of DFL. This work proposes GreenDFL, a fully implementable\nframework that has been integrated into a real-world DFL platform. GreenDFL\nsystematically analyzes the impact of various factors, including hardware\naccelerators, model architecture, communication medium, data distribution,\nnetwork topology, and federation size, on the sustainability of DFL systems.\nBesides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node\nselection algorithm (GreenDFL-SN) are developed to optimize energy efficiency\nand reduce carbon emissions in DFL training. Empirical experiments are\nconducted on multiple datasets, measuring energy consumption and carbon\nemissions at different phases of the DFL lifecycle. The proposed GreenDFL\nprovides a comprehensive and practical approach for assessing the\nsustainability of DFL systems. Furthermore, it offers best practices for\nimproving environmental efficiency in DFL, making sustainability considerations\nmore actionable in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Alberto Huertas Celdrán"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Gérôme Bovet"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05212v1",
                "updated": "2025-03-07T08:04:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    4,
                    25,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T08:04:25Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    4,
                    25,
                    4,
                    66,
                    0
                ],
                "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Updating? No More Model Editing! Just Selective Contextual\n  Reasoning"
                },
                "summary": "As real-world knowledge evolves, the information embedded within large\nlanguage models (LLMs) can become outdated, inadequate, or erroneous. Model\nediting has emerged as a prominent approach for updating LLMs' knowledge with\nminimal computational costs and parameter changes. This approach typically\nidentifies and adjusts specific model parameters associated with newly acquired\nknowledge. However, existing methods often underestimate the adverse effects\nthat parameter modifications can have on broadly distributed knowledge. More\ncritically, post-edit LLMs frequently struggle with multi-hop reasoning and\ncontinuous knowledge updates. Although various studies have discussed these\nshortcomings, there is a lack of comprehensive evaluation. In this paper, we\nprovide an evaluation of ten model editing methods along four dimensions:\nreliability, generalization, locality, and portability. Results confirm that\nall ten popular model editing methods show significant shortcomings across\nmultiple dimensions, suggesting model editing is less promising. We then\npropose a straightforward method called Selective Contextual Reasoning (SCR),\nfor knowledge updating. SCR does not modify model parameters but harnesses\nLLM's inherent contextual reasoning capabilities utilizing the updated\nknowledge pieces. Under SCR, an LLM first assesses whether an incoming query\nfalls within the scope of an external knowledge base. If it does, the relevant\nexternal knowledge texts are contextualized to enhance reasoning; otherwise,\nthe query is answered directly. We evaluate SCR against the ten model editing\nmethods on two counterfactual datasets with three backbone LLMs. Empirical\nresults confirm the effectiveness and efficiency of contextual reasoning for\nknowledge updating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As real-world knowledge evolves, the information embedded within large\nlanguage models (LLMs) can become outdated, inadequate, or erroneous. Model\nediting has emerged as a prominent approach for updating LLMs' knowledge with\nminimal computational costs and parameter changes. This approach typically\nidentifies and adjusts specific model parameters associated with newly acquired\nknowledge. However, existing methods often underestimate the adverse effects\nthat parameter modifications can have on broadly distributed knowledge. More\ncritically, post-edit LLMs frequently struggle with multi-hop reasoning and\ncontinuous knowledge updates. Although various studies have discussed these\nshortcomings, there is a lack of comprehensive evaluation. In this paper, we\nprovide an evaluation of ten model editing methods along four dimensions:\nreliability, generalization, locality, and portability. Results confirm that\nall ten popular model editing methods show significant shortcomings across\nmultiple dimensions, suggesting model editing is less promising. We then\npropose a straightforward method called Selective Contextual Reasoning (SCR),\nfor knowledge updating. SCR does not modify model parameters but harnesses\nLLM's inherent contextual reasoning capabilities utilizing the updated\nknowledge pieces. Under SCR, an LLM first assesses whether an incoming query\nfalls within the scope of an external knowledge base. If it does, the relevant\nexternal knowledge texts are contextualized to enhance reasoning; otherwise,\nthe query is answered directly. We evaluate SCR against the ten model editing\nmethods on two counterfactual datasets with three backbone LLMs. Empirical\nresults confirm the effectiveness and efficiency of contextual reasoning for\nknowledge updating."
                },
                "authors": [
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05206v1",
                "updated": "2025-03-07T07:54:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    54,
                    43,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:54:43Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    54,
                    43,
                    4,
                    66,
                    0
                ],
                "title": "Operationalizing Cybersecurity Knowledge: Design, Implementation &\n  Evaluation of a Knowledge Management System for CACAO Playbooks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operationalizing Cybersecurity Knowledge: Design, Implementation &\n  Evaluation of a Knowledge Management System for CACAO Playbooks"
                },
                "summary": "Modern cybersecurity threats are growing in complexity, targeting\nincreasingly intricate & interconnected systems. To effectively defend against\nthese evolving threats, security teams utilize automation & orchestration to\nenhance response efficiency and consistency. In that sense, cybersecurity\nplaybooks are key enablers, providing a structured, reusable, and continuously\nimproving approach to incident response, enabling organizations to codify\nrequirements, domain expertise, and best practices and automate decision-making\nprocesses to the extent possible. The emerging Collaborative Automated Course\nof Action Operations (CACAO) standard defines a common machine-processable\nschema for cybersecurity playbooks, facilitating interoperability for their\nexchange and ensuring the ability to orchestrate and automate cybersecurity\noperations. However, despite its potential and the fact that it is a relatively\nnew standardization work, there is a lack of tools to support its adoption and,\nin particular, the management & lifecycle development of CACAO playbooks,\nlimiting their practical deployment. Motivated by the above, this work presents\nthe design, development, and evaluation of a Knowledge Management System (KMS)\nfor managing CACAO cybersecurity playbooks throughout their lifecycle,\nproviding essential tools to streamline playbook management. Using open\ntechnologies & standards, the proposed approach fosters standards-based\ninteroperability & enhances the usability of state-of-the-art cybersecurity\norchestration & automation primitives. To encourage adoption, the resulting\nimplementation is released as open-source, which, to the extent of our\nknowledge, comprises the first publicly available & documented work in this\ndomain, supporting the broader uptake of CACAO playbooks & promoting the\nwidespread use of interoperable automation and orchestration mechanisms in\ncybersecurity operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cybersecurity threats are growing in complexity, targeting\nincreasingly intricate & interconnected systems. To effectively defend against\nthese evolving threats, security teams utilize automation & orchestration to\nenhance response efficiency and consistency. In that sense, cybersecurity\nplaybooks are key enablers, providing a structured, reusable, and continuously\nimproving approach to incident response, enabling organizations to codify\nrequirements, domain expertise, and best practices and automate decision-making\nprocesses to the extent possible. The emerging Collaborative Automated Course\nof Action Operations (CACAO) standard defines a common machine-processable\nschema for cybersecurity playbooks, facilitating interoperability for their\nexchange and ensuring the ability to orchestrate and automate cybersecurity\noperations. However, despite its potential and the fact that it is a relatively\nnew standardization work, there is a lack of tools to support its adoption and,\nin particular, the management & lifecycle development of CACAO playbooks,\nlimiting their practical deployment. Motivated by the above, this work presents\nthe design, development, and evaluation of a Knowledge Management System (KMS)\nfor managing CACAO cybersecurity playbooks throughout their lifecycle,\nproviding essential tools to streamline playbook management. Using open\ntechnologies & standards, the proposed approach fosters standards-based\ninteroperability & enhances the usability of state-of-the-art cybersecurity\norchestration & automation primitives. To encourage adoption, the resulting\nimplementation is released as open-source, which, to the extent of our\nknowledge, comprises the first publicly available & documented work in this\ndomain, supporting the broader uptake of CACAO playbooks & promoting the\nwidespread use of interoperable automation and orchestration mechanisms in\ncybersecurity operations."
                },
                "authors": [
                    {
                        "name": "Orestis Tsirakis"
                    },
                    {
                        "name": "Konstantinos Fysarakis"
                    },
                    {
                        "name": "Vasileios Mavroeidis"
                    },
                    {
                        "name": "Ioannis Papaefstathiou"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Papaefstathiou"
                },
                "author": "Ioannis Papaefstathiou",
                "arxiv_comment": "This preprint has not been peer-reviewed. It is a preliminary version\n  of a research article that has been submitted for journal publication. The\n  final, peer-reviewed version may differ from this preprint. Associated GitHub\n  page available at: https://github.com/Orestistsira/cacao-knowledge-base",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05203v1",
                "updated": "2025-03-07T07:48:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    48,
                    30,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:48:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    48,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge\n  Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge\n  Graph Retrieval-Augmented Generation"
                },
                "summary": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, train-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost. Code is coming soon at\nhttps://github.com/hrwang00/path-pooling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, train-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost. Code is coming soon at\nhttps://github.com/hrwang00/path-pooling."
                },
                "authors": [
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05200v1",
                "updated": "2025-03-07T07:44:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    44,
                    31,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:44:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    44,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "ORANSight-2.0: Foundational LLMs for O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORANSight-2.0: Foundational LLMs for O-RAN"
                },
                "summary": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\naimed at developing specialized foundational LLMs tailored for O-RAN. Built on\n18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes\nmodels ranging from 1 to 70B parameters, significantly reducing reliance on\nproprietary, closed-source models while enhancing performance for O-RAN. At the\ncore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation\n(RAG) based instruction-tuning framework that employs two LLM agents to create\nhigh-quality instruction-tuning datasets. The generated dataset is then used to\nfine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate\nORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code\ngeneration and codebase understanding in the context of srsRAN, a widely used\n5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for\nassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate\nthat ORANSight-2.0 models outperform general-purpose and closed-source models,\nsuch as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on\nsrsRANBench, achieving superior performance while maintaining lower\ncomputational and energy costs. We also experiment with RAG-augmented variants\nof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,\ndemonstrating costs for training, standard inference, and RAG-augmented\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\naimed at developing specialized foundational LLMs tailored for O-RAN. Built on\n18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes\nmodels ranging from 1 to 70B parameters, significantly reducing reliance on\nproprietary, closed-source models while enhancing performance for O-RAN. At the\ncore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation\n(RAG) based instruction-tuning framework that employs two LLM agents to create\nhigh-quality instruction-tuning datasets. The generated dataset is then used to\nfine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate\nORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code\ngeneration and codebase understanding in the context of srsRAN, a widely used\n5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for\nassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate\nthat ORANSight-2.0 models outperform general-purpose and closed-source models,\nsuch as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on\nsrsRANBench, achieving superior performance while maintaining lower\ncomputational and energy costs. We also experiment with RAG-augmented variants\nof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,\ndemonstrating costs for training, standard inference, and RAG-augmented\ninference."
                },
                "authors": [
                    {
                        "name": "Pranshav Gajjar"
                    },
                    {
                        "name": "Vijay K. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vijay K. Shah"
                },
                "author": "Vijay K. Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16927v2",
                "updated": "2025-03-07T07:28:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    28,
                    39,
                    4,
                    66,
                    0
                ],
                "published": "2025-02-24T07:37:29Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    7,
                    37,
                    29,
                    0,
                    55,
                    0
                ],
                "title": "BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for\n  Fast Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for\n  Fast Training and Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) structure scales the Transformer-based large\nlanguage models (LLMs) and improves their performance with only the sub-linear\nincrease in computation resources. Recently, a fine-grained DeepSeekMoE\nstructure is proposed, which can further improve the computing efficiency of\nMoE without performance degradation. However, the All-to-All communication\nintroduced by MoE has become a bottleneck, especially for the fine-grained\nstructure, which typically involves and activates more experts, hence\ncontributing to heavier communication overhead.\n  In this paper, we propose a novel MoE structure named BigMac, which is also\nfine-grained but with high communication efficiency. The innovation of BigMac\nis mainly due to that we abandon the\n\\textbf{c}ommunicate-\\textbf{d}escend-\\textbf{a}scend-\\textbf{c}ommunicate\n(CDAC) manner used by fine-grained MoE, which leads to the All-to-All\ncommunication always taking place at the highest dimension. Instead, BigMac\ndesigns an efficient\n\\textbf{d}escend-\\textbf{c}ommunicate-\\textbf{c}ommunicate-\\textbf{a}scend\n(DCCA) manner. Specifically, we add a descending and ascending projection at\nthe entrance and exit of the expert, respectively, which enables the\ncommunication to perform at a very low dimension. Furthermore, to adapt to\nDCCA, we re-design the structure of small experts, ensuring that the expert in\nBigMac has enough complexity to address tokens. Experimental results show that\nBigMac achieves comparable or even better model quality than fine-grained MoEs\nwith the same number of experts and a similar number of total parameters.\nEqually importantly, BigMac reduces the end-to-end latency by up to\n3.09$\\times$ for training and increases the throughput by up to 3.11$\\times$\nfor inference on state-of-the-art AI computing frameworks including Megatron,\nTutel, and DeepSpeed-Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) structure scales the Transformer-based large\nlanguage models (LLMs) and improves their performance with only the sub-linear\nincrease in computation resources. Recently, a fine-grained DeepSeekMoE\nstructure is proposed, which can further improve the computing efficiency of\nMoE without performance degradation. However, the All-to-All communication\nintroduced by MoE has become a bottleneck, especially for the fine-grained\nstructure, which typically involves and activates more experts, hence\ncontributing to heavier communication overhead.\n  In this paper, we propose a novel MoE structure named BigMac, which is also\nfine-grained but with high communication efficiency. The innovation of BigMac\nis mainly due to that we abandon the\n\\textbf{c}ommunicate-\\textbf{d}escend-\\textbf{a}scend-\\textbf{c}ommunicate\n(CDAC) manner used by fine-grained MoE, which leads to the All-to-All\ncommunication always taking place at the highest dimension. Instead, BigMac\ndesigns an efficient\n\\textbf{d}escend-\\textbf{c}ommunicate-\\textbf{c}ommunicate-\\textbf{a}scend\n(DCCA) manner. Specifically, we add a descending and ascending projection at\nthe entrance and exit of the expert, respectively, which enables the\ncommunication to perform at a very low dimension. Furthermore, to adapt to\nDCCA, we re-design the structure of small experts, ensuring that the expert in\nBigMac has enough complexity to address tokens. Experimental results show that\nBigMac achieves comparable or even better model quality than fine-grained MoEs\nwith the same number of experts and a similar number of total parameters.\nEqually importantly, BigMac reduces the end-to-end latency by up to\n3.09$\\times$ for training and increases the throughput by up to 3.11$\\times$\nfor inference on state-of-the-art AI computing frameworks including Megatron,\nTutel, and DeepSpeed-Inference."
                },
                "authors": [
                    {
                        "name": "Zewen Jin"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Jiaan Zhu"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Zhenyu Ming"
                    },
                    {
                        "name": "Cheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Li"
                },
                "author": "Cheng Li",
                "arxiv_comment": "Typo Fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05193v1",
                "updated": "2025-03-07T07:28:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    28,
                    32,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:28:32Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    28,
                    32,
                    4,
                    66,
                    0
                ],
                "title": "Memory-augmented Query Reconstruction for LLM-based Knowledge Graph\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-augmented Query Reconstruction for LLM-based Knowledge Graph\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance on\nknowledge graph question answering (KGQA) tasks by planning and interacting\nwith knowledge graphs. However, existing methods often confuse tool utilization\nwith knowledge reasoning, harming readability of model outputs and giving rise\nto hallucinatory tool invocations, which hinder the advancement of KGQA. To\naddress this issue, we propose Memory-augmented Query Reconstruction for\nLLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation\ntasks using LLM-built query memory. By establishing a memory module with\nexplicit descriptions of query statements, the proposed MemQ facilitates the\nKGQA process with natural language reasoning and memory-augmented query\nreconstruction. Meanwhile, we design an effective and readable reasoning to\nenhance the LLM's reasoning capability in KGQA. Experimental results that MemQ\nachieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance on\nknowledge graph question answering (KGQA) tasks by planning and interacting\nwith knowledge graphs. However, existing methods often confuse tool utilization\nwith knowledge reasoning, harming readability of model outputs and giving rise\nto hallucinatory tool invocations, which hinder the advancement of KGQA. To\naddress this issue, we propose Memory-augmented Query Reconstruction for\nLLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation\ntasks using LLM-built query memory. By establishing a memory module with\nexplicit descriptions of query statements, the proposed MemQ facilitates the\nKGQA process with natural language reasoning and memory-augmented query\nreconstruction. Meanwhile, we design an effective and readable reasoning to\nenhance the LLM's reasoning capability in KGQA. Experimental results that MemQ\nachieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ."
                },
                "authors": [
                    {
                        "name": "Mufan Xu"
                    },
                    {
                        "name": "Gewen Liang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05188v1",
                "updated": "2025-03-07T07:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    20,
                    24,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    20,
                    24,
                    4,
                    66,
                    0
                ],
                "title": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM\n  Reasoning"
                },
                "summary": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT."
                },
                "authors": [
                    {
                        "name": "Jiachun Li"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Jiexin Xu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Xiaojian Jiang"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "18 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12643v2",
                "updated": "2025-03-07T07:14:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    14,
                    33,
                    4,
                    66,
                    0
                ],
                "published": "2024-12-17T08:07:16Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    7,
                    16,
                    1,
                    352,
                    0
                ],
                "title": "LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering"
                },
                "summary": "Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm. To tackle this issue, we propose READS to reformulate the KGQA\nprocess into discriminative subtasks, which simplifies the search space for\neach subtasks. Based on the subtasks, we design a new corresponding\ndiscriminative inference strategy to conduct the reasoning for KGQA, thereby\nalleviating hallucination and ungrounded reasoning issues in LLMs. Experimental\nresults show that the proposed approach outperforms multiple strong comparison\nmethods, along with achieving state-of-the-art performance on widely used\nbenchmarks WebQSP and CWQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm. To tackle this issue, we propose READS to reformulate the KGQA\nprocess into discriminative subtasks, which simplifies the search space for\neach subtasks. Based on the subtasks, we design a new corresponding\ndiscriminative inference strategy to conduct the reasoning for KGQA, thereby\nalleviating hallucination and ungrounded reasoning issues in LLMs. Experimental\nresults show that the proposed approach outperforms multiple strong comparison\nmethods, along with achieving state-of-the-art performance on widely used\nbenchmarks WebQSP and CWQ."
                },
                "authors": [
                    {
                        "name": "Mufan Xu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05185v1",
                "updated": "2025-03-07T07:13:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    13,
                    59,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T07:13:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    13,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance"
                },
                "summary": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench."
                },
                "authors": [
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Junfeng Li"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Huanbo Luan"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05179v1",
                "updated": "2025-03-07T06:57:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    57,
                    17,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T06:57:17Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    57,
                    17,
                    4,
                    66,
                    0
                ],
                "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching"
                },
                "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT."
                },
                "authors": [
                    {
                        "name": "Simon A. Aytes"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05174v1",
                "updated": "2025-03-07T06:40:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    40,
                    6,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T06:40:06Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    40,
                    6,
                    4,
                    66,
                    0
                ],
                "title": "SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image\n  via 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image\n  via 3D Gaussian Splatting"
                },
                "summary": "6-DoF pose estimation is a fundamental task in computer vision with\nwide-ranging applications in augmented reality and robotics. Existing single\nRGB-based methods often compromise accuracy due to their reliance on initial\npose estimates and susceptibility to rotational ambiguity, while approaches\nrequiring depth sensors or multi-view setups incur significant deployment\ncosts. To address these limitations, we introduce SplatPose, a novel framework\nthat synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural\narchitecture to achieve high-precision pose estimation using only a single RGB\nimage. Central to our approach is the Dual-Attention Ray Scoring Network\n(DARS-Net), which innovatively decouples positional and angular alignment\nthrough geometry-domain attention mechanisms, explicitly modeling directional\ndependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine\noptimization pipeline progressively refines pose estimates by aligning dense 2D\nfeatures between query images and 3DGS-synthesized views, effectively\ncorrecting feature misalignment and depth errors from sparse ray sampling.\nExperiments on three benchmark datasets demonstrate that SplatPose achieves\nstate-of-the-art 6-DoF pose estimation accuracy in single RGB settings,\nrivaling approaches that depend on depth or multi-view images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF pose estimation is a fundamental task in computer vision with\nwide-ranging applications in augmented reality and robotics. Existing single\nRGB-based methods often compromise accuracy due to their reliance on initial\npose estimates and susceptibility to rotational ambiguity, while approaches\nrequiring depth sensors or multi-view setups incur significant deployment\ncosts. To address these limitations, we introduce SplatPose, a novel framework\nthat synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural\narchitecture to achieve high-precision pose estimation using only a single RGB\nimage. Central to our approach is the Dual-Attention Ray Scoring Network\n(DARS-Net), which innovatively decouples positional and angular alignment\nthrough geometry-domain attention mechanisms, explicitly modeling directional\ndependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine\noptimization pipeline progressively refines pose estimates by aligning dense 2D\nfeatures between query images and 3DGS-synthesized views, effectively\ncorrecting feature misalignment and depth errors from sparse ray sampling.\nExperiments on three benchmark datasets demonstrate that SplatPose achieves\nstate-of-the-art 6-DoF pose estimation accuracy in single RGB settings,\nrivaling approaches that depend on depth or multi-view images."
                },
                "authors": [
                    {
                        "name": "Linqi Yang"
                    },
                    {
                        "name": "Xiongwei Zhao"
                    },
                    {
                        "name": "Qihao Sun"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Ao Chen"
                    },
                    {
                        "name": "Peng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Kang"
                },
                "author": "Peng Kang",
                "arxiv_comment": "Submitted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12262v3",
                "updated": "2025-03-07T06:18:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    18,
                    3,
                    4,
                    66,
                    0
                ],
                "published": "2024-09-18T18:47:58Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    47,
                    58,
                    2,
                    262,
                    0
                ],
                "title": "Bootstrapping Object-level Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Object-level Planning with Large Language Models"
                },
                "summary": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation."
                },
                "authors": [
                    {
                        "name": "David Paulius"
                    },
                    {
                        "name": "Alejandro Agostini"
                    },
                    {
                        "name": "Benedict Quartey"
                    },
                    {
                        "name": "George Konidaris"
                    }
                ],
                "author_detail": {
                    "name": "George Konidaris"
                },
                "author": "George Konidaris",
                "arxiv_comment": "Accepted to ICRA 2025; 11 pages (6 pages + 1 page references + 4\n  pages appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02295v2",
                "updated": "2025-03-07T06:16:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    6,
                    16,
                    34,
                    4,
                    66,
                    0
                ],
                "published": "2025-01-04T14:08:52Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    14,
                    8,
                    52,
                    5,
                    4,
                    0
                ],
                "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language\n  Models through Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit vs. Implicit: Investigating Social Bias in Large Language\n  Models through Self-Reflection"
                },
                "summary": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies."
                },
                "authors": [
                    {
                        "name": "Yachao Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]