[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v1",
                "updated": "2025-04-15T14:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v1",
                "updated": "2025-04-14T00:29:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables. Code can shared at request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Pérez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castaño"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castaño"
                },
                "author": "Manuel Gamero-Castaño",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.11456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11456v1",
                "updated": "2025-04-15T17:59:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    51,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:59:51Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    51,
                    1,
                    105,
                    0
                ],
                "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning"
                },
                "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath."
                },
                "authors": [
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11455v1",
                "updated": "2025-04-15T17:59:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    46,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:59:46Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    46,
                    1,
                    105,
                    0
                ],
                "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL"
                },
                "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR."
                },
                "authors": [
                    {
                        "name": "Junke Wang"
                    },
                    {
                        "name": "Zhi Tian"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "technical report, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11451v1",
                "updated": "2025-04-15T17:58:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    58,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:58:16Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    58,
                    16,
                    1,
                    105,
                    0
                ],
                "title": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond"
                },
                "summary": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/"
                },
                "authors": [
                    {
                        "name": "Minghua Liu"
                    },
                    {
                        "name": "Mikaela Angelina Uy"
                    },
                    {
                        "name": "Donglai Xiang"
                    },
                    {
                        "name": "Hao Su"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Nicholas Sharp"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "https://research.nvidia.com/labs/toronto-ai/partfield-release/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11446v1",
                "updated": "2025-04-15T17:56:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    56,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:56:24Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    56,
                    24,
                    1,
                    105,
                    0
                ],
                "title": "eXplainable AI for data driven control: an inverse optimal control\n  approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eXplainable AI for data driven control: an inverse optimal control\n  approach"
                },
                "summary": "Understanding the behavior of black-box data-driven controllers is a key\nchallenge in modern control design. In this work, we propose an eXplainable AI\n(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local\nexplanations for the behavior of a controller operating around a given region.\nSpecifically, we extract the weights assigned to tracking errors and control\neffort in the implicit cost function that a black-box controller is optimizing,\noffering a more transparent and interpretable representation of the\ncontroller's underlying objectives. This approach presents connections with\nwell-established XAI techniques, such as Local Interpretable Model-agnostic\nExplanations (LIME) since it is still based on a local approximation of the\ncontrol policy. However, rather being limited to a standard sensitivity\nanalysis, the explanation provided by our method relies on the solution of an\ninverse Linear Quadratic (LQ) problem, offering a structured and more\ncontrol-relevant perspective. Numerical examples demonstrate that the inferred\ncost function consistently provides a deeper understanding of the controller's\ndecision-making process, shedding light on otherwise counterintuitive or\nunexpected phenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the behavior of black-box data-driven controllers is a key\nchallenge in modern control design. In this work, we propose an eXplainable AI\n(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local\nexplanations for the behavior of a controller operating around a given region.\nSpecifically, we extract the weights assigned to tracking errors and control\neffort in the implicit cost function that a black-box controller is optimizing,\noffering a more transparent and interpretable representation of the\ncontroller's underlying objectives. This approach presents connections with\nwell-established XAI techniques, such as Local Interpretable Model-agnostic\nExplanations (LIME) since it is still based on a local approximation of the\ncontrol policy. However, rather being limited to a standard sensitivity\nanalysis, the explanation provided by our method relies on the solution of an\ninverse Linear Quadratic (LQ) problem, offering a structured and more\ncontrol-relevant perspective. Numerical examples demonstrate that the inferred\ncost function consistently provides a deeper understanding of the controller's\ndecision-making process, shedding light on otherwise counterintuitive or\nunexpected phenomena."
                },
                "authors": [
                    {
                        "name": "Federico Porcari"
                    },
                    {
                        "name": "Donatello Materassi"
                    },
                    {
                        "name": "Simone Formentin"
                    }
                ],
                "author_detail": {
                    "name": "Simone Formentin"
                },
                "author": "Simone Formentin",
                "arxiv_comment": "Submitted to CDC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11442v1",
                "updated": "2025-04-15T17:55:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    55,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:55:20Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    55,
                    20,
                    1,
                    105,
                    0
                ],
                "title": "TextArena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextArena"
                },
                "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/."
                },
                "authors": [
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Bobby Cheng"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "work in progress; 5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10479v2",
                "updated": "2025-04-15T17:50:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    50,
                    27,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models"
                },
                "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."
                },
                "authors": [
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Xingguang Wei"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Haomin Wang"
                    },
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Dengnian Chen"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Tan Jiang"
                    },
                    {
                        "name": "Jiapeng Luo"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Wenwen Qu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Penglong Jiao"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11431v1",
                "updated": "2025-04-15T17:41:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    41,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:41:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    41,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models"
                },
                "summary": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "arxiv_comment": "To appear in ICWSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11426v1",
                "updated": "2025-04-15T17:38:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    47,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:38:47Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    47,
                    1,
                    105,
                    0
                ],
                "title": "A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v2",
                "updated": "2025-04-15T17:38:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11423v1",
                "updated": "2025-04-15T17:37:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    37,
                    50,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:37:50Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    37,
                    50,
                    1,
                    105,
                    0
                ],
                "title": "ADT: Tuning Diffusion Models with Adversarial Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADT: Tuning Diffusion Models with Adversarial Supervision"
                },
                "summary": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality."
                },
                "authors": [
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Lujundong Li"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11420v1",
                "updated": "2025-04-15T17:35:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    35,
                    56,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:35:56Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    35,
                    56,
                    1,
                    105,
                    0
                ],
                "title": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\n  Composing Informative Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\n  Composing Informative Contexts"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    }
                ],
                "author_detail": {
                    "name": "Sinno Jialin Pan"
                },
                "author": "Sinno Jialin Pan",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11409v1",
                "updated": "2025-04-15T17:26:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning"
                },
                "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier."
                },
                "authors": [
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Yashaswi Karnati"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v4",
                "updated": "2025-04-15T17:19:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    19,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "AI agents can coordinate beyond human scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents can coordinate beyond human scale"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in collaborative tasks\ninvolving multiple agents, forming an \"AI agent society: where agents interact\nand influence one another. Whether such groups can spontaneously coordinate on\narbitrary decisions without external influence - a hallmark of self-organized\nregulation in human societies - remains an open question. Here we investigate\nthe stability of groups formed by AI agents by applying methods from complexity\nscience and principles from behavioral sciences. We find that LLMs can\nspontaneously form cohesive groups, and that their opinion dynamics is governed\nby a majority force coefficient, which determines whether coordination is\nachievable. This majority force diminishes as group size increases, leading to\na critical group size beyond which coordination becomes practically\nunattainable and stability is lost. Notably, this critical group size grows\nexponentially with the language capabilities of the models, and for the most\nadvanced LLMs, it exceeds the typical size of informal human groups. Our\nfindings highlight intrinsic limitations in the self-organization of AI agent\nsocieties and have implications for the design of collaborative AI systems\nwhere coordination is desired or could represent a treat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in collaborative tasks\ninvolving multiple agents, forming an \"AI agent society: where agents interact\nand influence one another. Whether such groups can spontaneously coordinate on\narbitrary decisions without external influence - a hallmark of self-organized\nregulation in human societies - remains an open question. Here we investigate\nthe stability of groups formed by AI agents by applying methods from complexity\nscience and principles from behavioral sciences. We find that LLMs can\nspontaneously form cohesive groups, and that their opinion dynamics is governed\nby a majority force coefficient, which determines whether coordination is\nachievable. This majority force diminishes as group size increases, leading to\na critical group size beyond which coordination becomes practically\nunattainable and stability is lost. Notably, this critical group size grows\nexponentially with the language capabilities of the models, and for the most\nadvanced LLMs, it exceeds the typical size of informal human groups. Our\nfindings highlight intrinsic limitations in the self-organization of AI agent\nsocieties and have implications for the design of collaborative AI systems\nwhere coordination is desired or could represent a treat."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07491v2",
                "updated": "2025-04-15T17:14:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    14,
                    37,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-10T06:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Kimi-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-VL Technical Report"
                },
                "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chenlin Zhang"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Dikang Du"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Lin Sui"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Qizheng Gu"
                    },
                    {
                        "name": "Runjie Zhou"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianhui Song"
                    },
                    {
                        "name": "Tongtong Bai"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Xiaokun Yuan"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Yongsheng Kang"
                    },
                    {
                        "name": "Yuanxin Liu"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Zijia Zhao"
                    },
                    {
                        "name": "Ziwei Chen"
                    },
                    {
                        "name": "Zongyu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zongyu Lin"
                },
                "author": "Zongyu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11389v1",
                "updated": "2025-04-15T16:58:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    15,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:58:15Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    15,
                    1,
                    105,
                    0
                ],
                "title": "VideoPanda: Video Panoramic Diffusion with Multi-view Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoPanda: Video Panoramic Diffusion with Multi-view Attention"
                },
                "summary": "High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results."
                },
                "authors": [
                    {
                        "name": "Kevin Xie"
                    },
                    {
                        "name": "Amirmojtaba Sabour"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Despoina Paschalidou"
                    },
                    {
                        "name": "Greg Klar"
                    },
                    {
                        "name": "Umar Iqbal"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Xiaohui Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Zeng"
                },
                "author": "Xiaohui Zeng",
                "arxiv_comment": "Project website at\n  https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11381v1",
                "updated": "2025-04-15T16:53:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    53,
                    31,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:53:31Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    53,
                    31,
                    1,
                    105,
                    0
                ],
                "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankAlign: A Ranking View of the Generator-Validator Gap in Large\n  Language Models"
                },
                "summary": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items."
                },
                "authors": [
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Wenxuan Ding"
                    },
                    {
                        "name": "Katrin Erk"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11373v1",
                "updated": "2025-04-15T16:37:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    32,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:37:32Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    32,
                    1,
                    105,
                    0
                ],
                "title": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions"
                },
                "summary": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems."
                },
                "authors": [
                    {
                        "name": "Wang Bill Zhu"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Ching Ying Lin"
                    },
                    {
                        "name": "Jade Law"
                    },
                    {
                        "name": "Mazen Jizzini"
                    },
                    {
                        "name": "Jorge J. Nieva"
                    },
                    {
                        "name": "Ruishan Liu"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11364v1",
                "updated": "2025-04-15T16:30:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    30,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:30:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    30,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Teaching Large Language Models to Reason through Learning and Forgetting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Large Language Models to Reason through Learning and Forgetting"
                },
                "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$."
                },
                "authors": [
                    {
                        "name": "Tianwei Ni"
                    },
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Sapana Chaudhary"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11358v1",
                "updated": "2025-04-15T16:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    26,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:26:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    26,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks"
                },
                "summary": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in IEEE Symposium on Security and Privacy, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11343v1",
                "updated": "2025-04-15T16:15:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    15,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:15:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    15,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce"
                },
                "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Hanze Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hanze Dong"
                },
                "author": "Hanze Dong",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11336v1",
                "updated": "2025-04-15T16:09:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    9,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:09:06Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    9,
                    6,
                    1,
                    105,
                    0
                ],
                "title": "Looking beyond the next token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looking beyond the next token"
                },
                "summary": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm."
                },
                "authors": [
                    {
                        "name": "Abitha Thankaraj"
                    },
                    {
                        "name": "Yiding Jiang"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Yonatan Bisk"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Bisk"
                },
                "author": "Yonatan Bisk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11330v1",
                "updated": "2025-04-15T16:04:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    4,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:04:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    4,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Decorrelation in Complex Wave Scattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decorrelation in Complex Wave Scattering"
                },
                "summary": "Phenomena involving multiple scattering, despite having attracted\nconsiderable attention in physics for decades, continue to generate unexpected\nand counterintuitive behaviours prompting further studies. For optical\nscattering, the memory effect well predicts fourth order statistics, i.e. the\nintensity correlation, as long as the scattering strength and depth are within\ncertain bounds. The memory effect has found a wide range of applications, where\nits limitations also become apparent: for example, in imaging through turbid\nmedia, decorrelation due to multiscattering in thick samples has been shown to\nrestrict the field of view. However, to our knowledge, no comprehensive\nmechanism exists to date that can account for decorrelation precisely. In this\npaper, we quantify how the scatterer's own statistics determine such\nlimitations. We show that the ensemble statistics of the backscattered field\nmay be decomposed into two terms: one expresses surface scattering, where\nstatistical distributions of multiscale structure features may be inferred from\nour previous works; while the second term originates from the underlying\nscattering volume and is diffusive. The new framework agrees well with\nexperiments, including the prediction of a new quasipower law for fluctuations\ninduced by the single realization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phenomena involving multiple scattering, despite having attracted\nconsiderable attention in physics for decades, continue to generate unexpected\nand counterintuitive behaviours prompting further studies. For optical\nscattering, the memory effect well predicts fourth order statistics, i.e. the\nintensity correlation, as long as the scattering strength and depth are within\ncertain bounds. The memory effect has found a wide range of applications, where\nits limitations also become apparent: for example, in imaging through turbid\nmedia, decorrelation due to multiscattering in thick samples has been shown to\nrestrict the field of view. However, to our knowledge, no comprehensive\nmechanism exists to date that can account for decorrelation precisely. In this\npaper, we quantify how the scatterer's own statistics determine such\nlimitations. We show that the ensemble statistics of the backscattered field\nmay be decomposed into two terms: one expresses surface scattering, where\nstatistical distributions of multiscale structure features may be inferred from\nour previous works; while the second term originates from the underlying\nscattering volume and is diffusive. The new framework agrees well with\nexperiments, including the prediction of a new quasipower law for fluctuations\ninduced by the single realization."
                },
                "authors": [
                    {
                        "name": "Qihang Zhang"
                    },
                    {
                        "name": "Haoyu Yue"
                    },
                    {
                        "name": "Ninghe Liu"
                    },
                    {
                        "name": "Danlin Xu"
                    },
                    {
                        "name": "Renjie Zhou"
                    },
                    {
                        "name": "Liangcai Cao"
                    },
                    {
                        "name": "George Barbastathis"
                    }
                ],
                "author_detail": {
                    "name": "George Barbastathis"
                },
                "author": "George Barbastathis",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11329v1",
                "updated": "2025-04-15T16:03:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    3,
                    10,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:03:10Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    3,
                    10,
                    1,
                    105,
                    0
                ],
                "title": "Hunting for Maxwell's Demon in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunting for Maxwell's Demon in the Wild"
                },
                "summary": "The apparent paradox of Maxwell's demon motivated the development of\ninformation thermodynamics and, more recently, engineering advances enabling\nthe creation of nanoscale information engines. From these advances, it is now\nunderstood that nanoscale machines like the molecular motors within cells can\nin principle operate as Maxwell demons. This motivates the question: does\ninformation help power molecular motors? Answering this would seemingly require\nsimultaneous measurement of all system degrees of freedom, which is generally\nintractable in single-molecule experiments. To overcome this limitation, we\nderive a statistical estimator to infer both the direction and magnitude of\nsubsystem heat flows, and thus to determine whether -- and how strongly -- a\nmotor operates as a Maxwell demon. The estimator uses only trajectory\nmeasurements for a single degree of freedom. We demonstrate the estimator by\napplying it to simulations of an experimental realization of an information\nengine and a kinesin molecular motor. Our results show that kinesin transitions\nto a Maxwell-demon mechanism in the presence of nonequilibrium noise, with a\ncorresponding increase in velocity consistent with experiments. These findings\nsuggest that molecular motors may have evolved to leverage active fluctuations\nwithin cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The apparent paradox of Maxwell's demon motivated the development of\ninformation thermodynamics and, more recently, engineering advances enabling\nthe creation of nanoscale information engines. From these advances, it is now\nunderstood that nanoscale machines like the molecular motors within cells can\nin principle operate as Maxwell demons. This motivates the question: does\ninformation help power molecular motors? Answering this would seemingly require\nsimultaneous measurement of all system degrees of freedom, which is generally\nintractable in single-molecule experiments. To overcome this limitation, we\nderive a statistical estimator to infer both the direction and magnitude of\nsubsystem heat flows, and thus to determine whether -- and how strongly -- a\nmotor operates as a Maxwell demon. The estimator uses only trajectory\nmeasurements for a single degree of freedom. We demonstrate the estimator by\napplying it to simulations of an experimental realization of an information\nengine and a kinesin molecular motor. Our results show that kinesin transitions\nto a Maxwell-demon mechanism in the presence of nonequilibrium noise, with a\ncorresponding increase in velocity consistent with experiments. These findings\nsuggest that molecular motors may have evolved to leverage active fluctuations\nwithin cells."
                },
                "authors": [
                    {
                        "name": "Johan du Buisson"
                    },
                    {
                        "name": "Jannik Ehrich"
                    },
                    {
                        "name": "Matthew P. Leighton"
                    },
                    {
                        "name": "Avijit Kundu"
                    },
                    {
                        "name": "Tushar K. Saha"
                    },
                    {
                        "name": "John Bechhoefer"
                    },
                    {
                        "name": "David A. Sivak"
                    }
                ],
                "author_detail": {
                    "name": "David A. Sivak"
                },
                "author": "David A. Sivak",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09347v2",
                "updated": "2025-04-15T15:55:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    55,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-12T21:32:42Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    21,
                    32,
                    42,
                    5,
                    102,
                    0
                ],
                "title": "Inferring Outcome Means of Exponential Family Distributions Estimated by\n  Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Outcome Means of Exponential Family Distributions Estimated by\n  Deep Neural Networks"
                },
                "summary": "While deep neural networks (DNNs) are widely used for prediction, inference\non DNN-estimated subject-specific means for categorical or exponential family\noutcomes remains underexplored. We address this by proposing a DNN estimator\nunder generalized nonparametric regression models (GNRMs) and developing a\nrigorous inference framework. Unlike existing approaches that assume\nindependence between prediction errors and inputs to establish the error bound,\na condition often violated in GNRMs, we allow for dependence and our\ntheoretical analysis demonstrates the feasibility of drawing inference under\nGNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM)\nthat leverages U-statistics and the Hoeffding decomposition to construct\nreliable confidence intervals for DNN estimates. We show that, under GNRM\nsettings, ESM enables model-free variance estimation and accounts for\nheterogeneity among individuals in the population. Through simulations under\nnonparametric logistic, Poisson, and binomial regression models, we demonstrate\nthe effectiveness and efficiency of our method. We further apply the method to\nthe electronic Intensive Care Unit (eICU) dataset, a large-scale collection of\nanonymized health records from ICU patients, to predict ICU readmission risk\nand offer patient-centric insights for clinical decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep neural networks (DNNs) are widely used for prediction, inference\non DNN-estimated subject-specific means for categorical or exponential family\noutcomes remains underexplored. We address this by proposing a DNN estimator\nunder generalized nonparametric regression models (GNRMs) and developing a\nrigorous inference framework. Unlike existing approaches that assume\nindependence between prediction errors and inputs to establish the error bound,\na condition often violated in GNRMs, we allow for dependence and our\ntheoretical analysis demonstrates the feasibility of drawing inference under\nGNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM)\nthat leverages U-statistics and the Hoeffding decomposition to construct\nreliable confidence intervals for DNN estimates. We show that, under GNRM\nsettings, ESM enables model-free variance estimation and accounts for\nheterogeneity among individuals in the population. Through simulations under\nnonparametric logistic, Poisson, and binomial regression models, we demonstrate\nthe effectiveness and efficiency of our method. We further apply the method to\nthe electronic Intensive Care Unit (eICU) dataset, a large-scale collection of\nanonymized health records from ICU patients, to predict ICU readmission risk\nand offer patient-centric insights for clinical decision-making."
                },
                "authors": [
                    {
                        "name": "Xuran Meng"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "44 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11307v1",
                "updated": "2025-04-15T15:48:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    48,
                    51,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:48:51Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    48,
                    51,
                    1,
                    105,
                    0
                ],
                "title": "Uncertainty Estimation for Trust Attribution to Speed-of-Sound\n  Reconstruction with Variational Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Estimation for Trust Attribution to Speed-of-Sound\n  Reconstruction with Variational Networks"
                },
                "summary": "Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making."
                },
                "authors": [
                    {
                        "name": "Sonia Laguna"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Can Deniz Bezek"
                    },
                    {
                        "name": "Monika Farkas"
                    },
                    {
                        "name": "Dieter Schweizer"
                    },
                    {
                        "name": "Rahel A. Kubik-Huch"
                    },
                    {
                        "name": "Orcun Goksel"
                    }
                ],
                "author_detail": {
                    "name": "Orcun Goksel"
                },
                "author": "Orcun Goksel",
                "arxiv_comment": "Published at the International Journal of Computer Assisted Radiology\n  and Surgery. Presented at the 16th International Conference on Information\n  Processing in Computer-Assisted Interventions 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11301v1",
                "updated": "2025-04-15T15:44:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:44:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments."
                },
                "authors": [
                    {
                        "name": "Yangyang Zhuang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12144v2",
                "updated": "2025-04-15T15:42:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    42,
                    40,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-10T09:13:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    13,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models"
                },
                "summary": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings."
                },
                "authors": [
                    {
                        "name": "Chang-Jin Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Yun Tang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "arxiv_comment": "Submitted to Computers in Human Behavior Reports. 54 pages (main\n  text), 12 pages (appendix), and 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09727v2",
                "updated": "2025-04-15T15:38:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    38,
                    22,
                    1,
                    105,
                    0
                ],
                "published": "2024-01-18T05:06:39Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    5,
                    6,
                    39,
                    3,
                    18,
                    0
                ],
                "title": "Lateral Phishing With Large Language Models: A Large Organization\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lateral Phishing With Large Language Models: A Large Organization\n  Comparative Study"
                },
                "summary": "The emergence of Large Language Models (LLMs) has heightened the threat of\nphishing emails by enabling the generation of highly targeted, personalized,\nand automated attacks. Traditionally, many phishing emails have been\ncharacterized by typos, errors, and poor language. These errors can be\nmitigated by LLMs, potentially lowering the barrier for attackers. Despite\nthis, there is a lack of large-scale studies comparing the effectiveness of\nLLM-generated lateral phishing emails to those crafted by humans. Current\nliterature does not adequately address the comparative effectiveness of LLM and\nhuman-generated lateral phishing emails in a real-world, large-scale\norganizational setting, especially considering the potential for LLMs to\ngenerate more convincing and error-free phishing content. To address this gap,\nwe conducted a pioneering study within a large university, targeting its\nworkforce of approximately 9,000 individuals including faculty, staff,\nadministrators, and student workers. Our results indicate that LLM-generated\nlateral phishing emails are as effective as those written by communications\nprofessionals, emphasizing the critical threat posed by LLMs in leading\nphishing campaigns. We break down the results of the overall phishing\nexperiment, comparing vulnerability between departments and job roles.\nFurthermore, to gather qualitative data, we administered a detailed\nquestionnaire, revealing insights into the reasons and motivations behind\nvulnerable employee's actions. This study contributes to the understanding of\ncyber security threats in educational institutions and provides a comprehensive\ncomparison of LLM and human-generated phishing emails' effectiveness,\nconsidering the potential for LLMs to generate more convincing content. The\nfindings highlight the need for enhanced user education and system defenses to\nmitigate the growing threat of AI-powered phishing attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has heightened the threat of\nphishing emails by enabling the generation of highly targeted, personalized,\nand automated attacks. Traditionally, many phishing emails have been\ncharacterized by typos, errors, and poor language. These errors can be\nmitigated by LLMs, potentially lowering the barrier for attackers. Despite\nthis, there is a lack of large-scale studies comparing the effectiveness of\nLLM-generated lateral phishing emails to those crafted by humans. Current\nliterature does not adequately address the comparative effectiveness of LLM and\nhuman-generated lateral phishing emails in a real-world, large-scale\norganizational setting, especially considering the potential for LLMs to\ngenerate more convincing and error-free phishing content. To address this gap,\nwe conducted a pioneering study within a large university, targeting its\nworkforce of approximately 9,000 individuals including faculty, staff,\nadministrators, and student workers. Our results indicate that LLM-generated\nlateral phishing emails are as effective as those written by communications\nprofessionals, emphasizing the critical threat posed by LLMs in leading\nphishing campaigns. We break down the results of the overall phishing\nexperiment, comparing vulnerability between departments and job roles.\nFurthermore, to gather qualitative data, we administered a detailed\nquestionnaire, revealing insights into the reasons and motivations behind\nvulnerable employee's actions. This study contributes to the understanding of\ncyber security threats in educational institutions and provides a comprehensive\ncomparison of LLM and human-generated phishing emails' effectiveness,\nconsidering the potential for LLMs to generate more convincing content. The\nfindings highlight the need for enhanced user education and system defenses to\nmitigate the growing threat of AI-powered phishing attacks."
                },
                "authors": [
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Athanasios Galiopoulos"
                    },
                    {
                        "name": "Emet Bethany"
                    },
                    {
                        "name": "Mohammad Bahrami Karkevandi"
                    },
                    {
                        "name": "Nicole Beebe"
                    },
                    {
                        "name": "Nishant Vishwamitra"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "arxiv_doi": "10.1109/ACCESS.2025.3555500",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3555500",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.09727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in IEEE Access. This version includes\n  revisions following peer review",
                "arxiv_journal_ref": "IEEE Access, 13, 60684-60701",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11289v1",
                "updated": "2025-04-15T15:29:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    29,
                    11,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:29:11Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    29,
                    11,
                    1,
                    105,
                    0
                ],
                "title": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion\n  Transformer"
                },
                "summary": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT."
                },
                "authors": [
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Longxiang Tang"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Changxin Gao"
                    },
                    {
                        "name": "Yuehuan Wang"
                    },
                    {
                        "name": "Nong Sang"
                    }
                ],
                "author_detail": {
                    "name": "Nong Sang"
                },
                "author": "Nong Sang",
                "arxiv_comment": "The training and inference code (based on Wan2.1) is available at\n  https://github.com/ali-vilab/UniAnimate-DiT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01663v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01663v6",
                "updated": "2025-04-15T15:28:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    28,
                    28,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-02T06:07:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    6,
                    7,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models"
                },
                "summary": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "JingSong Yang"
                    }
                ],
                "author_detail": {
                    "name": "JingSong Yang"
                },
                "author": "JingSong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01663v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01663v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17807v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17807v5",
                "updated": "2025-04-15T15:28:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    28,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2024-06-23T11:58:26Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    11,
                    58,
                    26,
                    6,
                    175,
                    0
                ],
                "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary"
                },
                "summary": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yiling Tao"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Sun Li Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Sun Li Tianyu Shi"
                },
                "author": "Sun Li Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17807v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17807v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11286v1",
                "updated": "2025-04-15T15:26:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    26,
                    28,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:26:28Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    26,
                    28,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Medical Image Restoration via Reliability Guided Learning in\n  Frequency Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Medical Image Restoration via Reliability Guided Learning in\n  Frequency Domain"
                },
                "summary": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Pengcheng Zheng"
                    },
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Jiaxin Huang"
                    },
                    {
                        "name": "Bohao Chen"
                    },
                    {
                        "name": "Ju Liu"
                    },
                    {
                        "name": "Yazhou Ren"
                    },
                    {
                        "name": "Xiaorong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorong Pu"
                },
                "author": "Xiaorong Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11281v1",
                "updated": "2025-04-15T15:21:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    21,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:21:09Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    21,
                    9,
                    1,
                    105,
                    0
                ],
                "title": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\n  Fine-Print Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\n  Fine-Print Injections"
                },
                "summary": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Zhiping Zhang"
                    },
                    {
                        "name": "Bingcan Guo"
                    },
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Ibrahim Khalilov"
                    },
                    {
                        "name": "Simret A Gebreegziabher"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11279v1",
                "updated": "2025-04-15T15:18:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    18,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:18:58Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    18,
                    58,
                    1,
                    105,
                    0
                ],
                "title": "Simulation-based inference for stochastic nonlinear mixed-effects models\n  with applications in systems biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference for stochastic nonlinear mixed-effects models\n  with applications in systems biology"
                },
                "summary": "The analysis of data from multiple experiments, such as observations of\nseveral individuals, is commonly approached using mixed-effects models, which\naccount for variation between individuals through hierarchical representations.\nThis makes mixed-effects models widely applied in fields such as biology,\npharmacokinetics, and sociology. In this work, we propose a novel methodology\nfor scalable Bayesian inference in hierarchical mixed-effects models. Our\nframework first constructs amortized approximations of the likelihood and the\nposterior distribution, which are then rapidly refined for each individual\ndataset, to ultimately approximate the parameters posterior across many\nindividuals. The framework is easily trainable, as it uses mixtures of experts\nbut without neural networks, leading to parsimonious yet expressive surrogate\nmodels of the likelihood and the posterior. We demonstrate the effectiveness of\nour methodology using challenging stochastic models, such as mixed-effects\nstochastic differential equations emerging in systems biology-driven problems.\nHowever, the approach is broadly applicable and can accommodate both stochastic\nand deterministic models. We show that our approach can seamlessly handle\ninference for many parameters. Additionally, we applied our method to a\nreal-data case study of mRNA transfection. When compared to exact\npseudomarginal Bayesian inference, our approach proved to be both fast and\ncompetitive in terms of statistical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of data from multiple experiments, such as observations of\nseveral individuals, is commonly approached using mixed-effects models, which\naccount for variation between individuals through hierarchical representations.\nThis makes mixed-effects models widely applied in fields such as biology,\npharmacokinetics, and sociology. In this work, we propose a novel methodology\nfor scalable Bayesian inference in hierarchical mixed-effects models. Our\nframework first constructs amortized approximations of the likelihood and the\nposterior distribution, which are then rapidly refined for each individual\ndataset, to ultimately approximate the parameters posterior across many\nindividuals. The framework is easily trainable, as it uses mixtures of experts\nbut without neural networks, leading to parsimonious yet expressive surrogate\nmodels of the likelihood and the posterior. We demonstrate the effectiveness of\nour methodology using challenging stochastic models, such as mixed-effects\nstochastic differential equations emerging in systems biology-driven problems.\nHowever, the approach is broadly applicable and can accommodate both stochastic\nand deterministic models. We show that our approach can seamlessly handle\ninference for many parameters. Additionally, we applied our method to a\nreal-data case study of mRNA transfection. When compared to exact\npseudomarginal Bayesian inference, our approach proved to be both fast and\ncompetitive in terms of statistical accuracy."
                },
                "authors": [
                    {
                        "name": "Henrik Häggström"
                    },
                    {
                        "name": "Sebastian Persson"
                    },
                    {
                        "name": "Marija Cvijovic"
                    },
                    {
                        "name": "Umberto Picchini"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Picchini"
                },
                "author": "Umberto Picchini",
                "arxiv_comment": "36 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11277v1",
                "updated": "2025-04-15T15:16:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    16,
                    45,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:16:45Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    16,
                    45,
                    1,
                    105,
                    0
                ],
                "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs"
                },
                "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance."
                },
                "authors": [
                    {
                        "name": "Guocong Li"
                    },
                    {
                        "name": "Weize Liu"
                    },
                    {
                        "name": "Yihang Wu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Shuaihan Huang"
                    },
                    {
                        "name": "Hongxia Xu"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11271v1",
                "updated": "2025-04-15T15:12:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    12,
                    57,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:12:57Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    12,
                    57,
                    1,
                    105,
                    0
                ],
                "title": "Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient\n  Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient\n  Image Super-Resolution"
                },
                "summary": "Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR."
                },
                "authors": [
                    {
                        "name": "Xinning Chai"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Zhengxue Cheng"
                    },
                    {
                        "name": "Yingsheng Qin"
                    },
                    {
                        "name": "Yucai Yang"
                    },
                    {
                        "name": "Li Song"
                    }
                ],
                "author_detail": {
                    "name": "Li Song"
                },
                "author": "Li Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11269v1",
                "updated": "2025-04-15T15:10:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    10,
                    59,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:10:59Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    10,
                    59,
                    1,
                    105,
                    0
                ],
                "title": "Minimax asymptotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimax asymptotics"
                },
                "summary": "In this paper, we consider asymptotics of the optimal value and the optimal\nsolutions of parametric minimax estimation problems. Specifically, we consider\nestimators of the optimal value and the optimal solutions in a sample minimax\nproblem that approximates the true population problem and study the limiting\ndistributions of these estimators as the sample size tends to infinity. The\nmain technical tool we employ in our analysis is the theory of sensitivity\nanalysis of parameterized mathematical optimization problems. Our results go\nwell beyond the existing literature and show that these limiting distributions\nare highly non-Gaussian in general and normal in simple specific cases. These\nresults open up the way for the development of statistical inference methods in\nparametric minimax problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider asymptotics of the optimal value and the optimal\nsolutions of parametric minimax estimation problems. Specifically, we consider\nestimators of the optimal value and the optimal solutions in a sample minimax\nproblem that approximates the true population problem and study the limiting\ndistributions of these estimators as the sample size tends to infinity. The\nmain technical tool we employ in our analysis is the theory of sensitivity\nanalysis of parameterized mathematical optimization problems. Our results go\nwell beyond the existing literature and show that these limiting distributions\nare highly non-Gaussian in general and normal in simple specific cases. These\nresults open up the way for the development of statistical inference methods in\nparametric minimax problems."
                },
                "authors": [
                    {
                        "name": "Mika Meitz"
                    },
                    {
                        "name": "Alexander Shapiro"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Shapiro"
                },
                "author": "Alexander Shapiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v3",
                "updated": "2025-04-15T15:09:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    9,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art education\n. Then we compare the answers given by DeepSeek-R1 in the seven aspects with\nthe answers given by o1-preview. DeepSeek-R1 performs well in the humanities\nand social sciences, answering most questions correctly and logically, and can\ngive reasonable analysis processes and explanations. Compared with o1-preview,\nit can automatically generate reasoning processes and provide more detailed\nexplanations, which is suitable for beginners or people who need to have a\ndetailed understanding of this knowledge, while o1-preview is more suitable for\nquick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art education\n. Then we compare the answers given by DeepSeek-R1 in the seven aspects with\nthe answers given by o1-preview. DeepSeek-R1 performs well in the humanities\nand social sciences, answering most questions correctly and logically, and can\ngive reasonable analysis processes and explanations. Compared with o1-preview,\nit can automatically generate reasoning processes and provide more detailed\nexplanations, which is suitable for beginners or people who need to have a\ndetailed understanding of this knowledge, while o1-preview is more suitable for\nquick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "52 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06778v2",
                "updated": "2025-04-15T15:09:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    9,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-09T10:58:54Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    58,
                    54,
                    2,
                    99,
                    0
                ],
                "title": "CAFA: a Controllable Automatic Foley Artist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAFA: a Controllable Automatic Foley Artist"
                },
                "summary": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations."
                },
                "authors": [
                    {
                        "name": "Roi Benita"
                    },
                    {
                        "name": "Michael Finkelson"
                    },
                    {
                        "name": "Tavi Halperin"
                    },
                    {
                        "name": "Gleb Sterkin"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "arxiv_comment": "Renamed paper to \"CAFA: a Controllable Automatic Foley Artist\" from\n  \"Controllable Automatic Foley Artist\". Updated link to demo page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10331v2",
                "updated": "2025-04-15T15:06:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    6,
                    33,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T15:39:31Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    39,
                    31,
                    0,
                    104,
                    0
                ],
                "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis"
                },
                "summary": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Fenggen Yu"
                    },
                    {
                        "name": "Huiyao Xu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10356v2",
                "updated": "2025-04-15T15:02:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    2,
                    53,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T16:05:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages"
                },
                "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."
                },
                "authors": [
                    {
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "name": "Nikolay Bogoychev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Bogoychev"
                },
                "author": "Nikolay Bogoychev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11249v1",
                "updated": "2025-04-15T14:46:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    46,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:46:25Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    46,
                    25,
                    1,
                    105,
                    0
                ],
                "title": "Cryo-em images are intrinsically low dimensional",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryo-em images are intrinsically low dimensional"
                },
                "summary": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry."
                },
                "authors": [
                    {
                        "name": "Luke Evans"
                    },
                    {
                        "name": "Octavian-Vlad Murad"
                    },
                    {
                        "name": "Lars Dingeldein"
                    },
                    {
                        "name": "Pilar Cossio"
                    },
                    {
                        "name": "Roberto Covino"
                    },
                    {
                        "name": "Marina Meila"
                    }
                ],
                "author_detail": {
                    "name": "Marina Meila"
                },
                "author": "Marina Meila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11243v1",
                "updated": "2025-04-15T14:43:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    43,
                    19,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:43:19Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    43,
                    19,
                    1,
                    105,
                    0
                ],
                "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Safety Requirements Derivation Using Agent-based RAG"
                },
                "summary": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods."
                },
                "authors": [
                    {
                        "name": "Balahari Vignesh Balu"
                    },
                    {
                        "name": "Florian Geissler"
                    },
                    {
                        "name": "Francesco Carella"
                    },
                    {
                        "name": "Joao-Vitor Zacchi"
                    },
                    {
                        "name": "Josef Jiru"
                    },
                    {
                        "name": "Nuria Mata"
                    },
                    {
                        "name": "Reinhard Stolle"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard Stolle"
                },
                "author": "Reinhard Stolle",
                "arxiv_comment": "9 pages, 3 figures",
                "arxiv_journal_ref": "Proceedings of the AAAI-make Spring Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07995v2",
                "updated": "2025-04-15T14:41:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    41,
                    45,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-08T19:16:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    16,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "SafeChat: A Framework for Building Trustworthy Collaborative Assistants\n  and a Case Study of its Usefulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeChat: A Framework for Building Trustworthy Collaborative Assistants\n  and a Case Study of its Usefulness"
                },
                "summary": "Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot."
                },
                "authors": [
                    {
                        "name": "Biplav Srivastava"
                    },
                    {
                        "name": "Kausik Lakkaraju"
                    },
                    {
                        "name": "Nitin Gupta"
                    },
                    {
                        "name": "Vansh Nagpal"
                    },
                    {
                        "name": "Bharath C. Muppasani"
                    },
                    {
                        "name": "Sara E. Jones"
                    }
                ],
                "author_detail": {
                    "name": "Sara E. Jones"
                },
                "author": "Sara E. Jones",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11239v1",
                "updated": "2025-04-15T14:40:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    29,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:40:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling\n  Reasoning Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling\n  Reasoning Benchmark for LLMs"
                },
                "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI)."
                },
                "authors": [
                    {
                        "name": "Chang Yang"
                    },
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Junzhe Jiang"
                    },
                    {
                        "name": "Qi Jiang"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Yanchen Deng"
                    },
                    {
                        "name": "Shuxin Li"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Florian T. Pokorny"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Xinrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinrun Wang"
                },
                "author": "Xinrun Wang",
                "arxiv_comment": "Preliminary work, 10 pages for main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08897v2",
                "updated": "2025-04-15T14:40:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    7,
                    1,
                    105,
                    0
                ],
                "published": "2025-01-15T16:06:10Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "title": "Automated Retrosynthesis Planning of Macromolecules Using Large Language\n  Models and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Retrosynthesis Planning of Macromolecules Using Large Language\n  Models and Knowledge Graphs"
                },
                "summary": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs. By leveraging LLMs' powerful capabilities for extracting and\nrecognizing chemical substance names, and storing the extracted data in a\nstructured knowledge graph, our system fully automates the retrieval of\nrelevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. By considering the complex interdependencies among chemical\nreactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is\nproposed to help identify all valid multi-branched reaction pathways, which\narise when a single product decomposes into multiple reaction intermediates. In\ncontrast, previous studies were limited to cases where a product decomposes\ninto at most one reaction intermediate. This work represents the first attempt\nto develop a fully automated retrosynthesis planning agent tailored specially\nfor macromolecules powered by LLMs. Applied to polyimide synthesis, our new\napproach constructs a retrosynthetic pathway tree with hundreds of pathways and\nrecommends optimized routes, including both known and novel pathways. This\ndemonstrates utilizing LLMs for literature consultation to accomplish specific\ntasks is possible and crucial for future materials research, given the vast\namount of materials-related literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs. By leveraging LLMs' powerful capabilities for extracting and\nrecognizing chemical substance names, and storing the extracted data in a\nstructured knowledge graph, our system fully automates the retrieval of\nrelevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. By considering the complex interdependencies among chemical\nreactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is\nproposed to help identify all valid multi-branched reaction pathways, which\narise when a single product decomposes into multiple reaction intermediates. In\ncontrast, previous studies were limited to cases where a product decomposes\ninto at most one reaction intermediate. This work represents the first attempt\nto develop a fully automated retrosynthesis planning agent tailored specially\nfor macromolecules powered by LLMs. Applied to polyimide synthesis, our new\napproach constructs a retrosynthetic pathway tree with hundreds of pathways and\nrecommends optimized routes, including both known and novel pathways. This\ndemonstrates utilizing LLMs for literature consultation to accomplish specific\ntasks is possible and crucial for future materials research, given the vast\namount of materials-related literature."
                },
                "authors": [
                    {
                        "name": "Qinyu Ma"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_doi": "10.1002/marc.202500065",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/marc.202500065",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.08897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The source code of RetroSynthesisAgent is available at\n  https://github.com/QinyuMa316/RetroSynthesisAgent",
                "arxiv_journal_ref": "Macromol. Rapid Commun. 2025, 2500065",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07355v3",
                "updated": "2025-04-15T14:38:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    38,
                    13,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-10T09:48:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models"
                },
                "summary": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology."
                },
                "authors": [
                    {
                        "name": "Andrea Caria"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caria"
                },
                "author": "Andrea Caria",
                "arxiv_comment": "needs major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04917v2",
                "updated": "2025-04-15T14:36:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    14,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-07T11:07:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    7,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "Why am I seeing this: Democratizing End User Auditing for Online Content\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why am I seeing this: Democratizing End User Auditing for Online Content\n  Recommendations"
                },
                "summary": "Personalized recommendation systems tailor content based on user attributes,\nwhich are either provided or inferred from private data. Research suggests that\nusers often hypothesize about reasons behind contents they encounter (e.g., \"I\nsee this jewelry ad because I am a woman\"), but they lack the means to confirm\nthese hypotheses due to the opaqueness of these systems. This hinders informed\ndecision-making about privacy and system use and contributes to the lack of\nalgorithmic accountability. To address these challenges, we introduce a new\ninteractive sandbox approach. This approach creates sets of synthetic user\npersonas and corresponding personal data that embody realistic variations in\npersonal attributes, allowing users to test their hypotheses by observing how a\nwebsite's algorithms respond to these personas. We tested the sandbox in the\ncontext of targeted advertisement. Our user study demonstrates its usability,\nusefulness, and effectiveness in empowering end-user auditing in a case study\nof targeting ads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized recommendation systems tailor content based on user attributes,\nwhich are either provided or inferred from private data. Research suggests that\nusers often hypothesize about reasons behind contents they encounter (e.g., \"I\nsee this jewelry ad because I am a woman\"), but they lack the means to confirm\nthese hypotheses due to the opaqueness of these systems. This hinders informed\ndecision-making about privacy and system use and contributes to the lack of\nalgorithmic accountability. To address these challenges, we introduce a new\ninteractive sandbox approach. This approach creates sets of synthetic user\npersonas and corresponding personal data that embody realistic variations in\npersonal attributes, allowing users to test their hypotheses by observing how a\nwebsite's algorithms respond to these personas. We tested the sandbox in the\ncontext of targeted advertisement. Our user study demonstrates its usability,\nusefulness, and effectiveness in empowering end-user auditing in a case study\nof targeting ads."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Leyang Li"
                    },
                    {
                        "name": "Luke Cao"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Toby Jia-jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-jun Li"
                },
                "author": "Toby Jia-jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11233v1",
                "updated": "2025-04-15T14:36:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:36:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "AutoRAN: Automated and Zero-Touch Open RAN Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRAN: Automated and Zero-Touch Open RAN Systems"
                },
                "summary": "[...] This paper presents AutoRAN, an automated, intent-driven framework for\nzero-touch provisioning of open, programmable cellular networks. Leveraging\ncloud-native principles, AutoRAN employs virtualization, declarative\ninfrastructure-as-code templates, and disaggregated micro-services to abstract\nphysical resources and protocol stacks. Its orchestration engine integrates\nLanguage Models (LLMs) to translate high-level intents into machine-readable\nconfigurations, enabling closed-loop control via telemetry-driven\nobservability. Implemented on a multi-architecture OpenShift cluster with\nheterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access\nNetwork (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of\nO-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS\ncore, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines.\nExperimental results demonstrate that AutoRAN is capable of deploying an\nend-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput,\nvalidating its ability to streamline configuration, accelerate testing, and\nreduce manual intervention with similar performance than non cloud-based\nimplementations. With its novel LLM-assisted intent translation mechanism, and\nperformance-optimized automation workflow for multi-vendor environments,\nAutoRAN has the potential of advancing the robustness of next-generation\ncellular supply chains through reproducible, intent-based provisioning across\npublic and private deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[...] This paper presents AutoRAN, an automated, intent-driven framework for\nzero-touch provisioning of open, programmable cellular networks. Leveraging\ncloud-native principles, AutoRAN employs virtualization, declarative\ninfrastructure-as-code templates, and disaggregated micro-services to abstract\nphysical resources and protocol stacks. Its orchestration engine integrates\nLanguage Models (LLMs) to translate high-level intents into machine-readable\nconfigurations, enabling closed-loop control via telemetry-driven\nobservability. Implemented on a multi-architecture OpenShift cluster with\nheterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access\nNetwork (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of\nO-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS\ncore, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines.\nExperimental results demonstrate that AutoRAN is capable of deploying an\nend-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput,\nvalidating its ability to streamline configuration, accelerate testing, and\nreduce manual intervention with similar performance than non cloud-based\nimplementations. With its novel LLM-assisted intent translation mechanism, and\nperformance-optimized automation workflow for multi-vendor environments,\nAutoRAN has the potential of advancing the robustness of next-generation\ncellular supply chains through reproducible, intent-based provisioning across\npublic and private deployments."
                },
                "authors": [
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Ravis Shirkhani"
                    },
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Leonardo Bonati"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Michele Polese"
                    }
                ],
                "author_detail": {
                    "name": "Michele Polese"
                },
                "author": "Michele Polese",
                "arxiv_comment": "17 pages, 15 figures, 6 listings, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03624v3",
                "updated": "2025-04-15T14:36:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    1,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-04T17:41:58Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    41,
                    58,
                    4,
                    94,
                    0
                ],
                "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models"
                },
                "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo."
                },
                "authors": [
                    {
                        "name": "NVIDIA"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Aaron Blakeman"
                    },
                    {
                        "name": "Aarti Basant"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Adithya Renduchintala"
                    },
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Alexis Bjorlin"
                    },
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Anna Shors"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Ashwin Poojary"
                    },
                    {
                        "name": "Ayush Dattagupta"
                    },
                    {
                        "name": "Balaram Buddharaju"
                    },
                    {
                        "name": "Bobby Chen"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Brian Butterfield"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Carlo del Mundo"
                    },
                    {
                        "name": "Chengyu Dong"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Christopher Parisien"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Danny Yin"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Ding Ma"
                    },
                    {
                        "name": "Dmytro Pykhtar"
                    },
                    {
                        "name": "Dong Ahn"
                    },
                    {
                        "name": "Duncan Riach"
                    },
                    {
                        "name": "Dusan Stosic"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Eric Chung"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Ewa Dobrowolska"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Gargi Prasad"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Guilin Liu"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Helen Ngo"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "Jared Casper"
                    },
                    {
                        "name": "Jarno Seppanen"
                    },
                    {
                        "name": "Jason Lu"
                    },
                    {
                        "name": "Jason Sewall"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Jinze Xue"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Jon Barker"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Kirthi Sivamani"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Kumar Anik"
                    },
                    {
                        "name": "Kunlun Li"
                    },
                    {
                        "name": "Lawrence McAfee"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Lindsey Pavao"
                    },
                    {
                        "name": "Luis Vega"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Maciej Bala"
                    },
                    {
                        "name": "Maer Rodrigues de Melo"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Marta Stepniewska-Dziubinska"
                    },
                    {
                        "name": "Matthieu Le"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Michael Andersch"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Miguel Martinez"
                    },
                    {
                        "name": "Mike Chrzanowski"
                    },
                    {
                        "name": "Mike Ranzinger"
                    },
                    {
                        "name": "Mikolaj Blaz"
                    },
                    {
                        "name": "Misha Smelyanskiy"
                    },
                    {
                        "name": "Mohamed Fawzy"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Oleg Rybakov"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Osvald Nitski"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Pasha Shamis"
                    },
                    {
                        "name": "Paulius Micikevicius"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Peter Dykas"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Pierre-Yves Aquilanti"
                    },
                    {
                        "name": "Piotr Bialecki"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Przemek Tredak"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Rahul Kandu"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Ruoxi Zhang"
                    },
                    {
                        "name": "Sabrina Kavanaugh"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Samuel Kriman"
                    },
                    {
                        "name": "Sangkug Lym"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Selvaraj Anandaraj"
                    },
                    {
                        "name": "Seonmyeong Bak"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Sharon Clay"
                    },
                    {
                        "name": "Shelby Thomas"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Shubham Pachori"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Sirshak Das"
                    },
                    {
                        "name": "Slawek Kierat"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Sriharsha Niverty"
                    },
                    {
                        "name": "Stefania Alborghetti"
                    },
                    {
                        "name": "Suseella Panguluri"
                    },
                    {
                        "name": "Swetha Bhendigeri"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Szymon Migacz"
                    },
                    {
                        "name": "Tal Shiri"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Tyler Poon"
                    },
                    {
                        "name": "Ushnish De"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Varun Singh"
                    },
                    {
                        "name": "Vijay Korthikanti"
                    },
                    {
                        "name": "Vitaly Kurin"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Wenliang Dai"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Xiaowei Ren"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Zhongbo Zhu"
                    },
                    {
                        "name": "Zhuolin Yang"
                    },
                    {
                        "name": "Zijia Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zijia Chen"
                },
                "author": "Zijia Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19000v3",
                "updated": "2025-04-15T14:35:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    35,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2024-11-28T09:04:39Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    9,
                    4,
                    39,
                    3,
                    333,
                    0
                ],
                "title": "An AI-driven multimodal smart home platform for continuous monitoring\n  and intelligent assistance in post-stroke patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI-driven multimodal smart home platform for continuous monitoring\n  and intelligent assistance in post-stroke patients"
                },
                "summary": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Ruizhi Zhang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Zihe Zhao"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Junliang Chen"
                    },
                    {
                        "name": "Yanning Dai"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Ruoyu Juan"
                    },
                    {
                        "name": "Qiaoying Li"
                    },
                    {
                        "name": "Ruimou Xie"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Xinkai Zhou"
                    },
                    {
                        "name": "Yunjia Xia"
                    },
                    {
                        "name": "Jianan Chen"
                    },
                    {
                        "name": "Fanghao Lu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ninglli Wang"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Hubin Zhao"
                    },
                    {
                        "name": "Luigi G. Occhipinti"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipinti"
                },
                "author": "Luigi G. Occhipinti",
                "arxiv_comment": "5 figures, 41 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03369v2",
                "updated": "2025-04-15T14:29:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    29,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2024-06-05T15:24:20Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    15,
                    24,
                    20,
                    2,
                    157,
                    0
                ],
                "title": "Posterior and variational inference for deep neural networks with\n  heavy-tailed weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior and variational inference for deep neural networks with\n  heavy-tailed weights"
                },
                "summary": "We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support."
                },
                "authors": [
                    {
                        "name": "Ismaël Castillo"
                    },
                    {
                        "name": "Paul Egels"
                    }
                ],
                "author_detail": {
                    "name": "Paul Egels"
                },
                "author": "Paul Egels",
                "arxiv_comment": "58 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11227v1",
                "updated": "2025-04-15T14:28:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    28,
                    48,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:28:48Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    28,
                    48,
                    1,
                    105,
                    0
                ],
                "title": "VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax\n  Computation in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax\n  Computation in Transformers"
                },
                "summary": "While Transformers are dominated by Floating-Point (FP)\nMatrix-Multiplications, their aggressive acceleration through dedicated\nhardware or many-core programmable systems has shifted the performance\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\nthe most demanding step. To address this, we design a custom arithmetic block\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\nthe RISC-V cores of a compute cluster, through custom Instruction Set\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\%. By\noptimizing the software kernels to leverage the extension, we execute Softmax\nwith 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the\nbaseline cluster, achieving an 8.2$\\times$ performance improvement and\n4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\nto efficiently execute end-to-end inference of pre-trained Transformer models,\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$\nreduction in latency and energy consumption, respectively, without requiring\nre-training and with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers are dominated by Floating-Point (FP)\nMatrix-Multiplications, their aggressive acceleration through dedicated\nhardware or many-core programmable systems has shifted the performance\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\nthe most demanding step. To address this, we design a custom arithmetic block\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\nthe RISC-V cores of a compute cluster, through custom Instruction Set\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\%. By\noptimizing the software kernels to leverage the extension, we execute Softmax\nwith 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the\nbaseline cluster, achieving an 8.2$\\times$ performance improvement and\n4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\nto efficiently execute end-to-end inference of pre-trained Transformer models,\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$\nreduction in latency and energy consumption, respectively, without requiring\nre-training and with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Gamze Islamoglu"
                    },
                    {
                        "name": "Andrea Belano"
                    },
                    {
                        "name": "Viviane Potocnik"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04383v2",
                "updated": "2025-04-15T14:07:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    7,
                    31,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-06T06:23:27Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    6,
                    23,
                    27,
                    6,
                    96,
                    0
                ],
                "title": "Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning"
                },
                "summary": "Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models."
                },
                "authors": [
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "David Acuna"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Jaehun Jung"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Code and data will be publicly released upon internal approval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20502v3",
                "updated": "2025-04-15T14:06:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    6,
                    28,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-27T16:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    16,
                    28,
                    28,
                    6,
                    301,
                    0
                ],
                "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation"
                },
                "summary": "Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon."
                },
                "authors": [
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Jeongsoo Choi"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Hefei Ling"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Accepted at ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v3",
                "updated": "2025-04-15T14:06:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    6,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09983v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09983v3",
                "updated": "2025-04-15T14:03:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    3,
                    0,
                    1,
                    105,
                    0
                ],
                "published": "2024-06-14T12:46:35Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    12,
                    46,
                    35,
                    4,
                    166,
                    0
                ],
                "title": "Epidemic-induced local awareness behavior inferred from surveys and\n  genetic sequence data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epidemic-induced local awareness behavior inferred from surveys and\n  genetic sequence data"
                },
                "summary": "Behavior-disease models suggest that pandemics can be contained\ncost-effectively if individuals take preventive actions when disease prevalence\nrises among their close contacts. However, assessing local awareness behavior\nin real-world datasets remains a challenge. Through the analysis of mutation\npatterns in clinical genetic sequence data, we propose an efficient approach to\nquantify the impact of local awareness by identifying superspreading events and\nassigning containment scores to them.\n  We validate the proposed containment score as a proxy for local awareness in\nsimulation experiments, and find that it was correlated positively with policy\nstringency during the COVID-19 pandemic. Finally, we observe a temporary drop\nin the containment score during the Omicron wave in the United Kingdom,\nmatching a survey experiment we carried out in Hungary during the corresponding\nperiod of the pandemic. Our findings bring important insight into the field of\nawareness modeling through the analysis of large-scale genetic sequence data,\none of the most promising data sources in epidemics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior-disease models suggest that pandemics can be contained\ncost-effectively if individuals take preventive actions when disease prevalence\nrises among their close contacts. However, assessing local awareness behavior\nin real-world datasets remains a challenge. Through the analysis of mutation\npatterns in clinical genetic sequence data, we propose an efficient approach to\nquantify the impact of local awareness by identifying superspreading events and\nassigning containment scores to them.\n  We validate the proposed containment score as a proxy for local awareness in\nsimulation experiments, and find that it was correlated positively with policy\nstringency during the COVID-19 pandemic. Finally, we observe a temporary drop\nin the containment score during the Omicron wave in the United Kingdom,\nmatching a survey experiment we carried out in Hungary during the corresponding\nperiod of the pandemic. Our findings bring important insight into the field of\nawareness modeling through the analysis of large-scale genetic sequence data,\none of the most promising data sources in epidemics research."
                },
                "authors": [
                    {
                        "name": "Gergely Ódor"
                    },
                    {
                        "name": "Márton Karsai"
                    }
                ],
                "author_detail": {
                    "name": "Márton Karsai"
                },
                "author": "Márton Karsai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09983v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09983v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07772v3",
                "updated": "2025-04-15T13:57:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    57,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-01-14T01:07:30Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    1,
                    7,
                    30,
                    1,
                    14,
                    0
                ],
                "title": "Bridging Root-$n$ and Non-standard Asymptotics: Adaptive Inference in\n  M-Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Root-$n$ and Non-standard Asymptotics: Adaptive Inference in\n  M-Estimation"
                },
                "summary": "This manuscript studies a general approach to construct confidence sets for\nthe solution of population-level optimization, commonly referred to as\nM-estimation. Statistical inference for M-estimation poses significant\nchallenges due to the non-standard limiting behaviors of the corresponding\nestimator, which arise in settings with increasing dimension of parameters,\nnon-smooth objectives, or constraints. We propose a simple and unified method\nthat guarantees validity in both regular and irregular cases. Moreover, we\nprovide a comprehensive width analysis of the proposed confidence set, showing\nthat the convergence rate of the diameter is adaptive to the unknown degree of\ninstance-specific regularity. We apply the proposed method to several\nhigh-dimensional and irregular statistical problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript studies a general approach to construct confidence sets for\nthe solution of population-level optimization, commonly referred to as\nM-estimation. Statistical inference for M-estimation poses significant\nchallenges due to the non-standard limiting behaviors of the corresponding\nestimator, which arise in settings with increasing dimension of parameters,\nnon-smooth objectives, or constraints. We propose a simple and unified method\nthat guarantees validity in both regular and irregular cases. Moreover, we\nprovide a comprehensive width analysis of the proposed confidence set, showing\nthat the convergence rate of the diameter is adaptive to the unknown degree of\ninstance-specific regularity. We apply the proposed method to several\nhigh-dimensional and irregular statistical problems."
                },
                "authors": [
                    {
                        "name": "Kenta Takatsu"
                    },
                    {
                        "name": "Arun Kumar Kuchibhotla"
                    }
                ],
                "author_detail": {
                    "name": "Arun Kumar Kuchibhotla"
                },
                "author": "Arun Kumar Kuchibhotla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11199v1",
                "updated": "2025-04-15T13:56:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    56,
                    14,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:56:14Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    56,
                    14,
                    1,
                    105,
                    0
                ],
                "title": "Video Summarization with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Summarization with Large Language Models"
                },
                "summary": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content."
                },
                "authors": [
                    {
                        "name": "Min Jung Lee"
                    },
                    {
                        "name": "Dayoung Gong"
                    },
                    {
                        "name": "Minsu Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsu Cho"
                },
                "author": "Minsu Cho",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11197v2",
                "updated": "2025-04-16T03:32:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    32,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    53,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance"
                },
                "summary": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead."
                },
                "authors": [
                    {
                        "name": "Shangyu Liu"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Xiaoyao Huang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Jie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Wu"
                },
                "author": "Jie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11195v1",
                "updated": "2025-04-15T13:49:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    49,
                    31,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:49:31Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    49,
                    31,
                    1,
                    105,
                    0
                ],
                "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models\n  through Test-Time Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-TPT: Improving Adversarial Robustness of Vision-Language Models\n  through Test-Time Prompt Tuning"
                },
                "summary": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT."
                },
                "authors": [
                    {
                        "name": "Lijun Sheng"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Zilei Wang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11188v1",
                "updated": "2025-04-15T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    47,
                    35,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    47,
                    35,
                    1,
                    105,
                    0
                ],
                "title": "Clinically Interpretable Survival Risk Stratification in Head and Neck\n  Cancer Using Bayesian Networks and Markov Blankets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinically Interpretable Survival Risk Stratification in Head and Neck\n  Cancer Using Bayesian Networks and Markov Blankets"
                },
                "summary": "Purpose: To identify a clinically interpretable subset of survival-relevant\nfeatures in HN cancer using Bayesian Network (BN) and evaluate its prognostic\nand causal utility. Methods and Materials: We used the RADCURE dataset,\nconsisting of 3,346 patients with H&N cancer treated with definitive\n(chemo)radiotherapy. A probabilistic BN was constructed to model dependencies\namong clinical, anatomical, and treatment variables. The Markov Blanket (MB) of\ntwo-year survival (SVy2) was extracted and used to train a logistic regression\nmodel. After excluding incomplete cases, a temporal split yielded a train/test\n(2,174/820) dataset using 2007 as the cutoff year. Model performance was\nassessed using area under the ROC curve (AUC), C-index, and Kaplan-Meier (KM)\nsurvival stratification. Model fit was further evaluated using a log-likelihood\nratio (LLR) test. Causal inference was performed using do-calculus\ninterventions on MB variables. Results: The MB of SVy2 included 6 clinically\nrelevant features: ECOG performance status, T-stage, HPV status, disease site,\nthe primary gross tumor volume (GTVp), and treatment modality. The model\nachieved an AUC of 0.65 and C-index of 0.78 on the test dataset, significantly\nstratifying patients into high- and low-risk groups (log-rank p < 0.01). Model\nfit was further supported by a log-likelihood ratio of 70.32 (p < 0.01).\nSubgroup analyses revealed strong performance in HPV-negative (AUC = 0.69,\nC-index = 0.76), T4 (AUC = 0.69, C-index = 0.80), and large-GTV (AUC = 0.67,\nC-index = 0.75) cohorts, each showing significant KM separation. Causal\nanalysis further supported the positive survival impact of ECOG 0, HPV-positive\nstatus, and chemoradiation. Conclusions: A compact, MB-derived BN model can\nrobustly stratify survival risk in HN cancer. The model enables explainable\nprognostication and supports individualized decision-making across key clinical\nsubgroups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To identify a clinically interpretable subset of survival-relevant\nfeatures in HN cancer using Bayesian Network (BN) and evaluate its prognostic\nand causal utility. Methods and Materials: We used the RADCURE dataset,\nconsisting of 3,346 patients with H&N cancer treated with definitive\n(chemo)radiotherapy. A probabilistic BN was constructed to model dependencies\namong clinical, anatomical, and treatment variables. The Markov Blanket (MB) of\ntwo-year survival (SVy2) was extracted and used to train a logistic regression\nmodel. After excluding incomplete cases, a temporal split yielded a train/test\n(2,174/820) dataset using 2007 as the cutoff year. Model performance was\nassessed using area under the ROC curve (AUC), C-index, and Kaplan-Meier (KM)\nsurvival stratification. Model fit was further evaluated using a log-likelihood\nratio (LLR) test. Causal inference was performed using do-calculus\ninterventions on MB variables. Results: The MB of SVy2 included 6 clinically\nrelevant features: ECOG performance status, T-stage, HPV status, disease site,\nthe primary gross tumor volume (GTVp), and treatment modality. The model\nachieved an AUC of 0.65 and C-index of 0.78 on the test dataset, significantly\nstratifying patients into high- and low-risk groups (log-rank p < 0.01). Model\nfit was further supported by a log-likelihood ratio of 70.32 (p < 0.01).\nSubgroup analyses revealed strong performance in HPV-negative (AUC = 0.69,\nC-index = 0.76), T4 (AUC = 0.69, C-index = 0.80), and large-GTV (AUC = 0.67,\nC-index = 0.75) cohorts, each showing significant KM separation. Causal\nanalysis further supported the positive survival impact of ECOG 0, HPV-positive\nstatus, and chemoradiation. Conclusions: A compact, MB-derived BN model can\nrobustly stratify survival risk in HN cancer. The model enables explainable\nprognostication and supports individualized decision-making across key clinical\nsubgroups."
                },
                "authors": [
                    {
                        "name": "Keyur D. Shah"
                    },
                    {
                        "name": "Ibrahim Chamseddine"
                    },
                    {
                        "name": "Xiaohan Yuan"
                    },
                    {
                        "name": "Sibo Tian"
                    },
                    {
                        "name": "Richard Qiu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Anees Dhabaan"
                    },
                    {
                        "name": "Hania Al-Hallaq"
                    },
                    {
                        "name": "David S. Yu"
                    },
                    {
                        "name": "Harald Paganetti"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "arxiv_comment": "24 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11186v1",
                "updated": "2025-04-15T13:42:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    42,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:42:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    42,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in\n  Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in\n  Ophthalmology: A Head-to-Head Evaluation on 5,888 Items"
                },
                "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications."
                },
                "authors": [
                    {
                        "name": "Minjie Zou"
                    },
                    {
                        "name": "Sahana Srinivasan"
                    },
                    {
                        "name": "Thaddaeus Wai Soon Lo"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Gabriel Dawei Yang"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Maxwell Singer"
                    },
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "Kelvin Li"
                    },
                    {
                        "name": "Robert Chang"
                    },
                    {
                        "name": "Marcus Tan"
                    },
                    {
                        "name": "David Ziyou Chen"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih Chung Tham"
                    }
                ],
                "author_detail": {
                    "name": "Yih Chung Tham"
                },
                "author": "Yih Chung Tham",
                "arxiv_comment": "83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v5",
                "updated": "2025-04-15T13:38:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    38,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11182v1",
                "updated": "2025-04-15T13:37:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    37,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:37:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    37,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations"
                },
                "summary": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner."
                },
                "authors": [
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06543v2",
                "updated": "2025-04-15T13:36:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    36,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-09T14:54:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Challenges and Opportunities for Visual Analytics in Jurisprudence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Opportunities for Visual Analytics in Jurisprudence"
                },
                "summary": "Legal exploration, analysis, and interpretation remain complex and demanding\ntasks, even for experienced legal scholars, due to the domain-specific\nlanguage, tacit legal concepts, and intentional ambiguities embedded in legal\ntexts. In related, text-based domains, Visual Analytics (VA) and Large Language\nModels (LLMs) have become indispensable tools for navigating documents,\nrepresenting knowledge, and supporting analytical reasoning. However, legal\nscholarship presents distinct challenges: it requires managing formal legal\nstructure, drawing on tacit domain knowledge, and documenting intricate and\naccurate reasoning processes - needs that current VA systems designs and LLMs\nfail to address adequately. We identify previously unexamined key challenges\nand underexplored opportunities in applying VA to jurisprudence to explore how\nthese technologies might better serve the legal domain. Based on\nsemi-structured interviews with nine legal experts, we find a significant gap\nin tools and means that can externalize tacit legal knowledge in a form that is\nboth explicit and machine-interpretable. Hence, we propose leveraging\ninteractive visualization for this articulation, teaching the machine relevant\nsemantic relationships between legal documents that inform the predictions of\nLLMs, facilitating the enhanced navigation between hierarchies of legal\ncollections. This work introduces a user-centered VA workflow to the\njurisprudential context, recognizing tacit legal knowledge and expert\nexperience as vital components in deriving legal insight, comparing it with\nestablished practices in other text-based domains, and outlining a research\nagenda that offers future guidance for researchers in Visual Analytics for law\nand beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal exploration, analysis, and interpretation remain complex and demanding\ntasks, even for experienced legal scholars, due to the domain-specific\nlanguage, tacit legal concepts, and intentional ambiguities embedded in legal\ntexts. In related, text-based domains, Visual Analytics (VA) and Large Language\nModels (LLMs) have become indispensable tools for navigating documents,\nrepresenting knowledge, and supporting analytical reasoning. However, legal\nscholarship presents distinct challenges: it requires managing formal legal\nstructure, drawing on tacit domain knowledge, and documenting intricate and\naccurate reasoning processes - needs that current VA systems designs and LLMs\nfail to address adequately. We identify previously unexamined key challenges\nand underexplored opportunities in applying VA to jurisprudence to explore how\nthese technologies might better serve the legal domain. Based on\nsemi-structured interviews with nine legal experts, we find a significant gap\nin tools and means that can externalize tacit legal knowledge in a form that is\nboth explicit and machine-interpretable. Hence, we propose leveraging\ninteractive visualization for this articulation, teaching the machine relevant\nsemantic relationships between legal documents that inform the predictions of\nLLMs, facilitating the enhanced navigation between hierarchies of legal\ncollections. This work introduces a user-centered VA workflow to the\njurisprudential context, recognizing tacit legal knowledge and expert\nexperience as vital components in deriving legal insight, comparing it with\nestablished practices in other text-based domains, and outlining a research\nagenda that offers future guidance for researchers in Visual Analytics for law\nand beyond."
                },
                "authors": [
                    {
                        "name": "Daniel Fürst"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "39 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10342v2",
                "updated": "2025-04-15T13:34:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    34,
                    53,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T15:50:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    50,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge"
                },
                "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yibo Kong"
                    },
                    {
                        "name": "Zecheng Li"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "arxiv_comment": "56 pages, 43 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11174v1",
                "updated": "2025-04-15T13:27:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    27,
                    57,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:27:57Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    27,
                    57,
                    1,
                    105,
                    0
                ],
                "title": "Algorithmic thresholds in combinatorial optimization depend on the time\n  scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic thresholds in combinatorial optimization depend on the time\n  scaling"
                },
                "summary": "In the last decades, many efforts have focused on analyzing typical-case\nhardness in optimization and inference problems. Some recent work has pointed\nout that polynomial algorithms exist, running with a time that grows more than\nlinearly with the system size, which can do better than linear algorithms,\nfinding solutions to random problems in a wider range of parameters. However, a\ntheory for polynomial and superlinear algorithms is in general lacking. In this\npaper, we examine the performance of the Simulated Annealing algorithm, a\nstandard, versatile, and robust choice for solving optimization and inference\nproblems, in the prototypical random $K$-Sat problem. For the first time, we\nshow that the algorithmic thresholds depend on the time scaling of the\nalgorithm with the size of the system. Indeed, one can identify not just one,\nbut different thresholds for linear, quadratic, cubic regimes (and so on). This\nobservation opens new directions in studying the typical case hardness in\noptimization problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last decades, many efforts have focused on analyzing typical-case\nhardness in optimization and inference problems. Some recent work has pointed\nout that polynomial algorithms exist, running with a time that grows more than\nlinearly with the system size, which can do better than linear algorithms,\nfinding solutions to random problems in a wider range of parameters. However, a\ntheory for polynomial and superlinear algorithms is in general lacking. In this\npaper, we examine the performance of the Simulated Annealing algorithm, a\nstandard, versatile, and robust choice for solving optimization and inference\nproblems, in the prototypical random $K$-Sat problem. For the first time, we\nshow that the algorithmic thresholds depend on the time scaling of the\nalgorithm with the size of the system. Indeed, one can identify not just one,\nbut different thresholds for linear, quadratic, cubic regimes (and so on). This\nobservation opens new directions in studying the typical case hardness in\noptimization problems."
                },
                "authors": [
                    {
                        "name": "M. C. Angelini"
                    },
                    {
                        "name": "M. Avila-González"
                    },
                    {
                        "name": "F. D'Amico"
                    },
                    {
                        "name": "D. Machado"
                    },
                    {
                        "name": "R. Mulet"
                    },
                    {
                        "name": "F. Ricci-Tersenghi"
                    }
                ],
                "author_detail": {
                    "name": "F. Ricci-Tersenghi"
                },
                "author": "F. Ricci-Tersenghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06372v2",
                "updated": "2025-04-15T13:25:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    25,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-08T18:44:33Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    18,
                    44,
                    33,
                    1,
                    98,
                    0
                ],
                "title": "A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior"
                },
                "summary": "Inference and estimation are fundamental aspects of statistics, system\nidentification and machine learning. For most inference problems, prior\nknowledge is available on the system to be modeled, and Bayesian analysis is a\nnatural framework to impose such prior information in the form of a prior\ndistribution. However, in many situations, coming out with a fully specified\nprior distribution is not easy, as prior knowledge might be too vague, so\npractitioners prefer to use a prior distribution that is as `ignorant' or\n`uninformative' as possible, in the sense of not imposing subjective beliefs,\nwhile still supporting reliable statistical analysis. Jeffreys prior is an\nappealing uninformative prior because it offers two important benefits: (i) it\nis invariant under any re-parameterization of the model, (ii) it encodes the\nintrinsic geometric structure of the parameter space through the Fisher\ninformation matrix, which in turn enhances the diversity of parameter samples.\nDespite these benefits, drawing samples from Jeffreys prior is a challenging\ntask. In this paper, we propose a general sampling scheme using the\nMetropolis-Adjusted Langevin Algorithm that enables sampling of parameter\nvalues from Jeffreys prior, and provide numerical illustrations of our approach\nthrough several examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and estimation are fundamental aspects of statistics, system\nidentification and machine learning. For most inference problems, prior\nknowledge is available on the system to be modeled, and Bayesian analysis is a\nnatural framework to impose such prior information in the form of a prior\ndistribution. However, in many situations, coming out with a fully specified\nprior distribution is not easy, as prior knowledge might be too vague, so\npractitioners prefer to use a prior distribution that is as `ignorant' or\n`uninformative' as possible, in the sense of not imposing subjective beliefs,\nwhile still supporting reliable statistical analysis. Jeffreys prior is an\nappealing uninformative prior because it offers two important benefits: (i) it\nis invariant under any re-parameterization of the model, (ii) it encodes the\nintrinsic geometric structure of the parameter space through the Fisher\ninformation matrix, which in turn enhances the diversity of parameter samples.\nDespite these benefits, drawing samples from Jeffreys prior is a challenging\ntask. In this paper, we propose a general sampling scheme using the\nMetropolis-Adjusted Langevin Algorithm that enables sampling of parameter\nvalues from Jeffreys prior, and provide numerical illustrations of our approach\nthrough several examples."
                },
                "authors": [
                    {
                        "name": "Yibo Shi"
                    },
                    {
                        "name": "Braghadeesh Lakshminarayanan"
                    },
                    {
                        "name": "Cristian R. Rojas"
                    }
                ],
                "author_detail": {
                    "name": "Cristian R. Rojas"
                },
                "author": "Cristian R. Rojas",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06137v2",
                "updated": "2025-04-15T13:20:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    20,
                    51,
                    1,
                    105,
                    0
                ],
                "published": "2023-09-12T11:21:54Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    11,
                    21,
                    54,
                    1,
                    255,
                    0
                ],
                "title": "Gaia's brightest very metal-poor (VMP) stars. Metallicity catalogue of a\n  thousand VMP stars from Gaia's radial velocity spectrometer spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaia's brightest very metal-poor (VMP) stars. Metallicity catalogue of a\n  thousand VMP stars from Gaia's radial velocity spectrometer spectra"
                },
                "summary": "Context. Gaia DR3 has offered the scientific community a remarkable dataset\nof approximately one million spectra acquired with the Radial Velocity\nSpectrometer (RVS) in the Calcium II triplet region, that is well-suited to\nidentify very metal-poor (VMP) stars. However, over 40% of these spectra have\nno released parameters by Gaia's GSP Spec pipeline in the domain of VMP stars,\nwhereas VMP stars are key tracers of early Galactic evolution. Aims. We aim to\nprovide spectroscopic metallicities for VMP stars using Gaia RVS spectra,\nthereby producing a catalogue of bright VMP stars distributed over the full sky\nthat can serve as the basis to study early chemical evolution throughout the\nGalaxy. Methods. We select VMP stars using photometric metallicities from the\nliterature and analyse the Gaia RVS spectra to infer spectroscopic\nmetallicities for these stars. Results. The inferred metallicities agree very\nwell with literature high-resolution metallicities with a median systematic\noffset of 0.1 dex and standard deviation of $\\sim$0.15 dex. The purity of this\nsample in the VMP regime is $\\sim$80% with outliers representing a mere\n$\\sim$3%. Conclusions. We make available an all-sky catalogue of $\\sim$1500\nstars with reliable spectroscopic metallicities down to [Fe/H]$\\sim$-4.0, of\nwhich $\\sim$1000 are VMP stars. More than 75% of these stars have either no\nmetallicity value in the literature to date or are flagged to be unreliable in\ntheir literature metallicity estimates. This catalogue of bright (G<13) VMP\nstars is three times larger than the current sample of well-studied VMP stars\nin the literature in this magnitude range, making it ideal for high-resolution\nspectroscopic follow-up and to study the properties of VMP stars in different\nparts of our Galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Gaia DR3 has offered the scientific community a remarkable dataset\nof approximately one million spectra acquired with the Radial Velocity\nSpectrometer (RVS) in the Calcium II triplet region, that is well-suited to\nidentify very metal-poor (VMP) stars. However, over 40% of these spectra have\nno released parameters by Gaia's GSP Spec pipeline in the domain of VMP stars,\nwhereas VMP stars are key tracers of early Galactic evolution. Aims. We aim to\nprovide spectroscopic metallicities for VMP stars using Gaia RVS spectra,\nthereby producing a catalogue of bright VMP stars distributed over the full sky\nthat can serve as the basis to study early chemical evolution throughout the\nGalaxy. Methods. We select VMP stars using photometric metallicities from the\nliterature and analyse the Gaia RVS spectra to infer spectroscopic\nmetallicities for these stars. Results. The inferred metallicities agree very\nwell with literature high-resolution metallicities with a median systematic\noffset of 0.1 dex and standard deviation of $\\sim$0.15 dex. The purity of this\nsample in the VMP regime is $\\sim$80% with outliers representing a mere\n$\\sim$3%. Conclusions. We make available an all-sky catalogue of $\\sim$1500\nstars with reliable spectroscopic metallicities down to [Fe/H]$\\sim$-4.0, of\nwhich $\\sim$1000 are VMP stars. More than 75% of these stars have either no\nmetallicity value in the literature to date or are flagged to be unreliable in\ntheir literature metallicity estimates. This catalogue of bright (G<13) VMP\nstars is three times larger than the current sample of well-studied VMP stars\nin the literature in this magnitude range, making it ideal for high-resolution\nspectroscopic follow-up and to study the properties of VMP stars in different\nparts of our Galaxy."
                },
                "authors": [
                    {
                        "name": "Akshara Viswanathan"
                    },
                    {
                        "name": "Else Starkenburg"
                    },
                    {
                        "name": "Tadafumi Matsuno"
                    },
                    {
                        "name": "Kim A. Venn"
                    },
                    {
                        "name": "Nicolas F. Martin"
                    },
                    {
                        "name": "Nicolas Longeard"
                    },
                    {
                        "name": "Anke Ardern-Arentsen"
                    },
                    {
                        "name": "Raymond G. Carlberg"
                    },
                    {
                        "name": "Sebastien Fabbro"
                    },
                    {
                        "name": "Georges Kordopatis"
                    },
                    {
                        "name": "Martin Montelius"
                    },
                    {
                        "name": "Federico Sestito"
                    },
                    {
                        "name": "Zhen Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Yuan"
                },
                "author": "Zhen Yuan",
                "arxiv_doi": "10.1051/0004-6361/202347944",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202347944",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.06137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published version. Catalogue of VMP stars from Gaia DR3 RVS spectra\n  is made available public here\n  https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/683/L11",
                "arxiv_journal_ref": "A&A 683, L11 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11171v1",
                "updated": "2025-04-15T13:17:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    39,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:17:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TerraMind: Large-Scale Generative Multimodality for Earth Observation"
                },
                "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license."
                },
                "authors": [
                    {
                        "name": "Johannes Jakubik"
                    },
                    {
                        "name": "Felix Yang"
                    },
                    {
                        "name": "Benedikt Blumenstiel"
                    },
                    {
                        "name": "Erik Scheurer"
                    },
                    {
                        "name": "Rocco Sedona"
                    },
                    {
                        "name": "Stefano Maurogiovanni"
                    },
                    {
                        "name": "Jente Bosmans"
                    },
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Valerio Marsocci"
                    },
                    {
                        "name": "Niklas Kopp"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Thomas Brunschwiler"
                    },
                    {
                        "name": "Gabriele Cavallaro"
                    },
                    {
                        "name": "Juan Bernabe-Moreno"
                    },
                    {
                        "name": "Nicolas Longépé"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Longépé"
                },
                "author": "Nicolas Longépé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11170v2",
                "updated": "2025-04-16T08:50:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:17:14Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    14,
                    1,
                    105,
                    0
                ],
                "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space"
                },
                "summary": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance."
                },
                "authors": [
                    {
                        "name": "Taewook Kang"
                    },
                    {
                        "name": "Bum-Jae You"
                    },
                    {
                        "name": "Juyoun Park"
                    },
                    {
                        "name": "Yisoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yisoo Lee"
                },
                "author": "Yisoo Lee",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11169v1",
                "updated": "2025-04-15T13:16:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    46,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:16:46Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    46,
                    1,
                    105,
                    0
                ],
                "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos"
                },
                "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context."
                },
                "authors": [
                    {
                        "name": "Laura De Grazia"
                    },
                    {
                        "name": "Pol Pastells"
                    },
                    {
                        "name": "Mauro Vázquez Chas"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Danae Sánchez Villegas"
                    },
                    {
                        "name": "Mireia Farrús"
                    },
                    {
                        "name": "Mariona Taulé"
                    }
                ],
                "author_detail": {
                    "name": "Mariona Taulé"
                },
                "author": "Mariona Taulé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v1",
                "updated": "2025-04-15T13:16:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "12 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13124v2",
                "updated": "2025-04-15T13:09:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    9,
                    12,
                    1,
                    105,
                    0
                ],
                "published": "2024-05-21T18:00:11Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    18,
                    0,
                    11,
                    1,
                    142,
                    0
                ],
                "title": "The Pristine survey: XXV. The very metal-poor Galaxy: Chemodynamics\n  through the follow-up of the Pristine-Gaia synthetic catalogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pristine survey: XXV. The very metal-poor Galaxy: Chemodynamics\n  through the follow-up of the Pristine-Gaia synthetic catalogue"
                },
                "summary": "The Pristine-\\textit{Gaia} synthetic catalogue provides reliable photometric\nmetallicities for $\\sim$30 million FGK stars using the Pristine survey model\nand Gaia XP spectra. We perform the first low-to-medium-resolution\nspectroscopic follow-up of bright (G<15) and distant (up to 35 kpc) very and\nextremely metal-poor (V/EMP, [Fe/H]<-2.5) red giant branch stars from this. We\nuse Isaac Newton Telescope/Intermediate Dispersion Spectrograph (INT/IDS)\nobservations centred around the calcium triplet region ideal for V/EMP stars.\nWe find that 76\\% of our stars indeed have [Fe/H]<-2.5 with these inferred\nspectroscopic metallicities and only 3\\% are outliers with [Fe/H] > -2.0. We\nreport a success rate of 77\\% and 38\\% in finding stars with [Fe/H]<-2.5 and\n-3.0 respectively. This will allow for 10,000-20,000 homogeneously analysed EMP\nstars using the WEAVE survey follow-up of Pristine EMP candidates. We associate\n20\\%, 46\\%, and 34\\% of the stars to be confined to the disc plane, or to have\ninner and outer halo orbits, respectively. We also associate these V/EMP stars\nto known accretion events such as Gaia-Enceladus-Sausage (GES), LMS-1/Wukong,\nThamnos, Helmi streams, Sagittarius, Sequoia, etc. For the stars that orbit\nclose to the disc plane, we find that the prograde region with low vertical\naction is overdense with a significance of 4$\\sigma$ as compared to its\nretrograde counterpart. We also find three new (brightest) members of the most\nmetal-poor stellar stream, C-19, one of which is 50$^\\circ$ away from the main\nbody of the stream. Our measured mean metallicity, velocity dispersion, and\nstream width are consistent with the literature, but our results favour a\nhigher distance ($\\sim$21.5 kpc) for the stream. We publish a catalogue (and 1D\nspectra) of 215 V/EMP stars from this spectroscopic follow-up and showcase the\npower of chemokinematic analysis of V/EMP end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pristine-\\textit{Gaia} synthetic catalogue provides reliable photometric\nmetallicities for $\\sim$30 million FGK stars using the Pristine survey model\nand Gaia XP spectra. We perform the first low-to-medium-resolution\nspectroscopic follow-up of bright (G<15) and distant (up to 35 kpc) very and\nextremely metal-poor (V/EMP, [Fe/H]<-2.5) red giant branch stars from this. We\nuse Isaac Newton Telescope/Intermediate Dispersion Spectrograph (INT/IDS)\nobservations centred around the calcium triplet region ideal for V/EMP stars.\nWe find that 76\\% of our stars indeed have [Fe/H]<-2.5 with these inferred\nspectroscopic metallicities and only 3\\% are outliers with [Fe/H] > -2.0. We\nreport a success rate of 77\\% and 38\\% in finding stars with [Fe/H]<-2.5 and\n-3.0 respectively. This will allow for 10,000-20,000 homogeneously analysed EMP\nstars using the WEAVE survey follow-up of Pristine EMP candidates. We associate\n20\\%, 46\\%, and 34\\% of the stars to be confined to the disc plane, or to have\ninner and outer halo orbits, respectively. We also associate these V/EMP stars\nto known accretion events such as Gaia-Enceladus-Sausage (GES), LMS-1/Wukong,\nThamnos, Helmi streams, Sagittarius, Sequoia, etc. For the stars that orbit\nclose to the disc plane, we find that the prograde region with low vertical\naction is overdense with a significance of 4$\\sigma$ as compared to its\nretrograde counterpart. We also find three new (brightest) members of the most\nmetal-poor stellar stream, C-19, one of which is 50$^\\circ$ away from the main\nbody of the stream. Our measured mean metallicity, velocity dispersion, and\nstream width are consistent with the literature, but our results favour a\nhigher distance ($\\sim$21.5 kpc) for the stream. We publish a catalogue (and 1D\nspectra) of 215 V/EMP stars from this spectroscopic follow-up and showcase the\npower of chemokinematic analysis of V/EMP end."
                },
                "authors": [
                    {
                        "name": "Akshara Viswanathan"
                    },
                    {
                        "name": "Zhen Yuan"
                    },
                    {
                        "name": "Anke Ardern-Arentsen"
                    },
                    {
                        "name": "Else Starkenburg"
                    },
                    {
                        "name": "Nicolas F. Martin"
                    },
                    {
                        "name": "Kris Youakim"
                    },
                    {
                        "name": "Rodrigo A. Ibata"
                    },
                    {
                        "name": "Federico Sestito"
                    },
                    {
                        "name": "Tadafumi Matsuno"
                    },
                    {
                        "name": "Carlos Allende Prieto"
                    },
                    {
                        "name": "Freya Barwell"
                    },
                    {
                        "name": "Manuel Bayer"
                    },
                    {
                        "name": "Amandine Doliva-Dolinsky"
                    },
                    {
                        "name": "Emma Fernandez-Alvar"
                    },
                    {
                        "name": "Pablo M. Galan-de Anta"
                    },
                    {
                        "name": "Kiran Jhass"
                    },
                    {
                        "name": "Nicolas Longeard"
                    },
                    {
                        "name": "Jose Maria Arroyo-Polonio"
                    },
                    {
                        "name": "Pol Massana"
                    },
                    {
                        "name": "Martin Montelius"
                    },
                    {
                        "name": "Samuel Rusterucci"
                    },
                    {
                        "name": "Judith Santos"
                    },
                    {
                        "name": "Guillaume F. Thomas"
                    },
                    {
                        "name": "Sara Vitali"
                    },
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Paige Yarker"
                    },
                    {
                        "name": "Xianhao Ye"
                    },
                    {
                        "name": "David S. Aguado"
                    },
                    {
                        "name": "Felipe Gran"
                    },
                    {
                        "name": "Julio Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Julio Navarro"
                },
                "author": "Julio Navarro",
                "arxiv_doi": "10.1051/0004-6361/202450819",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450819",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.13124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published version. The catalogue of 215 V/EMP stars and their reduced\n  1D spectra is available here\n  https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/695/A112",
                "arxiv_journal_ref": "A&A 695, A112 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11147v1",
                "updated": "2025-04-15T12:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    48,
                    23,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T12:48:23Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    48,
                    23,
                    1,
                    105,
                    0
                ],
                "title": "Robust Bayesian Inference for Censored Survival Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian Inference for Censored Survival Models"
                },
                "summary": "This paper proposes a robust Bayesian accelerated failure time model for\ncensored survival data. We develop a new family of life-time distributions\nusing a scale mixture of the generalized gamma distributions, where we propose\na novel super heavy-tailed distribution as a mixing density. We theoretically\nshow that, under some conditions, the proposed method satisfies the full\nposterior robustness, which guarantees robustness of point estimation as well\nas uncertainty quantification. For posterior computation, we employ an integral\nexpression of the proposed heavy-tailed distribution to develop an efficient\nposterior computation algorithm based on the Markov chain Monte Carlo. The\nperformance of the proposed method is illustrated through numerical experiments\nand real data example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a robust Bayesian accelerated failure time model for\ncensored survival data. We develop a new family of life-time distributions\nusing a scale mixture of the generalized gamma distributions, where we propose\na novel super heavy-tailed distribution as a mixing density. We theoretically\nshow that, under some conditions, the proposed method satisfies the full\nposterior robustness, which guarantees robustness of point estimation as well\nas uncertainty quantification. For posterior computation, we employ an integral\nexpression of the proposed heavy-tailed distribution to develop an efficient\nposterior computation algorithm based on the Markov chain Monte Carlo. The\nperformance of the proposed method is illustrated through numerical experiments\nand real data example."
                },
                "authors": [
                    {
                        "name": "Yasuyuki Hamura"
                    },
                    {
                        "name": "Takahiro Onizuka"
                    },
                    {
                        "name": "Shintaro Hashimoto"
                    },
                    {
                        "name": "Shonosuke Sugasawa"
                    }
                ],
                "author_detail": {
                    "name": "Shonosuke Sugasawa"
                },
                "author": "Shonosuke Sugasawa",
                "arxiv_comment": "51 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11143v1",
                "updated": "2025-04-15T12:44:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    44,
                    53,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T12:44:53Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    44,
                    53,
                    1,
                    105,
                    0
                ],
                "title": "Taming Consistency Distillation for Accelerated Human Image Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Consistency Distillation for Accelerated Human Image Animation"
                },
                "summary": "Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Changxin Gao"
                    },
                    {
                        "name": "Yuehuan Wang"
                    },
                    {
                        "name": "Nong Sang"
                    }
                ],
                "author_detail": {
                    "name": "Nong Sang"
                },
                "author": "Nong Sang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11121v1",
                "updated": "2025-04-15T12:09:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    9,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T12:09:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    9,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "$B$ meson decays to vector charmonium(like) states and a $K$ meson: the\n  role of final-state interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$B$ meson decays to vector charmonium(like) states and a $K$ meson: the\n  role of final-state interactions"
                },
                "summary": "A series of vector charmonium(like) states, accompanied by a $K$ meson, have\nbeen observed in the decays of $B$ meson. These processes are color-suppressed\nat the quark level, as inferred from topological diagram analysis. In this\nwork, we calculate the branching fractions of the decays $B \\to \\psi K$, where\n$\\psi$ denotes the charmonium(like) states $\\psi(1S)$, $\\psi(2S)$,\n$\\psi(4040)$, $\\psi(3770)$, and $\\psi(4160)$. Our analysis incorporates both\nshort-distance (naive factorization approach) and long-distance (final-state\ninteractions) contributions. Within reasonable parameters, our results align\nwith experimental data except for the $ \\psi(4160)$, suggesting its possible\nexotic nature. Furthermore, we find that long-distance contributions dominate\nthese decay processes, highlighting the crucial role of final-state\ninteractions in the productions of charmonium(like) states in $B$ decays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A series of vector charmonium(like) states, accompanied by a $K$ meson, have\nbeen observed in the decays of $B$ meson. These processes are color-suppressed\nat the quark level, as inferred from topological diagram analysis. In this\nwork, we calculate the branching fractions of the decays $B \\to \\psi K$, where\n$\\psi$ denotes the charmonium(like) states $\\psi(1S)$, $\\psi(2S)$,\n$\\psi(4040)$, $\\psi(3770)$, and $\\psi(4160)$. Our analysis incorporates both\nshort-distance (naive factorization approach) and long-distance (final-state\ninteractions) contributions. Within reasonable parameters, our results align\nwith experimental data except for the $ \\psi(4160)$, suggesting its possible\nexotic nature. Furthermore, we find that long-distance contributions dominate\nthese decay processes, highlighting the crucial role of final-state\ninteractions in the productions of charmonium(like) states in $B$ decays."
                },
                "authors": [
                    {
                        "name": "Qi-Wei Yuan"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Ming-Zhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Zhu Liu"
                },
                "author": "Ming-Zhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04118v2",
                "updated": "2025-04-15T12:01:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    1,
                    11,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-05T09:19:22Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    9,
                    19,
                    22,
                    5,
                    95,
                    0
                ],
                "title": "Joint Analysis of Constraints on f(R) Parametrization from Recent\n  Cosmological Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Analysis of Constraints on f(R) Parametrization from Recent\n  Cosmological Observations"
                },
                "summary": "In this study, we present constraints on the parameters of three well-known\n$f(R)$ gravity models, viz. (i) Hu-Sawicki, (ii) Starobinsky, and (iii) ArcTanh\nby using a joint analysis of recent cosmological observations. We perform\nanalytical approximations for the Hubble parameter, $H(z)$, and cosmological\ndistances in terms of the Hubble constant $(H_0)$, matter density\n$(\\Omega_{m0})$, and a deviation parameter $b$ for each model. Our analysis\nuses data from four cosmological observations: (a) Hubble parameter\nmeasurements (Cosmic Chronometers), (b) Type Ia Supernovae (Union 3.0), (c)\nBaryon Acoustic Oscillations (DESI-2024), and (d) Gamma-Ray Bursts (GRBs). We\nfirst optimize the models using each dataset independently, and subsequently,\nwe perform a comprehensive joint analysis combining all four datasets. Our\nresults show that the Hu-Sawicki and ArcTanh models do not deviate\nsignificantly from the $\\Lambda$CDM model at 68\\% confidence level for\nindividual datasets and remain consistent at 99\\% confidence level in the joint\nanalysis. In contrast, the Starobinsky model shows a strong deviation and\nappears as a viable alternative to $\\Lambda$CDM. We also constrain the\ntransition redshift parameter ($z_t$), and check that the obtained value agrees\nwith the values inferred from both early-time measurement (Planck) and\nlate-time data from Type Ia Supernovae. These results support the potential\nsupport of $f(R)$ gravity to explain the late-time cosmic acceleration\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present constraints on the parameters of three well-known\n$f(R)$ gravity models, viz. (i) Hu-Sawicki, (ii) Starobinsky, and (iii) ArcTanh\nby using a joint analysis of recent cosmological observations. We perform\nanalytical approximations for the Hubble parameter, $H(z)$, and cosmological\ndistances in terms of the Hubble constant $(H_0)$, matter density\n$(\\Omega_{m0})$, and a deviation parameter $b$ for each model. Our analysis\nuses data from four cosmological observations: (a) Hubble parameter\nmeasurements (Cosmic Chronometers), (b) Type Ia Supernovae (Union 3.0), (c)\nBaryon Acoustic Oscillations (DESI-2024), and (d) Gamma-Ray Bursts (GRBs). We\nfirst optimize the models using each dataset independently, and subsequently,\nwe perform a comprehensive joint analysis combining all four datasets. Our\nresults show that the Hu-Sawicki and ArcTanh models do not deviate\nsignificantly from the $\\Lambda$CDM model at 68\\% confidence level for\nindividual datasets and remain consistent at 99\\% confidence level in the joint\nanalysis. In contrast, the Starobinsky model shows a strong deviation and\nappears as a viable alternative to $\\Lambda$CDM. We also constrain the\ntransition redshift parameter ($z_t$), and check that the obtained value agrees\nwith the values inferred from both early-time measurement (Planck) and\nlate-time data from Type Ia Supernovae. These results support the potential\nsupport of $f(R)$ gravity to explain the late-time cosmic acceleration\neffectively."
                },
                "authors": [
                    {
                        "name": "Darshan Kumar"
                    },
                    {
                        "name": "Praveen Kumar Dhankar"
                    },
                    {
                        "name": "Saibal Ray"
                    },
                    {
                        "name": "Fengge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fengge Zhang"
                },
                "arxiv_affiliation": "HNAS",
                "author": "Fengge Zhang",
                "arxiv_comment": "15 pages, 13 figures, and 6 tables; v2: references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11109v1",
                "updated": "2025-04-15T11:56:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:56:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Fine-Tuning Large Language Models on Quantum Optimization Problems for\n  Circuit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Language Models on Quantum Optimization Problems for\n  Circuit Generation"
                },
                "summary": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware."
                },
                "authors": [
                    {
                        "name": "Linus Jern"
                    },
                    {
                        "name": "Valter Uotila"
                    },
                    {
                        "name": "Cong Yu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "12 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11108v1",
                "updated": "2025-04-15T11:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Vision Language Models on German Factual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vision Language Models on German Factual Data"
                },
                "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."
                },
                "authors": [
                    {
                        "name": "René Peinl"
                    },
                    {
                        "name": "Vincent Tischler"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Tischler"
                },
                "author": "Vincent Tischler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11104v1",
                "updated": "2025-04-15T11:52:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    52,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:52:20Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    52,
                    20,
                    1,
                    105,
                    0
                ],
                "title": "Using LLMs as prompt modifier to avoid biases in AI image generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs as prompt modifier to avoid biases in AI image generators"
                },
                "summary": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/"
                },
                "authors": [
                    {
                        "name": "René Peinl"
                    }
                ],
                "author_detail": {
                    "name": "René Peinl"
                },
                "author": "René Peinl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11101v2",
                "updated": "2025-04-16T03:22:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    22,
                    14,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T11:51:18Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    51,
                    18,
                    1,
                    105,
                    0
                ],
                "title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR"
                },
                "summary": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout."
                },
                "authors": [
                    {
                        "name": "Yulong Zhang"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Xinyue Huang"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09378v2",
                "updated": "2025-04-15T11:49:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    49,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-13T00:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    0,
                    1,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs"
                },
                "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success."
                },
                "authors": [
                    {
                        "name": "Kartik Ravisankar"
                    },
                    {
                        "name": "Hyojung Han"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11631v2",
                "updated": "2025-04-15T11:47:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    47,
                    19,
                    1,
                    105,
                    0
                ],
                "published": "2025-02-17T10:20:39Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    20,
                    39,
                    0,
                    48,
                    0
                ],
                "title": "Advancing the heralded photon-number-state characterization by\n  understanding the interplay of experimental settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing the heralded photon-number-state characterization by\n  understanding the interplay of experimental settings"
                },
                "summary": "We theoretically explore the properties of heralded number states including\nup to three photons that are generated from single-mode twin beams. We\ninvestigate the effects of different parameters involved in the state\npreparation by using the fidelity, normalized second-order factorial moment of\nphoton number for the heralded state $(g^{(2)}_h)$, and photon-number parity as\nfigures of merit. Especially, the photon-number parity offers a practical and\nrobust tool for inferring the target state quality by capturing the\ncontamination of all undesired photon-number contributions. We focus on\nexpressing our results in terms of experimentally easily accessible parameters\nsuch as the coincidences-to-accidentals ratio and the detection efficiencies.\nOur results identify the optimal parameter regions for generating high quality\nphoton-number states by heralding and provide useful insights for advancing\ntheir use in quantum technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We theoretically explore the properties of heralded number states including\nup to three photons that are generated from single-mode twin beams. We\ninvestigate the effects of different parameters involved in the state\npreparation by using the fidelity, normalized second-order factorial moment of\nphoton number for the heralded state $(g^{(2)}_h)$, and photon-number parity as\nfigures of merit. Especially, the photon-number parity offers a practical and\nrobust tool for inferring the target state quality by capturing the\ncontamination of all undesired photon-number contributions. We focus on\nexpressing our results in terms of experimentally easily accessible parameters\nsuch as the coincidences-to-accidentals ratio and the detection efficiencies.\nOur results identify the optimal parameter regions for generating high quality\nphoton-number states by heralding and provide useful insights for advancing\ntheir use in quantum technologies."
                },
                "authors": [
                    {
                        "name": "Daniel Borrero Landazabal"
                    },
                    {
                        "name": "Kaisa Laiho"
                    }
                ],
                "author_detail": {
                    "name": "Kaisa Laiho"
                },
                "author": "Kaisa Laiho",
                "arxiv_comment": "15 pages content, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11094v1",
                "updated": "2025-04-15T11:40:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "title": "Evaluation Report on MCP Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation Report on MCP Servers"
                },
                "summary": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions."
                },
                "authors": [
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xuanrui Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Gao"
                },
                "author": "Jinyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15756v2",
                "updated": "2025-04-15T11:39:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    39,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-21T08:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    8,
                    15,
                    45,
                    0,
                    295,
                    0
                ],
                "title": "Automated Proof Generation for Rust Code via Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Proof Generation for Rust Code via Self-Evolution"
                },
                "summary": "Ensuring correctness is crucial for code generation. Formal verification\noffers a definitive assurance of correctness, but demands substantial human\neffort in proof construction and hence raises a pressing need for automation.\nThe primary obstacle lies in the severe lack of data-there is much fewer proofs\nthan code snippets for Large Language Models (LLMs) to train upon. In this\npaper, we introduce SAFE, a framework that overcomes the lack of human-written\nproofs to enable automated proof generation of Rust code. SAFE establishes a\nself-evolving cycle where data synthesis and fine-tuning collaborate to enhance\nthe model capability, leveraging the definitive power of a symbolic verifier in\ntelling correct proofs from incorrect ones. SAFE also re-purposes the large\nnumber of synthesized incorrect proofs to train the self-debugging capability\nof the fine-tuned models, empowering them to fix incorrect proofs based on the\nverifier's feedback. SAFE demonstrates superior efficiency and precision\ncompared to GPT-4o. Through tens of thousands of synthesized proofs and the\nself-debugging mechanism, we improve the capability of open-source models,\ninitially unacquainted with formal verification, to automatically write proofs\nfor Rust code. This advancement leads to a significant improvement in\nperformance, achieving a 52.52% accuracy rate in a benchmark crafted by human\nexperts, a significant leap over GPT-4o's performance of 14.39%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring correctness is crucial for code generation. Formal verification\noffers a definitive assurance of correctness, but demands substantial human\neffort in proof construction and hence raises a pressing need for automation.\nThe primary obstacle lies in the severe lack of data-there is much fewer proofs\nthan code snippets for Large Language Models (LLMs) to train upon. In this\npaper, we introduce SAFE, a framework that overcomes the lack of human-written\nproofs to enable automated proof generation of Rust code. SAFE establishes a\nself-evolving cycle where data synthesis and fine-tuning collaborate to enhance\nthe model capability, leveraging the definitive power of a symbolic verifier in\ntelling correct proofs from incorrect ones. SAFE also re-purposes the large\nnumber of synthesized incorrect proofs to train the self-debugging capability\nof the fine-tuned models, empowering them to fix incorrect proofs based on the\nverifier's feedback. SAFE demonstrates superior efficiency and precision\ncompared to GPT-4o. Through tens of thousands of synthesized proofs and the\nself-debugging mechanism, we improve the capability of open-source models,\ninitially unacquainted with formal verification, to automatically write proofs\nfor Rust code. This advancement leads to a significant improvement in\nperformance, achieving a 52.52% accuracy rate in a benchmark crafted by human\nexperts, a significant leap over GPT-4o's performance of 14.39%."
                },
                "authors": [
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Md Rakib Hossain Misu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Nan Duan"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Shuvendu K Lahiri"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Lidong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Zhou"
                },
                "author": "Lidong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20708v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20708v4",
                "updated": "2025-04-15T11:34:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    34,
                    30,
                    1,
                    105,
                    0
                ],
                "published": "2024-07-30T10:04:16Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    4,
                    16,
                    1,
                    212,
                    0
                ],
                "title": "Integer-Valued Training and Spike-Driven Inference Spiking Neural\n  Network for High-performance and Energy-efficient Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integer-Valued Training and Spike-Driven Inference Spiking Neural\n  Network for High-performance and Energy-efficient Object Detection"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO"
                },
                "authors": [
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Man Yao"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "arxiv_comment": "Accepted by ECCV2024; 19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20708v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20708v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11085v1",
                "updated": "2025-04-15T11:31:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    31,
                    17,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:31:17Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    31,
                    17,
                    1,
                    105,
                    0
                ],
                "title": "TD-Suite: All Batteries Included Framework for Technical Debt\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-Suite: All Batteries Included Framework for Technical Debt\n  Classification"
                },
                "summary": "Recognizing that technical debt is a persistent and significant challenge\nrequiring sophisticated management tools, TD-Suite offers a comprehensive\nsoftware framework specifically engineered to automate the complex task of its\nclassification within software projects. It leverages the advanced natural\nlanguage understanding of state-of-the-art transformer models to analyze\ntextual artifacts, such as developer discussions in issue reports, where subtle\nindicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from\ninitial data ingestion and rigorous preprocessing to model training, thorough\nevaluation, and final inference. This allows it to support both straightforward\nbinary classification (debt or no debt) and more valuable, identifying specific\ncategories like code, design, or documentation debt, thus enabling more\ntargeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world,\noften imbalanced, datasets, TD-Suite incorporates critical training\nmethodologies: k-fold cross-validation assesses generalization capability,\nearly stopping mechanisms prevent overfitting to the training data, and class\nweighting strategies effectively address skewed data distributions. Beyond core\nfunctionality, and acknowledging the growing importance of sustainability, the\nframework integrates tracking and reporting of carbon emissions associated with\nthe computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container\nsetup, simplifying model interaction, evaluation, and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing that technical debt is a persistent and significant challenge\nrequiring sophisticated management tools, TD-Suite offers a comprehensive\nsoftware framework specifically engineered to automate the complex task of its\nclassification within software projects. It leverages the advanced natural\nlanguage understanding of state-of-the-art transformer models to analyze\ntextual artifacts, such as developer discussions in issue reports, where subtle\nindicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from\ninitial data ingestion and rigorous preprocessing to model training, thorough\nevaluation, and final inference. This allows it to support both straightforward\nbinary classification (debt or no debt) and more valuable, identifying specific\ncategories like code, design, or documentation debt, thus enabling more\ntargeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world,\noften imbalanced, datasets, TD-Suite incorporates critical training\nmethodologies: k-fold cross-validation assesses generalization capability,\nearly stopping mechanisms prevent overfitting to the training data, and class\nweighting strategies effectively address skewed data distributions. Beyond core\nfunctionality, and acknowledging the growing importance of sustainability, the\nframework integrates tracking and reporting of carbon emissions associated with\nthe computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container\nsetup, simplifying model interaction, evaluation, and inference."
                },
                "authors": [
                    {
                        "name": "Karthik Shivashankar"
                    },
                    {
                        "name": "Antonio Martini"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Martini"
                },
                "author": "Antonio Martini",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11083v1",
                "updated": "2025-04-15T11:29:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    29,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:29:09Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    29,
                    9,
                    1,
                    105,
                    0
                ],
                "title": "QAMA: Quantum annealing multi-head attention operator with classical\n  deep learning framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QAMA: Quantum annealing multi-head attention operator with classical\n  deep learning framework"
                },
                "summary": "As large language models scale up, the conventional attention mechanism faces\ncritical challenges of exponential growth in memory consumption and energy\ncosts. Quantum annealing computing, with its inherent advantages in\ncomputational efficiency and low energy consumption, offers an innovative\ndirection for constructing novel deep learning architectures. This study\nproposes the first Quantum Annealing-based Multi-head Attention (QAMA)\nmechanism, achieving seamless compatibility with classical attention\narchitectures through quadratic unconstrained binary optimization (QUBO)\nmodeling of forward propagation and energy-based backpropagation. The method\ninnovatively leverages the quantum bit interaction characteristics of Ising\nmodels to optimize the conventional $O(n^2)$ spatiotemporal complexity into\nlinear resource consumption. Integrated with the optical computing advantages\nof coherent Ising machines (CIM), the system maintains millisecond-level\nreal-time responsiveness while significantly reducing energy consumption. Our\nkey contributions include: Theoretical proofs establish QAMA mathematical\nequivalence to classical attention mechanisms; Dual optimization of multi-head\nspecificity and long-range information capture via QUBO constraints; Explicit\ngradient proofs for the Ising energy equation are utilized to implement\ngradient conduction as the only path in the computational graph as a layer;\nProposed soft selection mechanism overcoming traditional binary attention\nlimitations to approximate continuous weights. Experiments on QBoson CPQC\nquantum computer show QAMA achieves comparable accuracy to classical operators\nwhile reducing inference time to millisecond level and improving solution\nquality. This work pioneers architectural-level integration of quantum\ncomputing and deep learning, applicable to any attention-based model, driving\nparadigm innovation in AI foundational computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models scale up, the conventional attention mechanism faces\ncritical challenges of exponential growth in memory consumption and energy\ncosts. Quantum annealing computing, with its inherent advantages in\ncomputational efficiency and low energy consumption, offers an innovative\ndirection for constructing novel deep learning architectures. This study\nproposes the first Quantum Annealing-based Multi-head Attention (QAMA)\nmechanism, achieving seamless compatibility with classical attention\narchitectures through quadratic unconstrained binary optimization (QUBO)\nmodeling of forward propagation and energy-based backpropagation. The method\ninnovatively leverages the quantum bit interaction characteristics of Ising\nmodels to optimize the conventional $O(n^2)$ spatiotemporal complexity into\nlinear resource consumption. Integrated with the optical computing advantages\nof coherent Ising machines (CIM), the system maintains millisecond-level\nreal-time responsiveness while significantly reducing energy consumption. Our\nkey contributions include: Theoretical proofs establish QAMA mathematical\nequivalence to classical attention mechanisms; Dual optimization of multi-head\nspecificity and long-range information capture via QUBO constraints; Explicit\ngradient proofs for the Ising energy equation are utilized to implement\ngradient conduction as the only path in the computational graph as a layer;\nProposed soft selection mechanism overcoming traditional binary attention\nlimitations to approximate continuous weights. Experiments on QBoson CPQC\nquantum computer show QAMA achieves comparable accuracy to classical operators\nwhile reducing inference time to millisecond level and improving solution\nquality. This work pioneers architectural-level integration of quantum\ncomputing and deep learning, applicable to any attention-based model, driving\nparadigm innovation in AI foundational computing."
                },
                "authors": [
                    {
                        "name": "Peng Du"
                    },
                    {
                        "name": "Shuolei Wang"
                    },
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Jinjing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Jinjing Shi"
                },
                "author": "Jinjing Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11082v1",
                "updated": "2025-04-15T11:28:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    28,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:28:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    28,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "DeepMLF: Multimodal language model with learnable tokens for deep fusion\n  in sentiment analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMLF: Multimodal language model with learnable tokens for deep fusion\n  in sentiment analysis"
                },
                "summary": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization."
                },
                "authors": [
                    {
                        "name": "Efthymios Georgiou"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    },
                    {
                        "name": "Yannis Avrithis"
                    },
                    {
                        "name": "Alexandros Potamianos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Potamianos"
                },
                "author": "Alexandros Potamianos",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11075v1",
                "updated": "2025-04-15T11:16:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    16,
                    27,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:16:27Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    16,
                    27,
                    1,
                    105,
                    0
                ],
                "title": "Emergence of Goal-Directed Behaviors via Active Inference with\n  Self-Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of Goal-Directed Behaviors via Active Inference with\n  Self-Prior"
                },
                "summary": "Infants often exhibit goal-directed behaviors, such as reaching for a sensory\nstimulus, even when no external reward criterion is provided. These\nintrinsically motivated behaviors facilitate spontaneous exploration and\nlearning of the body and environment during early developmental stages.\nAlthough computational modeling can offer insight into the mechanisms\nunderlying such behaviors, many existing studies on intrinsic motivation focus\nprimarily on how exploration contributes to acquiring external rewards. In this\npaper, we propose a novel density model for an agent's own multimodal sensory\nexperiences, called the \"self-prior,\" and investigate whether it can\nautonomously induce goal-directed behavior. Integrated within an active\ninference framework based on the free energy principle, the self-prior\ngenerates behavioral references purely from an intrinsic process that minimizes\nmismatches between average past sensory experiences and current observations.\nThis mechanism is also analogous to the acquisition and utilization of a body\nschema through continuous interaction with the environment. We examine this\napproach in a simulated environment and confirm that the agent spontaneously\nreaches toward a tactile stimulus. Our study implements intrinsically motivated\nbehavior shaped by the agent's own sensory experiences, demonstrating the\nspontaneous emergence of intentional behavior during early development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infants often exhibit goal-directed behaviors, such as reaching for a sensory\nstimulus, even when no external reward criterion is provided. These\nintrinsically motivated behaviors facilitate spontaneous exploration and\nlearning of the body and environment during early developmental stages.\nAlthough computational modeling can offer insight into the mechanisms\nunderlying such behaviors, many existing studies on intrinsic motivation focus\nprimarily on how exploration contributes to acquiring external rewards. In this\npaper, we propose a novel density model for an agent's own multimodal sensory\nexperiences, called the \"self-prior,\" and investigate whether it can\nautonomously induce goal-directed behavior. Integrated within an active\ninference framework based on the free energy principle, the self-prior\ngenerates behavioral references purely from an intrinsic process that minimizes\nmismatches between average past sensory experiences and current observations.\nThis mechanism is also analogous to the acquisition and utilization of a body\nschema through continuous interaction with the environment. We examine this\napproach in a simulated environment and confirm that the agent spontaneously\nreaches toward a tactile stimulus. Our study implements intrinsically motivated\nbehavior shaped by the agent's own sensory experiences, demonstrating the\nspontaneous emergence of intentional behavior during early development."
                },
                "authors": [
                    {
                        "name": "Dongmin Kim"
                    },
                    {
                        "name": "Hoshinori Kanazawa"
                    },
                    {
                        "name": "Naoto Yoshida"
                    },
                    {
                        "name": "Yasuo Kuniyoshi"
                    }
                ],
                "author_detail": {
                    "name": "Yasuo Kuniyoshi"
                },
                "author": "Yasuo Kuniyoshi",
                "arxiv_comment": "20 pages, Code is available at\n  https://github.com/kim135797531/self-prior",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T40, 68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.6; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11050v1",
                "updated": "2025-04-15T10:34:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    34,
                    23,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T10:34:23Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    34,
                    23,
                    1,
                    105,
                    0
                ],
                "title": "Leveraging LLMs and attention-mechanism for automatic annotation of\n  historical maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs and attention-mechanism for automatic annotation of\n  historical maps"
                },
                "summary": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis."
                },
                "authors": [
                    {
                        "name": "Yunshuang Yuan"
                    },
                    {
                        "name": "Monika Sester"
                    }
                ],
                "author_detail": {
                    "name": "Monika Sester"
                },
                "author": "Monika Sester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15392v3",
                "updated": "2025-04-15T10:25:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    25,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-01-26T04:19:43Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    4,
                    19,
                    43,
                    6,
                    26,
                    0
                ],
                "title": "Faster Configuration Performance Bug Testing with Neural Dual-level\n  Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Configuration Performance Bug Testing with Neural Dual-level\n  Prioritization"
                },
                "summary": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools."
                },
                "authors": [
                    {
                        "name": "Youpeng Ma"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Ke Li"
                    }
                ],
                "author_detail": {
                    "name": "Ke Li"
                },
                "author": "Ke Li",
                "arxiv_comment": "accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08929v3",
                "updated": "2025-04-15T10:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    24,
                    14,
                    1,
                    105,
                    0
                ],
                "published": "2023-06-15T08:02:07Z",
                "published_parsed": [
                    2023,
                    6,
                    15,
                    8,
                    2,
                    7,
                    3,
                    166,
                    0
                ],
                "title": "Inferring Communities of Interest in Collaborative Learning-based\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Communities of Interest in Collaborative Learning-based\n  Recommender Systems"
                },
                "summary": "Collaborative-learning-based recommender systems, such as those employing\nFederated Learning (FL) and Gossip Learning (GL), allow users to train models\nwhile keeping their history of liked items on their devices. While these\nmethods were seen as promising for enhancing privacy, recent research has shown\nthat collaborative learning can be vulnerable to various privacy attacks. In\nthis paper, we propose a novel attack called Community Inference Attack (CIA),\nwhich enables an adversary to identify community members based on a set of\ntarget items. What sets CIA apart is its efficiency: it operates at low\ncomputational cost by eliminating the need for training surrogate models.\nInstead, it uses a comparison-based approach, inferring sensitive information\nby comparing users' models rather than targeting any specific individual model.\nTo evaluate the effectiveness of CIA, we conduct experiments on three\nreal-world recommendation datasets using two recommendation models under both\nFederated and Gossip-like settings. The results demonstrate that CIA can be up\nto 10 times more accurate than random guessing. Additionally, we evaluate two\nmitigation strategies: Differentially Private Stochastic Gradient Descent\n(DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive\nmodel parameters. Our findings suggest that the Share less strategy offers a\nbetter privacy-utility trade-off, especially in GL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative-learning-based recommender systems, such as those employing\nFederated Learning (FL) and Gossip Learning (GL), allow users to train models\nwhile keeping their history of liked items on their devices. While these\nmethods were seen as promising for enhancing privacy, recent research has shown\nthat collaborative learning can be vulnerable to various privacy attacks. In\nthis paper, we propose a novel attack called Community Inference Attack (CIA),\nwhich enables an adversary to identify community members based on a set of\ntarget items. What sets CIA apart is its efficiency: it operates at low\ncomputational cost by eliminating the need for training surrogate models.\nInstead, it uses a comparison-based approach, inferring sensitive information\nby comparing users' models rather than targeting any specific individual model.\nTo evaluate the effectiveness of CIA, we conduct experiments on three\nreal-world recommendation datasets using two recommendation models under both\nFederated and Gossip-like settings. The results demonstrate that CIA can be up\nto 10 times more accurate than random guessing. Additionally, we evaluate two\nmitigation strategies: Differentially Private Stochastic Gradient Descent\n(DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive\nmodel parameters. Our findings suggest that the Share less strategy offers a\nbetter privacy-utility trade-off, especially in GL."
                },
                "authors": [
                    {
                        "name": "Yacine Belal"
                    },
                    {
                        "name": "Sonia Ben Mokhtar"
                    },
                    {
                        "name": "Mohamed Maouche"
                    },
                    {
                        "name": "Anthony Simonet-Boulogne"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Simonet-Boulogne"
                },
                "author": "Anthony Simonet-Boulogne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.6; I.2.11; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11044v1",
                "updated": "2025-04-15T10:12:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    12,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T10:12:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    12,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "On relative universality, regression operator, and conditional\n  independence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On relative universality, regression operator, and conditional\n  independence"
                },
                "summary": "The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation."
                },
                "authors": [
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Ben Jones"
                    },
                    {
                        "name": "Andreas Artemiou"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Artemiou"
                },
                "author": "Andreas Artemiou",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11042v1",
                "updated": "2025-04-15T10:07:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    7,
                    33,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T10:07:33Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    7,
                    33,
                    1,
                    105,
                    0
                ],
                "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews"
                },
                "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)"
                },
                "authors": [
                    {
                        "name": "Sukannya Purkayastha"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "29 pages, 18 Figures, 15 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11035v1",
                "updated": "2025-04-15T09:58:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    58,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:58:06Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    58,
                    6,
                    1,
                    105,
                    0
                ],
                "title": "A conceptual synthesis of causal assumptions for causal discovery and\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A conceptual synthesis of causal assumptions for causal discovery and\n  inference"
                },
                "summary": "This work presents a conceptual synthesis of causal discovery and inference\nframeworks, with a focus on how foundational assumptions -- causal sufficiency,\ncausal faithfulness, and the causal Markov condition -- are formalized and\noperationalized across methodological traditions. Through structured tables and\ncomparative summaries, I map core assumptions, tasks, and analytical choices\nfrom multiple causal frameworks, highlighting their connections and\ndifferences. The synthesis provides practical guidance for researchers\ndesigning causal studies, especially in settings where observational or\nexperimental constraints challenge standard approaches. This guide spans all\nphases of causal analysis, including question formulation, formalization of\nbackground knowledge, selection of appropriate frameworks, choice of study\ndesign or algorithm, and interpretation. It is intended as a tool to support\nrigorous causal reasoning across diverse empirical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a conceptual synthesis of causal discovery and inference\nframeworks, with a focus on how foundational assumptions -- causal sufficiency,\ncausal faithfulness, and the causal Markov condition -- are formalized and\noperationalized across methodological traditions. Through structured tables and\ncomparative summaries, I map core assumptions, tasks, and analytical choices\nfrom multiple causal frameworks, highlighting their connections and\ndifferences. The synthesis provides practical guidance for researchers\ndesigning causal studies, especially in settings where observational or\nexperimental constraints challenge standard approaches. This guide spans all\nphases of causal analysis, including question formulation, formalization of\nbackground knowledge, selection of appropriate frameworks, choice of study\ndesign or algorithm, and interpretation. It is intended as a tool to support\nrigorous causal reasoning across diverse empirical domains."
                },
                "authors": [
                    {
                        "name": "Hannah E. Correia"
                    }
                ],
                "author_detail": {
                    "name": "Hannah E. Correia"
                },
                "author": "Hannah E. Correia",
                "arxiv_comment": "40 pages, 1 color figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62A01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11025v1",
                "updated": "2025-04-15T09:50:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    50,
                    0,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:50:00Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    50,
                    0,
                    1,
                    105,
                    0
                ],
                "title": "Optimal inference for the mean of random functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal inference for the mean of random functions"
                },
                "summary": "We study estimation and inference for the mean of real-valued random\nfunctions defined on a hypercube. The independent random functions are observed\non a discrete, random subset of design points, possibly with heteroscedastic\nnoise. We propose a novel optimal-rate estimator based on Fourier series\nexpansions and establish a sharp non-asymptotic error bound in $L^2-$norm.\nAdditionally, we derive a non-asymptotic Gaussian approximation bound for our\nestimated Fourier coefficients. Pointwise and uniform confidence sets are\nconstructed. Our approach is made adaptive by a plug-in estimator for the\nH\\\"older regularity of the mean function, for which we derive non-asymptotic\nconcentration bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study estimation and inference for the mean of real-valued random\nfunctions defined on a hypercube. The independent random functions are observed\non a discrete, random subset of design points, possibly with heteroscedastic\nnoise. We propose a novel optimal-rate estimator based on Fourier series\nexpansions and establish a sharp non-asymptotic error bound in $L^2-$norm.\nAdditionally, we derive a non-asymptotic Gaussian approximation bound for our\nestimated Fourier coefficients. Pointwise and uniform confidence sets are\nconstructed. Our approach is made adaptive by a plug-in estimator for the\nH\\\"older regularity of the mean function, for which we derive non-asymptotic\nconcentration bounds."
                },
                "authors": [
                    {
                        "name": "Omar Kassi"
                    },
                    {
                        "name": "Valentin Patilea"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Patilea"
                },
                "author": "Valentin Patilea",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.11456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11456v1",
                "updated": "2025-04-15T17:59:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    51,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:59:51Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    59,
                    51,
                    1,
                    105,
                    0
                ],
                "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning"
                },
                "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath."
                },
                "authors": [
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11442v1",
                "updated": "2025-04-15T17:55:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    55,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:55:20Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    55,
                    20,
                    1,
                    105,
                    0
                ],
                "title": "TextArena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextArena"
                },
                "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/."
                },
                "authors": [
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Bobby Cheng"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "work in progress; 5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11438v1",
                "updated": "2025-04-15T17:53:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    53,
                    18,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:53:18Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    53,
                    18,
                    1,
                    105,
                    0
                ],
                "title": "Mamba-Based Ensemble learning for White Blood Cell Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-Based Ensemble learning for White Blood Cell Classification"
                },
                "summary": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification."
                },
                "authors": [
                    {
                        "name": "Lewis Clifton"
                    },
                    {
                        "name": "Xin Tian"
                    },
                    {
                        "name": "Duangdao Palasuwan"
                    },
                    {
                        "name": "Phandee Watanaboonyongcharoen"
                    },
                    {
                        "name": "Ponlapat Rojnuckarin"
                    },
                    {
                        "name": "Nantheera Anantrasirichai"
                    }
                ],
                "author_detail": {
                    "name": "Nantheera Anantrasirichai"
                },
                "author": "Nantheera Anantrasirichai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11434v1",
                "updated": "2025-04-15T17:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    35,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    35,
                    1,
                    105,
                    0
                ],
                "title": "Enhancing Out-of-Distribution Detection with Extended Logit\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Out-of-Distribution Detection with Extended Logit\n  Normalization"
                },
                "summary": "Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Xixi Liu"
                    },
                    {
                        "name": "Jonas Unger"
                    },
                    {
                        "name": "Gabriel Eilertsen"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Eilertsen"
                },
                "author": "Gabriel Eilertsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10479v2",
                "updated": "2025-04-15T17:50:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    50,
                    27,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models"
                },
                "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."
                },
                "authors": [
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Xingguang Wei"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Haomin Wang"
                    },
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Dengnian Chen"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Tan Jiang"
                    },
                    {
                        "name": "Jiapeng Luo"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Wenwen Qu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Penglong Jiao"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11431v1",
                "updated": "2025-04-15T17:41:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    41,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:41:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    41,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models"
                },
                "summary": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "arxiv_comment": "To appear in ICWSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11426v1",
                "updated": "2025-04-15T17:38:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    47,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:38:47Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    47,
                    1,
                    105,
                    0
                ],
                "title": "A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v2",
                "updated": "2025-04-15T17:38:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    38,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11420v1",
                "updated": "2025-04-15T17:35:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    35,
                    56,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:35:56Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    35,
                    56,
                    1,
                    105,
                    0
                ],
                "title": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\n  Composing Informative Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\n  Composing Informative Contexts"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    }
                ],
                "author_detail": {
                    "name": "Sinno Jialin Pan"
                },
                "author": "Sinno Jialin Pan",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11417v1",
                "updated": "2025-04-15T17:33:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    33,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:33:06Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    33,
                    6,
                    1,
                    105,
                    0
                ],
                "title": "A tutorial on simulating nonlinear behaviors of flexible structures with\n  the discrete differential geometry (DDG) method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tutorial on simulating nonlinear behaviors of flexible structures with\n  the discrete differential geometry (DDG) method"
                },
                "summary": "Flexible elastic structures, such as beams, rods, ribbons, plates, and\nshells, exhibit complex nonlinear dynamical behaviors that are central to a\nwide range of engineering and scientific applications, including soft robotics,\ndeployable structures, and biomedical devices. While various numerical methods\nhave been developed to simulate these behaviors, many conventional approaches\nstruggle to simultaneously capture geometric and material nonlinearities, as\nwell as nonlinear external interactions, particularly in highly deformable and\ndynamically evolving systems. The Discrete Differential Geometry (DDG) method\nhas emerged as a robust and efficient numerical framework that intrinsically\npreserves geometric properties, accommodates material nonlinearity, and\naccurately models interactions with external environments and fields. By\ndirectly discretizing geometric and mechanical quantities, DDG provides an\naccurate, stable, and efficient approach to modeling flexible structures,\naddressing key limitations of traditional numerical methods. This tutorial\nprovides a systematic introduction to the DDG method for simulating nonlinear\nbehaviors in flexible structures. It covers DDG theory, simulation frameworks,\nand MATLAB implementation, with examples spanning dynamic systems, geometric\nand material nonlinearities, and external interactions like magnetics and\nfluids, culminating in practical insights and future directions. By offering a\ncomprehensive and practical guide, together with open-source MATLAB code, this\ntutorial aims to facilitate the broader adoption of DDG-based numerical tools\namong researchers and engineers in computational mechanics, applied\nmathematics, and structural design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible elastic structures, such as beams, rods, ribbons, plates, and\nshells, exhibit complex nonlinear dynamical behaviors that are central to a\nwide range of engineering and scientific applications, including soft robotics,\ndeployable structures, and biomedical devices. While various numerical methods\nhave been developed to simulate these behaviors, many conventional approaches\nstruggle to simultaneously capture geometric and material nonlinearities, as\nwell as nonlinear external interactions, particularly in highly deformable and\ndynamically evolving systems. The Discrete Differential Geometry (DDG) method\nhas emerged as a robust and efficient numerical framework that intrinsically\npreserves geometric properties, accommodates material nonlinearity, and\naccurately models interactions with external environments and fields. By\ndirectly discretizing geometric and mechanical quantities, DDG provides an\naccurate, stable, and efficient approach to modeling flexible structures,\naddressing key limitations of traditional numerical methods. This tutorial\nprovides a systematic introduction to the DDG method for simulating nonlinear\nbehaviors in flexible structures. It covers DDG theory, simulation frameworks,\nand MATLAB implementation, with examples spanning dynamic systems, geometric\nand material nonlinearities, and external interactions like magnetics and\nfluids, culminating in practical insights and future directions. By offering a\ncomprehensive and practical guide, together with open-source MATLAB code, this\ntutorial aims to facilitate the broader adoption of DDG-based numerical tools\namong researchers and engineers in computational mechanics, applied\nmathematics, and structural design."
                },
                "authors": [
                    {
                        "name": "Weicheng Huang"
                    },
                    {
                        "name": "Zhuonan Hao"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Dezhong Tong"
                    },
                    {
                        "name": "Kexin Guo"
                    },
                    {
                        "name": "Yingchao Zhang"
                    },
                    {
                        "name": "Huajian Gao"
                    },
                    {
                        "name": "K. Jimmy Hsia"
                    },
                    {
                        "name": "Mingchao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingchao Liu"
                },
                "author": "Mingchao Liu",
                "arxiv_comment": "87 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11409v1",
                "updated": "2025-04-15T17:26:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning"
                },
                "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier."
                },
                "authors": [
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Yashaswi Karnati"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v4",
                "updated": "2025-04-15T17:19:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    19,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "AI agents can coordinate beyond human scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents can coordinate beyond human scale"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in collaborative tasks\ninvolving multiple agents, forming an \"AI agent society: where agents interact\nand influence one another. Whether such groups can spontaneously coordinate on\narbitrary decisions without external influence - a hallmark of self-organized\nregulation in human societies - remains an open question. Here we investigate\nthe stability of groups formed by AI agents by applying methods from complexity\nscience and principles from behavioral sciences. We find that LLMs can\nspontaneously form cohesive groups, and that their opinion dynamics is governed\nby a majority force coefficient, which determines whether coordination is\nachievable. This majority force diminishes as group size increases, leading to\na critical group size beyond which coordination becomes practically\nunattainable and stability is lost. Notably, this critical group size grows\nexponentially with the language capabilities of the models, and for the most\nadvanced LLMs, it exceeds the typical size of informal human groups. Our\nfindings highlight intrinsic limitations in the self-organization of AI agent\nsocieties and have implications for the design of collaborative AI systems\nwhere coordination is desired or could represent a treat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in collaborative tasks\ninvolving multiple agents, forming an \"AI agent society: where agents interact\nand influence one another. Whether such groups can spontaneously coordinate on\narbitrary decisions without external influence - a hallmark of self-organized\nregulation in human societies - remains an open question. Here we investigate\nthe stability of groups formed by AI agents by applying methods from complexity\nscience and principles from behavioral sciences. We find that LLMs can\nspontaneously form cohesive groups, and that their opinion dynamics is governed\nby a majority force coefficient, which determines whether coordination is\nachievable. This majority force diminishes as group size increases, leading to\na critical group size beyond which coordination becomes practically\nunattainable and stability is lost. Notably, this critical group size grows\nexponentially with the language capabilities of the models, and for the most\nadvanced LLMs, it exceeds the typical size of informal human groups. Our\nfindings highlight intrinsic limitations in the self-organization of AI agent\nsocieties and have implications for the design of collaborative AI systems\nwhere coordination is desired or could represent a treat."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07491v2",
                "updated": "2025-04-15T17:14:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    14,
                    37,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-10T06:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Kimi-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-VL Technical Report"
                },
                "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chenlin Zhang"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Dikang Du"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Lin Sui"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Qizheng Gu"
                    },
                    {
                        "name": "Runjie Zhou"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianhui Song"
                    },
                    {
                        "name": "Tongtong Bai"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Xiaokun Yuan"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Yongsheng Kang"
                    },
                    {
                        "name": "Yuanxin Liu"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Zijia Zhao"
                    },
                    {
                        "name": "Ziwei Chen"
                    },
                    {
                        "name": "Zongyu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zongyu Lin"
                },
                "author": "Zongyu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11400v1",
                "updated": "2025-04-15T17:14:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    14,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T17:14:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    14,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "FlowUnits: Extending Dataflow for the Edge-to-Cloud Computing Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowUnits: Extending Dataflow for the Edge-to-Cloud Computing Continuum"
                },
                "summary": "This paper introduces FlowUnits, a novel programming and deployment model\nthat extends the traditional dataflow paradigm to address the unique challenges\nof edge-to-cloud computing environments. While conventional dataflow systems\noffer significant advantages for large-scale data processing in homogeneous\ncloud settings, they fall short when deployed across distributed, heterogeneous\ninfrastructures. FlowUnits addresses three critical limitations of current\napproaches: lack of locality awareness, insufficient resource adaptation, and\nabsence of dynamic update mechanisms. FlowUnits organize processing operators\ninto cohesive, independently manageable components that can be transparently\nreplicated across different regions, efficiently allocated on nodes with\nappropriate hardware capabilities, and dynamically updated without disrupting\nongoing computations. We implement and evaluate the FlowUnits model within\nRenoir, an existing dataflow system, demonstrating significant improvements in\ndeployment flexibility and resource utilization across the computing continuum.\nOur approach maintains the simplicity of dataflow while enabling seamless\nintegration of edge and cloud resources into unified data processing pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FlowUnits, a novel programming and deployment model\nthat extends the traditional dataflow paradigm to address the unique challenges\nof edge-to-cloud computing environments. While conventional dataflow systems\noffer significant advantages for large-scale data processing in homogeneous\ncloud settings, they fall short when deployed across distributed, heterogeneous\ninfrastructures. FlowUnits addresses three critical limitations of current\napproaches: lack of locality awareness, insufficient resource adaptation, and\nabsence of dynamic update mechanisms. FlowUnits organize processing operators\ninto cohesive, independently manageable components that can be transparently\nreplicated across different regions, efficiently allocated on nodes with\nappropriate hardware capabilities, and dynamically updated without disrupting\nongoing computations. We implement and evaluate the FlowUnits model within\nRenoir, an existing dataflow system, demonstrating significant improvements in\ndeployment flexibility and resource utilization across the computing continuum.\nOur approach maintains the simplicity of dataflow while enabling seamless\nintegration of edge and cloud resources into unified data processing pipelines."
                },
                "authors": [
                    {
                        "name": "Fabio Chini"
                    },
                    {
                        "name": "Luca De Martini"
                    },
                    {
                        "name": "Alessandro Margara"
                    },
                    {
                        "name": "Gianpaolo Cugola"
                    }
                ],
                "author_detail": {
                    "name": "Gianpaolo Cugola"
                },
                "author": "Gianpaolo Cugola",
                "arxiv_comment": "Preprint. Accepted at the 2nd Workshop on Engineering Techniques for\n  Distributed Computing Continuum Systems (EDCCS), co-located with IEEE ICDCS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19887v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19887v5",
                "updated": "2025-04-16T09:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    24,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-25T17:51:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "AI threats to national security can be countered through an incident\n  regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI threats to national security can be countered through an incident\n  regime"
                },
                "summary": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security."
                },
                "authors": [
                    {
                        "name": "Alejandro Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ortega"
                },
                "author": "Alejandro Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19887v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19887v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11381v1",
                "updated": "2025-04-15T16:53:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    53,
                    31,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:53:31Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    53,
                    31,
                    1,
                    105,
                    0
                ],
                "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankAlign: A Ranking View of the Generator-Validator Gap in Large\n  Language Models"
                },
                "summary": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items."
                },
                "authors": [
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Wenxuan Ding"
                    },
                    {
                        "name": "Katrin Erk"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11373v1",
                "updated": "2025-04-15T16:37:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    32,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:37:32Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    32,
                    1,
                    105,
                    0
                ],
                "title": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions"
                },
                "summary": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems."
                },
                "authors": [
                    {
                        "name": "Wang Bill Zhu"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Ching Ying Lin"
                    },
                    {
                        "name": "Jade Law"
                    },
                    {
                        "name": "Mazen Jizzini"
                    },
                    {
                        "name": "Jorge J. Nieva"
                    },
                    {
                        "name": "Ruishan Liu"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11372v1",
                "updated": "2025-04-15T16:37:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    23,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:37:23Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    37,
                    23,
                    1,
                    105,
                    0
                ],
                "title": "A Review of Traffic Wave Suppression Strategies: Variable Speed Limit\n  vs. Jam-Absorption Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Traffic Wave Suppression Strategies: Variable Speed Limit\n  vs. Jam-Absorption Driving"
                },
                "summary": "The main form of freeway traffic congestion is the familiar stop-and-go wave,\ncharacterized by wide moving jams that propagate indefinitely upstream provided\nenough traffic demand. They cause severe, long-lasting adverse effects, such as\nreduced traffic efficiency, increased driving risks, and higher vehicle\nemissions. This underscores the crucial importance of artificial intervention\nin the propagation of stop-and-go waves. Over the past two decades, two\nprominent strategies for stop-and-go wave suppression have emerged: variable\nspeed limit (VSL) and jam-absorption driving (JAD). Although they share similar\nresearch motivations, objectives, and theoretical foundations, the development\nof these strategies has remained relatively disconnected. To synthesize\nfragmented advances and drive the field forward, this paper first provides a\ncomprehensive review of the achievements in the stop-and-go wave\nsuppression-oriented VSL and JAD, respectively. It then focuses on bridging the\ntwo areas and identifying research opportunities from the following\nperspectives: fundamental diagrams, traffic dynamics modeling, traffic state\nestimation and prediction, stochasticity, scenarios for strategy validation,\nand field tests and practical deployment. We expect that through this review,\none area can effectively address its limitations by identifying and leveraging\nthe strengths of the other, thus promoting the overall research goal of freeway\nstop-and-go wave suppression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main form of freeway traffic congestion is the familiar stop-and-go wave,\ncharacterized by wide moving jams that propagate indefinitely upstream provided\nenough traffic demand. They cause severe, long-lasting adverse effects, such as\nreduced traffic efficiency, increased driving risks, and higher vehicle\nemissions. This underscores the crucial importance of artificial intervention\nin the propagation of stop-and-go waves. Over the past two decades, two\nprominent strategies for stop-and-go wave suppression have emerged: variable\nspeed limit (VSL) and jam-absorption driving (JAD). Although they share similar\nresearch motivations, objectives, and theoretical foundations, the development\nof these strategies has remained relatively disconnected. To synthesize\nfragmented advances and drive the field forward, this paper first provides a\ncomprehensive review of the achievements in the stop-and-go wave\nsuppression-oriented VSL and JAD, respectively. It then focuses on bridging the\ntwo areas and identifying research opportunities from the following\nperspectives: fundamental diagrams, traffic dynamics modeling, traffic state\nestimation and prediction, stochasticity, scenarios for strategy validation,\nand field tests and practical deployment. We expect that through this review,\none area can effectively address its limitations by identifying and leveraging\nthe strengths of the other, thus promoting the overall research goal of freeway\nstop-and-go wave suppression."
                },
                "authors": [
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Jorge Laval"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Ryosuke Nishi"
                    },
                    {
                        "name": "Cathy Wu"
                    }
                ],
                "author_detail": {
                    "name": "Cathy Wu"
                },
                "author": "Cathy Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11368v1",
                "updated": "2025-04-15T16:32:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    32,
                    15,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:32:15Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    32,
                    15,
                    1,
                    105,
                    0
                ],
                "title": "From Gaze to Insight: Bridging Human Visual Attention and Vision\n  Language Model Explanation for Weakly-Supervised Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Gaze to Insight: Bridging Human Visual Attention and Vision\n  Language Model Explanation for Weakly-Supervised Medical Image Segmentation"
                },
                "summary": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git."
                },
                "authors": [
                    {
                        "name": "Jingkun Chen"
                    },
                    {
                        "name": "Haoran Duan"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Vicente Grau"
                    },
                    {
                        "name": "Jungong Han"
                    }
                ],
                "author_detail": {
                    "name": "Jungong Han"
                },
                "author": "Jungong Han",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11358v1",
                "updated": "2025-04-15T16:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    26,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:26:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    26,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks"
                },
                "summary": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in IEEE Symposium on Security and Privacy, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11346v1",
                "updated": "2025-04-15T16:19:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    19,
                    7,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:19:07Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    19,
                    7,
                    1,
                    105,
                    0
                ],
                "title": "Seedream 3.0 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seedream 3.0 Technical Report"
                },
                "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality."
                },
                "authors": [
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Lixue Gong"
                    },
                    {
                        "name": "Qiushan Guo"
                    },
                    {
                        "name": "Xiaoxia Hou"
                    },
                    {
                        "name": "Zhichao Lai"
                    },
                    {
                        "name": "Fanshi Li"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Xiaochen Lian"
                    },
                    {
                        "name": "Chao Liao"
                    },
                    {
                        "name": "Liyang Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yichun Shi"
                    },
                    {
                        "name": "Shiqi Sun"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Zhi Tian"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Xuanda Wang"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Guofeng Wu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Zhonghua Zhai"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Shijia Zhao"
                    },
                    {
                        "name": "Jianchao Yang"
                    },
                    {
                        "name": "Weilin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weilin Huang"
                },
                "author": "Weilin Huang",
                "arxiv_comment": "Seedream 3.0 Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11343v1",
                "updated": "2025-04-15T16:15:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    15,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:15:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    15,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce"
                },
                "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Hanze Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hanze Dong"
                },
                "author": "Hanze Dong",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11338v1",
                "updated": "2025-04-15T16:12:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    12,
                    7,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:12:07Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    12,
                    7,
                    1,
                    105,
                    0
                ],
                "title": "Transformer-Based Model for Cold Start Mitigation in FaaS Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-Based Model for Cold Start Mitigation in FaaS Architecture"
                },
                "summary": "Serverless architectures, particularly the Function as a Service (FaaS)\nmodel, have become a cornerstone of modern cloud computing due to their ability\nto simplify resource management and enhance application deployment agility.\nHowever, a significant challenge remains: the cold start problem. This\nphenomenon occurs when an idle FaaS function is invoked, requiring a full\ninitialization process, which increases latency and degrades user experience.\nExisting solutions for cold start mitigation are limited in terms of invocation\npattern generalization and implementation complexity. In this study, we propose\nan innovative approach leveraging Transformer models to mitigate the impact of\ncold starts in FaaS architectures. Our solution excels in accurately modeling\nfunction initialization delays and optimizing serverless system performance.\nExperimental evaluation using a public dataset provided by Azure demonstrates a\nsignificant reduction in cold start times, reaching up to 79\\% compared to\nconventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless architectures, particularly the Function as a Service (FaaS)\nmodel, have become a cornerstone of modern cloud computing due to their ability\nto simplify resource management and enhance application deployment agility.\nHowever, a significant challenge remains: the cold start problem. This\nphenomenon occurs when an idle FaaS function is invoked, requiring a full\ninitialization process, which increases latency and degrades user experience.\nExisting solutions for cold start mitigation are limited in terms of invocation\npattern generalization and implementation complexity. In this study, we propose\nan innovative approach leveraging Transformer models to mitigate the impact of\ncold starts in FaaS architectures. Our solution excels in accurately modeling\nfunction initialization delays and optimizing serverless system performance.\nExperimental evaluation using a public dataset provided by Azure demonstrates a\nsignificant reduction in cold start times, reaching up to 79\\% compared to\nconventional methods."
                },
                "authors": [
                    {
                        "name": "Alexandre Savi Fayam Mbala Mouen"
                    },
                    {
                        "name": "Jerry Lacmou Zeutouo"
                    },
                    {
                        "name": "Vianney Kengne Tchendji"
                    }
                ],
                "author_detail": {
                    "name": "Vianney Kengne Tchendji"
                },
                "author": "Vianney Kengne Tchendji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11305v1",
                "updated": "2025-04-15T15:45:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    45,
                    59,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:45:59Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    45,
                    59,
                    1,
                    105,
                    0
                ],
                "title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable\n  Wood Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable\n  Wood Defect Detection"
                },
                "summary": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jincheng Kang"
                    },
                    {
                        "name": "Yi Cen"
                    },
                    {
                        "name": "Yigang Cen"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yuhan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Liu"
                },
                "author": "Yuhan Liu",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11301v1",
                "updated": "2025-04-15T15:44:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:44:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments."
                },
                "authors": [
                    {
                        "name": "Yangyang Zhuang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11300v1",
                "updated": "2025-04-15T15:43:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    43,
                    39,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:43:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    43,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Multi-Orbiter Continuous Lunar Beaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Orbiter Continuous Lunar Beaming"
                },
                "summary": "In this work, free-space optics-based continuous wireless power transmission\nbetween multiple low lunar orbit satellites and a solar panel on the lunar\nrover located at the lunar south pole are investigated based on the\ntime-varying harvested power and overall system efficiency metrics. The\nperformances are compared between a solar panel with the tracking ability and a\nfixed solar panel that induces \\textit{the cosine effect} due to the\ntime-dependent angle of incidence (AoI). In our work, the Systems Tool Kit\n(STK) high-precision orbit propagator, which calculates the ephemeris data\nprecisely, is utilized. Interestingly, orbiter deployments in constellations\nchange significantly during a Moon revolution; thus, short-duration simulations\ncannot be used straightforwardly. In our work, many satellite configurations\nare assessed to be able to find a Cislunar constellation that establishes a\ncontinuous line-of-sight (LoS) between the solar panel and at least a single\nLLO satellite. It is found that 40-satellite schemes enable the establishment\nof a continuous WPT system model. Besides, a satellite selection method (SSM)\nis introduced so that only the best LoS link among multiple simultaneous links\nfrom multiple satellites will be active for optimum efficiency. Our benchmark\nsystem of a 40-satellite quadruple orbit scheme is compared with 30-satellite\nand a single satellite schemes based on the average harvested powers and\noverall system efficiencies 27.3 days so the trade-off options can be assessed\nfrom the multiple Cislunar models. The outcomes show that the average system\nefficiencies of single, 30-satellite, and 40-satellite schemes are 2.84%,\n32.33%, and 33.29%, respectively, for the tracking panel and 0.97%, 18.33%, and\n20.44%, respectively, for the fixed solar panel case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, free-space optics-based continuous wireless power transmission\nbetween multiple low lunar orbit satellites and a solar panel on the lunar\nrover located at the lunar south pole are investigated based on the\ntime-varying harvested power and overall system efficiency metrics. The\nperformances are compared between a solar panel with the tracking ability and a\nfixed solar panel that induces \\textit{the cosine effect} due to the\ntime-dependent angle of incidence (AoI). In our work, the Systems Tool Kit\n(STK) high-precision orbit propagator, which calculates the ephemeris data\nprecisely, is utilized. Interestingly, orbiter deployments in constellations\nchange significantly during a Moon revolution; thus, short-duration simulations\ncannot be used straightforwardly. In our work, many satellite configurations\nare assessed to be able to find a Cislunar constellation that establishes a\ncontinuous line-of-sight (LoS) between the solar panel and at least a single\nLLO satellite. It is found that 40-satellite schemes enable the establishment\nof a continuous WPT system model. Besides, a satellite selection method (SSM)\nis introduced so that only the best LoS link among multiple simultaneous links\nfrom multiple satellites will be active for optimum efficiency. Our benchmark\nsystem of a 40-satellite quadruple orbit scheme is compared with 30-satellite\nand a single satellite schemes based on the average harvested powers and\noverall system efficiencies 27.3 days so the trade-off options can be assessed\nfrom the multiple Cislunar models. The outcomes show that the average system\nefficiencies of single, 30-satellite, and 40-satellite schemes are 2.84%,\n32.33%, and 33.29%, respectively, for the tracking panel and 0.97%, 18.33%, and\n20.44%, respectively, for the fixed solar panel case."
                },
                "authors": [
                    {
                        "name": "Baris Donmez"
                    },
                    {
                        "name": "Yanni Jiwan-Mercier"
                    },
                    {
                        "name": "Sebastien Loranger"
                    },
                    {
                        "name": "Gunes Karabulut Kurt"
                    }
                ],
                "author_detail": {
                    "name": "Gunes Karabulut Kurt"
                },
                "author": "Gunes Karabulut Kurt",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12144v2",
                "updated": "2025-04-15T15:42:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    42,
                    40,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-10T09:13:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    13,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models"
                },
                "summary": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings."
                },
                "authors": [
                    {
                        "name": "Chang-Jin Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Yun Tang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "arxiv_comment": "Submitted to Computers in Human Behavior Reports. 54 pages (main\n  text), 12 pages (appendix), and 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09727v2",
                "updated": "2025-04-15T15:38:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    38,
                    22,
                    1,
                    105,
                    0
                ],
                "published": "2024-01-18T05:06:39Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    5,
                    6,
                    39,
                    3,
                    18,
                    0
                ],
                "title": "Lateral Phishing With Large Language Models: A Large Organization\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lateral Phishing With Large Language Models: A Large Organization\n  Comparative Study"
                },
                "summary": "The emergence of Large Language Models (LLMs) has heightened the threat of\nphishing emails by enabling the generation of highly targeted, personalized,\nand automated attacks. Traditionally, many phishing emails have been\ncharacterized by typos, errors, and poor language. These errors can be\nmitigated by LLMs, potentially lowering the barrier for attackers. Despite\nthis, there is a lack of large-scale studies comparing the effectiveness of\nLLM-generated lateral phishing emails to those crafted by humans. Current\nliterature does not adequately address the comparative effectiveness of LLM and\nhuman-generated lateral phishing emails in a real-world, large-scale\norganizational setting, especially considering the potential for LLMs to\ngenerate more convincing and error-free phishing content. To address this gap,\nwe conducted a pioneering study within a large university, targeting its\nworkforce of approximately 9,000 individuals including faculty, staff,\nadministrators, and student workers. Our results indicate that LLM-generated\nlateral phishing emails are as effective as those written by communications\nprofessionals, emphasizing the critical threat posed by LLMs in leading\nphishing campaigns. We break down the results of the overall phishing\nexperiment, comparing vulnerability between departments and job roles.\nFurthermore, to gather qualitative data, we administered a detailed\nquestionnaire, revealing insights into the reasons and motivations behind\nvulnerable employee's actions. This study contributes to the understanding of\ncyber security threats in educational institutions and provides a comprehensive\ncomparison of LLM and human-generated phishing emails' effectiveness,\nconsidering the potential for LLMs to generate more convincing content. The\nfindings highlight the need for enhanced user education and system defenses to\nmitigate the growing threat of AI-powered phishing attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has heightened the threat of\nphishing emails by enabling the generation of highly targeted, personalized,\nand automated attacks. Traditionally, many phishing emails have been\ncharacterized by typos, errors, and poor language. These errors can be\nmitigated by LLMs, potentially lowering the barrier for attackers. Despite\nthis, there is a lack of large-scale studies comparing the effectiveness of\nLLM-generated lateral phishing emails to those crafted by humans. Current\nliterature does not adequately address the comparative effectiveness of LLM and\nhuman-generated lateral phishing emails in a real-world, large-scale\norganizational setting, especially considering the potential for LLMs to\ngenerate more convincing and error-free phishing content. To address this gap,\nwe conducted a pioneering study within a large university, targeting its\nworkforce of approximately 9,000 individuals including faculty, staff,\nadministrators, and student workers. Our results indicate that LLM-generated\nlateral phishing emails are as effective as those written by communications\nprofessionals, emphasizing the critical threat posed by LLMs in leading\nphishing campaigns. We break down the results of the overall phishing\nexperiment, comparing vulnerability between departments and job roles.\nFurthermore, to gather qualitative data, we administered a detailed\nquestionnaire, revealing insights into the reasons and motivations behind\nvulnerable employee's actions. This study contributes to the understanding of\ncyber security threats in educational institutions and provides a comprehensive\ncomparison of LLM and human-generated phishing emails' effectiveness,\nconsidering the potential for LLMs to generate more convincing content. The\nfindings highlight the need for enhanced user education and system defenses to\nmitigate the growing threat of AI-powered phishing attacks."
                },
                "authors": [
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Athanasios Galiopoulos"
                    },
                    {
                        "name": "Emet Bethany"
                    },
                    {
                        "name": "Mohammad Bahrami Karkevandi"
                    },
                    {
                        "name": "Nicole Beebe"
                    },
                    {
                        "name": "Nishant Vishwamitra"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "arxiv_doi": "10.1109/ACCESS.2025.3555500",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3555500",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.09727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in IEEE Access. This version includes\n  revisions following peer review",
                "arxiv_journal_ref": "IEEE Access, 13, 60684-60701",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01663v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01663v6",
                "updated": "2025-04-15T15:28:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    28,
                    28,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-02T06:07:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    6,
                    7,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models"
                },
                "summary": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "JingSong Yang"
                    }
                ],
                "author_detail": {
                    "name": "JingSong Yang"
                },
                "author": "JingSong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01663v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01663v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17807v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17807v5",
                "updated": "2025-04-15T15:28:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    28,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2024-06-23T11:58:26Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    11,
                    58,
                    26,
                    6,
                    175,
                    0
                ],
                "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary"
                },
                "summary": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yiling Tao"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Sun Li Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Sun Li Tianyu Shi"
                },
                "author": "Sun Li Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17807v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17807v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01089v2",
                "updated": "2025-04-15T15:24:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    24,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-02-03T06:18:29Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    18,
                    29,
                    0,
                    34,
                    0
                ],
                "title": "Advanced Architectures Integrated with Agentic AI for Next-Generation\n  Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Architectures Integrated with Agentic AI for Next-Generation\n  Wireless Networks"
                },
                "summary": "This paper investigates a range of cutting-edge technologies and\narchitectural innovations aimed at simplifying network operations, reducing\noperational expenditure (OpEx), and enabling the deployment of new service\nmodels. The focus is on (i) Proposing novel, more efficient 6G architectures,\nwith both Control and User planes enabling the seamless expansion of services,\nwhile addressing long-term 6G network evolution. (ii) Exploring advanced\ntechniques for constrained artificial intelligence (AI) operations,\nparticularly the design of AI agents for real-time learning, optimizing energy\nconsumption, and the allocation of computational resources. (iii) Identifying\ntechnologies and architectures that support the orchestration of backend\nservices using serverless computing models across multiple domains,\nparticularly for vertical industries. (iv) Introducing optically-based,\nultra-high-speed, low-latency network architectures, with fast optical\nswitching and real-time control, replacing conventional electronic switching to\nreduce power consumption by an order of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a range of cutting-edge technologies and\narchitectural innovations aimed at simplifying network operations, reducing\noperational expenditure (OpEx), and enabling the deployment of new service\nmodels. The focus is on (i) Proposing novel, more efficient 6G architectures,\nwith both Control and User planes enabling the seamless expansion of services,\nwhile addressing long-term 6G network evolution. (ii) Exploring advanced\ntechniques for constrained artificial intelligence (AI) operations,\nparticularly the design of AI agents for real-time learning, optimizing energy\nconsumption, and the allocation of computational resources. (iii) Identifying\ntechnologies and architectures that support the orchestration of backend\nservices using serverless computing models across multiple domains,\nparticularly for vertical industries. (iv) Introducing optically-based,\nultra-high-speed, low-latency network architectures, with fast optical\nswitching and real-time control, replacing conventional electronic switching to\nreduce power consumption by an order of magnitude."
                },
                "authors": [
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Sunder Ali Khowaja"
                    },
                    {
                        "name": "Keshav Singh"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "6 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11281v1",
                "updated": "2025-04-15T15:21:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    21,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:21:09Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    21,
                    9,
                    1,
                    105,
                    0
                ],
                "title": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\n  Fine-Print Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\n  Fine-Print Injections"
                },
                "summary": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Zhiping Zhang"
                    },
                    {
                        "name": "Bingcan Guo"
                    },
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Ibrahim Khalilov"
                    },
                    {
                        "name": "Simret A Gebreegziabher"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11277v1",
                "updated": "2025-04-15T15:16:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    16,
                    45,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T15:16:45Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    16,
                    45,
                    1,
                    105,
                    0
                ],
                "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs"
                },
                "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance."
                },
                "authors": [
                    {
                        "name": "Guocong Li"
                    },
                    {
                        "name": "Weize Liu"
                    },
                    {
                        "name": "Yihang Wu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Shuaihan Huang"
                    },
                    {
                        "name": "Hongxia Xu"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v3",
                "updated": "2025-04-15T15:09:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    9,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art education\n. Then we compare the answers given by DeepSeek-R1 in the seven aspects with\nthe answers given by o1-preview. DeepSeek-R1 performs well in the humanities\nand social sciences, answering most questions correctly and logically, and can\ngive reasonable analysis processes and explanations. Compared with o1-preview,\nit can automatically generate reasoning processes and provide more detailed\nexplanations, which is suitable for beginners or people who need to have a\ndetailed understanding of this knowledge, while o1-preview is more suitable for\nquick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art education\n. Then we compare the answers given by DeepSeek-R1 in the seven aspects with\nthe answers given by o1-preview. DeepSeek-R1 performs well in the humanities\nand social sciences, answering most questions correctly and logically, and can\ngive reasonable analysis processes and explanations. Compared with o1-preview,\nit can automatically generate reasoning processes and provide more detailed\nexplanations, which is suitable for beginners or people who need to have a\ndetailed understanding of this knowledge, while o1-preview is more suitable for\nquick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "52 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10356v2",
                "updated": "2025-04-15T15:02:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    2,
                    53,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T16:05:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages"
                },
                "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."
                },
                "authors": [
                    {
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "name": "Nikolay Bogoychev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Bogoychev"
                },
                "author": "Nikolay Bogoychev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11243v1",
                "updated": "2025-04-15T14:43:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    43,
                    19,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:43:19Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    43,
                    19,
                    1,
                    105,
                    0
                ],
                "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Safety Requirements Derivation Using Agent-based RAG"
                },
                "summary": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods."
                },
                "authors": [
                    {
                        "name": "Balahari Vignesh Balu"
                    },
                    {
                        "name": "Florian Geissler"
                    },
                    {
                        "name": "Francesco Carella"
                    },
                    {
                        "name": "Joao-Vitor Zacchi"
                    },
                    {
                        "name": "Josef Jiru"
                    },
                    {
                        "name": "Nuria Mata"
                    },
                    {
                        "name": "Reinhard Stolle"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard Stolle"
                },
                "author": "Reinhard Stolle",
                "arxiv_comment": "9 pages, 3 figures",
                "arxiv_journal_ref": "Proceedings of the AAAI-make Spring Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07995v2",
                "updated": "2025-04-15T14:41:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    41,
                    45,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-08T19:16:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    16,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "SafeChat: A Framework for Building Trustworthy Collaborative Assistants\n  and a Case Study of its Usefulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeChat: A Framework for Building Trustworthy Collaborative Assistants\n  and a Case Study of its Usefulness"
                },
                "summary": "Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot."
                },
                "authors": [
                    {
                        "name": "Biplav Srivastava"
                    },
                    {
                        "name": "Kausik Lakkaraju"
                    },
                    {
                        "name": "Nitin Gupta"
                    },
                    {
                        "name": "Vansh Nagpal"
                    },
                    {
                        "name": "Bharath C. Muppasani"
                    },
                    {
                        "name": "Sara E. Jones"
                    }
                ],
                "author_detail": {
                    "name": "Sara E. Jones"
                },
                "author": "Sara E. Jones",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11239v1",
                "updated": "2025-04-15T14:40:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    29,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:40:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling\n  Reasoning Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling\n  Reasoning Benchmark for LLMs"
                },
                "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI)."
                },
                "authors": [
                    {
                        "name": "Chang Yang"
                    },
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Junzhe Jiang"
                    },
                    {
                        "name": "Qi Jiang"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Yanchen Deng"
                    },
                    {
                        "name": "Shuxin Li"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Florian T. Pokorny"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Xinrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinrun Wang"
                },
                "author": "Xinrun Wang",
                "arxiv_comment": "Preliminary work, 10 pages for main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08897v2",
                "updated": "2025-04-15T14:40:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    40,
                    7,
                    1,
                    105,
                    0
                ],
                "published": "2025-01-15T16:06:10Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "title": "Automated Retrosynthesis Planning of Macromolecules Using Large Language\n  Models and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Retrosynthesis Planning of Macromolecules Using Large Language\n  Models and Knowledge Graphs"
                },
                "summary": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs. By leveraging LLMs' powerful capabilities for extracting and\nrecognizing chemical substance names, and storing the extracted data in a\nstructured knowledge graph, our system fully automates the retrieval of\nrelevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. By considering the complex interdependencies among chemical\nreactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is\nproposed to help identify all valid multi-branched reaction pathways, which\narise when a single product decomposes into multiple reaction intermediates. In\ncontrast, previous studies were limited to cases where a product decomposes\ninto at most one reaction intermediate. This work represents the first attempt\nto develop a fully automated retrosynthesis planning agent tailored specially\nfor macromolecules powered by LLMs. Applied to polyimide synthesis, our new\napproach constructs a retrosynthetic pathway tree with hundreds of pathways and\nrecommends optimized routes, including both known and novel pathways. This\ndemonstrates utilizing LLMs for literature consultation to accomplish specific\ntasks is possible and crucial for future materials research, given the vast\namount of materials-related literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs. By leveraging LLMs' powerful capabilities for extracting and\nrecognizing chemical substance names, and storing the extracted data in a\nstructured knowledge graph, our system fully automates the retrieval of\nrelevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. By considering the complex interdependencies among chemical\nreactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is\nproposed to help identify all valid multi-branched reaction pathways, which\narise when a single product decomposes into multiple reaction intermediates. In\ncontrast, previous studies were limited to cases where a product decomposes\ninto at most one reaction intermediate. This work represents the first attempt\nto develop a fully automated retrosynthesis planning agent tailored specially\nfor macromolecules powered by LLMs. Applied to polyimide synthesis, our new\napproach constructs a retrosynthetic pathway tree with hundreds of pathways and\nrecommends optimized routes, including both known and novel pathways. This\ndemonstrates utilizing LLMs for literature consultation to accomplish specific\ntasks is possible and crucial for future materials research, given the vast\namount of materials-related literature."
                },
                "authors": [
                    {
                        "name": "Qinyu Ma"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_doi": "10.1002/marc.202500065",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/marc.202500065",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.08897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The source code of RetroSynthesisAgent is available at\n  https://github.com/QinyuMa316/RetroSynthesisAgent",
                "arxiv_journal_ref": "Macromol. Rapid Commun. 2025, 2500065",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07355v3",
                "updated": "2025-04-15T14:38:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    38,
                    13,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-10T09:48:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models"
                },
                "summary": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology."
                },
                "authors": [
                    {
                        "name": "Andrea Caria"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caria"
                },
                "author": "Andrea Caria",
                "arxiv_comment": "needs major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11233v1",
                "updated": "2025-04-15T14:36:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:36:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    36,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "AutoRAN: Automated and Zero-Touch Open RAN Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRAN: Automated and Zero-Touch Open RAN Systems"
                },
                "summary": "[...] This paper presents AutoRAN, an automated, intent-driven framework for\nzero-touch provisioning of open, programmable cellular networks. Leveraging\ncloud-native principles, AutoRAN employs virtualization, declarative\ninfrastructure-as-code templates, and disaggregated micro-services to abstract\nphysical resources and protocol stacks. Its orchestration engine integrates\nLanguage Models (LLMs) to translate high-level intents into machine-readable\nconfigurations, enabling closed-loop control via telemetry-driven\nobservability. Implemented on a multi-architecture OpenShift cluster with\nheterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access\nNetwork (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of\nO-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS\ncore, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines.\nExperimental results demonstrate that AutoRAN is capable of deploying an\nend-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput,\nvalidating its ability to streamline configuration, accelerate testing, and\nreduce manual intervention with similar performance than non cloud-based\nimplementations. With its novel LLM-assisted intent translation mechanism, and\nperformance-optimized automation workflow for multi-vendor environments,\nAutoRAN has the potential of advancing the robustness of next-generation\ncellular supply chains through reproducible, intent-based provisioning across\npublic and private deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[...] This paper presents AutoRAN, an automated, intent-driven framework for\nzero-touch provisioning of open, programmable cellular networks. Leveraging\ncloud-native principles, AutoRAN employs virtualization, declarative\ninfrastructure-as-code templates, and disaggregated micro-services to abstract\nphysical resources and protocol stacks. Its orchestration engine integrates\nLanguage Models (LLMs) to translate high-level intents into machine-readable\nconfigurations, enabling closed-loop control via telemetry-driven\nobservability. Implemented on a multi-architecture OpenShift cluster with\nheterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access\nNetwork (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of\nO-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS\ncore, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines.\nExperimental results demonstrate that AutoRAN is capable of deploying an\nend-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput,\nvalidating its ability to streamline configuration, accelerate testing, and\nreduce manual intervention with similar performance than non cloud-based\nimplementations. With its novel LLM-assisted intent translation mechanism, and\nperformance-optimized automation workflow for multi-vendor environments,\nAutoRAN has the potential of advancing the robustness of next-generation\ncellular supply chains through reproducible, intent-based provisioning across\npublic and private deployments."
                },
                "authors": [
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Ravis Shirkhani"
                    },
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Leonardo Bonati"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Michele Polese"
                    }
                ],
                "author_detail": {
                    "name": "Michele Polese"
                },
                "author": "Michele Polese",
                "arxiv_comment": "17 pages, 15 figures, 6 listings, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19000v3",
                "updated": "2025-04-15T14:35:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    35,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2024-11-28T09:04:39Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    9,
                    4,
                    39,
                    3,
                    333,
                    0
                ],
                "title": "An AI-driven multimodal smart home platform for continuous monitoring\n  and intelligent assistance in post-stroke patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI-driven multimodal smart home platform for continuous monitoring\n  and intelligent assistance in post-stroke patients"
                },
                "summary": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Ruizhi Zhang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Zihe Zhao"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Junliang Chen"
                    },
                    {
                        "name": "Yanning Dai"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Ruoyu Juan"
                    },
                    {
                        "name": "Qiaoying Li"
                    },
                    {
                        "name": "Ruimou Xie"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Xinkai Zhou"
                    },
                    {
                        "name": "Yunjia Xia"
                    },
                    {
                        "name": "Jianan Chen"
                    },
                    {
                        "name": "Fanghao Lu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ninglli Wang"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Hubin Zhao"
                    },
                    {
                        "name": "Luigi G. Occhipinti"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipinti"
                },
                "author": "Luigi G. Occhipinti",
                "arxiv_comment": "5 figures, 41 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11230v1",
                "updated": "2025-04-15T14:30:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    30,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:30:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    30,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image"
                },
                "summary": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page."
                },
                "authors": [
                    {
                        "name": "Jingshun Huang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhu"
                },
                "author": "Yi Zhu",
                "arxiv_comment": "To appear in CVPR 2025 (Highlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11206v1",
                "updated": "2025-04-15T14:10:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    10,
                    5,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:10:05Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    10,
                    5,
                    1,
                    105,
                    0
                ],
                "title": "Investigation of optimal transfers to retrograde co-orbital orbits in\n  the Earth-Moon system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigation of optimal transfers to retrograde co-orbital orbits in\n  the Earth-Moon system"
                },
                "summary": "Recent findings on retrograde co-orbital mean-motion resonances in the\nEarth-Moon system, highlight the potential use of spacecraft in retrograde\nresonances. Based on these discoveries, this study investigates retrograde\nco-orbital resonances within the Earth-Moon system, focusing on both optimal\nand sub-optimal orbital transfers to such configurations. The paper provides a\ncomprehensive analysis of retrograde co-orbital resonances, optimization\ntechniques to evaluate and enhance the performance of bi-impulsive transfers to\nthese configurations. The results reveal the feasibility of low-cost transfers,\nwhich could support a range of future missions, including space exploration and\nsatellite deployment. Combining advanced optimization processes, we obtained\nsolutions for orbital transfers for different arrival points in retrograde\nco-orbitals improving mission efficiency and offering a cost-effective approach\nto interplanetary exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings on retrograde co-orbital mean-motion resonances in the\nEarth-Moon system, highlight the potential use of spacecraft in retrograde\nresonances. Based on these discoveries, this study investigates retrograde\nco-orbital resonances within the Earth-Moon system, focusing on both optimal\nand sub-optimal orbital transfers to such configurations. The paper provides a\ncomprehensive analysis of retrograde co-orbital resonances, optimization\ntechniques to evaluate and enhance the performance of bi-impulsive transfers to\nthese configurations. The results reveal the feasibility of low-cost transfers,\nwhich could support a range of future missions, including space exploration and\nsatellite deployment. Combining advanced optimization processes, we obtained\nsolutions for orbital transfers for different arrival points in retrograde\nco-orbitals improving mission efficiency and offering a cost-effective approach\nto interplanetary exploration."
                },
                "authors": [
                    {
                        "name": "G. A. Carita"
                    },
                    {
                        "name": "M. H. M. Morais"
                    },
                    {
                        "name": "S. Aljbaae"
                    },
                    {
                        "name": "A. F. B. A. Prado"
                    }
                ],
                "author_detail": {
                    "name": "A. F. B. A. Prado"
                },
                "author": "A. F. B. A. Prado",
                "arxiv_comment": "Accepted for publication in Astrophysics and Space Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v3",
                "updated": "2025-04-15T14:06:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    6,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11199v1",
                "updated": "2025-04-15T13:56:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    56,
                    14,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:56:14Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    56,
                    14,
                    1,
                    105,
                    0
                ],
                "title": "Video Summarization with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Summarization with Large Language Models"
                },
                "summary": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content."
                },
                "authors": [
                    {
                        "name": "Min Jung Lee"
                    },
                    {
                        "name": "Dayoung Gong"
                    },
                    {
                        "name": "Minsu Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsu Cho"
                },
                "author": "Minsu Cho",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09441v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09441v3",
                "updated": "2025-04-15T13:55:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    55,
                    47,
                    1,
                    105,
                    0
                ],
                "published": "2024-09-14T13:51:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    13,
                    51,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for\n  Quadrupedal Robot Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for\n  Quadrupedal Robot Locomotion"
                },
                "summary": "A core strength of Model Predictive Control (MPC) for quadrupedal locomotion\nhas been its ability to enforce constraints and provide interpretability of the\nsequence of commands over the horizon. However, despite being able to plan, MPC\nstruggles to scale with task complexity, often failing to achieve robust\nbehavior on rapidly changing surfaces. On the other hand, model-free\nReinforcement Learning (RL) methods have outperformed MPC on multiple terrains,\nshowing emergent motions but inherently lack any ability to handle constraints\nor perform planning. To address these limitations, we propose a framework that\nintegrates proprioceptive planning with RL, allowing for agile and safe\nlocomotion behaviors through the horizon. Inspired by MPC, we incorporate an\ninternal model that includes a velocity estimator and a Dreamer module. During\ntraining, the framework learns an expert policy and an internal model that are\nco-dependent, facilitating exploration for improved locomotion behaviors.\nDuring deployment, the Dreamer module solves an infinite-horizon MPC problem,\nadapting actions and velocity commands to respect the constraints. We validate\nthe robustness of our training framework through ablation studies on internal\nmodel components and demonstrate improved robustness to training noise.\nFinally, we evaluate our approach across multi-terrain scenarios in both\nsimulation and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core strength of Model Predictive Control (MPC) for quadrupedal locomotion\nhas been its ability to enforce constraints and provide interpretability of the\nsequence of commands over the horizon. However, despite being able to plan, MPC\nstruggles to scale with task complexity, often failing to achieve robust\nbehavior on rapidly changing surfaces. On the other hand, model-free\nReinforcement Learning (RL) methods have outperformed MPC on multiple terrains,\nshowing emergent motions but inherently lack any ability to handle constraints\nor perform planning. To address these limitations, we propose a framework that\nintegrates proprioceptive planning with RL, allowing for agile and safe\nlocomotion behaviors through the horizon. Inspired by MPC, we incorporate an\ninternal model that includes a velocity estimator and a Dreamer module. During\ntraining, the framework learns an expert policy and an internal model that are\nco-dependent, facilitating exploration for improved locomotion behaviors.\nDuring deployment, the Dreamer module solves an infinite-horizon MPC problem,\nadapting actions and velocity commands to respect the constraints. We validate\nthe robustness of our training framework through ablation studies on internal\nmodel components and demonstrate improved robustness to training noise.\nFinally, we evaluate our approach across multi-terrain scenarios in both\nsimulation and hardware."
                },
                "authors": [
                    {
                        "name": "Aditya Shirwatkar"
                    },
                    {
                        "name": "Naman Saxena"
                    },
                    {
                        "name": "Kishore Chandra"
                    },
                    {
                        "name": "Shishir Kolathaya"
                    }
                ],
                "author_detail": {
                    "name": "Shishir Kolathaya"
                },
                "author": "Shishir Kolathaya",
                "arxiv_comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09441v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11197v2",
                "updated": "2025-04-16T03:32:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    32,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    53,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance"
                },
                "summary": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead."
                },
                "authors": [
                    {
                        "name": "Shangyu Liu"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Xiaoyao Huang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Jie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Wu"
                },
                "author": "Jie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14488v2",
                "updated": "2025-04-15T13:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    49,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2025-01-24T13:42:00Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    42,
                    0,
                    4,
                    24,
                    0
                ],
                "title": "Breaking the Pre-Planning Barrier: Adaptive Real-Time Coordination of\n  Heterogeneous UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Pre-Planning Barrier: Adaptive Real-Time Coordination of\n  Heterogeneous UAVs"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) offer significant potential in dynamic,\nperception-intensive tasks such as search and rescue and environmental\nmonitoring; however, their effectiveness is severely restricted by conventional\npre-planned routing methods, which lack the flexibility to respond in real-time\nto evolving task demands, unexpected disturbances, and localized view\nlimitations in real-world scenarios. To address this fundamental limitation, we\nintroduce a novel multi-agent reinforcement learning framework named\n\\textbf{H}eterogeneous \\textbf{G}raph \\textbf{A}ttention \\textbf{M}ulti-agent\nDeep Deterministic Policy Gradient (HGAM), uniquely designed to enable adaptive\nreal-time coordination between mission UAVs (MUAVs) and charging UAVs (CUAVs).\nHGAM specifically addresses the previously unsolved challenge of enabling\nprecise, decentralized continuous-action coordination solely based on local,\nheterogeneous graph-based observations. Extensive simulations demonstrate that\nHGAM substantially surpasses existing methods, achieving, for example, a 30\\%\nimprovement in data collection coverage and a 20\\% increase in charging\nefficiency, providing crucial insights and foundations for the future\ndeployment of intelligent, flexible UAV networks in complex, dynamic\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) offer significant potential in dynamic,\nperception-intensive tasks such as search and rescue and environmental\nmonitoring; however, their effectiveness is severely restricted by conventional\npre-planned routing methods, which lack the flexibility to respond in real-time\nto evolving task demands, unexpected disturbances, and localized view\nlimitations in real-world scenarios. To address this fundamental limitation, we\nintroduce a novel multi-agent reinforcement learning framework named\n\\textbf{H}eterogeneous \\textbf{G}raph \\textbf{A}ttention \\textbf{M}ulti-agent\nDeep Deterministic Policy Gradient (HGAM), uniquely designed to enable adaptive\nreal-time coordination between mission UAVs (MUAVs) and charging UAVs (CUAVs).\nHGAM specifically addresses the previously unsolved challenge of enabling\nprecise, decentralized continuous-action coordination solely based on local,\nheterogeneous graph-based observations. Extensive simulations demonstrate that\nHGAM substantially surpasses existing methods, achieving, for example, a 30\\%\nimprovement in data collection coverage and a 20\\% increase in charging\nefficiency, providing crucial insights and foundations for the future\ndeployment of intelligent, flexible UAV networks in complex, dynamic\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11186v1",
                "updated": "2025-04-15T13:42:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    42,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:42:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    42,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in\n  Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in\n  Ophthalmology: A Head-to-Head Evaluation on 5,888 Items"
                },
                "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications."
                },
                "authors": [
                    {
                        "name": "Minjie Zou"
                    },
                    {
                        "name": "Sahana Srinivasan"
                    },
                    {
                        "name": "Thaddaeus Wai Soon Lo"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Gabriel Dawei Yang"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Maxwell Singer"
                    },
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "Kelvin Li"
                    },
                    {
                        "name": "Robert Chang"
                    },
                    {
                        "name": "Marcus Tan"
                    },
                    {
                        "name": "David Ziyou Chen"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih Chung Tham"
                    }
                ],
                "author_detail": {
                    "name": "Yih Chung Tham"
                },
                "author": "Yih Chung Tham",
                "arxiv_comment": "83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v5",
                "updated": "2025-04-15T13:38:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    38,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11182v1",
                "updated": "2025-04-15T13:37:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    37,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:37:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    37,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations"
                },
                "summary": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner."
                },
                "authors": [
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06543v2",
                "updated": "2025-04-15T13:36:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    36,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-09T14:54:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Challenges and Opportunities for Visual Analytics in Jurisprudence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Opportunities for Visual Analytics in Jurisprudence"
                },
                "summary": "Legal exploration, analysis, and interpretation remain complex and demanding\ntasks, even for experienced legal scholars, due to the domain-specific\nlanguage, tacit legal concepts, and intentional ambiguities embedded in legal\ntexts. In related, text-based domains, Visual Analytics (VA) and Large Language\nModels (LLMs) have become indispensable tools for navigating documents,\nrepresenting knowledge, and supporting analytical reasoning. However, legal\nscholarship presents distinct challenges: it requires managing formal legal\nstructure, drawing on tacit domain knowledge, and documenting intricate and\naccurate reasoning processes - needs that current VA systems designs and LLMs\nfail to address adequately. We identify previously unexamined key challenges\nand underexplored opportunities in applying VA to jurisprudence to explore how\nthese technologies might better serve the legal domain. Based on\nsemi-structured interviews with nine legal experts, we find a significant gap\nin tools and means that can externalize tacit legal knowledge in a form that is\nboth explicit and machine-interpretable. Hence, we propose leveraging\ninteractive visualization for this articulation, teaching the machine relevant\nsemantic relationships between legal documents that inform the predictions of\nLLMs, facilitating the enhanced navigation between hierarchies of legal\ncollections. This work introduces a user-centered VA workflow to the\njurisprudential context, recognizing tacit legal knowledge and expert\nexperience as vital components in deriving legal insight, comparing it with\nestablished practices in other text-based domains, and outlining a research\nagenda that offers future guidance for researchers in Visual Analytics for law\nand beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal exploration, analysis, and interpretation remain complex and demanding\ntasks, even for experienced legal scholars, due to the domain-specific\nlanguage, tacit legal concepts, and intentional ambiguities embedded in legal\ntexts. In related, text-based domains, Visual Analytics (VA) and Large Language\nModels (LLMs) have become indispensable tools for navigating documents,\nrepresenting knowledge, and supporting analytical reasoning. However, legal\nscholarship presents distinct challenges: it requires managing formal legal\nstructure, drawing on tacit domain knowledge, and documenting intricate and\naccurate reasoning processes - needs that current VA systems designs and LLMs\nfail to address adequately. We identify previously unexamined key challenges\nand underexplored opportunities in applying VA to jurisprudence to explore how\nthese technologies might better serve the legal domain. Based on\nsemi-structured interviews with nine legal experts, we find a significant gap\nin tools and means that can externalize tacit legal knowledge in a form that is\nboth explicit and machine-interpretable. Hence, we propose leveraging\ninteractive visualization for this articulation, teaching the machine relevant\nsemantic relationships between legal documents that inform the predictions of\nLLMs, facilitating the enhanced navigation between hierarchies of legal\ncollections. This work introduces a user-centered VA workflow to the\njurisprudential context, recognizing tacit legal knowledge and expert\nexperience as vital components in deriving legal insight, comparing it with\nestablished practices in other text-based domains, and outlining a research\nagenda that offers future guidance for researchers in Visual Analytics for law\nand beyond."
                },
                "authors": [
                    {
                        "name": "Daniel Fürst"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "39 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07014v3",
                "updated": "2025-04-15T13:31:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    31,
                    27,
                    1,
                    105,
                    0
                ],
                "published": "2024-07-09T16:33:43Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    33,
                    43,
                    1,
                    191,
                    0
                ],
                "title": "An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model\n  Integrated Cost Function for Optimizing SNN Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model\n  Integrated Cost Function for Optimizing SNN Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) emulate the spiking behavior of biological\nneurons and are typically deployed on distributed-memory neuromorphic hardware.\nThe deployment of a SNN usually requires partitioning the network and mapping\nthese partitions onto the hardware's processing units.\n  However, finding optimal deployment configurations is an NP-hard problem,\noften addressed through optimization algorithms. While some objectives (e.g.,\nmemory utilization and chip count) are static, others (e.g., communication\nlatency and energy efficiency) depend on the network's dynamic behavior,\nnecessitating dynamic-aware optimization.\n  To address this, we model SNN dynamics using an Ising-type pairwise\ninteraction framework, bridging microscopic neuron interactions with\nmacroscopic network behavior. We optimize deployment by exploring the parameter\nand configuration spaces of the Ising model.\n  We evaluate our approach on two SNNs deployed on the sPyNNaker neuromorphic\nplatform. Initial results suggest that the method underperforms, potentially\ndue to the Ising model's equilibrium assumptions and the architectural\ncomplexity of real-world neuromorphic hardware, highlighting limitations in its\ncurrent formulation.\n  Update: The method proposed is with a equilibrium-dynamics SNN assumption,\nand the original paper does not mention this. The paper needs to be revisited\nand reuploaded after further experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) emulate the spiking behavior of biological\nneurons and are typically deployed on distributed-memory neuromorphic hardware.\nThe deployment of a SNN usually requires partitioning the network and mapping\nthese partitions onto the hardware's processing units.\n  However, finding optimal deployment configurations is an NP-hard problem,\noften addressed through optimization algorithms. While some objectives (e.g.,\nmemory utilization and chip count) are static, others (e.g., communication\nlatency and energy efficiency) depend on the network's dynamic behavior,\nnecessitating dynamic-aware optimization.\n  To address this, we model SNN dynamics using an Ising-type pairwise\ninteraction framework, bridging microscopic neuron interactions with\nmacroscopic network behavior. We optimize deployment by exploring the parameter\nand configuration spaces of the Ising model.\n  We evaluate our approach on two SNNs deployed on the sPyNNaker neuromorphic\nplatform. Initial results suggest that the method underperforms, potentially\ndue to the Ising model's equilibrium assumptions and the architectural\ncomplexity of real-world neuromorphic hardware, highlighting limitations in its\ncurrent formulation.\n  Update: The method proposed is with a equilibrium-dynamics SNN assumption,\nand the original paper does not mention this. The paper needs to be revisited\nand reuploaded after further experiments."
                },
                "authors": [
                    {
                        "name": "Wanhong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wanhong Huang"
                },
                "author": "Wanhong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11169v1",
                "updated": "2025-04-15T13:16:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    46,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:16:46Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    46,
                    1,
                    105,
                    0
                ],
                "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos"
                },
                "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context."
                },
                "authors": [
                    {
                        "name": "Laura De Grazia"
                    },
                    {
                        "name": "Pol Pastells"
                    },
                    {
                        "name": "Mauro Vázquez Chas"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Danae Sánchez Villegas"
                    },
                    {
                        "name": "Mireia Farrús"
                    },
                    {
                        "name": "Mariona Taulé"
                    }
                ],
                "author_detail": {
                    "name": "Mariona Taulé"
                },
                "author": "Mariona Taulé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v1",
                "updated": "2025-04-15T13:16:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "12 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11163v1",
                "updated": "2025-04-15T13:11:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    11,
                    47,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:11:47Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    11,
                    47,
                    1,
                    105,
                    0
                ],
                "title": "The Robotability Score: Enabling Harmonious Robot Navigation on Urban\n  Streets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Robotability Score: Enabling Harmonious Robot Navigation on Urban\n  Streets"
                },
                "summary": "This paper introduces the Robotability Score ($R$), a novel metric that\nquantifies the suitability of urban environments for autonomous robot\nnavigation. Through expert interviews and surveys, we identify and weigh key\nfeatures contributing to R for wheeled robots on urban streets. Our findings\nreveal that pedestrian density, crowd dynamics and pedestrian flow are the most\ncritical factors, collectively accounting for 28% of the total score. Computing\nrobotability across New York City yields significant variation; the area of\nhighest R is 3.0 times more \"robotable\" than the area of lowest R. Deployments\nof a physical robot on high and low robotability areas show the adequacy of the\nscore in anticipating the ease of robot navigation. This new framework for\nevaluating urban landscapes aims to reduce uncertainty in robot deployment\nwhile respecting established mobility patterns and urban planning principles,\ncontributing to the discourse on harmonious human-robot environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Robotability Score ($R$), a novel metric that\nquantifies the suitability of urban environments for autonomous robot\nnavigation. Through expert interviews and surveys, we identify and weigh key\nfeatures contributing to R for wheeled robots on urban streets. Our findings\nreveal that pedestrian density, crowd dynamics and pedestrian flow are the most\ncritical factors, collectively accounting for 28% of the total score. Computing\nrobotability across New York City yields significant variation; the area of\nhighest R is 3.0 times more \"robotable\" than the area of lowest R. Deployments\nof a physical robot on high and low robotability areas show the adequacy of the\nscore in anticipating the ease of robot navigation. This new framework for\nevaluating urban landscapes aims to reduce uncertainty in robot deployment\nwhile respecting established mobility patterns and urban planning principles,\ncontributing to the discourse on harmonious human-robot environments."
                },
                "authors": [
                    {
                        "name": "Matt Franchi"
                    },
                    {
                        "name": "Maria Teresa Parreira"
                    },
                    {
                        "name": "Fanjun Bu"
                    },
                    {
                        "name": "Wendy Ju"
                    }
                ],
                "author_detail": {
                    "name": "Wendy Ju"
                },
                "author": "Wendy Ju",
                "arxiv_comment": "Accepted to CHI '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11162v1",
                "updated": "2025-04-15T13:11:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    11,
                    26,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T13:11:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    11,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "Scalable Transceiver Design for Multi-User Communication in FDD Massive\n  MIMO Systems via Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Transceiver Design for Multi-User Communication in FDD Massive\n  MIMO Systems via Deep Learning"
                },
                "summary": "This paper addresses the joint transceiver design, including pilot\ntransmission, channel feature extraction and feedback, as well as precoding,\nfor low-overhead downlink massive multiple-input multiple-output (MIMO)\ncommunication in frequency-division duplex (FDD) systems. Although deep\nlearning (DL) has shown great potential in tackling this problem, existing\nmethods often suffer from poor scalability in practical systems, as the\nsolution obtained in the training phase merely works for a fixed feedback\ncapacity and a fixed number of users in the deployment phase. To address this\nlimitation, we propose a novel DL-based framework comprised of choreographed\nneural networks, which can utilize one training phase to generate all the\ntransceiver solutions used in the deployment phase with varying sizes of\nfeedback codebooks and numbers of users. The proposed framework includes a\nresidual vector-quantized variational autoencoder (RVQ-VAE) for efficient\nchannel feedback and an edge graph attention network (EGAT) for robust\nmultiuser precoding. It can adapt to different feedback capacities by flexibly\nadjusting the RVQ codebook sizes using the hierarchical codebook structure, and\nscale with the number of users through a feedback module sharing scheme and the\ninherent scalability of EGAT. Moreover, a progressive training strategy is\nproposed to further enhance data transmission performance and generalization\ncapability. Numerical results on a real-world dataset demonstrate the superior\nscalability and performance of our approach over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the joint transceiver design, including pilot\ntransmission, channel feature extraction and feedback, as well as precoding,\nfor low-overhead downlink massive multiple-input multiple-output (MIMO)\ncommunication in frequency-division duplex (FDD) systems. Although deep\nlearning (DL) has shown great potential in tackling this problem, existing\nmethods often suffer from poor scalability in practical systems, as the\nsolution obtained in the training phase merely works for a fixed feedback\ncapacity and a fixed number of users in the deployment phase. To address this\nlimitation, we propose a novel DL-based framework comprised of choreographed\nneural networks, which can utilize one training phase to generate all the\ntransceiver solutions used in the deployment phase with varying sizes of\nfeedback codebooks and numbers of users. The proposed framework includes a\nresidual vector-quantized variational autoencoder (RVQ-VAE) for efficient\nchannel feedback and an edge graph attention network (EGAT) for robust\nmultiuser precoding. It can adapt to different feedback capacities by flexibly\nadjusting the RVQ codebook sizes using the hierarchical codebook structure, and\nscale with the number of users through a feedback module sharing scheme and the\ninherent scalability of EGAT. Moreover, a progressive training strategy is\nproposed to further enhance data transmission performance and generalization\ncapability. Numerical results on a real-world dataset demonstrate the superior\nscalability and performance of our approach over existing methods."
                },
                "authors": [
                    {
                        "name": "Lin Zhu"
                    },
                    {
                        "name": "Weifeng Zhu"
                    },
                    {
                        "name": "Shuowen Zhang"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Liang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Liu"
                },
                "author": "Liang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11109v1",
                "updated": "2025-04-15T11:56:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:56:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Fine-Tuning Large Language Models on Quantum Optimization Problems for\n  Circuit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Language Models on Quantum Optimization Problems for\n  Circuit Generation"
                },
                "summary": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware."
                },
                "authors": [
                    {
                        "name": "Linus Jern"
                    },
                    {
                        "name": "Valter Uotila"
                    },
                    {
                        "name": "Cong Yu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "12 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11108v1",
                "updated": "2025-04-15T11:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Vision Language Models on German Factual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vision Language Models on German Factual Data"
                },
                "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."
                },
                "authors": [
                    {
                        "name": "René Peinl"
                    },
                    {
                        "name": "Vincent Tischler"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Tischler"
                },
                "author": "Vincent Tischler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11104v1",
                "updated": "2025-04-15T11:52:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    52,
                    20,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:52:20Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    52,
                    20,
                    1,
                    105,
                    0
                ],
                "title": "Using LLMs as prompt modifier to avoid biases in AI image generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs as prompt modifier to avoid biases in AI image generators"
                },
                "summary": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/"
                },
                "authors": [
                    {
                        "name": "René Peinl"
                    }
                ],
                "author_detail": {
                    "name": "René Peinl"
                },
                "author": "René Peinl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11101v2",
                "updated": "2025-04-16T03:22:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    22,
                    14,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T11:51:18Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    51,
                    18,
                    1,
                    105,
                    0
                ],
                "title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR"
                },
                "summary": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout."
                },
                "authors": [
                    {
                        "name": "Yulong Zhang"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Xinyue Huang"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09378v2",
                "updated": "2025-04-15T11:49:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    49,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-13T00:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    0,
                    1,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs"
                },
                "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success."
                },
                "authors": [
                    {
                        "name": "Kartik Ravisankar"
                    },
                    {
                        "name": "Hyojung Han"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11094v1",
                "updated": "2025-04-15T11:40:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "title": "Evaluation Report on MCP Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation Report on MCP Servers"
                },
                "summary": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions."
                },
                "authors": [
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xuanrui Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Gao"
                },
                "author": "Jinyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15756v2",
                "updated": "2025-04-15T11:39:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    39,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2024-10-21T08:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    8,
                    15,
                    45,
                    0,
                    295,
                    0
                ],
                "title": "Automated Proof Generation for Rust Code via Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Proof Generation for Rust Code via Self-Evolution"
                },
                "summary": "Ensuring correctness is crucial for code generation. Formal verification\noffers a definitive assurance of correctness, but demands substantial human\neffort in proof construction and hence raises a pressing need for automation.\nThe primary obstacle lies in the severe lack of data-there is much fewer proofs\nthan code snippets for Large Language Models (LLMs) to train upon. In this\npaper, we introduce SAFE, a framework that overcomes the lack of human-written\nproofs to enable automated proof generation of Rust code. SAFE establishes a\nself-evolving cycle where data synthesis and fine-tuning collaborate to enhance\nthe model capability, leveraging the definitive power of a symbolic verifier in\ntelling correct proofs from incorrect ones. SAFE also re-purposes the large\nnumber of synthesized incorrect proofs to train the self-debugging capability\nof the fine-tuned models, empowering them to fix incorrect proofs based on the\nverifier's feedback. SAFE demonstrates superior efficiency and precision\ncompared to GPT-4o. Through tens of thousands of synthesized proofs and the\nself-debugging mechanism, we improve the capability of open-source models,\ninitially unacquainted with formal verification, to automatically write proofs\nfor Rust code. This advancement leads to a significant improvement in\nperformance, achieving a 52.52% accuracy rate in a benchmark crafted by human\nexperts, a significant leap over GPT-4o's performance of 14.39%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring correctness is crucial for code generation. Formal verification\noffers a definitive assurance of correctness, but demands substantial human\neffort in proof construction and hence raises a pressing need for automation.\nThe primary obstacle lies in the severe lack of data-there is much fewer proofs\nthan code snippets for Large Language Models (LLMs) to train upon. In this\npaper, we introduce SAFE, a framework that overcomes the lack of human-written\nproofs to enable automated proof generation of Rust code. SAFE establishes a\nself-evolving cycle where data synthesis and fine-tuning collaborate to enhance\nthe model capability, leveraging the definitive power of a symbolic verifier in\ntelling correct proofs from incorrect ones. SAFE also re-purposes the large\nnumber of synthesized incorrect proofs to train the self-debugging capability\nof the fine-tuned models, empowering them to fix incorrect proofs based on the\nverifier's feedback. SAFE demonstrates superior efficiency and precision\ncompared to GPT-4o. Through tens of thousands of synthesized proofs and the\nself-debugging mechanism, we improve the capability of open-source models,\ninitially unacquainted with formal verification, to automatically write proofs\nfor Rust code. This advancement leads to a significant improvement in\nperformance, achieving a 52.52% accuracy rate in a benchmark crafted by human\nexperts, a significant leap over GPT-4o's performance of 14.39%."
                },
                "authors": [
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Md Rakib Hossain Misu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Nan Duan"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Shuvendu K Lahiri"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Lidong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Zhou"
                },
                "author": "Lidong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11082v1",
                "updated": "2025-04-15T11:28:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    28,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:28:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    28,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "DeepMLF: Multimodal language model with learnable tokens for deep fusion\n  in sentiment analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMLF: Multimodal language model with learnable tokens for deep fusion\n  in sentiment analysis"
                },
                "summary": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization."
                },
                "authors": [
                    {
                        "name": "Efthymios Georgiou"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    },
                    {
                        "name": "Yannis Avrithis"
                    },
                    {
                        "name": "Alexandros Potamianos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Potamianos"
                },
                "author": "Alexandros Potamianos",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11079v1",
                "updated": "2025-04-15T11:24:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    24,
                    43,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:24:43Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    24,
                    43,
                    1,
                    105,
                    0
                ],
                "title": "Scalability and Maintainability Challenges and Solutions in Machine\n  Learning: Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalability and Maintainability Challenges and Solutions in Machine\n  Learning: Systematic Literature Review"
                },
                "summary": "This systematic literature review examines the critical challenges and\nsolutions related to scalability and maintainability in Machine Learning (ML)\nsystems. As ML applications become increasingly complex and widespread across\nindustries, the need to balance system scalability with long-term\nmaintainability has emerged as a significant concern. This review synthesizes\ncurrent research and practices addressing these dual challenges across the\nentire ML life-cycle, from data engineering to model deployment in production.\nWe analyzed 124 papers to identify and categorize 41 maintainability challenges\nand 13 scalability challenges, along with their corresponding solutions. Our\nfindings reveal intricate inter dependencies between scalability and\nmaintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining\nmaintainability and scalability challenges in data engineering, model\nengineering, and ML system development. We explore how these challenges\nmanifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and\npractitioners in the field of ML systems. It aims to guide future research\ndirections, inform best practices, and contribute to the development of more\nrobust, efficient, and sustainable ML applications across various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This systematic literature review examines the critical challenges and\nsolutions related to scalability and maintainability in Machine Learning (ML)\nsystems. As ML applications become increasingly complex and widespread across\nindustries, the need to balance system scalability with long-term\nmaintainability has emerged as a significant concern. This review synthesizes\ncurrent research and practices addressing these dual challenges across the\nentire ML life-cycle, from data engineering to model deployment in production.\nWe analyzed 124 papers to identify and categorize 41 maintainability challenges\nand 13 scalability challenges, along with their corresponding solutions. Our\nfindings reveal intricate inter dependencies between scalability and\nmaintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining\nmaintainability and scalability challenges in data engineering, model\nengineering, and ML system development. We explore how these challenges\nmanifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and\npractitioners in the field of ML systems. It aims to guide future research\ndirections, inform best practices, and contribute to the development of more\nrobust, efficient, and sustainable ML applications across various domains."
                },
                "authors": [
                    {
                        "name": "Karthik Shivashankar"
                    },
                    {
                        "name": "Ghadi S. Al Hajj"
                    },
                    {
                        "name": "Antonio Martini"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Martini"
                },
                "author": "Antonio Martini",
                "arxiv_comment": "Minor Revision ACM Computing Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04009v2",
                "updated": "2025-04-15T11:12:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    12,
                    9,
                    1,
                    105,
                    0
                ],
                "published": "2025-02-06T12:11:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    11,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "A Critical Analysis of Deployed Use Cases for Quantum Key Distribution\n  and Comparison with Post-Quantum Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Analysis of Deployed Use Cases for Quantum Key Distribution\n  and Comparison with Post-Quantum Cryptography"
                },
                "summary": "Quantum Key Distribution (QKD) is currently being discussed as a technology\nto safeguard communication in a future where quantum computers compromise\ntraditional public-key cryptosystems. In this paper, we conduct a comprehensive\nsecurity evaluation of QKD-based solutions, focusing on real-world use cases\nsourced from academic literature and industry reports. We analyze these use\ncases, assess their security and identify the possible advantages of deploying\nQKD-based solutions. We further compare QKD-based solutions with Post-Quantum\nCryptography (PQC), the alternative approach to achieving security when quantum\ncomputers compromise traditional public-key cryptosystems, evaluating their\nrespective suitability for each scenario. Based on this comparative analysis,\nwe critically discuss and comment on which use cases QKD is suited for,\nconsidering factors such as implementation complexity, scalability, and\nlong-term security. Our findings contribute to a better understanding of the\nrole QKD could play in future cryptographic infrastructures and offer guidance\nto decision-makers considering the deployment of QKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) is currently being discussed as a technology\nto safeguard communication in a future where quantum computers compromise\ntraditional public-key cryptosystems. In this paper, we conduct a comprehensive\nsecurity evaluation of QKD-based solutions, focusing on real-world use cases\nsourced from academic literature and industry reports. We analyze these use\ncases, assess their security and identify the possible advantages of deploying\nQKD-based solutions. We further compare QKD-based solutions with Post-Quantum\nCryptography (PQC), the alternative approach to achieving security when quantum\ncomputers compromise traditional public-key cryptosystems, evaluating their\nrespective suitability for each scenario. Based on this comparative analysis,\nwe critically discuss and comment on which use cases QKD is suited for,\nconsidering factors such as implementation complexity, scalability, and\nlong-term security. Our findings contribute to a better understanding of the\nrole QKD could play in future cryptographic infrastructures and offer guidance\nto decision-makers considering the deployment of QKD."
                },
                "authors": [
                    {
                        "name": "Nick Aquina"
                    },
                    {
                        "name": "Bruno Cimoli"
                    },
                    {
                        "name": "Soumya Das"
                    },
                    {
                        "name": "Kathrin Hövelmanns"
                    },
                    {
                        "name": "Fiona Johanna Weber"
                    },
                    {
                        "name": "Chigo Okonkwo"
                    },
                    {
                        "name": "Simon Rommel"
                    },
                    {
                        "name": "Boris Škorić"
                    },
                    {
                        "name": "Idelfonso Tafur Monroy"
                    },
                    {
                        "name": "Sebastian Verschoor"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Verschoor"
                },
                "author": "Sebastian Verschoor",
                "arxiv_comment": "Accepted for publication in EPJ Quantum Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10190v2",
                "updated": "2025-04-15T10:59:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    59,
                    35,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T12:50:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    50,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "Differentially Private 2D Human Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private 2D Human Pose Estimation"
                },
                "summary": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications."
                },
                "authors": [
                    {
                        "name": "Kaushik Bhargav Sivangi"
                    },
                    {
                        "name": "Idris Zakariyya"
                    },
                    {
                        "name": "Paul Henderson"
                    },
                    {
                        "name": "Fani Deligianni"
                    }
                ],
                "author_detail": {
                    "name": "Fani Deligianni"
                },
                "author": "Fani Deligianni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11050v1",
                "updated": "2025-04-15T10:34:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    34,
                    23,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T10:34:23Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    34,
                    23,
                    1,
                    105,
                    0
                ],
                "title": "Leveraging LLMs and attention-mechanism for automatic annotation of\n  historical maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs and attention-mechanism for automatic annotation of\n  historical maps"
                },
                "summary": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis."
                },
                "authors": [
                    {
                        "name": "Yunshuang Yuan"
                    },
                    {
                        "name": "Monika Sester"
                    }
                ],
                "author_detail": {
                    "name": "Monika Sester"
                },
                "author": "Monika Sester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11042v1",
                "updated": "2025-04-15T10:07:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    7,
                    33,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T10:07:33Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    10,
                    7,
                    33,
                    1,
                    105,
                    0
                ],
                "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews"
                },
                "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)"
                },
                "authors": [
                    {
                        "name": "Sukannya Purkayastha"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "29 pages, 18 Figures, 15 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11009v1",
                "updated": "2025-04-15T09:29:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    29,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:29:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    29,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal\n  Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal\n  Critique"
                },
                "summary": "Visual language models (VLMs) have demonstrated strong performance across\ndiverse multimodal reasoning tasks but still face challenges such as\nhallucinations, resulting in incorrect reasoning outcomes. Inspired by recent\nresearch on external feedback mechanisms in large language models (LLMs), we\npropose a multimodal actor-critic framework to enhance VLM reasoning\ncapabilities. Specifically, the actor model generates step-by-step reasoning\npaths based on image and text inputs, while the critic model evaluates these\nreasoning paths and provides corrective feedback. The actor model iteratively\nrefines its reasoning based on the feedback until the reasoning outcome is\ndeemed satisfactory by the critic model. To reduce reliance on costly manual\nannotations, we introduce an automated method for constructing multimodal\ncritique datasets. By leveraging Monte Carlo Tree Search (MCTS), we\nsystematically guide the actor model to explore diverse reasoning paths. To\nobtain critique data for correcting erroneous reasoning steps, we prompt an\nannotator model to compare pairs of reasoning paths diverging from a shared\nancestor node - one leading to a correct conclusion and the other to an\nincorrect one. This approach enables us to construct the MMC (MCTS-based\nMultimodal Critique) dataset, upon which we further develop a comprehensive\ntraining and inference pipeline. Extensive experiments conducted on several\npublic benchmark datasets and mainstream VLMs demonstrate that our approach\nsignificantly improves the performance of VLM on complex multimodal reasoning\ntasks, underscoring its effectiveness and wide applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language models (VLMs) have demonstrated strong performance across\ndiverse multimodal reasoning tasks but still face challenges such as\nhallucinations, resulting in incorrect reasoning outcomes. Inspired by recent\nresearch on external feedback mechanisms in large language models (LLMs), we\npropose a multimodal actor-critic framework to enhance VLM reasoning\ncapabilities. Specifically, the actor model generates step-by-step reasoning\npaths based on image and text inputs, while the critic model evaluates these\nreasoning paths and provides corrective feedback. The actor model iteratively\nrefines its reasoning based on the feedback until the reasoning outcome is\ndeemed satisfactory by the critic model. To reduce reliance on costly manual\nannotations, we introduce an automated method for constructing multimodal\ncritique datasets. By leveraging Monte Carlo Tree Search (MCTS), we\nsystematically guide the actor model to explore diverse reasoning paths. To\nobtain critique data for correcting erroneous reasoning steps, we prompt an\nannotator model to compare pairs of reasoning paths diverging from a shared\nancestor node - one leading to a correct conclusion and the other to an\nincorrect one. This approach enables us to construct the MMC (MCTS-based\nMultimodal Critique) dataset, upon which we further develop a comprehensive\ntraining and inference pipeline. Extensive experiments conducted on several\npublic benchmark datasets and mainstream VLMs demonstrate that our approach\nsignificantly improves the performance of VLM on complex multimodal reasoning\ntasks, underscoring its effectiveness and wide applicability."
                },
                "authors": [
                    {
                        "name": "Shuhang Liu"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    },
                    {
                        "name": "Feng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ma"
                },
                "author": "Feng Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03784v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03784v3",
                "updated": "2025-04-15T09:29:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    29,
                    6,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-03T16:16:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    16,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset."
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Francesco Quinzan"
                    },
                    {
                        "name": "Chengchung Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchung Shi"
                },
                "author": "Chengchung Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03784v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03784v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11007v1",
                "updated": "2025-04-15T09:26:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    26,
                    8,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:26:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    26,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network\n  Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network\n  Costs"
                },
                "summary": "Modern cloud-native applications increasingly utilise managed cloud services\nand containerisation technologies, such as Kubernetes, to achieve rapid\ntime-to-market and scalable deployments. Organisations must consider various\nfactors, including cost implications when deciding on a hosting platform for\ncontainerised applications as the usage grows. An emerging discipline called\nFinOps combines financial management and cloud operations to optimise costs in\ncloud-based applications. While prior research has explored system-level\noptimisation strategies for cost and resource efficiency in containerized\nsystems, analysing network costs in Kubernetes clusters remains underexplored.\nThis paper investigates the network usage and cost implications of\ncontainerised applications running on Kubernetes clusters. Using a methodology\nthat combines measurement analysis, experimentation, and cost modelling, we aim\nto provide organisations with actionable insights into network cost\noptimisation. Our findings highlight key considerations for analysing network\nexpenditures and evaluating the potential cost benefits of deploying\napplications on cloud providers. Overall, this paper contributes to the\nemerging FinOps discipline by addressing the financial and operational aspects\nof managing network costs in cloud-native environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cloud-native applications increasingly utilise managed cloud services\nand containerisation technologies, such as Kubernetes, to achieve rapid\ntime-to-market and scalable deployments. Organisations must consider various\nfactors, including cost implications when deciding on a hosting platform for\ncontainerised applications as the usage grows. An emerging discipline called\nFinOps combines financial management and cloud operations to optimise costs in\ncloud-based applications. While prior research has explored system-level\noptimisation strategies for cost and resource efficiency in containerized\nsystems, analysing network costs in Kubernetes clusters remains underexplored.\nThis paper investigates the network usage and cost implications of\ncontainerised applications running on Kubernetes clusters. Using a methodology\nthat combines measurement analysis, experimentation, and cost modelling, we aim\nto provide organisations with actionable insights into network cost\noptimisation. Our findings highlight key considerations for analysing network\nexpenditures and evaluating the potential cost benefits of deploying\napplications on cloud providers. Overall, this paper contributes to the\nemerging FinOps discipline by addressing the financial and operational aspects\nof managing network costs in cloud-native environments."
                },
                "authors": [
                    {
                        "name": "Rodrigo Mompo Redoli"
                    },
                    {
                        "name": "Amjad Ullah"
                    }
                ],
                "author_detail": {
                    "name": "Amjad Ullah"
                },
                "author": "Amjad Ullah",
                "arxiv_comment": "Paper accepted in the 39th International Conference on Advanced\n  Information Networking and Applications (AINA-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11004v1",
                "updated": "2025-04-15T09:20:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    20,
                    45,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:20:45Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    20,
                    45,
                    1,
                    105,
                    0
                ],
                "title": "Dynamic Compressing Prompts for Efficient Inference of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Compressing Prompts for Efficient Inference of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP."
                },
                "authors": [
                    {
                        "name": "Jinwu Hu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Yu Hu"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Qing Du"
                    }
                ],
                "author_detail": {
                    "name": "Qing Du"
                },
                "author": "Qing Du",
                "arxiv_comment": "Under review (submited in 2024.11)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11001v1",
                "updated": "2025-04-15T09:18:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    18,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T09:18:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    18,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReZero: Enhancing LLM search ability by trying one-more-time"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Thinh Le"
                    }
                ],
                "author_detail": {
                    "name": "Thinh Le"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Thinh Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13212v3",
                "updated": "2025-04-15T09:11:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    9,
                    11,
                    18,
                    1,
                    105,
                    0
                ],
                "published": "2024-11-20T11:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation"
                },
                "summary": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment."
                },
                "authors": [
                    {
                        "name": "David Otero"
                    },
                    {
                        "name": "Javier Parapar"
                    },
                    {
                        "name": "Álvaro Barreiro"
                    }
                ],
                "author_detail": {
                    "name": "Álvaro Barreiro"
                },
                "author": "Álvaro Barreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10987v1",
                "updated": "2025-04-15T08:59:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    59,
                    3,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T08:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    59,
                    3,
                    1,
                    105,
                    0
                ],
                "title": "Leveraging Vertical Public-Private Split for Improved Synthetic Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Vertical Public-Private Split for Improved Synthetic Data\n  Generation"
                },
                "summary": "Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of\nprivate and secure tabular-data sharing, producing artificial data that carries\nthrough the underlying statistical properties of the input data. This typically\ninvolves adding carefully calibrated statistical noise to guarantee individual\nprivacy, at the cost of synthetic data quality. Recent literature has explored\nscenarios where a small amount of public data is used to help enhance the\nquality of synthetic data. These methods study a horizontal public-private\npartitioning which assumes access to a small number of public rows that can be\nused for model initialization, providing a small utility gain. However,\nrealistic datasets often naturally consist of public and private attributes,\nmaking a vertical public-private partitioning relevant for practical synthetic\ndata deployments. We propose a novel framework that adapts horizontal\npublic-assisted methods into the vertical setting. We compare this framework\nagainst our alternative approach that uses conditional generation, highlighting\ninitial limitations of public-data assisted methods and proposing future\nresearch directions to address these challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of\nprivate and secure tabular-data sharing, producing artificial data that carries\nthrough the underlying statistical properties of the input data. This typically\ninvolves adding carefully calibrated statistical noise to guarantee individual\nprivacy, at the cost of synthetic data quality. Recent literature has explored\nscenarios where a small amount of public data is used to help enhance the\nquality of synthetic data. These methods study a horizontal public-private\npartitioning which assumes access to a small number of public rows that can be\nused for model initialization, providing a small utility gain. However,\nrealistic datasets often naturally consist of public and private attributes,\nmaking a vertical public-private partitioning relevant for practical synthetic\ndata deployments. We propose a novel framework that adapts horizontal\npublic-assisted methods into the vertical setting. We compare this framework\nagainst our alternative approach that uses conditional generation, highlighting\ninitial limitations of public-data assisted methods and proposing future\nresearch directions to address these challenges."
                },
                "authors": [
                    {
                        "name": "Samuel Maddock"
                    },
                    {
                        "name": "Shripad Gade"
                    },
                    {
                        "name": "Graham Cormode"
                    },
                    {
                        "name": "Will Bullock"
                    }
                ],
                "author_detail": {
                    "name": "Will Bullock"
                },
                "author": "Will Bullock",
                "arxiv_comment": "Accepted to the Synthetic Data x Data Access Problem (SynthData)\n  workshop @ ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07157v3",
                "updated": "2025-04-16T09:41:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    41,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-09T11:19:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance."
                },
                "authors": [
                    {
                        "name": "Xavier Sécheresse"
                    },
                    {
                        "name": "Jacques-Yves Guilbert--Ly"
                    },
                    {
                        "name": "Antoine Villedieu de Torcy"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Villedieu de Torcy"
                },
                "author": "Antoine Villedieu de Torcy",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19204v2",
                "updated": "2025-04-15T08:51:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    51,
                    16,
                    1,
                    105,
                    0
                ],
                "published": "2024-07-27T08:14:18Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    14,
                    18,
                    5,
                    209,
                    0
                ],
                "title": "Towards the Terminator Economy: Assessing Job Exposure to AI through\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Terminator Economy: Assessing Job Exposure to AI through\n  LLMs"
                },
                "summary": "AI and related technologies are reshaping jobs and tasks, either by\nautomating or augmenting human skills in the workplace. Many researchers have\nbeen working on estimating if and to what extent jobs and tasks are exposed to\nthe risk of being automatized by AI-related technologies. Our work tackles this\nissue through a data-driven approach by: (i) developing a reproducible\nframework that uses cutting-edge open-source large language models to assess\nthe current capabilities of AI and robotics in performing job-related tasks;\n(ii) formalizing and computing a measure of AI exposure by occupation, the Task\nExposure to AI (TEAI) index, and a measure of Task Replacement by AI (TRAI),\nboth validated through a human user evaluation and compared with the state of\nthe art.\n  Our results show that the TEAI index is positively correlated with cognitive,\nproblem-solving and management skills, while it is negatively correlated with\nsocial skills. Applying the index to the US, we obtain that about one-third of\nUS employment is highly exposed to AI, primarily in high-skill jobs requiring a\ngraduate or postgraduate level of education. We also find that AI exposure is\npositively associated with both employment and wage growth in 2003-2023,\nsuggesting that AI has an overall positive effect on productivity.\n  Considering specifically the TRAI index, we find that even in high-skill\noccupations, AI exhibits high variability in task substitution, suggesting that\nAI and humans complement each other within the same occupation, while the\nallocation of tasks within occupations is likely to change.\n  All results, models, and code are freely available online to allow the\ncommunity to reproduce our results, compare outcomes, and use our work as a\nbenchmark to monitor AI's progress over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and related technologies are reshaping jobs and tasks, either by\nautomating or augmenting human skills in the workplace. Many researchers have\nbeen working on estimating if and to what extent jobs and tasks are exposed to\nthe risk of being automatized by AI-related technologies. Our work tackles this\nissue through a data-driven approach by: (i) developing a reproducible\nframework that uses cutting-edge open-source large language models to assess\nthe current capabilities of AI and robotics in performing job-related tasks;\n(ii) formalizing and computing a measure of AI exposure by occupation, the Task\nExposure to AI (TEAI) index, and a measure of Task Replacement by AI (TRAI),\nboth validated through a human user evaluation and compared with the state of\nthe art.\n  Our results show that the TEAI index is positively correlated with cognitive,\nproblem-solving and management skills, while it is negatively correlated with\nsocial skills. Applying the index to the US, we obtain that about one-third of\nUS employment is highly exposed to AI, primarily in high-skill jobs requiring a\ngraduate or postgraduate level of education. We also find that AI exposure is\npositively associated with both employment and wage growth in 2003-2023,\nsuggesting that AI has an overall positive effect on productivity.\n  Considering specifically the TRAI index, we find that even in high-skill\noccupations, AI exhibits high variability in task substitution, suggesting that\nAI and humans complement each other within the same occupation, while the\nallocation of tasks within occupations is likely to change.\n  All results, models, and code are freely available online to allow the\ncommunity to reproduce our results, compare outcomes, and use our work as a\nbenchmark to monitor AI's progress over time."
                },
                "authors": [
                    {
                        "name": "Emilio Colombo"
                    },
                    {
                        "name": "Fabio Mercorio"
                    },
                    {
                        "name": "Mario Mezzanzanica"
                    },
                    {
                        "name": "Antonio Serino"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Serino"
                },
                "author": "Antonio Serino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05747v2",
                "updated": "2025-04-15T08:51:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    51,
                    5,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-08T07:24:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    24,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "SEA-LION: Southeast Asian Languages in One Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-LION: Southeast Asian Languages in One Network"
                },
                "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity."
                },
                "authors": [
                    {
                        "name": "Raymond Ng"
                    },
                    {
                        "name": "Thanh Ngan Nguyen"
                    },
                    {
                        "name": "Yuli Huang"
                    },
                    {
                        "name": "Ngee Chia Tai"
                    },
                    {
                        "name": "Wai Yi Leong"
                    },
                    {
                        "name": "Wei Qi Leong"
                    },
                    {
                        "name": "Xianbin Yong"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Nicholas Cheng"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Adithya Venkatadri Hulagadri"
                    },
                    {
                        "name": "Kok Wai Teng"
                    },
                    {
                        "name": "Yeo Yeow Tong"
                    },
                    {
                        "name": "Bryan Siow"
                    },
                    {
                        "name": "Wei Yi Teo"
                    },
                    {
                        "name": "Wayne Lau"
                    },
                    {
                        "name": "Choon Meng Tan"
                    },
                    {
                        "name": "Brandon Ong"
                    },
                    {
                        "name": "Zhi Hao Ong"
                    },
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Adwin Chan"
                    },
                    {
                        "name": "Sajeban Antonyrex"
                    },
                    {
                        "name": "Ren Lee"
                    },
                    {
                        "name": "Esther Choa"
                    },
                    {
                        "name": "David Ong Tat-Wee"
                    },
                    {
                        "name": "Bing Jie Darius Liu"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Leslie Teo"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Teo"
                },
                "author": "Leslie Teo",
                "arxiv_comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03786v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03786v3",
                "updated": "2025-04-15T08:51:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    51,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-03T17:43:45Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    45,
                    3,
                    93,
                    0
                ],
                "title": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on\n  Names to Identify Ingredients of Chinese Drugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on\n  Names to Identify Ingredients of Chinese Drugs"
                },
                "summary": "Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability."
                },
                "authors": [
                    {
                        "name": "Sifan Li"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03786v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03786v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10982v2",
                "updated": "2025-04-16T01:42:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    1,
                    42,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T08:46:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    46,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs"
                },
                "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18416v2",
                "updated": "2025-04-15T08:43:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    43,
                    7,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-24T13:08:34Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    8,
                    34,
                    1,
                    359,
                    0
                ],
                "title": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles"
                },
                "summary": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\nhttps://anonymous.4open.science/r/Muse-0086.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\nhttps://anonymous.4open.science/r/Muse-0086."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10050v2",
                "updated": "2025-04-15T08:42:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    42,
                    15,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T09:55:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    55,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "Emotional Strain and Frustration in LLM Interactions in Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Strain and Frustration in LLM Interactions in Software\n  Engineering"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into various daily\ntasks in Software Engineering such as coding and requirement elicitation.\nDespite their various capabilities and constant use, some interactions can lead\nto unexpected challenges (e.g. hallucinations or verbose answers) and, in turn,\ncause emotions that develop into frustration. Frustration can negatively impact\nengineers' productivity and well-being if they escalate into stress and\nburnout. In this paper, we assess the impact of LLM interactions on software\nengineers' emotional responses, specifically strains, and identify common\ncauses of frustration when interacting with LLMs at work. Based on 62 survey\nresponses from software engineers in industry and academia across various\ncompanies and universities, we found that a majority of our respondents\nexperience frustrations or other related emotions regardless of the nature of\ntheir work. Additionally, our results showed that frustration mainly stemmed\nfrom issues with correctness and less critical issues such as adaptability to\ncontext or specific format. While such issues may not cause frustration in\ngeneral, artefacts that do not follow certain preferences, standards, or best\npractices can make the output unusable without extensive modification, causing\nfrustration over time. In addition to the frustration triggers, our study\noffers guidelines to improve the software engineers' experience, aiming to\nminimise long-term consequences on mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into various daily\ntasks in Software Engineering such as coding and requirement elicitation.\nDespite their various capabilities and constant use, some interactions can lead\nto unexpected challenges (e.g. hallucinations or verbose answers) and, in turn,\ncause emotions that develop into frustration. Frustration can negatively impact\nengineers' productivity and well-being if they escalate into stress and\nburnout. In this paper, we assess the impact of LLM interactions on software\nengineers' emotional responses, specifically strains, and identify common\ncauses of frustration when interacting with LLMs at work. Based on 62 survey\nresponses from software engineers in industry and academia across various\ncompanies and universities, we found that a majority of our respondents\nexperience frustrations or other related emotions regardless of the nature of\ntheir work. Additionally, our results showed that frustration mainly stemmed\nfrom issues with correctness and less critical issues such as adaptability to\ncontext or specific format. While such issues may not cause frustration in\ngeneral, artefacts that do not follow certain preferences, standards, or best\npractices can make the output unusable without extensive modification, causing\nfrustration over time. In addition to the frustration triggers, our study\noffers guidelines to improve the software engineers' experience, aiming to\nminimise long-term consequences on mental health."
                },
                "authors": [
                    {
                        "name": "Cristina Martinez Montes"
                    },
                    {
                        "name": "Ranim Khojah"
                    }
                ],
                "author_detail": {
                    "name": "Ranim Khojah"
                },
                "author": "Ranim Khojah",
                "arxiv_comment": "Accepted in EASE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10978v1",
                "updated": "2025-04-15T08:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    39,
                    35,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T08:39:35Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    39,
                    35,
                    1,
                    105,
                    0
                ],
                "title": "AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent"
                },
                "summary": "Since human and environmental factors interfere, captured polyp images\nusually suffer from issues such as dim lighting, blur, and overexposure, which\npose challenges for downstream polyp segmentation tasks. To address the\nchallenges of noise-induced degradation in polyp images, we present AgentPolyp,\na novel framework integrating CLIP-based semantic guidance and dynamic image\nenhancement with a lightweight neural network for segmentation. The agent first\nevaluates image quality using CLIP-driven semantic analysis (e.g., identifying\n``low-contrast polyps with vascular textures\") and adapts reinforcement\nlearning strategies to dynamically apply multi-modal enhancement operations\n(e.g., denoising, contrast adjustment). A quality assessment feedback loop\noptimizes pixel-level enhancement and segmentation focus in a collaborative\nmanner, ensuring robust preprocessing before neural network segmentation. This\nmodular architecture supports plug-and-play extensions for various enhancement\nalgorithms and segmentation networks, meeting deployment requirements for\nendoscopic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since human and environmental factors interfere, captured polyp images\nusually suffer from issues such as dim lighting, blur, and overexposure, which\npose challenges for downstream polyp segmentation tasks. To address the\nchallenges of noise-induced degradation in polyp images, we present AgentPolyp,\na novel framework integrating CLIP-based semantic guidance and dynamic image\nenhancement with a lightweight neural network for segmentation. The agent first\nevaluates image quality using CLIP-driven semantic analysis (e.g., identifying\n``low-contrast polyps with vascular textures\") and adapts reinforcement\nlearning strategies to dynamically apply multi-modal enhancement operations\n(e.g., denoising, contrast adjustment). A quality assessment feedback loop\noptimizes pixel-level enhancement and segmentation focus in a collaborative\nmanner, ensuring robust preprocessing before neural network segmentation. This\nmodular architecture supports plug-and-play extensions for various enhancement\nalgorithms and segmentation networks, meeting deployment requirements for\nendoscopic devices."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Zhihua Zhang"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04620v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04620v6",
                "updated": "2025-04-15T08:32:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    32,
                    15,
                    1,
                    105,
                    0
                ],
                "published": "2024-02-07T07:07:02Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    7,
                    7,
                    2,
                    2,
                    38,
                    0
                ],
                "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients"
                },
                "summary": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot. CataractBot answers cataract surgery related questions\ninstantly using an LLM to query a curated knowledge base, and provides\nexpert-verified responses asynchronously. It has multimodal and multilingual\ncapabilities. In an in-the-wild deployment study with 49 patients and\nattendants, 4 doctors, and 2 patient coordinators, CataractBot demonstrated\npotential, providing anytime accessibility, saving time, accommodating diverse\nliteracy levels, alleviating power differences, and adding a privacy layer\nbetween patients and doctors. Users reported that their trust in the system was\nestablished through expert verification. Broadly, our results could inform\nfuture work on expert-mediated LLM bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot. CataractBot answers cataract surgery related questions\ninstantly using an LLM to query a curated knowledge base, and provides\nexpert-verified responses asynchronously. It has multimodal and multilingual\ncapabilities. In an in-the-wild deployment study with 49 patients and\nattendants, 4 doctors, and 2 patient coordinators, CataractBot demonstrated\npotential, providing anytime accessibility, saving time, accommodating diverse\nliteracy levels, alleviating power differences, and adding a privacy layer\nbetween patients and doctors. Users reported that their trust in the system was\nestablished through expert verification. Broadly, our results could inform\nfuture work on expert-mediated LLM bots."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Shreyas Kulkarni"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04620v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04620v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04222v2",
                "updated": "2025-04-15T08:30:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    30,
                    4,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-05T16:18:33Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    16,
                    18,
                    33,
                    5,
                    95,
                    0
                ],
                "title": "TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis\n  with Generic Traffic Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis\n  with Generic Traffic Representation"
                },
                "summary": "Machine learning (ML) powered network traffic analysis has been widely used\nfor the purpose of threat detection. Unfortunately, their generalization across\ndifferent tasks and unseen data is very limited. Large language models (LLMs),\nknown for their strong generalization capabilities, have shown promising\nperformance in various domains. However, their application to the traffic\nanalysis domain is limited due to significantly different characteristics of\nnetwork traffic. To address the issue, in this paper, we propose TrafficLLM,\nwhich introduces a dual-stage fine-tuning framework to learn generic traffic\nrepresentation from heterogeneous raw traffic data. The framework uses\ntraffic-domain tokenization, dual-stage tuning pipeline, and extensible\nadaptation to help LLM release generalization ability on dynamic traffic\nanalysis tasks, such that it enables traffic detection and traffic generation\nacross a wide range of downstream tasks. We evaluate TrafficLLM across 10\ndistinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of\n0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than\nexisting detection and generation methods. It also shows strong generalization\non unseen traffic with an 18.6% performance improvement. We further evaluate\nTrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy\nto scale and achieves accurate detection performance on enterprise traffic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) powered network traffic analysis has been widely used\nfor the purpose of threat detection. Unfortunately, their generalization across\ndifferent tasks and unseen data is very limited. Large language models (LLMs),\nknown for their strong generalization capabilities, have shown promising\nperformance in various domains. However, their application to the traffic\nanalysis domain is limited due to significantly different characteristics of\nnetwork traffic. To address the issue, in this paper, we propose TrafficLLM,\nwhich introduces a dual-stage fine-tuning framework to learn generic traffic\nrepresentation from heterogeneous raw traffic data. The framework uses\ntraffic-domain tokenization, dual-stage tuning pipeline, and extensible\nadaptation to help LLM release generalization ability on dynamic traffic\nanalysis tasks, such that it enables traffic detection and traffic generation\nacross a wide range of downstream tasks. We evaluate TrafficLLM across 10\ndistinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of\n0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than\nexisting detection and generation methods. It also shows strong generalization\non unseen traffic with an 18.6% performance improvement. We further evaluate\nTrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy\nto scale and achieves accurate detection performance on enterprise traffic."
                },
                "authors": [
                    {
                        "name": "Tianyu Cui"
                    },
                    {
                        "name": "Xinjie Lin"
                    },
                    {
                        "name": "Sijia Li"
                    },
                    {
                        "name": "Miao Chen"
                    },
                    {
                        "name": "Qilei Yin"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07991v2",
                "updated": "2025-04-15T08:28:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    28,
                    49,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-07T15:57:28Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    57,
                    28,
                    0,
                    97,
                    0
                ],
                "title": "SlicerNNInteractive: A 3D Slicer extension for nnInteractive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlicerNNInteractive: A 3D Slicer extension for nnInteractive"
                },
                "summary": "SlicerNNInteractive integrates nnInteractive, a state-of-the-art promptable\ndeep learning-based framework for 3D image segmentation, into the widely used\n3D Slicer platform. Our extension implements a client-server architecture that\ndecouples computationally intensive model inference from the client-side\ninterface. Therefore, SlicerNNInteractive eliminates heavy hardware constraints\non the client-side and enables better operating system compatibility than\nexisting plugins for nnInteractive. Running both the client and server-side on\na single machine is also possible, offering flexibility across different\ndeployment scenarios. The extension provides an intuitive user interface with\nall interaction types available in the original framework (point, bounding box,\nscribble, and lasso prompts), while including a comprehensive set of keyboard\nshortcuts for efficient workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlicerNNInteractive integrates nnInteractive, a state-of-the-art promptable\ndeep learning-based framework for 3D image segmentation, into the widely used\n3D Slicer platform. Our extension implements a client-server architecture that\ndecouples computationally intensive model inference from the client-side\ninterface. Therefore, SlicerNNInteractive eliminates heavy hardware constraints\non the client-side and enables better operating system compatibility than\nexisting plugins for nnInteractive. Running both the client and server-side on\na single machine is also possible, offering flexibility across different\ndeployment scenarios. The extension provides an intuitive user interface with\nall interaction types available in the original framework (point, bounding box,\nscribble, and lasso prompts), while including a comprehensive set of keyboard\nshortcuts for efficient workflow."
                },
                "authors": [
                    {
                        "name": "Coen de Vente"
                    },
                    {
                        "name": "Kiran Vaidhya Venkadesh"
                    },
                    {
                        "name": "Bram van Ginneken"
                    },
                    {
                        "name": "Clara I. Sánchez"
                    }
                ],
                "author_detail": {
                    "name": "Clara I. Sánchez"
                },
                "author": "Clara I. Sánchez",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v3",
                "updated": "2025-04-16T07:59:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    59,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15055v2",
                "updated": "2025-04-15T08:14:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    14,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2025-03-19T09:46:54Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    46,
                    54,
                    2,
                    78,
                    0
                ],
                "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation"
                },
                "summary": "We introduce Efficient LLM Token Extraction (ELTEX), a framework addressing\nthe critical challenge of LLM domain specialization by systematically\nextracting and integrating domain indicators throughout synthetic data\ngeneration. Unlike approaches relying on implicit knowledge transfer, ELTEX\nexplicitly leverages domain signals to maintain specialized knowledge\nintegrity. In our cybersecurity case study, ELTEX-enhanced data enables a\nfine-tuned Gemma-2B model to achieve performance competitive with GPT-4o on\nblockchain cyberattack classification while reducing computational\nrequirements. Our Google Sheets implementation makes ELTEX accessible to\nnon-technical users. Our contributions include: (1) the ELTEX framework; (2)\nGoogle Sheets Add-on implementation; (3) empirical validation showing how ELTEX\nbridges performance gaps between small and large models; and (4) a synthetic\ndataset of 11,448 texts for blockchain cyberattack detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Efficient LLM Token Extraction (ELTEX), a framework addressing\nthe critical challenge of LLM domain specialization by systematically\nextracting and integrating domain indicators throughout synthetic data\ngeneration. Unlike approaches relying on implicit knowledge transfer, ELTEX\nexplicitly leverages domain signals to maintain specialized knowledge\nintegrity. In our cybersecurity case study, ELTEX-enhanced data enables a\nfine-tuned Gemma-2B model to achieve performance competitive with GPT-4o on\nblockchain cyberattack classification while reducing computational\nrequirements. Our Google Sheets implementation makes ELTEX accessible to\nnon-technical users. Our contributions include: (1) the ELTEX framework; (2)\nGoogle Sheets Add-on implementation; (3) empirical validation showing how ELTEX\nbridges performance gaps between small and large models; and (4) a synthetic\ndataset of 11,448 texts for blockchain cyberattack detection."
                },
                "authors": [
                    {
                        "name": "Arina Razmyslovich"
                    },
                    {
                        "name": "Kseniia Murasheva"
                    },
                    {
                        "name": "Sofia Sedlova"
                    },
                    {
                        "name": "Julien Capitaine"
                    },
                    {
                        "name": "Eugene Dmitriev"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Dmitriev"
                },
                "author": "Eugene Dmitriev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22303v2",
                "updated": "2025-04-15T08:10:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    10,
                    39,
                    1,
                    105,
                    0
                ],
                "published": "2025-03-28T10:26:49Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    26,
                    49,
                    4,
                    87,
                    0
                ],
                "title": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering"
                },
                "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines."
                },
                "authors": [
                    {
                        "name": "Magdalena Kaiser"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "arxiv_doi": "10.1145/3701716.3715544",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715544",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.22303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025 Short Paper, 5 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08780v2",
                "updated": "2025-04-15T07:59:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    59,
                    2,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-05T06:16:43Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    6,
                    16,
                    43,
                    5,
                    95,
                    0
                ],
                "title": "How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs"
                },
                "summary": "We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage\nReranking to understand how relevance signals are learned and deployed by Large\nLanguage Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS\nMARCO under diverse LoRA configurations, we investigate how relevance modeling\nevolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the\nrelative importance of updated MHA vs. MLP components. Our ablations reveal\nwhich layers and projections within LoRA transformations are most critical for\nreranking accuracy. These findings offer fresh explanations into LoRA's\nadaptation mechanisms, setting the stage for deeper mechanistic studies in\nInformation Retrieval. All models used in this study have been shared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage\nReranking to understand how relevance signals are learned and deployed by Large\nLanguage Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS\nMARCO under diverse LoRA configurations, we investigate how relevance modeling\nevolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the\nrelative importance of updated MHA vs. MLP components. Our ablations reveal\nwhich layers and projections within LoRA transformations are most critical for\nreranking accuracy. These findings offer fresh explanations into LoRA's\nadaptation mechanisms, setting the stage for deeper mechanistic studies in\nInformation Retrieval. All models used in this study have been shared."
                },
                "authors": [
                    {
                        "name": "Atharva Nijasure"
                    },
                    {
                        "name": "Tanya Chowdhury"
                    },
                    {
                        "name": "James Allan"
                    }
                ],
                "author_detail": {
                    "name": "James Allan"
                },
                "author": "James Allan",
                "arxiv_comment": "Extended Abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10950v1",
                "updated": "2025-04-15T07:57:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    57,
                    5,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T07:57:05Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    57,
                    5,
                    1,
                    105,
                    0
                ],
                "title": "Unveiling Challenges for LLMs in Enterprise Data Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Challenges for LLMs in Enterprise Data Engineering"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential for\nautomating data engineering tasks on tabular data, giving enterprises a\nvaluable opportunity to reduce the high costs associated with manual data\nhandling. However, the enterprise domain introduces unique challenges that\nexisting LLM-based approaches for data engineering often overlook, such as\nlarge table sizes, more complex tasks, and the need for internal knowledge. To\nbridge these gaps, we identify key enterprise-specific challenges related to\ndata, tasks, and background knowledge and conduct a comprehensive study of\ntheir impact on recent LLMs for data engineering. Our analysis reveals that\nLLMs face substantial limitations in real-world enterprise scenarios, resulting\nin significant accuracy drops. Our findings contribute to a systematic\nunderstanding of LLMs for enterprise data engineering to support their adoption\nin industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential for\nautomating data engineering tasks on tabular data, giving enterprises a\nvaluable opportunity to reduce the high costs associated with manual data\nhandling. However, the enterprise domain introduces unique challenges that\nexisting LLM-based approaches for data engineering often overlook, such as\nlarge table sizes, more complex tasks, and the need for internal knowledge. To\nbridge these gaps, we identify key enterprise-specific challenges related to\ndata, tasks, and background knowledge and conduct a comprehensive study of\ntheir impact on recent LLMs for data engineering. Our analysis reveals that\nLLMs face substantial limitations in real-world enterprise scenarios, resulting\nin significant accuracy drops. Our findings contribute to a systematic\nunderstanding of LLMs for enterprise data engineering to support their adoption\nin industry."
                },
                "authors": [
                    {
                        "name": "Jan-Micha Bodensohn"
                    },
                    {
                        "name": "Ulf Brackmann"
                    },
                    {
                        "name": "Liane Vogel"
                    },
                    {
                        "name": "Anupam Sanghi"
                    },
                    {
                        "name": "Carsten Binnig"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Binnig"
                },
                "author": "Carsten Binnig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14936v2",
                "updated": "2025-04-15T07:53:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    53,
                    50,
                    1,
                    105,
                    0
                ],
                "published": "2025-03-19T06:44:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    44,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Enhancing Code LLM Training with Programmer Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLM Training with Programmer Attention"
                },
                "summary": "Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Zachary Karas"
                    },
                    {
                        "name": "Dung Thuy Nguyen"
                    },
                    {
                        "name": "Kevin Leach"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "arxiv_doi": "10.1145/3696630.3728510",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696630.3728510",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09421v2",
                "updated": "2025-04-15T07:52:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    52,
                    40,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-13T04:00:40Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    0,
                    40,
                    6,
                    103,
                    0
                ],
                "title": "ClinicalGPT-R1: Pushing reasoning capability of generalist disease\n  diagnosis with large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClinicalGPT-R1: Pushing reasoning capability of generalist disease\n  diagnosis with large language model"
                },
                "summary": "Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound."
                },
                "authors": [
                    {
                        "name": "Wuyang Lan"
                    },
                    {
                        "name": "Wenzheng Wang"
                    },
                    {
                        "name": "Changwei Ji"
                    },
                    {
                        "name": "Guoxing Yang"
                    },
                    {
                        "name": "Yongbo Zhang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Song Wu"
                    },
                    {
                        "name": "Guangyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Wang"
                },
                "author": "Guangyu Wang",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08389v2",
                "updated": "2025-04-15T07:44:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    44,
                    57,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-11T09:42:46Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    42,
                    46,
                    4,
                    101,
                    0
                ],
                "title": "Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm"
                },
                "summary": "Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Jiawei Lan"
                    },
                    {
                        "name": "Ye Tao"
                    },
                    {
                        "name": "Zhibiao Wang"
                    },
                    {
                        "name": "Haoyang Yu"
                    },
                    {
                        "name": "Wenhua Cui"
                    }
                ],
                "author_detail": {
                    "name": "Wenhua Cui"
                },
                "author": "Wenhua Cui",
                "arxiv_comment": "12 pages, 19 figures, 6 tables. Submitted to Engineering Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10936v1",
                "updated": "2025-04-15T07:32:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    32,
                    35,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T07:32:35Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    32,
                    35,
                    1,
                    105,
                    0
                ],
                "title": "Can LLMs Leverage Observational Data? Towards Data-Driven Causal\n  Discovery with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Leverage Observational Data? Towards Data-Driven Causal\n  Discovery with LLMs"
                },
                "summary": "Causal discovery traditionally relies on statistical methods applied to\nobservational data, often requiring large datasets and assumptions about\nunderlying causal structures. Recent advancements in Large Language Models\n(LLMs) have introduced new possibilities for causal discovery by providing\ndomain expert knowledge. However, it remains unclear whether LLMs can\neffectively process observational data for causal discovery. In this work, we\nexplore the potential of LLMs for data-driven causal discovery by integrating\nobservational data for LLM-based reasoning. Specifically, we examine whether\nLLMs can effectively utilize observational data through two prompting\nstrategies: pairwise prompting and breadth first search (BFS)-based prompting.\nIn both approaches, we incorporate the observational data directly into the\nprompt to assess LLMs' ability to infer causal relationships from such data.\nExperiments on benchmark datasets show that incorporating observational data\nenhances causal discovery, boosting F1 scores by up to 0.11 point using both\npairwise and BFS LLM-based prompting, while outperforming traditional\nstatistical causal discovery baseline by up to 0.52 points. Our findings\nhighlight the potential and limitations of LLMs for data-driven causal\ndiscovery, demonstrating their ability to move beyond textual metadata and\neffectively interpret and utilize observational data for more informed causal\nreasoning. Our studies lays the groundwork for future advancements toward fully\nLLM-driven causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery traditionally relies on statistical methods applied to\nobservational data, often requiring large datasets and assumptions about\nunderlying causal structures. Recent advancements in Large Language Models\n(LLMs) have introduced new possibilities for causal discovery by providing\ndomain expert knowledge. However, it remains unclear whether LLMs can\neffectively process observational data for causal discovery. In this work, we\nexplore the potential of LLMs for data-driven causal discovery by integrating\nobservational data for LLM-based reasoning. Specifically, we examine whether\nLLMs can effectively utilize observational data through two prompting\nstrategies: pairwise prompting and breadth first search (BFS)-based prompting.\nIn both approaches, we incorporate the observational data directly into the\nprompt to assess LLMs' ability to infer causal relationships from such data.\nExperiments on benchmark datasets show that incorporating observational data\nenhances causal discovery, boosting F1 scores by up to 0.11 point using both\npairwise and BFS LLM-based prompting, while outperforming traditional\nstatistical causal discovery baseline by up to 0.52 points. Our findings\nhighlight the potential and limitations of LLMs for data-driven causal\ndiscovery, demonstrating their ability to move beyond textual metadata and\neffectively interpret and utilize observational data for more informed causal\nreasoning. Our studies lays the groundwork for future advancements toward fully\nLLM-driven causal discovery."
                },
                "authors": [
                    {
                        "name": "Yuni Susanti"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "arxiv_journal_ref": "Causal-NeSy @ ESWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10925v1",
                "updated": "2025-04-15T07:12:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    12,
                    0,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T07:12:00Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    12,
                    0,
                    1,
                    105,
                    0
                ],
                "title": "Transfer Learning for Temporal Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning for Temporal Link Prediction"
                },
                "summary": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP."
                },
                "authors": [
                    {
                        "name": "Ayan Chatterjee"
                    },
                    {
                        "name": "Barbara Ikica"
                    },
                    {
                        "name": "Babak Ravandi"
                    },
                    {
                        "name": "John Palowitch"
                    }
                ],
                "author_detail": {
                    "name": "John Palowitch"
                },
                "author": "John Palowitch",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]