[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16158v1",
                "updated": "2024-12-20T18:59:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    59,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:59:59Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    59,
                    59,
                    4,
                    355,
                    0
                ],
                "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding"
                },
                "summary": "The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE."
                },
                "authors": [
                    {
                        "name": "Chenxin Tao"
                    },
                    {
                        "name": "Shiqian Su"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16155v1",
                "updated": "2024-12-20T18:58:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    58,
                    24,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:58:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    58,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can Generative Video Models Help Pose Estimation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Generative Video Models Help Pose Estimation?"
                },
                "summary": "Pairwise pose estimation from images with little or no overlap is an open\nchallenge in computer vision. Existing methods, even those trained on\nlarge-scale datasets, struggle in these scenarios due to the lack of\nidentifiable correspondences or visual overlap. Inspired by the human ability\nto infer spatial relationships from diverse scenes, we propose a novel\napproach, InterPose, that leverages the rich priors encoded within pre-trained\ngenerative video models. We propose to use a video model to hallucinate\nintermediate frames between two input images, effectively creating a dense,\nvisual transition, which significantly simplifies the problem of pose\nestimation. Since current video models can still produce implausible motion or\ninconsistent geometry, we introduce a self-consistency score that evaluates the\nconsistency of pose predictions from sampled videos. We demonstrate that our\napproach generalizes among three state-of-the-art video models and show\nconsistent improvements over the state-of-the-art DUSt3R on four diverse\ndatasets encompassing indoor, outdoor, and object-centric scenes. Our findings\nsuggest a promising avenue for improving pose estimation models by leveraging\nlarge generative models trained on vast amounts of video data, which is more\nreadily available than 3D data. See our project page for results:\nhttps://inter-pose.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise pose estimation from images with little or no overlap is an open\nchallenge in computer vision. Existing methods, even those trained on\nlarge-scale datasets, struggle in these scenarios due to the lack of\nidentifiable correspondences or visual overlap. Inspired by the human ability\nto infer spatial relationships from diverse scenes, we propose a novel\napproach, InterPose, that leverages the rich priors encoded within pre-trained\ngenerative video models. We propose to use a video model to hallucinate\nintermediate frames between two input images, effectively creating a dense,\nvisual transition, which significantly simplifies the problem of pose\nestimation. Since current video models can still produce implausible motion or\ninconsistent geometry, we introduce a self-consistency score that evaluates the\nconsistency of pose predictions from sampled videos. We demonstrate that our\napproach generalizes among three state-of-the-art video models and show\nconsistent improvements over the state-of-the-art DUSt3R on four diverse\ndatasets encompassing indoor, outdoor, and object-centric scenes. Our findings\nsuggest a promising avenue for improving pose estimation models by leveraging\nlarge generative models trained on vast amounts of video data, which is more\nreadily available than 3D data. See our project page for results:\nhttps://inter-pose.github.io/."
                },
                "authors": [
                    {
                        "name": "Ruojin Cai"
                    },
                    {
                        "name": "Jason Y. Zhang"
                    },
                    {
                        "name": "Philipp Henzler"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Ricardo Martin-Brualla"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Martin-Brualla"
                },
                "author": "Ricardo Martin-Brualla",
                "arxiv_comment": "Project page: https://inter-pose.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05451v2",
                "updated": "2024-12-20T18:57:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    57,
                    40,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-06T22:22:10Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    22,
                    22,
                    10,
                    4,
                    341,
                    0
                ],
                "title": "The Status of Neutrino Cosmology: Standard $Λ$CDM, Extensions, and\n  Tensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Status of Neutrino Cosmology: Standard $Λ$CDM, Extensions, and\n  Tensions"
                },
                "summary": "We examine the performance of the six-parameter $\\Lambda$CDM model and its\nextensions in light of recent cosmological observations, with particular focus\non neutrino properties inferred from cosmology. Using a broad suite of nine\ncombinations of datasets, with three separate analyses of the Planck Cosmic\nMicrowave Background (CMB) data, and three separate supernovae (SNe) survey\ndata, plus the recent DESI baryon acoustic oscillation (BAO) scale results, we\nderive constraints on the sum of neutrino masses ($\\Sigma m_\\nu$). Our results\nshow upper limits in the range of $\\Sigma m_\\nu < 76.9\\,\\mathrm{meV}$ to\n$\\Sigma m_\\nu < 108\\,\\mathrm{meV}$ (95\\% CL). The variation in the limits on\n$\\Sigma m_\\nu$ arises from the separate analyses of the Planck CMB data and the\nseparate supernovae datasets, as they relate to the inferred matter density and\nits relation to the sensitivity of the BAO scale and CMB lensing to $\\Sigma\nm_\\nu$. In the context of hierarchical mass models in $\\Lambda$CDM, we find a\n$1.47\\sigma$ preference for normal ordering (NO) over inverted ordering (IO),\nwith similar values of preference across all datasets. Despite the strong\nconstraints, an inclination towards massless neutrinos over NO remains weak at\n$1.36\\sigma$. We find that a ``negative'' neutrino mass, inferred from the\nshape of the likelihood in the physical regime, $\\Sigma m_\\nu > 0$, is only\npresent at less than $2\\sigma$. We confirm that models allowing extra\nrelativistic degrees of freedom, with $N_{\\rm eff} \\approx 3.5$, alleviate the\nHubble tension. Significantly, we find a $3.3\\sigma$ preference for a 0.1 eV\npartially thermalized sterile neutrino when the SH0ES $H_0$ measurement is\nincluded, a scale of interest in short-baseline oscillation experiment results.\n[abridged]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the performance of the six-parameter $\\Lambda$CDM model and its\nextensions in light of recent cosmological observations, with particular focus\non neutrino properties inferred from cosmology. Using a broad suite of nine\ncombinations of datasets, with three separate analyses of the Planck Cosmic\nMicrowave Background (CMB) data, and three separate supernovae (SNe) survey\ndata, plus the recent DESI baryon acoustic oscillation (BAO) scale results, we\nderive constraints on the sum of neutrino masses ($\\Sigma m_\\nu$). Our results\nshow upper limits in the range of $\\Sigma m_\\nu < 76.9\\,\\mathrm{meV}$ to\n$\\Sigma m_\\nu < 108\\,\\mathrm{meV}$ (95\\% CL). The variation in the limits on\n$\\Sigma m_\\nu$ arises from the separate analyses of the Planck CMB data and the\nseparate supernovae datasets, as they relate to the inferred matter density and\nits relation to the sensitivity of the BAO scale and CMB lensing to $\\Sigma\nm_\\nu$. In the context of hierarchical mass models in $\\Lambda$CDM, we find a\n$1.47\\sigma$ preference for normal ordering (NO) over inverted ordering (IO),\nwith similar values of preference across all datasets. Despite the strong\nconstraints, an inclination towards massless neutrinos over NO remains weak at\n$1.36\\sigma$. We find that a ``negative'' neutrino mass, inferred from the\nshape of the likelihood in the physical regime, $\\Sigma m_\\nu > 0$, is only\npresent at less than $2\\sigma$. We confirm that models allowing extra\nrelativistic degrees of freedom, with $N_{\\rm eff} \\approx 3.5$, alleviate the\nHubble tension. Significantly, we find a $3.3\\sigma$ preference for a 0.1 eV\npartially thermalized sterile neutrino when the SH0ES $H_0$ measurement is\nincluded, a scale of interest in short-baseline oscillation experiment results.\n[abridged]"
                },
                "authors": [
                    {
                        "name": "Helena García Escudero"
                    },
                    {
                        "name": "Kevork N. Abazajian"
                    }
                ],
                "author_detail": {
                    "name": "Kevork N. Abazajian"
                },
                "author": "Kevork N. Abazajian",
                "arxiv_comment": "20 pages, 7 figures, v2: updated discussion of geometry vs growth and\n  neutrino mass eigenstate modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16145v1",
                "updated": "2024-12-20T18:49:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    49,
                    45,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:49:45Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    49,
                    45,
                    4,
                    355,
                    0
                ],
                "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reinforcement Learning for LLM Multi-Step Reasoning"
                },
                "summary": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time."
                },
                "authors": [
                    {
                        "name": "Huaijie Wang"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Yilin Bao"
                    },
                    {
                        "name": "Ziran Yang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v1",
                "updated": "2024-12-20T18:31:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ndwula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16132v1",
                "updated": "2024-12-20T18:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    29,
                    49,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    29,
                    49,
                    4,
                    355,
                    0
                ],
                "title": "Data-Driven Mechanism Design: Jointly Eliciting Preferences and\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Mechanism Design: Jointly Eliciting Preferences and\n  Information"
                },
                "summary": "We study mechanism design when agents hold private information about both\ntheir preferences and a common payoff-relevant state. We show that standard\nmessage-driven mechanisms cannot implement socially efficient allocations when\nagents have multidimensional types, even under favorable conditions. To\novercome this limitation, we propose data-driven mechanisms that leverage\nadditional post-allocation information, modeled as an estimator of the\npayoff-relevant state. Our data-driven mechanisms extend the classic\nVickrey-Clarke-Groves class. We show that they achieve exact implementation in\nposterior equilibrium when the state is either fully revealed or the utility is\nlinear in an unbiased estimator. We also show that they achieve approximate\nimplementation with a consistent estimator, converging to exact implementation\nas the estimator converges, and present bounds on the convergence rate. We\ndemonstrate applications to digital advertising auctions and large language\nmodel (LLM)-based mechanisms, where user engagement naturally reveals relevant\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study mechanism design when agents hold private information about both\ntheir preferences and a common payoff-relevant state. We show that standard\nmessage-driven mechanisms cannot implement socially efficient allocations when\nagents have multidimensional types, even under favorable conditions. To\novercome this limitation, we propose data-driven mechanisms that leverage\nadditional post-allocation information, modeled as an estimator of the\npayoff-relevant state. Our data-driven mechanisms extend the classic\nVickrey-Clarke-Groves class. We show that they achieve exact implementation in\nposterior equilibrium when the state is either fully revealed or the utility is\nlinear in an unbiased estimator. We also show that they achieve approximate\nimplementation with a consistent estimator, converging to exact implementation\nas the estimator converges, and present bounds on the convergence rate. We\ndemonstrate applications to digital advertising auctions and large language\nmodel (LLM)-based mechanisms, where user engagement naturally reveals relevant\ninformation."
                },
                "authors": [
                    {
                        "name": "Dirk Bergemann"
                    },
                    {
                        "name": "Marek Bojko"
                    },
                    {
                        "name": "Paul Dütting"
                    },
                    {
                        "name": "Renato Paes Leme"
                    },
                    {
                        "name": "Haifeng Xu"
                    },
                    {
                        "name": "Song Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Song Zuo"
                },
                "author": "Song Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v4",
                "updated": "2024-12-20T18:11:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    11,
                    41,
                    4,
                    355,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16120v1",
                "updated": "2024-12-20T18:08:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    8,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:08:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    8,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation\n  Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation\n  Metrics"
                },
                "summary": "Evaluating the quality of machine-generated natural language content is a\nchallenging task in Natural Language Processing (NLP). Recently, large language\nmodels (LLMs) like GPT-4 have been employed for this purpose, but they are\ncomputationally expensive due to the extensive token usage required by complex\nevaluation prompts. In this paper, we propose a prompt optimization approach\nthat uses a smaller, fine-tuned language model to compress input data for\nevaluation prompt, thus reducing token usage and computational cost when using\nlarger LLMs for downstream evaluation. Our method involves a two-stage\nfine-tuning process: supervised fine-tuning followed by preference optimization\nto refine the model's outputs based on human preferences. We focus on Machine\nTranslation (MT) evaluation and utilize the GEMBA-MQM metric as a starting\npoint. Our results show a $2.37\\times$ reduction in token usage without any\nloss in evaluation quality. This work makes state-of-the-art LLM-based metrics\nlike GEMBA-MQM more cost-effective and efficient, enhancing their accessibility\nfor broader use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of machine-generated natural language content is a\nchallenging task in Natural Language Processing (NLP). Recently, large language\nmodels (LLMs) like GPT-4 have been employed for this purpose, but they are\ncomputationally expensive due to the extensive token usage required by complex\nevaluation prompts. In this paper, we propose a prompt optimization approach\nthat uses a smaller, fine-tuned language model to compress input data for\nevaluation prompt, thus reducing token usage and computational cost when using\nlarger LLMs for downstream evaluation. Our method involves a two-stage\nfine-tuning process: supervised fine-tuning followed by preference optimization\nto refine the model's outputs based on human preferences. We focus on Machine\nTranslation (MT) evaluation and utilize the GEMBA-MQM metric as a starting\npoint. Our results show a $2.37\\times$ reduction in token usage without any\nloss in evaluation quality. This work makes state-of-the-art LLM-based metrics\nlike GEMBA-MQM more cost-effective and efficient, enhancing their accessibility\nfor broader use."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16119v1",
                "updated": "2024-12-20T18:05:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    5,
                    22,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:05:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    5,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts"
                },
                "summary": "This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages."
                },
                "authors": [
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Salaar Masood"
                    },
                    {
                        "name": "Hamza Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Iqbal"
                },
                "author": "Hamza Iqbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17227v2",
                "updated": "2024-12-20T18:03:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    3,
                    21,
                    4,
                    355,
                    0
                ],
                "published": "2024-04-26T07:58:05Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    7,
                    58,
                    5,
                    4,
                    117,
                    0
                ],
                "title": "Trust Dynamics and Market Behavior in Cryptocurrency: A Comparative\n  Study of Centralized and Decentralized Exchanges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Dynamics and Market Behavior in Cryptocurrency: A Comparative\n  Study of Centralized and Decentralized Exchanges"
                },
                "summary": "In the rapidly evolving cryptocurrency landscape, trust is a critical yet\nunderexplored factor shaping market behaviors and driving user preferences\nbetween centralized exchanges (CEXs) and decentralized exchanges (DEXs).\nDespite its importance, trust remains challenging to measure, limiting the\nstudy of its effects on market dynamics. The collapse of FTX, a major CEX,\nprovides a unique natural experiment to examine the measurable impacts of trust\nand its sudden erosion on the cryptocurrency ecosystem. This pivotal event\nraised questions about the resilience of centralized trust systems and\naccelerated shifts toward decentralized alternatives. This research\ninvestigates the impacts of the FTX collapse on user trust, focusing on token\nvaluation, trading flows, and sentiment dynamics. Employing causal inference\nmethods, including Regression Discontinuity Design (RDD) and\nDifference-in-Differences (DID), we reveal significant declines in WETH prices\nand NetFlow from CEXs to DEXs, signaling a measurable transfer of trust.\nAdditionally, natural language processing methods, including topic modeling and\nsentiment analysis, uncover the complexities of user responses, highlighting\nshifts from functional discussions to emotional fragmentation in Binance's\ncommunity, while Uniswap's sentiment exhibits a gradual upward trend. Despite\ndata limitations and external influences, the findings underscore the intricate\ninterplay between trust, sentiment, and market behavior in the cryptocurrency\necosystem. By bridging blockchain analytics, behavioral finance, and\ndecentralized finance (DeFi), this study contributes to interdisciplinary\nresearch, offering a deeper understanding of distributed trust mechanisms and\nproviding critical insights for future investigations into the socio-technical\ndimensions of trust in digital economies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving cryptocurrency landscape, trust is a critical yet\nunderexplored factor shaping market behaviors and driving user preferences\nbetween centralized exchanges (CEXs) and decentralized exchanges (DEXs).\nDespite its importance, trust remains challenging to measure, limiting the\nstudy of its effects on market dynamics. The collapse of FTX, a major CEX,\nprovides a unique natural experiment to examine the measurable impacts of trust\nand its sudden erosion on the cryptocurrency ecosystem. This pivotal event\nraised questions about the resilience of centralized trust systems and\naccelerated shifts toward decentralized alternatives. This research\ninvestigates the impacts of the FTX collapse on user trust, focusing on token\nvaluation, trading flows, and sentiment dynamics. Employing causal inference\nmethods, including Regression Discontinuity Design (RDD) and\nDifference-in-Differences (DID), we reveal significant declines in WETH prices\nand NetFlow from CEXs to DEXs, signaling a measurable transfer of trust.\nAdditionally, natural language processing methods, including topic modeling and\nsentiment analysis, uncover the complexities of user responses, highlighting\nshifts from functional discussions to emotional fragmentation in Binance's\ncommunity, while Uniswap's sentiment exhibits a gradual upward trend. Despite\ndata limitations and external influences, the findings underscore the intricate\ninterplay between trust, sentiment, and market behavior in the cryptocurrency\necosystem. By bridging blockchain analytics, behavioral finance, and\ndecentralized finance (DeFi), this study contributes to interdisciplinary\nresearch, offering a deeper understanding of distributed trust mechanisms and\nproviding critical insights for future investigations into the socio-technical\ndimensions of trust in digital economies."
                },
                "authors": [
                    {
                        "name": "Xintong Wu"
                    },
                    {
                        "name": "Wanlin Deng"
                    },
                    {
                        "name": "Yutong Quan"
                    },
                    {
                        "name": "Luyao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Luyao Zhang"
                },
                "author": "Luyao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16117v1",
                "updated": "2024-12-20T18:01:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    1,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:01:58Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    1,
                    58,
                    4,
                    355,
                    0
                ],
                "title": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models"
                },
                "summary": "In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid."
                },
                "authors": [
                    {
                        "name": "Xiaohu Huang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "Efficient Video Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16112v1",
                "updated": "2024-12-20T17:57:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    57,
                    9,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:57:09Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    57,
                    9,
                    4,
                    355,
                    0
                ],
                "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\n  Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\n  Up"
                },
                "summary": "Diffusion Transformers (DiT) have become a leading architecture in image\ngeneration. However, the quadratic complexity of attention mechanisms, which\nare responsible for modeling token-wise relationships, results in significant\nlatency when generating high-resolution images. To address this issue, we aim\nat a linear attention mechanism in this paper that reduces the complexity of\npre-trained DiTs to linear. We begin our exploration with a comprehensive\nsummary of existing efficient attention mechanisms and identify four key\nfactors crucial for successful linearization of pre-trained DiTs: locality,\nformulation consistency, high-rank attention maps, and feature integrity. Based\non these insights, we introduce a convolution-like local attention strategy\ntermed CLEAR, which limits feature interactions to a local window around each\nquery token, and thus achieves linear complexity. Our experiments indicate\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\nDiT to a student model with linear complexity, yielding results comparable to\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\nand accelerates generation by 6.3 times for generating 8K-resolution images.\nFurthermore, we investigate favorable properties in the distilled attention\nlayers, such as zero-shot generalization cross various models and plugins, and\nimproved support for multi-GPU parallel inference. Models and codes are\navailable here: https://github.com/Huage001/CLEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become a leading architecture in image\ngeneration. However, the quadratic complexity of attention mechanisms, which\nare responsible for modeling token-wise relationships, results in significant\nlatency when generating high-resolution images. To address this issue, we aim\nat a linear attention mechanism in this paper that reduces the complexity of\npre-trained DiTs to linear. We begin our exploration with a comprehensive\nsummary of existing efficient attention mechanisms and identify four key\nfactors crucial for successful linearization of pre-trained DiTs: locality,\nformulation consistency, high-rank attention maps, and feature integrity. Based\non these insights, we introduce a convolution-like local attention strategy\ntermed CLEAR, which limits feature interactions to a local window around each\nquery token, and thus achieves linear complexity. Our experiments indicate\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\nDiT to a student model with linear complexity, yielding results comparable to\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\nand accelerates generation by 6.3 times for generating 8K-resolution images.\nFurthermore, we investigate favorable properties in the distilled attention\nlayers, such as zero-shot generalization cross various models and plugins, and\nimproved support for multi-GPU parallel inference. Models and codes are\navailable here: https://github.com/Huage001/CLEAR."
                },
                "authors": [
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17196v3",
                "updated": "2024-12-20T17:50:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    50,
                    38,
                    4,
                    355,
                    0
                ],
                "published": "2024-03-25T21:17:14Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    21,
                    17,
                    14,
                    0,
                    85,
                    0
                ],
                "title": "Text Understanding in GPT-4 vs Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Understanding in GPT-4 vs Humans"
                },
                "summary": "We examine whether a leading AI system GPT4 understands text as well as\nhumans do, first using a well-established standardized test of discourse\ncomprehension. On this test, GPT4 performs slightly, but not statistically\nsignificantly, better than humans given the very high level of human\nperformance. Both GPT4 and humans make correct inferences about information\nthat is not explicitly stated in the text, a critical test of understanding.\nNext, we use more difficult passages to determine whether that could allow\nlarger differences between GPT4 and humans. GPT4 does considerably better on\nthis more difficult text than do the high school and university students for\nwhom these the text passages are designed, as admission tests of student\nreading comprehension. Deeper exploration of GPT4 performance on material from\none of these admission tests reveals generally accepted signatures of genuine\nunderstanding, namely generalization and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine whether a leading AI system GPT4 understands text as well as\nhumans do, first using a well-established standardized test of discourse\ncomprehension. On this test, GPT4 performs slightly, but not statistically\nsignificantly, better than humans given the very high level of human\nperformance. Both GPT4 and humans make correct inferences about information\nthat is not explicitly stated in the text, a critical test of understanding.\nNext, we use more difficult passages to determine whether that could allow\nlarger differences between GPT4 and humans. GPT4 does considerably better on\nthis more difficult text than do the high school and university students for\nwhom these the text passages are designed, as admission tests of student\nreading comprehension. Deeper exploration of GPT4 performance on material from\none of these admission tests reveals generally accepted signatures of genuine\nunderstanding, namely generalization and inference."
                },
                "authors": [
                    {
                        "name": "Thomas R. Shultz"
                    },
                    {
                        "name": "Jamie M. Wise"
                    },
                    {
                        "name": "Ardavan Salehi Nobandegani"
                    }
                ],
                "author_detail": {
                    "name": "Ardavan Salehi Nobandegani"
                },
                "author": "Ardavan Salehi Nobandegani",
                "arxiv_comment": "22 pages, 2 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16100v1",
                "updated": "2024-12-20T17:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Logical Consistency of Large Language Models in Fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Consistency of Large Language Models in Fact-checking"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses -- a meaning-preserving change in the input query\nresults in an inconsistent response and attributes to vulnerabilities of LLMs\nsuch as hallucination, jailbreaking, etc. Consequently, existing research\nfocuses on simple paraphrasing-based consistency assessment of LLMs, and\nignores complex queries that necessitates an even better understanding of\nlogical reasoning by an LLM. Our work therefore addresses the logical\ninconsistency of LLMs under complex logical queries with primitive logical\noperators, e.g., negation, conjunction, and disjunction. As a test bed, we\nconsider retrieval-augmented LLMs on a fact-checking task involving\npropositional logic queries from real-world knowledge graphs (KGs). Our\ncontributions are three-fold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries as input and demonstrate that existing LLMs lack\nlogical consistency, specially on complex queries. Improvement: We employ\nsupervised fine-tuning to improve the logical consistency of LLMs on the\ncomplex fact-checking task with KG contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses -- a meaning-preserving change in the input query\nresults in an inconsistent response and attributes to vulnerabilities of LLMs\nsuch as hallucination, jailbreaking, etc. Consequently, existing research\nfocuses on simple paraphrasing-based consistency assessment of LLMs, and\nignores complex queries that necessitates an even better understanding of\nlogical reasoning by an LLM. Our work therefore addresses the logical\ninconsistency of LLMs under complex logical queries with primitive logical\noperators, e.g., negation, conjunction, and disjunction. As a test bed, we\nconsider retrieval-augmented LLMs on a fact-checking task involving\npropositional logic queries from real-world knowledge graphs (KGs). Our\ncontributions are three-fold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries as input and demonstrate that existing LLMs lack\nlogical consistency, specially on complex queries. Improvement: We employ\nsupervised fine-tuning to improve the logical consistency of LLMs on the\ncomplex fact-checking task with KG contexts."
                },
                "authors": [
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Sarah Hasan"
                    },
                    {
                        "name": "Naheed Anjum Arafat"
                    },
                    {
                        "name": "Arijit Khan"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Khan"
                },
                "author": "Arijit Khan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16089v1",
                "updated": "2024-12-20T17:34:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    34,
                    16,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:34:16Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    34,
                    16,
                    4,
                    355,
                    0
                ],
                "title": "The Evolution of LLM Adoption in Industry Data Curation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evolution of LLM Adoption in Industry Data Curation Practices"
                },
                "summary": "As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development."
                },
                "authors": [
                    {
                        "name": "Crystal Qian"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Emily Reif"
                    },
                    {
                        "name": "Grady Simon"
                    },
                    {
                        "name": "Nada Hussein"
                    },
                    {
                        "name": "Nathan Clement"
                    },
                    {
                        "name": "James Wexler"
                    },
                    {
                        "name": "Carrie J. Cai"
                    },
                    {
                        "name": "Michael Terry"
                    },
                    {
                        "name": "Minsuk Kahng"
                    }
                ],
                "author_detail": {
                    "name": "Minsuk Kahng"
                },
                "author": "Minsuk Kahng",
                "arxiv_comment": "19 pages, 4 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16086v1",
                "updated": "2024-12-20T17:33:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG"
                },
                "summary": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings."
                },
                "authors": [
                    {
                        "name": "Hasan Md Tusfiqur Alam"
                    },
                    {
                        "name": "Devansh Srivastav"
                    },
                    {
                        "name": "Md Abdul Kadir"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "arxiv_comment": "Accepted in ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16085v1",
                "updated": "2024-12-20T17:33:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    35,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:33:35Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    35,
                    4,
                    355,
                    0
                ],
                "title": "Efficient MedSAMs: Segment Anything in Medical Images on Laptop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient MedSAMs: Segment Anything in Medical Images on Laptop"
                },
                "summary": "Promptable segmentation foundation models have emerged as a transformative\napproach to addressing the diverse needs in medical images, but most existing\nmodels require expensive computing, posing a big barrier to their adoption in\nclinical practice. In this work, we organized the first international\ncompetition dedicated to promptable medical image segmentation, featuring a\nlarge-scale dataset spanning nine common imaging modalities from over 20\ndifferent institutions. The top teams developed lightweight segmentation\nfoundation models and implemented an efficient inference pipeline that\nsubstantially reduced computational requirements while maintaining\nstate-of-the-art segmentation accuracy. Moreover, the post-challenge phase\nadvanced the algorithms through the design of performance booster and\nreproducibility tasks, resulting in improved algorithms and validated\nreproducibility of the winning solution. Furthermore, the best-performing\nalgorithms have been incorporated into the open-source software with a\nuser-friendly interface to facilitate clinical adoption. The data and code are\npublicly available to foster the further development of medical image\nsegmentation foundation models and pave the way for impactful real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promptable segmentation foundation models have emerged as a transformative\napproach to addressing the diverse needs in medical images, but most existing\nmodels require expensive computing, posing a big barrier to their adoption in\nclinical practice. In this work, we organized the first international\ncompetition dedicated to promptable medical image segmentation, featuring a\nlarge-scale dataset spanning nine common imaging modalities from over 20\ndifferent institutions. The top teams developed lightweight segmentation\nfoundation models and implemented an efficient inference pipeline that\nsubstantially reduced computational requirements while maintaining\nstate-of-the-art segmentation accuracy. Moreover, the post-challenge phase\nadvanced the algorithms through the design of performance booster and\nreproducibility tasks, resulting in improved algorithms and validated\nreproducibility of the winning solution. Furthermore, the best-performing\nalgorithms have been incorporated into the open-source software with a\nuser-friendly interface to facilitate clinical adoption. The data and code are\npublicly available to foster the further development of medical image\nsegmentation foundation models and pave the way for impactful real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Sumin Kim"
                    },
                    {
                        "name": "Reza Asakereh"
                    },
                    {
                        "name": "Bao-Hiep Le"
                    },
                    {
                        "name": "Dang-Khoa Nguyen-Vu"
                    },
                    {
                        "name": "Alexander Pfefferle"
                    },
                    {
                        "name": "Muxin Wei"
                    },
                    {
                        "name": "Ruochen Gao"
                    },
                    {
                        "name": "Donghang Lyu"
                    },
                    {
                        "name": "Songxiao Yang"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Zdravko Marinov"
                    },
                    {
                        "name": "Marius Staring"
                    },
                    {
                        "name": "Haisheng Lu"
                    },
                    {
                        "name": "Thuy Thanh Dao"
                    },
                    {
                        "name": "Xincheng Ye"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Gianluca Brugnara"
                    },
                    {
                        "name": "Philipp Vollmuth"
                    },
                    {
                        "name": "Martha Foltyn-Dumitru"
                    },
                    {
                        "name": "Jaeyoung Cho"
                    },
                    {
                        "name": "Mustafa Ahmed Mahmutoglu"
                    },
                    {
                        "name": "Martin Bendszus"
                    },
                    {
                        "name": "Irada Pflüger"
                    },
                    {
                        "name": "Aditya Rastogi"
                    },
                    {
                        "name": "Dong Ni"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Guang-Quan Zhou"
                    },
                    {
                        "name": "Kaini Wang"
                    },
                    {
                        "name": "Nicholas Heller"
                    },
                    {
                        "name": "Nikolaos Papanikolopoulos"
                    },
                    {
                        "name": "Christopher Weight"
                    },
                    {
                        "name": "Yubing Tong"
                    },
                    {
                        "name": "Jayaram K Udupa"
                    },
                    {
                        "name": "Cahill J. Patrick"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Francisco Contijoch"
                    },
                    {
                        "name": "Elliot McVeigh"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Shucheng He"
                    },
                    {
                        "name": "Robert Haase"
                    },
                    {
                        "name": "Thomas Pinetz"
                    },
                    {
                        "name": "Alexander Radbruch"
                    },
                    {
                        "name": "Inga Krause"
                    },
                    {
                        "name": "Erich Kobler"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Yucheng Tang"
                    },
                    {
                        "name": "Haichun Yang"
                    },
                    {
                        "name": "Yuankai Huo"
                    },
                    {
                        "name": "Gongning Luo"
                    },
                    {
                        "name": "Kaisar Kushibar"
                    },
                    {
                        "name": "Jandos Amankulov"
                    },
                    {
                        "name": "Dias Toleshbayev"
                    },
                    {
                        "name": "Amangeldi Mukhamejan"
                    },
                    {
                        "name": "Jan Egger"
                    },
                    {
                        "name": "Antonio Pepe"
                    },
                    {
                        "name": "Christina Gsaxner"
                    },
                    {
                        "name": "Gijs Luijten"
                    },
                    {
                        "name": "Shohei Fujita"
                    },
                    {
                        "name": "Tomohiro Kikuchi"
                    },
                    {
                        "name": "Benedikt Wiestler"
                    },
                    {
                        "name": "Jan S. Kirschke"
                    },
                    {
                        "name": "Ezequiel de la Rosa"
                    },
                    {
                        "name": "Federico Bolelli"
                    },
                    {
                        "name": "Luca Lumetti"
                    },
                    {
                        "name": "Costantino Grana"
                    },
                    {
                        "name": "Kunpeng Xie"
                    },
                    {
                        "name": "Guomin Wu"
                    },
                    {
                        "name": "Behrus Puladi"
                    },
                    {
                        "name": "Carlos Martín-Isla"
                    },
                    {
                        "name": "Karim Lekadir"
                    },
                    {
                        "name": "Victor M. Campello"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Wayne Brisbane"
                    },
                    {
                        "name": "Hongxu Jiang"
                    },
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Wu Yuan"
                    },
                    {
                        "name": "Shuangle Li"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "CVPR 2024 MedSAM on Laptop Competition Summary:\n  https://www.codabench.org/competitions/1847/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16064v1",
                "updated": "2024-12-20T17:06:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    6,
                    55,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:06:55Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    6,
                    55,
                    4,
                    355,
                    0
                ],
                "title": "VaulTor: Putting the TEE in Tor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VaulTor: Putting the TEE in Tor"
                },
                "summary": "Online services that desire to operate anonymously routinely host themselves\nas 'Hidden Services' in the Tor network. However, these services are frequently\nthreatened by deanonymization attacks, whereby their IP address and location\nmay be inferred by the authorities. We present VaulTor, a novel architecture\nfor the Tor network to ensure an extra layer of security for the Hidden\nServices against deanonymization attacks. In this new architecture, a volunteer\n(vault) is incentivized to host the web application content on behalf of the\nHidden Service. The vault runs the hosted application in a Trusted Execution\nEnvironment (TEE) and becomes the point of contact for interested clients. This\nsetup can substantially reduce the uptime requirement of the original Hidden\nService provider and hence significantly decrease the chance of deanonymization\nattacks against them. We also show that the VaulTor architecture does not cause\nany noticeable performance degradation in accessing the hosted content (the\nperformance degradation ranges from 2.6-5.5%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online services that desire to operate anonymously routinely host themselves\nas 'Hidden Services' in the Tor network. However, these services are frequently\nthreatened by deanonymization attacks, whereby their IP address and location\nmay be inferred by the authorities. We present VaulTor, a novel architecture\nfor the Tor network to ensure an extra layer of security for the Hidden\nServices against deanonymization attacks. In this new architecture, a volunteer\n(vault) is incentivized to host the web application content on behalf of the\nHidden Service. The vault runs the hosted application in a Trusted Execution\nEnvironment (TEE) and becomes the point of contact for interested clients. This\nsetup can substantially reduce the uptime requirement of the original Hidden\nService provider and hence significantly decrease the chance of deanonymization\nattacks against them. We also show that the VaulTor architecture does not cause\nany noticeable performance degradation in accessing the hosted content (the\nperformance degradation ranges from 2.6-5.5%)."
                },
                "authors": [
                    {
                        "name": "Humza Ikram"
                    },
                    {
                        "name": "Rumaisa Habib"
                    },
                    {
                        "name": "Muaz Ali"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    }
                ],
                "author_detail": {
                    "name": "Zartash Afzal Uzmi"
                },
                "author": "Zartash Afzal Uzmi",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10958v2",
                "updated": "2024-12-20T16:59:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    59,
                    40,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-14T20:29:29Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    20,
                    29,
                    29,
                    5,
                    349,
                    0
                ],
                "title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer"
                },
                "summary": "Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VAE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VAE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Fangyi Chen"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Bhiksha Raj"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Code and model: https://github.com/Hhhhhhao/continuous_tokenizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16046v1",
                "updated": "2024-12-20T16:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    48,
                    52,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T16:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    48,
                    52,
                    4,
                    355,
                    0
                ],
                "title": "Segmentation of arbitrary features in very high resolution remote\n  sensing imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of arbitrary features in very high resolution remote\n  sensing imagery"
                },
                "summary": "Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper ."
                },
                "authors": [
                    {
                        "name": "Henry Cording"
                    },
                    {
                        "name": "Yves Plancherel"
                    },
                    {
                        "name": "Pablo Brito-Parada"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Brito-Parada"
                },
                "author": "Pablo Brito-Parada",
                "arxiv_comment": "Main article: 18 pages, 9 figures; appendix: 17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16714v2",
                "updated": "2024-12-20T16:26:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    26,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-22T05:51:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    5,
                    51,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment"
                },
                "summary": "Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment."
                },
                "authors": [
                    {
                        "name": "Mingzhi Wang"
                    },
                    {
                        "name": "Chengdong Ma"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Linjian Meng"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Jiancong Xiao"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Weijie J. Su"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06144v3",
                "updated": "2024-12-20T16:25:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    25,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-10T10:03:16Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    10,
                    3,
                    16,
                    0,
                    162,
                    0
                ],
                "title": "Language Models Resist Alignment: Evidence From Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Resist Alignment: Evidence From Data Compression"
                },
                "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment."
                },
                "authors": [
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Changye Li"
                    },
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Josef Dai"
                    },
                    {
                        "name": "Yunhuai Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "The five-page version has been accepted by NeurIPS 2024 Workshop\n  SoLaR. In the current version, we have conducted an in-depth expansion of\n  both the theoretical and experimental aspects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01345v2",
                "updated": "2024-12-20T16:18:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    18,
                    13,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-03T14:08:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    14,
                    8,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "BMRS: Bayesian Model Reduction for Structured Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BMRS: Bayesian Model Reduction for Structured Pruning"
                },
                "summary": "Modern neural networks are often massively overparameterized leading to high\ncompute costs during training and at inference. One effective method to improve\nboth the compute and energy efficiency of neural networks while maintaining\ngood performance is structured pruning, where full network structures\n(e.g.~neurons or convolutional filters) that have limited impact on the model\noutput are removed. In this work, we propose Bayesian Model Reduction for\nStructured pruning (BMRS), a fully end-to-end Bayesian method of structured\npruning. BMRS is based on two recent methods: Bayesian structured pruning with\nmultiplicative noise, and Bayesian model reduction (BMR), a method which allows\nefficient comparison of Bayesian models under a change in prior. We present two\nrealizations of BMRS derived from different priors which yield different\nstructured pruning characteristics: 1) BMRS_N with the truncated log-normal\nprior, which offers reliable compression rates and accuracy without the need\nfor tuning any thresholds and 2) BMRS_U with the truncated log-uniform prior\nthat can achieve more aggressive compression based on the boundaries of\ntruncation. Overall, we find that BMRS offers a theoretically grounded approach\nto structured pruning of neural networks yielding both high compression rates\nand accuracy. Experiments on multiple datasets and neural networks of varying\ncomplexity showed that the two BMRS methods offer a competitive\nperformance-efficiency trade-off compared to other pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern neural networks are often massively overparameterized leading to high\ncompute costs during training and at inference. One effective method to improve\nboth the compute and energy efficiency of neural networks while maintaining\ngood performance is structured pruning, where full network structures\n(e.g.~neurons or convolutional filters) that have limited impact on the model\noutput are removed. In this work, we propose Bayesian Model Reduction for\nStructured pruning (BMRS), a fully end-to-end Bayesian method of structured\npruning. BMRS is based on two recent methods: Bayesian structured pruning with\nmultiplicative noise, and Bayesian model reduction (BMR), a method which allows\nefficient comparison of Bayesian models under a change in prior. We present two\nrealizations of BMRS derived from different priors which yield different\nstructured pruning characteristics: 1) BMRS_N with the truncated log-normal\nprior, which offers reliable compression rates and accuracy without the need\nfor tuning any thresholds and 2) BMRS_U with the truncated log-uniform prior\nthat can achieve more aggressive compression based on the boundaries of\ntruncation. Overall, we find that BMRS offers a theoretically grounded approach\nto structured pruning of neural networks yielding both high compression rates\nand accuracy. Experiments on multiple datasets and neural networks of varying\ncomplexity showed that the two BMRS methods offer a competitive\nperformance-efficiency trade-off compared to other pruning methods."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Christian Igel"
                    },
                    {
                        "name": "Raghavendra Selvan"
                    }
                ],
                "author_detail": {
                    "name": "Raghavendra Selvan"
                },
                "author": "Raghavendra Selvan",
                "arxiv_comment": "NeurIPS 2024 Spotlight; 19 pages; 7 figures; 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16022v1",
                "updated": "2024-12-20T16:14:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    14,
                    43,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T16:14:43Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    14,
                    43,
                    4,
                    355,
                    0
                ],
                "title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Only Way is Ethics: A Guide to Ethical Research with Large Language\n  Models"
                },
                "summary": "There is a significant body of work looking at the ethical considerations of\nlarge language models (LLMs): critiquing tools to measure performance and\nharms; proposing toolkits to aid in ideation; discussing the risks to workers;\nconsidering legislation around privacy and security etc. As yet there is no\nwork that integrates these resources into a single practical guide that focuses\non LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',\nwhich we provide as an open and living resource for NLP practitioners, and\nthose tasked with evaluating the ethical implications of others' work. Our goal\nis to translate ethics literature into concrete recommendations and\nprovocations for thinking with clear first steps, aimed at computer scientists.\n'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's\nand Don'ts, which we present also in this paper. We likewise identify useful\ntoolkits to support ethical work. We refer the interested reader to the full\nLLM Ethics Whitepaper, which provides a succinct discussion of ethical\nconsiderations at each stage in a project lifecycle, as well as citations for\nthe hundreds of papers from which we drew our recommendations. The present\npaper can be thought of as a pocket guide to conducting ethical research with\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a significant body of work looking at the ethical considerations of\nlarge language models (LLMs): critiquing tools to measure performance and\nharms; proposing toolkits to aid in ideation; discussing the risks to workers;\nconsidering legislation around privacy and security etc. As yet there is no\nwork that integrates these resources into a single practical guide that focuses\non LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',\nwhich we provide as an open and living resource for NLP practitioners, and\nthose tasked with evaluating the ethical implications of others' work. Our goal\nis to translate ethics literature into concrete recommendations and\nprovocations for thinking with clear first steps, aimed at computer scientists.\n'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's\nand Don'ts, which we present also in this paper. We likewise identify useful\ntoolkits to support ethical work. We refer the interested reader to the full\nLLM Ethics Whitepaper, which provides a succinct discussion of ethical\nconsiderations at each stage in a project lifecycle, as well as citations for\nthe hundreds of papers from which we drew our recommendations. The present\npaper can be thought of as a pocket guide to conducting ethical research with\nLLMs."
                },
                "authors": [
                    {
                        "name": "Eddie L. Ungless"
                    },
                    {
                        "name": "Nikolas Vitsakis"
                    },
                    {
                        "name": "Zeerak Talat"
                    },
                    {
                        "name": "James Garforth"
                    },
                    {
                        "name": "Björn Ross"
                    },
                    {
                        "name": "Arno Onken"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "Accepted to COLING '25. This paper is the condensed pocket guide to\n  accompany our full LLM Ethics Whitepaper, available at arXiv:2410.19812, and\n  at https://github.com/MxEddie/Ethics-Whitepaper for suggested revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14426v2",
                "updated": "2024-12-20T15:57:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    57,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T00:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    0,
                    41,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs"
                },
                "summary": "Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively."
                },
                "authors": [
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Zhepeng Wang"
                    },
                    {
                        "name": "Runxue Bao"
                    },
                    {
                        "name": "Mengbing Wang"
                    },
                    {
                        "name": "Fangyi Li"
                    },
                    {
                        "name": "Yawen Wu"
                    },
                    {
                        "name": "Weiwen Jiang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Shangqian Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shangqian Gao"
                },
                "author": "Shangqian Gao",
                "arxiv_comment": "Updated a typo in the author list;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15993v1",
                "updated": "2024-12-20T15:41:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "title": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs"
                },
                "summary": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions."
                },
                "authors": [
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v4",
                "updated": "2024-12-20T15:26:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    26,
                    33,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15983v1",
                "updated": "2024-12-20T15:24:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    24,
                    28,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:24:28Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    24,
                    28,
                    4,
                    355,
                    0
                ],
                "title": "Never Reset Again: A Mathematical Framework for Continual Inference in\n  Recurrent Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Never Reset Again: A Mathematical Framework for Continual Inference in\n  Recurrent Neural Networks"
                },
                "summary": "Recurrent Neural Networks (RNNs) are widely used for sequential processing\nbut face fundamental limitations with continual inference due to state\nsaturation, requiring disruptive hidden state resets. However, reset-based\nmethods impose synchronization requirements with input boundaries and increase\ncomputational costs at inference. To address this, we propose an adaptive loss\nfunction that eliminates the need for resets during inference while preserving\nhigh accuracy over extended sequences. By combining cross-entropy and\nKullback-Leibler divergence, the loss dynamically modulates the gradient based\non input informativeness, allowing the network to differentiate meaningful data\nfrom noise and maintain stable representations over time. Experimental results\ndemonstrate that our reset-free approach outperforms traditional reset-based\nmethods when applied to a variety of RNNs, particularly in continual tasks,\nenhancing both the theoretical and practical capabilities of RNNs for streaming\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Neural Networks (RNNs) are widely used for sequential processing\nbut face fundamental limitations with continual inference due to state\nsaturation, requiring disruptive hidden state resets. However, reset-based\nmethods impose synchronization requirements with input boundaries and increase\ncomputational costs at inference. To address this, we propose an adaptive loss\nfunction that eliminates the need for resets during inference while preserving\nhigh accuracy over extended sequences. By combining cross-entropy and\nKullback-Leibler divergence, the loss dynamically modulates the gradient based\non input informativeness, allowing the network to differentiate meaningful data\nfrom noise and maintain stable representations over time. Experimental results\ndemonstrate that our reset-free approach outperforms traditional reset-based\nmethods when applied to a variety of RNNs, particularly in continual tasks,\nenhancing both the theoretical and practical capabilities of RNNs for streaming\napplications."
                },
                "authors": [
                    {
                        "name": "Bojian Yin"
                    },
                    {
                        "name": "Federico Corradi"
                    }
                ],
                "author_detail": {
                    "name": "Federico Corradi"
                },
                "author": "Federico Corradi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15973v1",
                "updated": "2024-12-20T15:18:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:18:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Legommenders: A Comprehensive Content-Based Recommendation Library with\n  LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legommenders: A Comprehensive Content-Based Recommendation Library with\n  LLM Support"
                },
                "summary": "We present Legommenders, a unique library designed for content-based\nrecommendation that enables the joint training of content encoders alongside\nbehavior and interaction modules, thereby facilitating the seamless integration\nof content understanding directly into the recommendation pipeline.\nLegommenders allows researchers to effortlessly create and analyze over 1,000\ndistinct models across 15 diverse datasets. Further, it supports the\nincorporation of contemporary large language models, both as feature encoder\nand data generator, offering a robust platform for developing state-of-the-art\nrecommendation models and enabling more personalized and effective content\ndelivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Legommenders, a unique library designed for content-based\nrecommendation that enables the joint training of content encoders alongside\nbehavior and interaction modules, thereby facilitating the seamless integration\nof content understanding directly into the recommendation pipeline.\nLegommenders allows researchers to effortlessly create and analyze over 1,000\ndistinct models across 15 diverse datasets. Further, it supports the\nincorporation of contemporary large language models, both as feature encoder\nand data generator, offering a robust platform for developing state-of-the-art\nrecommendation models and enabling more personalized and effective content\ndelivery."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10825v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10825v3",
                "updated": "2024-12-20T15:11:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    11,
                    13,
                    4,
                    355,
                    0
                ],
                "published": "2024-01-19T17:21:05Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    17,
                    21,
                    5,
                    4,
                    19,
                    0
                ],
                "title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study"
                },
                "summary": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Stanislas Morbieu"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10825v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10825v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68Q32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13682v2",
                "updated": "2024-12-20T15:08:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    8,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T10:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    10,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning"
                },
                "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios."
                },
                "authors": [
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Baizhi Chen"
                    },
                    {
                        "name": "Wen-Da Wei"
                    },
                    {
                        "name": "Guohao Cai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-feng Li"
                },
                "author": "Yu-feng Li",
                "arxiv_comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14622v2",
                "updated": "2024-12-20T15:06:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    6,
                    14,
                    4,
                    355,
                    0
                ],
                "published": "2024-03-21T17:59:35Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    17,
                    59,
                    35,
                    3,
                    81,
                    0
                ],
                "title": "Language Repository for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Repository for Long Video Understanding"
                },
                "summary": "Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Jongwoo Park"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v5",
                "updated": "2024-12-20T14:58:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    58,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Bytedance-Seed-Foundation-Code-Team"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Jianfeng Chen"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Wentao Chen"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Kaibo Liu"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Tingkai Liu"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Guanghan Ning"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Jiahao Su"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Yunzhe Tao"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Liang Xiang"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yongsheng Xiao"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Shikun Xu"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Jianbo Yuan"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yuyu Zhang"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Ming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhu"
                },
                "author": "Ming Zhu",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10643v2",
                "updated": "2024-12-20T14:55:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    55,
                    54,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-14T02:08:38Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    2,
                    8,
                    38,
                    5,
                    349,
                    0
                ],
                "title": "Scientific Realism vs. Anti-Realism: Toward a Common Ground",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Realism vs. Anti-Realism: Toward a Common Ground"
                },
                "summary": "The debate between scientific realism and anti-realism remains at a\nstalemate, making reconciliation seem hopeless. Yet, important work remains:\nexploring a common ground, even if only to uncover deeper points of\ndisagreement and, ideally, to benefit both sides of the debate. I propose such\na common ground. Specifically, many anti-realists, such as instrumentalists,\nhave yet to seriously engage with Sober's call to justify their preferred\nversion of Ockham's razor through a positive account. Meanwhile, realists face\na similar challenge: providing a non-circular explanation of how their version\nof Ockham's razor connects to truth. The common ground I propose addresses\nthese challenges for both sides; the key is to leverage the idea that everyone\nvalues some truths and to draw on insights from scientific fields that study\nscientific inference -- namely, statistics and machine learning. This common\nground also isolates a distinctively epistemic root of the irreconcilability in\nthe realism debate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The debate between scientific realism and anti-realism remains at a\nstalemate, making reconciliation seem hopeless. Yet, important work remains:\nexploring a common ground, even if only to uncover deeper points of\ndisagreement and, ideally, to benefit both sides of the debate. I propose such\na common ground. Specifically, many anti-realists, such as instrumentalists,\nhave yet to seriously engage with Sober's call to justify their preferred\nversion of Ockham's razor through a positive account. Meanwhile, realists face\na similar challenge: providing a non-circular explanation of how their version\nof Ockham's razor connects to truth. The common ground I propose addresses\nthese challenges for both sides; the key is to leverage the idea that everyone\nvalues some truths and to draw on insights from scientific fields that study\nscientific inference -- namely, statistics and machine learning. This common\nground also isolates a distinctively epistemic root of the irreconcilability in\nthe realism debate."
                },
                "authors": [
                    {
                        "name": "Hanti Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hanti Lin"
                },
                "author": "Hanti Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15957v1",
                "updated": "2024-12-20T14:51:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    51,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:51:12Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    51,
                    12,
                    4,
                    355,
                    0
                ],
                "title": "From General to Specific: Tailoring Large Language Models for\n  Personalized Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From General to Specific: Tailoring Large Language Models for\n  Personalized Healthcare"
                },
                "summary": "The rapid development of large language models (LLMs) has transformed many\nindustries, including healthcare. However, previous medical LLMs have largely\nfocused on leveraging general medical knowledge to provide responses, without\naccounting for patient variability and lacking true personalization at the\nindividual level. To address this, we propose a novel method called\npersonalized medical language model (PMLM), which explores and optimizes\npersonalized LLMs through recommendation systems and reinforcement learning\n(RL). Specifically, by utilizing self-informed and peer-informed\npersonalization, PMLM captures changes in behaviors and preferences to design\ninitial personalized prompts tailored to individual needs. We further refine\nthese initial personalized prompts through RL, ultimately enhancing the\nprecision of LLM guidance. Notably, the personalized prompt are hard prompt,\nwhich grants PMLM high adaptability and reusability, allowing it to directly\nleverage high-quality proprietary LLMs. We evaluate PMLM using real-world\nobstetrics and gynecology data, and the experimental results demonstrate that\nPMLM achieves personalized responses, and it provides more refined and\nindividualized services, offering a potential way for personalized medical\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has transformed many\nindustries, including healthcare. However, previous medical LLMs have largely\nfocused on leveraging general medical knowledge to provide responses, without\naccounting for patient variability and lacking true personalization at the\nindividual level. To address this, we propose a novel method called\npersonalized medical language model (PMLM), which explores and optimizes\npersonalized LLMs through recommendation systems and reinforcement learning\n(RL). Specifically, by utilizing self-informed and peer-informed\npersonalization, PMLM captures changes in behaviors and preferences to design\ninitial personalized prompts tailored to individual needs. We further refine\nthese initial personalized prompts through RL, ultimately enhancing the\nprecision of LLM guidance. Notably, the personalized prompt are hard prompt,\nwhich grants PMLM high adaptability and reusability, allowing it to directly\nleverage high-quality proprietary LLMs. We evaluate PMLM using real-world\nobstetrics and gynecology data, and the experimental results demonstrate that\nPMLM achieves personalized responses, and it provides more refined and\nindividualized services, offering a potential way for personalized medical\nLLMs."
                },
                "authors": [
                    {
                        "name": "Ruize Shi"
                    },
                    {
                        "name": "Hong Huang"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Kehan Yin"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Yun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yun Zhao"
                },
                "author": "Yun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15948v1",
                "updated": "2024-12-20T14:44:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    44,
                    11,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:44:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    44,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI\n  Refactoring"
                },
                "summary": "In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions."
                },
                "authors": [
                    {
                        "name": "Markus Borg"
                    }
                ],
                "author_detail": {
                    "name": "Markus Borg"
                },
                "author": "Markus Borg",
                "arxiv_comment": "Accepted for publication in the Proc. of the 2nd Workshop on\n  Integrated Development Environments, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15947v1",
                "updated": "2024-12-20T14:43:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    43,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:43:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    43,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Mamba-based Deep Learning Approaches for Sleep Staging on a Wireless\n  Multimodal Wearable System without Electroencephalography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-based Deep Learning Approaches for Sleep Staging on a Wireless\n  Multimodal Wearable System without Electroencephalography"
                },
                "summary": "Study Objectives: We investigate using Mamba-based deep learning approaches\nfor sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a\nminimally intrusive dual-sensor wireless wearable system measuring chest\nelectrocardiography (ECG), triaxial accelerometry, and temperature, as well as\nfinger photoplethysmography (PPG) and temperature.\n  Methods: We obtained wearable sensor recordings from 360 adults undergoing\nconcurrent clinical polysomnography (PSG) at a tertiary care sleep lab. PSG\nrecordings were scored according to AASM criteria. PSG and wearable sensor data\nwere automatically aligned using their ECG channels with manual confirmation by\nvisual inspection. We trained Mamba-based models with both\nconvolutional-recurrent neural network (CRNN) and the recurrent neural network\n(RNN) architectures on these recordings. Ensembling of model variants with\nsimilar architectures was performed.\n  Results: Our best approach, after ensembling, attains a 3-class (wake, NREM,\nREM) balanced accuracy of 83.50%, F1 score of 84.16%, Cohen's $\\kappa$ of\n72.68%, and a MCC score of 72.84%; a 4-class (wake, N1/N2, N3, REM) balanced\naccuracy of 74.64%, F1 score of 74.56%, Cohen's $\\kappa$ of 61.63%, and MCC\nscore of 62.04%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 64.30%,\nF1 score of 66.97%, Cohen's $\\kappa$ of 53.23%, MCC score of 54.38%.\n  Conclusions: Deep learning models can infer major sleep stages from a\nwearable system without electroencephalography (EEG) and can be successfully\napplied to data from adults attending a tertiary care sleep clinic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study Objectives: We investigate using Mamba-based deep learning approaches\nfor sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a\nminimally intrusive dual-sensor wireless wearable system measuring chest\nelectrocardiography (ECG), triaxial accelerometry, and temperature, as well as\nfinger photoplethysmography (PPG) and temperature.\n  Methods: We obtained wearable sensor recordings from 360 adults undergoing\nconcurrent clinical polysomnography (PSG) at a tertiary care sleep lab. PSG\nrecordings were scored according to AASM criteria. PSG and wearable sensor data\nwere automatically aligned using their ECG channels with manual confirmation by\nvisual inspection. We trained Mamba-based models with both\nconvolutional-recurrent neural network (CRNN) and the recurrent neural network\n(RNN) architectures on these recordings. Ensembling of model variants with\nsimilar architectures was performed.\n  Results: Our best approach, after ensembling, attains a 3-class (wake, NREM,\nREM) balanced accuracy of 83.50%, F1 score of 84.16%, Cohen's $\\kappa$ of\n72.68%, and a MCC score of 72.84%; a 4-class (wake, N1/N2, N3, REM) balanced\naccuracy of 74.64%, F1 score of 74.56%, Cohen's $\\kappa$ of 61.63%, and MCC\nscore of 62.04%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 64.30%,\nF1 score of 66.97%, Cohen's $\\kappa$ of 53.23%, MCC score of 54.38%.\n  Conclusions: Deep learning models can infer major sleep stages from a\nwearable system without electroencephalography (EEG) and can be successfully\napplied to data from adults attending a tertiary care sleep clinic."
                },
                "authors": [
                    {
                        "name": "Andrew H. Zhang"
                    },
                    {
                        "name": "Alex He-Mo"
                    },
                    {
                        "name": "Richard Fei Yin"
                    },
                    {
                        "name": "Chunlin Li"
                    },
                    {
                        "name": "Yuzhi Tang"
                    },
                    {
                        "name": "Dharmendra Gurve"
                    },
                    {
                        "name": "Nasim Montazeri Ghahjaverestan"
                    },
                    {
                        "name": "Maged Goubran"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Andrew S. P. Lim"
                    }
                ],
                "author_detail": {
                    "name": "Andrew S. P. Lim"
                },
                "author": "Andrew S. P. Lim",
                "arxiv_comment": "21 pages, 11 figures. Authors Andrew H. Zhang, Alex He-Mo, and\n  Richard Fei Yin contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12808v2",
                "updated": "2024-12-20T14:39:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    39,
                    34,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-17T11:25:55Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    25,
                    55,
                    1,
                    352,
                    0
                ],
                "title": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning"
                },
                "summary": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR."
                },
                "authors": [
                    {
                        "name": "Ziqi Qiu"
                    },
                    {
                        "name": "Jianxing Yu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Hanjiang Lai"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Qinliang Su"
                    },
                    {
                        "name": "Jian Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yin"
                },
                "author": "Jian Yin",
                "arxiv_comment": "In the experimental chapter, there is a problem with the experimental\n  setting and needs to be corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15931v1",
                "updated": "2024-12-20T14:23:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    23,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:23:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    23,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Large Language Model assisted Hybrid Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model assisted Hybrid Fuzzing"
                },
                "summary": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors."
                },
                "authors": [
                    {
                        "name": "Ruijie Meng"
                    },
                    {
                        "name": "Gregory J. Duck"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15921v1",
                "updated": "2024-12-20T14:13:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    13,
                    9,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:13:09Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    13,
                    9,
                    4,
                    355,
                    0
                ],
                "title": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning"
                },
                "summary": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "UNDER REVIEW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06984v2",
                "updated": "2024-12-20T14:05:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    5,
                    41,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-09T15:21:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    21,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Employing observability rank conditions for taking into account\n  experimental information a priori",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing observability rank conditions for taking into account\n  experimental information a priori"
                },
                "summary": "The concept of identifiability describes the possibility of inferring the\nparameters of a dynamic model by observing its output. It is common and useful\nto distinguish between structural and practical identifiability. The former\nproperty is fully determined by the model equations, while the latter is also\ninfluenced by the characteristics of the available experimental data.\nStructural identifiability can be determined by means of symbolic computations,\nwhich may be performed before collecting experimental data, and are hence\nsometimes called a priori analyses. Practical identifiability is typically\nassessed numerically, with methods that require simulations - and often also\noptimization - and are applied a posteriori. An approach to study structural\nlocal identifiability is to consider it as a particular case of observability,\nwhich is the possibility of inferring the internal state of a system from its\noutput. Thus, both properties can be analysed jointly, by building a\ngeneralized observability matrix and computing its rank. The aim of this paper\nis to investigate to which extent such observability-based methods can also\ninform about practical aspects related with the experimental setup, which are\nusually not approached in this way. To this end, we explore a number of\npossible extensions of the rank tests, and discuss the purposes for which they\ncan be informative as well as others for which they cannot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of identifiability describes the possibility of inferring the\nparameters of a dynamic model by observing its output. It is common and useful\nto distinguish between structural and practical identifiability. The former\nproperty is fully determined by the model equations, while the latter is also\ninfluenced by the characteristics of the available experimental data.\nStructural identifiability can be determined by means of symbolic computations,\nwhich may be performed before collecting experimental data, and are hence\nsometimes called a priori analyses. Practical identifiability is typically\nassessed numerically, with methods that require simulations - and often also\noptimization - and are applied a posteriori. An approach to study structural\nlocal identifiability is to consider it as a particular case of observability,\nwhich is the possibility of inferring the internal state of a system from its\noutput. Thus, both properties can be analysed jointly, by building a\ngeneralized observability matrix and computing its rank. The aim of this paper\nis to investigate to which extent such observability-based methods can also\ninform about practical aspects related with the experimental setup, which are\nusually not approached in this way. To this end, we explore a number of\npossible extensions of the rank tests, and discuss the purposes for which they\ncan be informative as well as others for which they cannot."
                },
                "authors": [
                    {
                        "name": "Alejandro F. Villaverde"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro F. Villaverde"
                },
                "author": "Alejandro F. Villaverde",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15902v1",
                "updated": "2024-12-20T13:54:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    54,
                    57,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:54:57Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    54,
                    57,
                    4,
                    355,
                    0
                ],
                "title": "On the Suitability of pre-trained foundational LLMs for Analysis in\n  German Legal Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Suitability of pre-trained foundational LLMs for Analysis in\n  German Legal Education"
                },
                "summary": "We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase."
                },
                "authors": [
                    {
                        "name": "Lorenz Wendlinger"
                    },
                    {
                        "name": "Christian Braun"
                    },
                    {
                        "name": "Abdullah Al Zubaer"
                    },
                    {
                        "name": "Simon Alexander Nonn"
                    },
                    {
                        "name": "Sarah Großkopf"
                    },
                    {
                        "name": "Christofer Fellicious"
                    },
                    {
                        "name": "Michael Granitzer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Granitzer"
                },
                "author": "Michael Granitzer",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15896v1",
                "updated": "2024-12-20T13:50:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "title": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models"
                },
                "summary": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting."
                },
                "authors": [
                    {
                        "name": "Manuel Pratelli"
                    },
                    {
                        "name": "John Bianchi"
                    },
                    {
                        "name": "Fabio Pinelli"
                    },
                    {
                        "name": "Marinella Petrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Marinella Petrocchi"
                },
                "author": "Marinella Petrocchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15893v1",
                "updated": "2024-12-20T13:48:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    48,
                    40,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:48:40Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    48,
                    40,
                    4,
                    355,
                    0
                ],
                "title": "Reproducibility of machine learning analyses of 21 cm reionization maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility of machine learning analyses of 21 cm reionization maps"
                },
                "summary": "Machine learning (ML) methods have become popular for parameter inference in\ncosmology, although their reliance on specific training data can cause\ndifficulties when applied across different data sets. By reproducing and\ntesting networks previously used in the field, and applied to 21cmFast and\nSimfast21 simulations, we show that convolutional neural networks (CNNs) often\nlearn to identify features of individual simulation boxes rather than the\nunderlying physics, limiting their applicability to real observations. We\nexamine the prediction of the neutral fraction and astrophysical parameters\nfrom 21 cm maps and find that networks typically fail to generalise to unseen\nsimulations. We explore a number of case studies to highlight factors that\nimprove or degrade network performance. These results emphasise the\nresponsibility on users to ensure ML models are applied correctly in 21 cm\ncosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) methods have become popular for parameter inference in\ncosmology, although their reliance on specific training data can cause\ndifficulties when applied across different data sets. By reproducing and\ntesting networks previously used in the field, and applied to 21cmFast and\nSimfast21 simulations, we show that convolutional neural networks (CNNs) often\nlearn to identify features of individual simulation boxes rather than the\nunderlying physics, limiting their applicability to real observations. We\nexamine the prediction of the neutral fraction and astrophysical parameters\nfrom 21 cm maps and find that networks typically fail to generalise to unseen\nsimulations. We explore a number of case studies to highlight factors that\nimprove or degrade network performance. These results emphasise the\nresponsibility on users to ensure ML models are applied correctly in 21 cm\ncosmology."
                },
                "authors": [
                    {
                        "name": "Kimeel Sooknunan"
                    },
                    {
                        "name": "Emma Chapman"
                    },
                    {
                        "name": "Luke Conaboy"
                    },
                    {
                        "name": "Daniel Mortlock"
                    },
                    {
                        "name": "Jonathan Pritchard"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Pritchard"
                },
                "author": "Jonathan Pritchard",
                "arxiv_comment": "14 pages, 12 figures, submitted to MNRAS, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15891v1",
                "updated": "2024-12-20T13:47:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    47,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:47:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    47,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain"
                },
                "summary": "Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand."
                },
                "authors": [
                    {
                        "name": "Camille Barboule"
                    },
                    {
                        "name": "Viet-Phi Huynh"
                    },
                    {
                        "name": "Adrien Bufort"
                    },
                    {
                        "name": "Yoan Chabot"
                    },
                    {
                        "name": "Géraldine Damnati"
                    },
                    {
                        "name": "Gwénolé Lecorvé"
                    }
                ],
                "author_detail": {
                    "name": "Gwénolé Lecorvé"
                },
                "author": "Gwénolé Lecorvé",
                "arxiv_comment": "30 pages (main: 13 pages, appendices: 17 pages), 1 figure, 22 tables,\n  achieved March 2024, released December 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01866v3",
                "updated": "2024-12-20T13:19:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    19,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-03T21:31:34Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    21,
                    31,
                    34,
                    5,
                    216,
                    0
                ],
                "title": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively."
                },
                "authors": [
                    {
                        "name": "Peyman Hosseini"
                    },
                    {
                        "name": "Ignacio Castro"
                    },
                    {
                        "name": "Iacopo Ghinassi"
                    },
                    {
                        "name": "Matthew Purver"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Purver"
                },
                "author": "Matthew Purver",
                "arxiv_comment": "12 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14170v2",
                "updated": "2024-12-20T13:15:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    15,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-23T04:54:37Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    4,
                    54,
                    37,
                    3,
                    144,
                    0
                ],
                "title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning"
                },
                "summary": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks."
                },
                "authors": [
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Linhao Luo"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yongli Hu"
                    },
                    {
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Baocai Yin"
                    }
                ],
                "author_detail": {
                    "name": "Baocai Yin"
                },
                "author": "Baocai Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14758v2",
                "updated": "2024-12-20T13:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    13,
                    52,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T11:39:10Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    39,
                    10,
                    3,
                    354,
                    0
                ],
                "title": "Semantic Foundations of Reductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Foundations of Reductive Reasoning"
                },
                "summary": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    },
                    {
                        "name": "David J. Pym"
                    }
                ],
                "author_detail": {
                    "name": "David J. Pym"
                },
                "author": "David J. Pym",
                "arxiv_journal_ref": "Topoi 2024, Special issue 'Meaning and Understanding via Proofs'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15860v1",
                "updated": "2024-12-20T12:55:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    55,
                    29,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T12:55:29Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    55,
                    29,
                    4,
                    355,
                    0
                ],
                "title": "Recovering the properties of the interstellar medium through integrated\n  spectroscopy: application to the z~0 ECO volume-limited star-forming galaxy\n  sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering the properties of the interstellar medium through integrated\n  spectroscopy: application to the z~0 ECO volume-limited star-forming galaxy\n  sample"
                },
                "summary": "Deriving physical parameters from integrated galaxy spectra is paramount to\ninterpret the cosmic evolution of star formation, chemical enrichment, and\nenergetic sources. We develop modeling techniques to characterize the ionized\ngas properties in the subset of 2052 star-forming galaxies from the\nvolume-limited, dwarf-dominated, z~0 ECO catalog. The MULTIGRIS statistical\nframework is used to evaluate the performance of various models using strong\nlines as constraints. The reference model involves physical parameters\ndistributed as power-laws with free parameter boundaries. Specifically, we use\ncombinations of 1D photoionization models (i.e., considering the propagation of\nradiation toward a single cloud) to match optical HII region lines, in order to\nprovide probability density functions of the inferred parameters. The inference\npredicts non-uniform physical conditions within galaxies. The integrated\nspectra of most galaxies are dominated by relatively low-excitation gas with a\nmetallicity around 0.3 solar. Using the average metallicity in galaxies, we\nprovide a new fit to the mass-metallicity relationship which is in line with\ndirect abundance method determinations from the calibrated range at low\nmetallicity to stacks at high metallicity. The average metallicity shows a\nweakly bimodal distribution which may be due related to external (e.g.,\nrefueling of non-cluster early-type galaxies above ~10^9.5 solar masses) or\ninternal processes (more efficient star-formation in metal-rich regions). The\nspecific line set used for inference affects the results and we identify\npotential issues with the use of the [SII] line doublet. Complex modelling\napproaches are limited by the inherent 1D model database as well as caveats\nregarding the gas geometry. Our results highlight, however, the possibility to\nextract useful and significant information from integrated spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving physical parameters from integrated galaxy spectra is paramount to\ninterpret the cosmic evolution of star formation, chemical enrichment, and\nenergetic sources. We develop modeling techniques to characterize the ionized\ngas properties in the subset of 2052 star-forming galaxies from the\nvolume-limited, dwarf-dominated, z~0 ECO catalog. The MULTIGRIS statistical\nframework is used to evaluate the performance of various models using strong\nlines as constraints. The reference model involves physical parameters\ndistributed as power-laws with free parameter boundaries. Specifically, we use\ncombinations of 1D photoionization models (i.e., considering the propagation of\nradiation toward a single cloud) to match optical HII region lines, in order to\nprovide probability density functions of the inferred parameters. The inference\npredicts non-uniform physical conditions within galaxies. The integrated\nspectra of most galaxies are dominated by relatively low-excitation gas with a\nmetallicity around 0.3 solar. Using the average metallicity in galaxies, we\nprovide a new fit to the mass-metallicity relationship which is in line with\ndirect abundance method determinations from the calibrated range at low\nmetallicity to stacks at high metallicity. The average metallicity shows a\nweakly bimodal distribution which may be due related to external (e.g.,\nrefueling of non-cluster early-type galaxies above ~10^9.5 solar masses) or\ninternal processes (more efficient star-formation in metal-rich regions). The\nspecific line set used for inference affects the results and we identify\npotential issues with the use of the [SII] line doublet. Complex modelling\napproaches are limited by the inherent 1D model database as well as caveats\nregarding the gas geometry. Our results highlight, however, the possibility to\nextract useful and significant information from integrated spectra."
                },
                "authors": [
                    {
                        "name": "V. Lebouteiller"
                    },
                    {
                        "name": "C. T. Richardson"
                    },
                    {
                        "name": "M. S. Polimera"
                    },
                    {
                        "name": "D. S. Carr"
                    },
                    {
                        "name": "Z. L. Hutchens"
                    },
                    {
                        "name": "S. J. Kannappan"
                    },
                    {
                        "name": "L. Ramambason"
                    },
                    {
                        "name": "A. J. Moffett"
                    },
                    {
                        "name": "M. Varese"
                    },
                    {
                        "name": "S. C. Madden"
                    }
                ],
                "author_detail": {
                    "name": "S. C. Madden"
                },
                "author": "S. C. Madden",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13975v3",
                "updated": "2024-12-20T12:52:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    52,
                    0,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-20T03:50:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    3,
                    50,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs"
                },
                "summary": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies."
                },
                "authors": [
                    {
                        "name": "Zhongshen Zeng"
                    },
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Pengguang Chen"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Linling Shen"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14276v2",
                "updated": "2024-12-20T12:45:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    45,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T19:15:17Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    19,
                    15,
                    17,
                    2,
                    353,
                    0
                ],
                "title": "Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data"
                },
                "summary": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Drai Paulen-Patterson"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "Accepted in Knowledge and Information Systems Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09569v2",
                "updated": "2024-12-20T12:25:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    25,
                    22,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-12T15:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    15,
                    33,
                    50,
                    5,
                    286,
                    0
                ],
                "title": "Are You Human? An Adversarial Benchmark to Expose LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are You Human? An Adversarial Benchmark to Expose LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations."
                },
                "authors": [
                    {
                        "name": "Gilad Gressel"
                    },
                    {
                        "name": "Rahul Pankajakshan"
                    },
                    {
                        "name": "Yisroel Mirsky"
                    }
                ],
                "author_detail": {
                    "name": "Yisroel Mirsky"
                },
                "author": "Yisroel Mirsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21868v2",
                "updated": "2024-12-20T12:22:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    22,
                    37,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-29T09:02:37Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    2,
                    37,
                    1,
                    303,
                    0
                ],
                "title": "Improving In-Context Learning with Small Language Model Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving In-Context Learning with Small Language Model Ensembles"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners."
                },
                "authors": [
                    {
                        "name": "M. Mehdi Mojarradi"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "Presented at NeurIPS 2024 Workshop on Adaptive Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15832v1",
                "updated": "2024-12-20T12:15:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    15,
                    54,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T12:15:54Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    15,
                    54,
                    4,
                    355,
                    0
                ],
                "title": "AIFS-CRPS: Ensemble forecasting using a model trained with a loss\n  function based on the Continuous Ranked Probability Score",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIFS-CRPS: Ensemble forecasting using a model trained with a loss\n  function based on the Continuous Ranked Probability Score"
                },
                "summary": "Over the last three decades, ensemble forecasts have become an integral part\nof forecasting the weather. They provide users with more complete information\nthan single forecasts as they permit to estimate the probability of weather\nevents by representing the sources of uncertainties and accounting for the\nday-to-day variability of error growth in the atmosphere. This paper presents a\nnovel approach to obtain a weather forecast model for ensemble forecasting with\nmachine-learning. AIFS-CRPS is a variant of the Artificial Intelligence\nForecasting System (AIFS) developed at ECMWF. Its loss function is based on a\nproper score, the Continuous Ranked Probability Score (CRPS). For the loss, the\nalmost fair CRPS is introduced because it approximately removes the bias in the\nscore due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The\ntrained model is stochastic and can generate as many exchangeable members as\ndesired and computationally feasible in inference. For medium-range forecasts\nAIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS)\nensemble for the majority of variables and lead times. For subseasonal\nforecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is\ncompetitive with the IFS ensemble when forecasts are evaluated as anomalies to\nremove the influence of model biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last three decades, ensemble forecasts have become an integral part\nof forecasting the weather. They provide users with more complete information\nthan single forecasts as they permit to estimate the probability of weather\nevents by representing the sources of uncertainties and accounting for the\nday-to-day variability of error growth in the atmosphere. This paper presents a\nnovel approach to obtain a weather forecast model for ensemble forecasting with\nmachine-learning. AIFS-CRPS is a variant of the Artificial Intelligence\nForecasting System (AIFS) developed at ECMWF. Its loss function is based on a\nproper score, the Continuous Ranked Probability Score (CRPS). For the loss, the\nalmost fair CRPS is introduced because it approximately removes the bias in the\nscore due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The\ntrained model is stochastic and can generate as many exchangeable members as\ndesired and computationally feasible in inference. For medium-range forecasts\nAIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS)\nensemble for the majority of variables and lead times. For subseasonal\nforecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is\ncompetitive with the IFS ensemble when forecasts are evaluated as anomalies to\nremove the influence of model biases."
                },
                "authors": [
                    {
                        "name": "Simon Lang"
                    },
                    {
                        "name": "Mihai Alexe"
                    },
                    {
                        "name": "Mariana C. A. Clare"
                    },
                    {
                        "name": "Christopher Roberts"
                    },
                    {
                        "name": "Rilwan Adewoyin"
                    },
                    {
                        "name": "Zied Ben Bouallègue"
                    },
                    {
                        "name": "Matthew Chantry"
                    },
                    {
                        "name": "Jesper Dramsch"
                    },
                    {
                        "name": "Peter D. Dueben"
                    },
                    {
                        "name": "Sara Hahner"
                    },
                    {
                        "name": "Pedro Maciel"
                    },
                    {
                        "name": "Ana Prieto-Nemesio"
                    },
                    {
                        "name": "Cathal O'Brien"
                    },
                    {
                        "name": "Florian Pinault"
                    },
                    {
                        "name": "Jan Polster"
                    },
                    {
                        "name": "Baudouin Raoult"
                    },
                    {
                        "name": "Steffen Tietsche"
                    },
                    {
                        "name": "Martin Leutbecher"
                    }
                ],
                "author_detail": {
                    "name": "Martin Leutbecher"
                },
                "author": "Martin Leutbecher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13993v3",
                "updated": "2024-12-20T12:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    6,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-07-19T02:48:54Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    48,
                    54,
                    4,
                    201,
                    0
                ],
                "title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models"
                },
                "summary": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    }
                ],
                "author_detail": {
                    "name": "Christoforus Yoga Haryanto"
                },
                "author": "Christoforus Yoga Haryanto",
                "arxiv_comment": "10 pages, 3 figures, 1 table, presented at the 51st International\n  Conference on Computers and Industrial Engineering (CIE51), 11 Dec 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15822v1",
                "updated": "2024-12-20T12:03:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    3,
                    33,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T12:03:33Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    3,
                    33,
                    4,
                    355,
                    0
                ],
                "title": "S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive\n  Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive\n  Knowledge Graph Completion"
                },
                "summary": "Inductive Knowledge Graph Completion (KGC) aims to infer missing facts\nbetween newly emerged entities within knowledge graphs (KGs), posing a\nsignificant challenge. While recent studies have shown promising results in\ninferring such entities through knowledge subgraph reasoning, they suffer from\n(i) the semantic inconsistencies of similar relations, and (ii) noisy\ninteractions inherent in KGs due to the presence of unconvincing knowledge for\nemerging entities. To address these challenges, we propose a Semantic\nStructure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to\nlearn adaptable general semantics and reliable structures to distill consistent\nsemantic knowledge while preserving reliable interactions within KGs.\nSpecifically, we introduce a semantic smoothing module over the enclosing\nsubgraphs to retain the universal semantic knowledge of relations. We\nincorporate a structure refining module to filter out unreliable interactions\nand offer additional knowledge, retaining robust structure surrounding target\nlinks. Extensive experiments conducted on three benchmark KGs demonstrate that\nS$^2$DN surpasses the performance of state-of-the-art models. These results\ndemonstrate the effectiveness of S$^2$DN in preserving semantic consistency and\nenhancing the robustness of filtering out unreliable interactions in\ncontaminated KGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Knowledge Graph Completion (KGC) aims to infer missing facts\nbetween newly emerged entities within knowledge graphs (KGs), posing a\nsignificant challenge. While recent studies have shown promising results in\ninferring such entities through knowledge subgraph reasoning, they suffer from\n(i) the semantic inconsistencies of similar relations, and (ii) noisy\ninteractions inherent in KGs due to the presence of unconvincing knowledge for\nemerging entities. To address these challenges, we propose a Semantic\nStructure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to\nlearn adaptable general semantics and reliable structures to distill consistent\nsemantic knowledge while preserving reliable interactions within KGs.\nSpecifically, we introduce a semantic smoothing module over the enclosing\nsubgraphs to retain the universal semantic knowledge of relations. We\nincorporate a structure refining module to filter out unreliable interactions\nand offer additional knowledge, retaining robust structure surrounding target\nlinks. Extensive experiments conducted on three benchmark KGs demonstrate that\nS$^2$DN surpasses the performance of state-of-the-art models. These results\ndemonstrate the effectiveness of S$^2$DN in preserving semantic consistency and\nenhancing the robustness of filtering out unreliable interactions in\ncontaminated KGs."
                },
                "authors": [
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Yujie Chen"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Xuan Lin"
                    },
                    {
                        "name": "Bosheng Song"
                    },
                    {
                        "name": "Xiangxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Zeng"
                },
                "author": "Xiangxiang Zeng",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v2",
                "updated": "2024-12-20T11:55:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    55,
                    35,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12581v2",
                "updated": "2024-12-20T11:49:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    49,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-17T06:20:39Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    20,
                    39,
                    1,
                    352,
                    0
                ],
                "title": "Understanding Emotional Body Expressions via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Emotional Body Expressions via Large Language Models"
                },
                "summary": "Emotion recognition based on body movements is vital in human-computer\ninteraction. However, existing emotion recognition methods predominantly focus\non enhancing classification accuracy, often neglecting the provision of textual\nexplanations to justify their classifications. In this paper, we propose an\nEmotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not\nonly recognizes emotions but also generates textual explanations by treating 3D\nbody movement data as unique input tokens within large language models (LLMs).\nSpecifically, we propose a multi-granularity skeleton tokenizer designed for\nLLMs, which separately extracts spatio-temporal tokens and semantic tokens from\nthe skeleton data. This approach allows LLMs to generate more nuanced\nclassification descriptions while maintaining robust classification\nperformance. Furthermore, we treat the skeleton sequence as a specific language\nand propose a unified skeleton token module. This module leverages the\nextensive background knowledge and language processing capabilities of LLMs to\naddress the challenges of joint training on heterogeneous datasets, thereby\nsignificantly enhancing recognition accuracy on individual datasets.\nExperimental results demonstrate that our model achieves recognition accuracy\ncomparable to existing methods. More importantly, with the support of\nbackground knowledge from LLMs, our model can generate detailed emotion\ndescriptions based on classification results, even when trained on a limited\namount of labeled skeleton data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition based on body movements is vital in human-computer\ninteraction. However, existing emotion recognition methods predominantly focus\non enhancing classification accuracy, often neglecting the provision of textual\nexplanations to justify their classifications. In this paper, we propose an\nEmotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not\nonly recognizes emotions but also generates textual explanations by treating 3D\nbody movement data as unique input tokens within large language models (LLMs).\nSpecifically, we propose a multi-granularity skeleton tokenizer designed for\nLLMs, which separately extracts spatio-temporal tokens and semantic tokens from\nthe skeleton data. This approach allows LLMs to generate more nuanced\nclassification descriptions while maintaining robust classification\nperformance. Furthermore, we treat the skeleton sequence as a specific language\nand propose a unified skeleton token module. This module leverages the\nextensive background knowledge and language processing capabilities of LLMs to\naddress the challenges of joint training on heterogeneous datasets, thereby\nsignificantly enhancing recognition accuracy on individual datasets.\nExperimental results demonstrate that our model achieves recognition accuracy\ncomparable to existing methods. More importantly, with the support of\nbackground knowledge from LLMs, our model can generate detailed emotion\ndescriptions based on classification results, even when trained on a limited\namount of labeled skeleton data."
                },
                "authors": [
                    {
                        "name": "Haifeng Lu"
                    },
                    {
                        "name": "Jiuyi Chen"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Runhao Zeng"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15809v1",
                "updated": "2024-12-20T11:40:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    40,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:40:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    40,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Prior-Posterior Derived-Predictive Consistency Checks for\n  Post-Estimation Calculated Quantities of Interest (QOI-Check)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior-Posterior Derived-Predictive Consistency Checks for\n  Post-Estimation Calculated Quantities of Interest (QOI-Check)"
                },
                "summary": "With flexible modeling software - such as the probabilistic programming\nlanguage Stan - growing in popularity, quantities of interest (QOIs) calculated\npost-estimation are increasingly desired and customly implemented, both by\nstatistical software developers and applied scientists. Examples of QOI include\nthe marginal expectation of a multilevel model with a non-linear link function,\nor an ANOVA decomposition of a bivariate regression spline. For this, the\nQOI-Check is introduced, a systematic approach to ensure proper calibration and\ncorrect interpretation of QOIs. It contributes to Bayesian Workflow, and aims\nto improve the interpretability and trust in post-estimation conclusions based\non QOIs. The QOI-Check builds upon Simulation Based Calibration (SBC), and the\nHoldout Predictive Check (HPC). SBC verifies computational reliability of\nBayesian inference algorithms by consistency check of posterior with prior when\nthe posterior is estimated on prior-predicted data, while HPC ensures robust\ninference by assessing consistency of model predictions with holdout data. SBC\nand HPC are combined in QOI-Checking for validating post-estimation QOI\ncalculation and interpretation in the context of a (hypothetical) population\ndefinition underlying the QOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With flexible modeling software - such as the probabilistic programming\nlanguage Stan - growing in popularity, quantities of interest (QOIs) calculated\npost-estimation are increasingly desired and customly implemented, both by\nstatistical software developers and applied scientists. Examples of QOI include\nthe marginal expectation of a multilevel model with a non-linear link function,\nor an ANOVA decomposition of a bivariate regression spline. For this, the\nQOI-Check is introduced, a systematic approach to ensure proper calibration and\ncorrect interpretation of QOIs. It contributes to Bayesian Workflow, and aims\nto improve the interpretability and trust in post-estimation conclusions based\non QOIs. The QOI-Check builds upon Simulation Based Calibration (SBC), and the\nHoldout Predictive Check (HPC). SBC verifies computational reliability of\nBayesian inference algorithms by consistency check of posterior with prior when\nthe posterior is estimated on prior-predicted data, while HPC ensures robust\ninference by assessing consistency of model predictions with holdout data. SBC\nand HPC are combined in QOI-Checking for validating post-estimation QOI\ncalculation and interpretation in the context of a (hypothetical) population\ndefinition underlying the QOI."
                },
                "authors": [
                    {
                        "name": "Holger Sennhenn-Reulen"
                    }
                ],
                "author_detail": {
                    "name": "Holger Sennhenn-Reulen"
                },
                "author": "Holger Sennhenn-Reulen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11532v3",
                "updated": "2024-12-20T11:25:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    25,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-18T12:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph"
                },
                "summary": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Ting Zhou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15803v1",
                "updated": "2024-12-20T11:24:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    24,
                    13,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:24:13Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    24,
                    13,
                    4,
                    355,
                    0
                ],
                "title": "WebLLM: A High-Performance In-Browser LLM Inference Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLLM: A High-Performance In-Browser LLM Inference Engine"
                },
                "summary": "Advancements in large language models (LLMs) have unlocked remarkable\ncapabilities. While deploying these models typically requires server-grade GPUs\nand cloud-based inference, the recent emergence of smaller open-source models\nand increasingly powerful consumer devices have made on-device deployment\npractical. The web browser as a platform for on-device deployment is\nuniversally accessible, provides a natural agentic environment, and\nconveniently abstracts out the different backends from diverse device vendors.\nTo address this opportunity, we introduce WebLLM, an open-source JavaScript\nframework that enables high-performance LLM inference entirely within web\nbrowsers. WebLLM provides an OpenAI-style API for seamless integration into web\napplications, and leverages WebGPU for efficient local GPU acceleration and\nWebAssembly for performant CPU computation. With machine learning compilers\nMLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming\nthe absence of performant WebGPU kernel libraries. Evaluations show that WebLLM\ncan retain up to 80% native performance on the same device, with room to\nfurther close the gap. WebLLM paves the way for universally accessible,\nprivacy-preserving, personalized, and locally powered LLM applications in web\nbrowsers. The code is available at: https://github.com/mlc-ai/web-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have unlocked remarkable\ncapabilities. While deploying these models typically requires server-grade GPUs\nand cloud-based inference, the recent emergence of smaller open-source models\nand increasingly powerful consumer devices have made on-device deployment\npractical. The web browser as a platform for on-device deployment is\nuniversally accessible, provides a natural agentic environment, and\nconveniently abstracts out the different backends from diverse device vendors.\nTo address this opportunity, we introduce WebLLM, an open-source JavaScript\nframework that enables high-performance LLM inference entirely within web\nbrowsers. WebLLM provides an OpenAI-style API for seamless integration into web\napplications, and leverages WebGPU for efficient local GPU acceleration and\nWebAssembly for performant CPU computation. With machine learning compilers\nMLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming\nthe absence of performant WebGPU kernel libraries. Evaluations show that WebLLM\ncan retain up to 80% native performance on the same device, with room to\nfurther close the gap. WebLLM paves the way for universally accessible,\nprivacy-preserving, personalized, and locally powered LLM applications in web\nbrowsers. The code is available at: https://github.com/mlc-ai/web-llm."
                },
                "authors": [
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yucheng Qin"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Meng-Shiun Yu"
                    },
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Sudeep Agarwal"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00587v2",
                "updated": "2024-12-20T11:22:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    22,
                    1,
                    4,
                    355,
                    0
                ],
                "published": "2024-09-01T02:43:33Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    2,
                    43,
                    33,
                    6,
                    245,
                    0
                ],
                "title": "FLUX that Plays Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLUX that Plays Music"
                },
                "summary": "This paper explores a simple extension of diffusion-based rectified flow\nTransformers for text-to-music generation, termed as FluxMusic. Generally,\nalong with design in advanced\nFlux\\footnote{https://github.com/black-forest-labs/flux} model, we transfers it\ninto a latent VAE space of mel-spectrum. It involves first applying a sequence\nof independent attention to the double text-music stream, followed by a stacked\nsingle music stream for denoised patch prediction. We employ multiple\npre-trained text encoders to sufficiently capture caption semantic information\nas well as inference flexibility. In between, coarse textual information, in\nconjunction with time step embeddings, is utilized in a modulation mechanism,\nwhile fine-grained textual details are concatenated with the music patch\nsequence as inputs. Through an in-depth study, we demonstrate that rectified\nflow training with an optimized architecture significantly outperforms\nestablished diffusion methods for the text-to-music task, as evidenced by\nvarious automatic metrics and human preference evaluations. Our experimental\ndata, code, and model weights are made publicly available at:\n\\url{https://github.com/feizc/FluxMusic}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a simple extension of diffusion-based rectified flow\nTransformers for text-to-music generation, termed as FluxMusic. Generally,\nalong with design in advanced\nFlux\\footnote{https://github.com/black-forest-labs/flux} model, we transfers it\ninto a latent VAE space of mel-spectrum. It involves first applying a sequence\nof independent attention to the double text-music stream, followed by a stacked\nsingle music stream for denoised patch prediction. We employ multiple\npre-trained text encoders to sufficiently capture caption semantic information\nas well as inference flexibility. In between, coarse textual information, in\nconjunction with time step embeddings, is utilized in a modulation mechanism,\nwhile fine-grained textual details are concatenated with the music patch\nsequence as inputs. Through an in-depth study, we demonstrate that rectified\nflow training with an optimized architecture significantly outperforms\nestablished diffusion methods for the text-to-music task, as evidenced by\nvarious automatic metrics and human preference evaluations. Our experimental\ndata, code, and model weights are made publicly available at:\n\\url{https://github.com/feizc/FluxMusic}."
                },
                "authors": [
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Changqian Yu"
                    },
                    {
                        "name": "Junshi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Junshi Huang"
                },
                "author": "Junshi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15798v1",
                "updated": "2024-12-20T11:15:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    15,
                    31,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:15:31Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    15,
                    31,
                    4,
                    355,
                    0
                ],
                "title": "Diffusion-Based Conditional Image Editing through Optimized Inference\n  with Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Based Conditional Image Editing through Optimized Inference\n  with Guidance"
                },
                "summary": "We present a simple but effective training-free approach for text-driven\nimage-to-image translation based on a pretrained text-to-image diffusion model.\nOur goal is to generate an image that aligns with the target task while\npreserving the structure and background of a source image. To this end, we\nderive the representation guidance with a combination of two objectives:\nmaximizing the similarity to the target prompt based on the CLIP score and\nminimizing the structural distance to the source latent variable. This guidance\nimproves the fidelity of the generated target image to the given target prompt\nwhile maintaining the structure integrity of the source image. To incorporate\nthe representation guidance component, we optimize the target latent variable\nof diffusion model's reverse process with the guidance. Experimental results\ndemonstrate that our method achieves outstanding image-to-image translation\nperformance on various tasks when combined with the pretrained Stable Diffusion\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple but effective training-free approach for text-driven\nimage-to-image translation based on a pretrained text-to-image diffusion model.\nOur goal is to generate an image that aligns with the target task while\npreserving the structure and background of a source image. To this end, we\nderive the representation guidance with a combination of two objectives:\nmaximizing the similarity to the target prompt based on the CLIP score and\nminimizing the structural distance to the source latent variable. This guidance\nimproves the fidelity of the generated target image to the given target prompt\nwhile maintaining the structure integrity of the source image. To incorporate\nthe representation guidance component, we optimize the target latent variable\nof diffusion model's reverse process with the guidance. Experimental results\ndemonstrate that our method achieves outstanding image-to-image translation\nperformance on various tasks when combined with the pretrained Stable Diffusion\nmodel."
                },
                "authors": [
                    {
                        "name": "Hyunsoo Lee"
                    },
                    {
                        "name": "Minsoo Kang"
                    },
                    {
                        "name": "Bohyung Han"
                    }
                ],
                "author_detail": {
                    "name": "Bohyung Han"
                },
                "author": "Bohyung Han",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15791v1",
                "updated": "2024-12-20T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    7,
                    51,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    7,
                    51,
                    4,
                    355,
                    0
                ],
                "title": "A Bayesian Approach for Earthquake Impact Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach for Earthquake Impact Modelling"
                },
                "summary": "Immediately following a disaster event, such as an earthquake, estimates of\nthe damage extent play a key role in informing the coordination of response and\nrecovery efforts. We develop a novel impact estimation tool that leverages a\ngeneralised Bayesian approach to generate earthquake impact estimates across\nthree impact types: mortality, population displacement, and building damage.\nInference is performed within a likelihood-free framework, and a\nscoring-rule-based posterior avoids information loss from non-sufficient\nsummary statistics. We propose an adaptation of existing scoring-rule-based\nloss functions that accommodates the use of an approximate Bayesian computation\nsequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results\ncomparable to those of two leading impact estimation tools in the prediction of\ntotal mortality when tested on a set of held-out past events. The proposed\nmethod provides four advantages over existing empirical approaches: modelling\nproduces a gridded spatial map of the estimated impact, predictions benefit\nfrom the Bayesian quantification and interpretation of uncertainty, there is\ndirect handling of multi-shock earthquake events, and the use of a joint model\nbetween impact types allows predictions to be updated as impact observations\nbecome available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immediately following a disaster event, such as an earthquake, estimates of\nthe damage extent play a key role in informing the coordination of response and\nrecovery efforts. We develop a novel impact estimation tool that leverages a\ngeneralised Bayesian approach to generate earthquake impact estimates across\nthree impact types: mortality, population displacement, and building damage.\nInference is performed within a likelihood-free framework, and a\nscoring-rule-based posterior avoids information loss from non-sufficient\nsummary statistics. We propose an adaptation of existing scoring-rule-based\nloss functions that accommodates the use of an approximate Bayesian computation\nsequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results\ncomparable to those of two leading impact estimation tools in the prediction of\ntotal mortality when tested on a set of held-out past events. The proposed\nmethod provides four advantages over existing empirical approaches: modelling\nproduces a gridded spatial map of the estimated impact, predictions benefit\nfrom the Bayesian quantification and interpretation of uncertainty, there is\ndirect handling of multi-shock earthquake events, and the use of a joint model\nbetween impact types allows predictions to be updated as impact observations\nbecome available."
                },
                "authors": [
                    {
                        "name": "Max Anderson Loake"
                    },
                    {
                        "name": "Hamish Patten"
                    },
                    {
                        "name": "David Steinsaltz"
                    }
                ],
                "author_detail": {
                    "name": "David Steinsaltz"
                },
                "author": "David Steinsaltz",
                "arxiv_comment": "24 pages, 21 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15790v1",
                "updated": "2024-12-20T11:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    5,
                    26,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    5,
                    26,
                    4,
                    355,
                    0
                ],
                "title": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning"
                },
                "summary": "The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine."
                },
                "authors": [
                    {
                        "name": "Heming Zhang"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Fuhai Li"
                    }
                ],
                "author_detail": {
                    "name": "Fuhai Li"
                },
                "author": "Fuhai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15772v1",
                "updated": "2024-12-20T10:43:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    43,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:43:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    43,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease\n  Detection based on Spontaneous Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease\n  Detection based on Spontaneous Speech"
                },
                "summary": "Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis."
                },
                "authors": [
                    {
                        "name": "Jonathan Heitz"
                    },
                    {
                        "name": "Gerold Schneider"
                    },
                    {
                        "name": "Nicolas Langer"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Langer"
                },
                "author": "Nicolas Langer",
                "arxiv_comment": "Accepted at the 31st International Conference on Computational\n  Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17662v3",
                "updated": "2024-12-20T10:35:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    35,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-04-26T19:07:30Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    19,
                    7,
                    30,
                    4,
                    117,
                    0
                ],
                "title": "Questioning the Unknown: Optimising Multi-Agent Collaboration in\n  Narrative-Driven Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning the Unknown: Optimising Multi-Agent Collaboration in\n  Narrative-Driven Games"
                },
                "summary": "We present Questum, a novel framework for Large Language Model (LLM)-based\nagents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including\nundefined state spaces, absent intermediate rewards, and the need for strategic\ninteraction in a continuous language domain. Questum addresses these\ncomplexities through a sensor-based representation of agent states, a\nquestion-targeting mechanism guided by information gain, and a pruning strategy\nto refine suspect lists and enhance decision-making efficiency. To enable\nsystematic evaluation, we propose WellPlay, a dataset comprising 1,482\ninferential questions across 12 games, categorised into objectives, reasoning,\nand relationships. Experiments demonstrate Questum's capacity to achieve\nsuperior performance in reasoning accuracy and efficiency compared to existing\napproaches, while also significantly improving the quality of agent-human\ninteractions in MMGs. This study advances the development of reasoning agents\nfor complex social and interactive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Questum, a novel framework for Large Language Model (LLM)-based\nagents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including\nundefined state spaces, absent intermediate rewards, and the need for strategic\ninteraction in a continuous language domain. Questum addresses these\ncomplexities through a sensor-based representation of agent states, a\nquestion-targeting mechanism guided by information gain, and a pruning strategy\nto refine suspect lists and enhance decision-making efficiency. To enable\nsystematic evaluation, we propose WellPlay, a dataset comprising 1,482\ninferential questions across 12 games, categorised into objectives, reasoning,\nand relationships. Experiments demonstrate Questum's capacity to achieve\nsuperior performance in reasoning accuracy and efficiency compared to existing\napproaches, while also significantly improving the quality of agent-human\ninteractions in MMGs. This study advances the development of reasoning agents\nfor complex social and interactive scenarios."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15758v1",
                "updated": "2024-12-20T10:24:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    24,
                    8,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:24:08Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    24,
                    8,
                    4,
                    355,
                    0
                ],
                "title": "Function Space Diversity for Uncertainty Prediction via Repulsive\n  Last-Layer Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function Space Diversity for Uncertainty Prediction via Repulsive\n  Last-Layer Ensembles"
                },
                "summary": "Bayesian inference in function space has gained attention due to its\nrobustness against overparameterization in neural networks. However,\napproximating the infinite-dimensional function space introduces several\nchallenges. In this work, we discuss function space inference via particle\noptimization and present practical modifications that improve uncertainty\nestimation and, most importantly, make it applicable for large and pretrained\nnetworks. First, we demonstrate that the input samples, where particle\npredictions are enforced to be diverse, are detrimental to the model\nperformance. While diversity on training data itself can lead to underfitting,\nthe use of label-destroying data augmentation, or unlabeled out-of-distribution\ndata can improve prediction diversity and uncertainty estimates. Furthermore,\nwe take advantage of the function space formulation, which imposes no\nrestrictions on network parameterization other than sufficient flexibility.\nInstead of using full deep ensembles to represent particles, we propose a\nsingle multi-headed network that introduces a minimal increase in parameters\nand computation. This allows seamless integration to pretrained networks, where\nthis repulsive last-layer ensemble can be used for uncertainty aware\nfine-tuning at minimal additional cost. We achieve competitive results in\ndisentangling aleatoric and epistemic uncertainty for active learning,\ndetecting out-of-domain data, and providing calibrated uncertainty estimates\nunder distribution shifts with minimal compute and memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference in function space has gained attention due to its\nrobustness against overparameterization in neural networks. However,\napproximating the infinite-dimensional function space introduces several\nchallenges. In this work, we discuss function space inference via particle\noptimization and present practical modifications that improve uncertainty\nestimation and, most importantly, make it applicable for large and pretrained\nnetworks. First, we demonstrate that the input samples, where particle\npredictions are enforced to be diverse, are detrimental to the model\nperformance. While diversity on training data itself can lead to underfitting,\nthe use of label-destroying data augmentation, or unlabeled out-of-distribution\ndata can improve prediction diversity and uncertainty estimates. Furthermore,\nwe take advantage of the function space formulation, which imposes no\nrestrictions on network parameterization other than sufficient flexibility.\nInstead of using full deep ensembles to represent particles, we propose a\nsingle multi-headed network that introduces a minimal increase in parameters\nand computation. This allows seamless integration to pretrained networks, where\nthis repulsive last-layer ensemble can be used for uncertainty aware\nfine-tuning at minimal additional cost. We achieve competitive results in\ndisentangling aleatoric and epistemic uncertainty for active learning,\ndetecting out-of-domain data, and providing calibrated uncertainty estimates\nunder distribution shifts with minimal compute and memory."
                },
                "authors": [
                    {
                        "name": "Sophie Steger"
                    },
                    {
                        "name": "Christian Knoll"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    },
                    {
                        "name": "Franz Pernkopf"
                    }
                ],
                "author_detail": {
                    "name": "Franz Pernkopf"
                },
                "author": "Franz Pernkopf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.02014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.02014v3",
                "updated": "2024-12-20T10:21:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    21,
                    14,
                    4,
                    355,
                    0
                ],
                "published": "2022-12-05T04:04:21Z",
                "published_parsed": [
                    2022,
                    12,
                    5,
                    4,
                    4,
                    21,
                    0,
                    339,
                    0
                ],
                "title": "Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding"
                },
                "summary": "Automatic parsing of human anatomies at the instance-level from 3D computed\ntomography (CT) is a prerequisite step for many clinical applications. The\npresence of pathologies, broken structures or limited field-of-view (FOV) can\nall make anatomy parsing algorithms vulnerable. In this work, we explore how to\nleverage and implement the successful detection-then-segmentation paradigm for\n3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering the complicated shapes, sizes, and orientations of\nanatomies, without loss of generality, we present a nine degrees of freedom\n(9-DoF) pose estimation solution in full 3D space using a novel single-stage,\nnon-hierarchical representation. Our whole framework is executed in a steerable\nmanner where any anatomy of interest can be directly retrieved to further boost\ninference efficiency. We have validated our method on three medical imaging\nparsing tasks: ribs, spine, and abdominal organs. For rib parsing, CT scans\nhave been annotated at the rib instance-level for quantitative evaluation,\nsimilarly for spine vertebrae and abdominal organs. Extensive experiments on\n9-DoF box detection and rib instance segmentation demonstrate the high\nefficiency and effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%), compared favorably against\nseveral strong baselines (e.g., CenterNet, FCOS, and nnU-Net). For spine\nparsing and abdominal multi-organ segmentation, our method achieves competitive\nresults on par with state-of-the-art methods on the public CTSpine1K dataset\nand FLARE22 competition, respectively. Our annotations, code, and models are\navailable at: https://github.com/alibaba-damo-academy/Med_Query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic parsing of human anatomies at the instance-level from 3D computed\ntomography (CT) is a prerequisite step for many clinical applications. The\npresence of pathologies, broken structures or limited field-of-view (FOV) can\nall make anatomy parsing algorithms vulnerable. In this work, we explore how to\nleverage and implement the successful detection-then-segmentation paradigm for\n3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering the complicated shapes, sizes, and orientations of\nanatomies, without loss of generality, we present a nine degrees of freedom\n(9-DoF) pose estimation solution in full 3D space using a novel single-stage,\nnon-hierarchical representation. Our whole framework is executed in a steerable\nmanner where any anatomy of interest can be directly retrieved to further boost\ninference efficiency. We have validated our method on three medical imaging\nparsing tasks: ribs, spine, and abdominal organs. For rib parsing, CT scans\nhave been annotated at the rib instance-level for quantitative evaluation,\nsimilarly for spine vertebrae and abdominal organs. Extensive experiments on\n9-DoF box detection and rib instance segmentation demonstrate the high\nefficiency and effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%), compared favorably against\nseveral strong baselines (e.g., CenterNet, FCOS, and nnU-Net). For spine\nparsing and abdominal multi-organ segmentation, our method achieves competitive\nresults on par with state-of-the-art methods on the public CTSpine1K dataset\nand FLARE22 competition, respectively. Our annotations, code, and models are\navailable at: https://github.com/alibaba-damo-academy/Med_Query."
                },
                "authors": [
                    {
                        "name": "Heng Guo"
                    },
                    {
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Le Lu"
                    },
                    {
                        "name": "Minfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Minfeng Xu"
                },
                "author": "Minfeng Xu",
                "arxiv_comment": "Accepted by IEEE Journal of Biomedical and Health Informatics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.02014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.02014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07934v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07934v4",
                "updated": "2024-12-20T10:14:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    14,
                    54,
                    4,
                    355,
                    0
                ],
                "published": "2024-07-10T16:03:04Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    3,
                    4,
                    2,
                    192,
                    0
                ],
                "title": "Identifying Macro Conditional Independencies and Macro Total Effects in\n  Summary Causal Graphs with Latent Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Macro Conditional Independencies and Macro Total Effects in\n  Summary Causal Graphs with Latent Confounding"
                },
                "summary": "Understanding causal relations in dynamic systems is essential in\nepidemiology. While causal inference methods have been extensively studied,\nthey often rely on fully specified causal graphs, which may not always be\navailable in complex dynamic systems. Partially specified causal graphs, and in\nparticular summary causal graphs (SCGs), provide a simplified representation of\ncausal relations between time series when working spacio-temporal data,\nomitting temporal information and focusing on causal structures between\nclusters of of temporal variables. Unlike fully specified causal graphs, SCGs\ncan contain cycles, which complicate their analysis and interpretation. In\naddition, their cluster-based nature introduces new challenges concerning the\ntypes of queries of interest: macro queries, which involve relationships\nbetween clusters represented as vertices in the graph, and micro queries, which\npertain to relationships between variables that are not directly visible\nthrough the vertices of the graph. In this paper, we first clearly distinguish\nbetween macro conditional independencies and micro conditional independencies\nand between macro total effects and micro total effects. Then, we demonstrate\nthe soundness and completeness of the d-separation to identify macro\nconditional independencies in SCGs. Furthermore, we establish that the\ndo-calculus is sound and complete for identifying macro total effects in SCGs.\nFinally, we give a graphical characterization for the non-identifiability of\nmacro total effects in SCGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relations in dynamic systems is essential in\nepidemiology. While causal inference methods have been extensively studied,\nthey often rely on fully specified causal graphs, which may not always be\navailable in complex dynamic systems. Partially specified causal graphs, and in\nparticular summary causal graphs (SCGs), provide a simplified representation of\ncausal relations between time series when working spacio-temporal data,\nomitting temporal information and focusing on causal structures between\nclusters of of temporal variables. Unlike fully specified causal graphs, SCGs\ncan contain cycles, which complicate their analysis and interpretation. In\naddition, their cluster-based nature introduces new challenges concerning the\ntypes of queries of interest: macro queries, which involve relationships\nbetween clusters represented as vertices in the graph, and micro queries, which\npertain to relationships between variables that are not directly visible\nthrough the vertices of the graph. In this paper, we first clearly distinguish\nbetween macro conditional independencies and micro conditional independencies\nand between macro total effects and micro total effects. Then, we demonstrate\nthe soundness and completeness of the d-separation to identify macro\nconditional independencies in SCGs. Furthermore, we establish that the\ndo-calculus is sound and complete for identifying macro total effects in SCGs.\nFinally, we give a graphical characterization for the non-identifiability of\nmacro total effects in SCGs."
                },
                "authors": [
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "arxiv_comment": "Accepted CI4TS Workshop at UAI2024. Accepted at AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07934v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07934v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15750v1",
                "updated": "2024-12-20T10:11:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    11,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:11:44Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    11,
                    44,
                    4,
                    355,
                    0
                ],
                "title": "Extracting Interpretable Task-Specific Circuits from Large Language\n  Models for Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Interpretable Task-Specific Circuits from Large Language\n  Models for Faster Inference"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, the size of LLMs is steadily increasing, hindering\ntheir application on computationally constrained environments. On the other\nhand, despite their general capabilities, there are many situations where only\none specific task is performed, rendering all other capabilities unnecessary\nand wasteful. This leads us to the following question: Is it possible to\nextract the minimal subset from an LLM that is able to perform a specific task\nin a faster, standalone manner? Recent works on Mechanistic Interpretability\n(MI) have shown that specific tasks are performed by a localized subset of\ncomponents, or circuit. However, current techniques used to identify the\ncircuit cannot be used to extract it for its standalone usage. In this work, we\npropose a novel approach to automatically extract the subset of the LLM that\nproperly performs a targeted task requiring no additional training and a small\namount of data samples. We evaluate our approach on different tasks and show\nthat the resulting models are (i) considerably smaller, reducing the number of\nparameters up to 82.77% and (ii) more interpretable, as they focus on the\ncircuit that is used to carry out the specific task, and can therefore be\nunderstood using MI techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, the size of LLMs is steadily increasing, hindering\ntheir application on computationally constrained environments. On the other\nhand, despite their general capabilities, there are many situations where only\none specific task is performed, rendering all other capabilities unnecessary\nand wasteful. This leads us to the following question: Is it possible to\nextract the minimal subset from an LLM that is able to perform a specific task\nin a faster, standalone manner? Recent works on Mechanistic Interpretability\n(MI) have shown that specific tasks are performed by a localized subset of\ncomponents, or circuit. However, current techniques used to identify the\ncircuit cannot be used to extract it for its standalone usage. In this work, we\npropose a novel approach to automatically extract the subset of the LLM that\nproperly performs a targeted task requiring no additional training and a small\namount of data samples. We evaluate our approach on different tasks and show\nthat the resulting models are (i) considerably smaller, reducing the number of\nparameters up to 82.77% and (ii) more interpretable, as they focus on the\ncircuit that is used to carry out the specific task, and can therefore be\nunderstood using MI techniques."
                },
                "authors": [
                    {
                        "name": "Jorge García-Carrasco"
                    },
                    {
                        "name": "Alejandro Maté"
                    },
                    {
                        "name": "Juan Trujillo"
                    }
                ],
                "author_detail": {
                    "name": "Juan Trujillo"
                },
                "author": "Juan Trujillo",
                "arxiv_comment": "Accepted to AAAI 25 Main Technical Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15748v1",
                "updated": "2024-12-20T10:06:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:06:52Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models"
                },
                "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole"
                },
                "authors": [
                    {
                        "name": "Shamus Sim"
                    },
                    {
                        "name": "Tyrone Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone Chen"
                },
                "author": "Tyrone Chen",
                "arxiv_comment": "16 pages, 5 figures, 2 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15745v1",
                "updated": "2024-12-20T10:04:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    4,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:04:59Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    4,
                    59,
                    4,
                    355,
                    0
                ],
                "title": "Dynamic Learning Rate Decay for Stochastic Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Learning Rate Decay for Stochastic Variational Inference"
                },
                "summary": "Like many optimization algorithms, Stochastic Variational Inference (SVI) is\nsensitive to the choice of the learning rate. If the learning rate is too\nsmall, the optimization process may be slow, and the algorithm might get stuck\nin local optima. On the other hand, if the learning rate is too large, the\nalgorithm may oscillate or diverge, failing to converge to a solution. Adaptive\nlearning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically\nadjust the learning rate based on the history of gradients. Nevertheless, if\nthe base learning rate is too large, the variational parameters might still\noscillate around the optimal solution. With learning rate schedules, the\nlearning rate can be reduced gradually to mitigate this problem. However, the\namount at which the learning rate should be decreased in each iteration is not\nknown a priori, which can significantly impact the performance of the\noptimization. In this work, we propose a method to decay the learning rate\nbased on the history of the variational parameters. We use an empirical measure\nto quantify the amount of oscillations against the progress of the variational\nparameters to adapt the learning rate. The approach requires little memory and\nis computationally efficient. We demonstrate in various numerical examples that\nour method reduces the sensitivity of the optimization performance to the\nlearning rate and that it can also be used in combination with other adaptive\nlearning rate methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like many optimization algorithms, Stochastic Variational Inference (SVI) is\nsensitive to the choice of the learning rate. If the learning rate is too\nsmall, the optimization process may be slow, and the algorithm might get stuck\nin local optima. On the other hand, if the learning rate is too large, the\nalgorithm may oscillate or diverge, failing to converge to a solution. Adaptive\nlearning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically\nadjust the learning rate based on the history of gradients. Nevertheless, if\nthe base learning rate is too large, the variational parameters might still\noscillate around the optimal solution. With learning rate schedules, the\nlearning rate can be reduced gradually to mitigate this problem. However, the\namount at which the learning rate should be decreased in each iteration is not\nknown a priori, which can significantly impact the performance of the\noptimization. In this work, we propose a method to decay the learning rate\nbased on the history of the variational parameters. We use an empirical measure\nto quantify the amount of oscillations against the progress of the variational\nparameters to adapt the learning rate. The approach requires little memory and\nis computationally efficient. We demonstrate in various numerical examples that\nour method reduces the sensitivity of the optimization performance to the\nlearning rate and that it can also be used in combination with other adaptive\nlearning rate methods."
                },
                "authors": [
                    {
                        "name": "Maximilian Dinkel"
                    },
                    {
                        "name": "Gil Robalo Rei"
                    },
                    {
                        "name": "Wolfgang A. Wall"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang A. Wall"
                },
                "author": "Wolfgang A. Wall",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15735v1",
                "updated": "2024-12-20T09:56:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    56,
                    17,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:56:17Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    56,
                    17,
                    4,
                    355,
                    0
                ],
                "title": "Prompt-based Unifying Inference Attack on Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based Unifying Inference Attack on Graph Neural Networks"
                },
                "summary": "Graph neural networks (GNNs) provide important prospective insights in\napplications such as social behavior analysis and financial risk analysis based\non their powerful learning capabilities on graph data. Nevertheless, GNNs'\npredictive performance relies on the quality of task-specific node labels, so\nit is common practice to improve the model's generalization ability in the\ndownstream execution of decision-making tasks through pre-training. Graph\nprompting is a prudent choice but risky without taking measures to prevent data\nleakage. In other words, in high-risk decision scenarios, prompt learning can\ninfer private information by accessing model parameters trained on private data\n(publishing model parameters in pre-training, i.e., without directly leaking\nthe raw data, is a tacitly accepted trend). However, myriad graph inference\nattacks necessitate tailored module design and processing to enhance inference\ncapabilities due to variations in supervision signals. In this paper, we\npropose a novel Prompt-based unifying Inference Attack framework on GNNs, named\nProIA. Specifically, ProIA retains the crucial topological information of the\ngraph during pre-training, enhancing the background knowledge of the inference\nattack model. It then utilizes a unified prompt and introduces additional\ndisentanglement factors in downstream attacks to adapt to task-relevant\nknowledge. Finally, extensive experiments show that ProIA enhances attack\ncapabilities and demonstrates remarkable adaptability to various inference\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) provide important prospective insights in\napplications such as social behavior analysis and financial risk analysis based\non their powerful learning capabilities on graph data. Nevertheless, GNNs'\npredictive performance relies on the quality of task-specific node labels, so\nit is common practice to improve the model's generalization ability in the\ndownstream execution of decision-making tasks through pre-training. Graph\nprompting is a prudent choice but risky without taking measures to prevent data\nleakage. In other words, in high-risk decision scenarios, prompt learning can\ninfer private information by accessing model parameters trained on private data\n(publishing model parameters in pre-training, i.e., without directly leaking\nthe raw data, is a tacitly accepted trend). However, myriad graph inference\nattacks necessitate tailored module design and processing to enhance inference\ncapabilities due to variations in supervision signals. In this paper, we\npropose a novel Prompt-based unifying Inference Attack framework on GNNs, named\nProIA. Specifically, ProIA retains the crucial topological information of the\ngraph during pre-training, enhancing the background knowledge of the inference\nattack model. It then utilizes a unified prompt and introduces additional\ndisentanglement factors in downstream attacks to adapt to task-relevant\nknowledge. Finally, extensive experiments show that ProIA enhances attack\ncapabilities and demonstrates remarkable adaptability to various inference\nattacks."
                },
                "authors": [
                    {
                        "name": "Yuecen Wei"
                    },
                    {
                        "name": "Xingcheng Fu"
                    },
                    {
                        "name": "Lingyun Liu"
                    },
                    {
                        "name": "Qingyun Sun"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Chunming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Chunming Hu"
                },
                "author": "Chunming Hu",
                "arxiv_comment": "Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v3",
                "updated": "2024-12-20T09:40:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    40,
                    54,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict arises as the LLM's evolving knowledge during\nSFT enables it to answer previously unanswerable questions, but the\nnow-answerable training samples still retain the original \"I don't know\"\nsupervision signals from the initial LLM state, leading to inconsistencies.\nThese conflicts cause the trained LLM to misclassify known questions as\nunknown, resulting in over-refusal. To address this issue, we introduce\nCertainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning\n(CRaFT). CRaFT centers on two main contributions: First, we additionally\nincorporate response certainty to selectively filter and modify data, reducing\nstatic conflicts. Second, we implement preliminary rehearsal training to\ncharacterize changes in the LLM's knowledge state, which helps mitigate dynamic\nconflicts during the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Code and data will be released at https://github.com/opendatalab/CRaFT .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict arises as the LLM's evolving knowledge during\nSFT enables it to answer previously unanswerable questions, but the\nnow-answerable training samples still retain the original \"I don't know\"\nsupervision signals from the initial LLM state, leading to inconsistencies.\nThese conflicts cause the trained LLM to misclassify known questions as\nunknown, resulting in over-refusal. To address this issue, we introduce\nCertainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning\n(CRaFT). CRaFT centers on two main contributions: First, we additionally\nincorporate response certainty to selectively filter and modify data, reducing\nstatic conflicts. Second, we implement preliminary rehearsal training to\ncharacterize changes in the LLM's knowledge state, which helps mitigate dynamic\nconflicts during the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Code and data will be released at https://github.com/opendatalab/CRaFT ."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15554v2",
                "updated": "2024-12-20T09:39:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    39,
                    6,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-21T00:59:50Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    0,
                    59,
                    50,
                    0,
                    295,
                    0
                ],
                "title": "A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning\n  Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple\n  Random Operating Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning\n  Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple\n  Random Operating Conditions"
                },
                "summary": "The nonlinear and unstable aerodynamic interference generated by the tandem\nwings of such biomimetic systems poses substantial challenges for motion\ncontrol, especially under multiple random operating conditions. To address\nthese challenges, the Concerto Reinforcement Learning Extension (CRL2E)\nalgorithm has been developed. This plug-and-play, fully on-the-job, real-time\nreinforcement learning algorithm incorporates a novel Physics-Inspired\nRule-Based Policy Composer Strategy with a Perturbation Module alongside a\nlightweight network optimized for real-time control. To validate the\nperformance and the rationality of the module design, experiments were\nconducted under six challenging operating conditions, comparing seven different\nalgorithms. The results demonstrate that the CRL2E algorithm achieves safe and\nstable training within the first 500 steps, improving tracking accuracy by 14\nto 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization,\nand Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally,\nCRL2E significantly enhances performance under various random operating\nconditions, with improvements in tracking accuracy ranging from 8.3% to 60.4%\ncompared to the Concerto Reinforcement Learning (CRL) algorithm. The\nconvergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm\nwith only the Composer Perturbation and 43.52% to 65.85% faster than the CRL\nalgorithm when both the Composer Perturbation and Time-Interleaved Capability\nPerturbation are introduced, especially in conditions where the standard CRL\nstruggles to converge. Hardware tests indicate that the optimized lightweight\nnetwork structure excels in weight loading and average inference time, meeting\nreal-time control requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear and unstable aerodynamic interference generated by the tandem\nwings of such biomimetic systems poses substantial challenges for motion\ncontrol, especially under multiple random operating conditions. To address\nthese challenges, the Concerto Reinforcement Learning Extension (CRL2E)\nalgorithm has been developed. This plug-and-play, fully on-the-job, real-time\nreinforcement learning algorithm incorporates a novel Physics-Inspired\nRule-Based Policy Composer Strategy with a Perturbation Module alongside a\nlightweight network optimized for real-time control. To validate the\nperformance and the rationality of the module design, experiments were\nconducted under six challenging operating conditions, comparing seven different\nalgorithms. The results demonstrate that the CRL2E algorithm achieves safe and\nstable training within the first 500 steps, improving tracking accuracy by 14\nto 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization,\nand Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally,\nCRL2E significantly enhances performance under various random operating\nconditions, with improvements in tracking accuracy ranging from 8.3% to 60.4%\ncompared to the Concerto Reinforcement Learning (CRL) algorithm. The\nconvergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm\nwith only the Composer Perturbation and 43.52% to 65.85% faster than the CRL\nalgorithm when both the Composer Perturbation and Time-Interleaved Capability\nPerturbation are introduced, especially in conditions where the standard CRL\nstruggles to converge. Hardware tests indicate that the optimized lightweight\nnetwork structure excels in weight loading and average inference time, meeting\nreal-time control requirements."
                },
                "authors": [
                    {
                        "name": "Zhang Minghao"
                    },
                    {
                        "name": "Song Bifeng"
                    },
                    {
                        "name": "Yang Xiaojun"
                    },
                    {
                        "name": "Wang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Liang"
                },
                "author": "Wang Liang",
                "arxiv_comment": "To prevent potential misunderstandings or negative impacts on the\n  community, I am requesting the withdrawal of my submission due to the\n  discovery of critical errors and major flaws in the work. Recent discussions\n  with researchers in the field have identified significant defects that\n  compromise the validity of the results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15714v1",
                "updated": "2024-12-20T09:37:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:37:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLife: Automatic Life Journaling with Smartphones and LLMs"
                },
                "summary": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals."
                },
                "authors": [
                    {
                        "name": "Huatao Xu"
                    },
                    {
                        "name": "Panron Tong"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15712v1",
                "updated": "2024-12-20T09:33:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:33:31Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining"
                },
                "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."
                },
                "authors": [
                    {
                        "name": "Maike Züfle"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15702v1",
                "updated": "2024-12-20T09:24:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    24,
                    50,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:24:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    24,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing\n  Programming Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing\n  Programming Feedback"
                },
                "summary": "Despite the growing use of large language models (LLMs) for providing\nfeedback, limited research has explored how to achieve high-quality feedback.\nThis case study introduces an evaluation framework to assess different\nzero-shot prompt engineering methods. We varied the prompts systematically and\nanalyzed the provided feedback on programming errors in R. The results suggest\nthat prompts suggesting a stepwise procedure increase the precision, while\nomitting explicit specifications about which provided data to analyze improves\nerror identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing use of large language models (LLMs) for providing\nfeedback, limited research has explored how to achieve high-quality feedback.\nThis case study introduces an evaluation framework to assess different\nzero-shot prompt engineering methods. We varied the prompts systematically and\nanalyzed the provided feedback on programming errors in R. The results suggest\nthat prompts suggesting a stepwise procedure increase the precision, while\nomitting explicit specifications about which provided data to analyze improves\nerror identification."
                },
                "authors": [
                    {
                        "name": "Niklas Ippisch"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Jan Simson"
                    },
                    {
                        "name": "Jacob Beck"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Malte Schierholz"
                    }
                ],
                "author_detail": {
                    "name": "Malte Schierholz"
                },
                "author": "Malte Schierholz",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15676v1",
                "updated": "2024-12-20T08:46:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    46,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:46:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Code Review Automation Via Multi-task Federated LLM -- An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Automation Via Multi-task Federated LLM -- An Empirical\n  Study"
                },
                "summary": "Code review is a crucial process before deploying code to production, as it\nvalidates the code, provides suggestions for improvements, and identifies\nerrors such as missed edge cases. In projects with regular production releases,\nthe effort required for peer code-reviews remains high. Consequently, there has\nbeen significant interest from software engineering (SE) researchers in\nautomating the code review process. Previous research on code review automation\nhas typically approached the task as three independent sub-tasks: review\nnecessity prediction, review comment generation, and code refinement. Our study\nattempts to (i) leverage the relationships between the sub-tasks of code review\nautomation, by developing a multi-task model that addresses all tasks in an\nintegrated manner, and (ii) increase model robustness on unseen data via\ncollaborative large language model (LLM) modeling, while retaining the\nproprietary nature of code, by using federated learning (FL). The study\nexplores five simple techniques for multi-task training, including two\nsequential methods, one parallel method, and two cumulative methods. The\nresults indicate that sequentially training a federated LLM (FedLLM) for our\ncode review multi-task use case is less efficient in terms of time,\ncomputation, and performance metrics, compared to training separate models for\neach task. Because sequential training demonstrates catastrophic forgetting,\nalternatively cumulative fine-tuning for multi-task training performs better\nthan training models for individual tasks. This study highlights the need for\nresearch focused on effective fine-tuning of multi-task FedLLMs for SE tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a crucial process before deploying code to production, as it\nvalidates the code, provides suggestions for improvements, and identifies\nerrors such as missed edge cases. In projects with regular production releases,\nthe effort required for peer code-reviews remains high. Consequently, there has\nbeen significant interest from software engineering (SE) researchers in\nautomating the code review process. Previous research on code review automation\nhas typically approached the task as three independent sub-tasks: review\nnecessity prediction, review comment generation, and code refinement. Our study\nattempts to (i) leverage the relationships between the sub-tasks of code review\nautomation, by developing a multi-task model that addresses all tasks in an\nintegrated manner, and (ii) increase model robustness on unseen data via\ncollaborative large language model (LLM) modeling, while retaining the\nproprietary nature of code, by using federated learning (FL). The study\nexplores five simple techniques for multi-task training, including two\nsequential methods, one parallel method, and two cumulative methods. The\nresults indicate that sequentially training a federated LLM (FedLLM) for our\ncode review multi-task use case is less efficient in terms of time,\ncomputation, and performance metrics, compared to training separate models for\neach task. Because sequential training demonstrates catastrophic forgetting,\nalternatively cumulative fine-tuning for multi-task training performs better\nthan training models for individual tasks. This study highlights the need for\nresearch focused on effective fine-tuning of multi-task FedLLMs for SE tasks."
                },
                "authors": [
                    {
                        "name": "Jahnavi Kumar"
                    },
                    {
                        "name": "Sridhar Chimalakonda"
                    }
                ],
                "author_detail": {
                    "name": "Sridhar Chimalakonda"
                },
                "author": "Sridhar Chimalakonda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14373v2",
                "updated": "2024-12-20T08:46:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    30,
                    4,
                    355,
                    0
                ],
                "published": "2024-02-22T08:26:56Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    8,
                    26,
                    56,
                    3,
                    53,
                    0
                ],
                "title": "Small Language Models as Effective Guides for Large Language Models in\n  Chinese Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models as Effective Guides for Large Language Models in\n  Chinese Relation Extraction"
                },
                "summary": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\npaid to this problem using LLM approaches. Therefore, in this paper, we propose\nSLCoLM, a model collaboration framework, to mitigate the data long-tail\nproblem. In our framework, we use the ``\\textit{Training-Guide-Predict}''\nstrategy to combine the strengths of small pre-trained language models (SLMs)\nand LLMs, where a task-specific SLM framework acts as a guider, transfers task\nknowledge to the LLM and guides the LLM in performing RE tasks. Our experiments\non an ancient Chinese RE dataset rich in relation types show that the approach\nfacilitates RE of long-tail relation types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\npaid to this problem using LLM approaches. Therefore, in this paper, we propose\nSLCoLM, a model collaboration framework, to mitigate the data long-tail\nproblem. In our framework, we use the ``\\textit{Training-Guide-Predict}''\nstrategy to combine the strengths of small pre-trained language models (SLMs)\nand LLMs, where a task-specific SLM framework acts as a guider, transfers task\nknowledge to the LLM and guides the LLM in performing RE tasks. Our experiments\non an ancient Chinese RE dataset rich in relation types show that the approach\nfacilitates RE of long-tail relation types."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "13 pages, 9 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v3",
                "updated": "2024-12-20T08:38:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    38,
                    38,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12106v2",
                "updated": "2024-12-20T08:35:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    35,
                    24,
                    4,
                    355,
                    0
                ],
                "published": "2024-09-18T16:26:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models"
                },
                "summary": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Hanjun Fang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15669v1",
                "updated": "2024-12-20T08:35:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    35,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:35:12Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    35,
                    12,
                    4,
                    355,
                    0
                ],
                "title": "WigglyEyes: Inferring Eye Movements from Keypress Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WigglyEyes: Inferring Eye Movements from Keypress Data"
                },
                "summary": "We present a model for inferring where users look during interaction based on\nkeypress data only. Given a key log, it outputs a scanpath that tells,\nmoment-by-moment, how the user had moved eyes while entering those keys. The\nmodel can be used as a proxy for human data in cases where collecting real eye\ntracking data is expensive or impossible. Our technical insight is three-fold:\nfirst, we present an inference architecture that considers the individual\ncharacteristics of the user, inferred as a low-dimensional parameter vector;\nsecond, we present a novel loss function for synchronizing inferred eye\nmovements with the keypresses; third, we train the model using a hybrid\napproach with both human data and synthetically generated data. The approach\ncan be applied in interactive systems where predictive models of user behavior\nare available. We report results from evaluation in the challenging case of\ntouchscreen typing, where the model accurately inferred real eye movements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a model for inferring where users look during interaction based on\nkeypress data only. Given a key log, it outputs a scanpath that tells,\nmoment-by-moment, how the user had moved eyes while entering those keys. The\nmodel can be used as a proxy for human data in cases where collecting real eye\ntracking data is expensive or impossible. Our technical insight is three-fold:\nfirst, we present an inference architecture that considers the individual\ncharacteristics of the user, inferred as a low-dimensional parameter vector;\nsecond, we present a novel loss function for synchronizing inferred eye\nmovements with the keypresses; third, we train the model using a hybrid\napproach with both human data and synthetically generated data. The approach\ncan be applied in interactive systems where predictive models of user behavior\nare available. We report results from evaluation in the challenging case of\ntouchscreen typing, where the model accurately inferred real eye movements."
                },
                "authors": [
                    {
                        "name": "Yujun Zhu"
                    },
                    {
                        "name": "Danqing Shi"
                    },
                    {
                        "name": "Hee-Seung Moon"
                    },
                    {
                        "name": "Antti Oulasvirta"
                    }
                ],
                "author_detail": {
                    "name": "Antti Oulasvirta"
                },
                "author": "Antti Oulasvirta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15666v1",
                "updated": "2024-12-20T08:30:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    30,
                    40,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:30:40Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    30,
                    40,
                    4,
                    355,
                    0
                ],
                "title": "A survey on FPGA-based accelerator for ML models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey on FPGA-based accelerator for ML models"
                },
                "summary": "This paper thoroughly surveys machine learning (ML) algorithms acceleration\nin hardware accelerators, focusing on Field-Programmable Gate Arrays (FPGAs).\nIt reviews 287 out of 1138 papers from the past six years, sourced from four\ntop FPGA conferences. Such selection underscores the increasing integration of\nML and FPGA technologies and their mutual importance in technological\nadvancement. Research clearly emphasises inference acceleration (81\\%) compared\nto training acceleration (13\\%). Additionally, the findings reveals that CNN\ndominates current FPGA acceleration research while emerging models like GNN\nshow obvious growth trends. The categorization of the FPGA research papers\nreveals a wide range of topics, demonstrating the growing relevance of ML in\nFPGA research. This comprehensive analysis provides valuable insights into the\ncurrent trends and future directions of FPGA research in the context of ML\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper thoroughly surveys machine learning (ML) algorithms acceleration\nin hardware accelerators, focusing on Field-Programmable Gate Arrays (FPGAs).\nIt reviews 287 out of 1138 papers from the past six years, sourced from four\ntop FPGA conferences. Such selection underscores the increasing integration of\nML and FPGA technologies and their mutual importance in technological\nadvancement. Research clearly emphasises inference acceleration (81\\%) compared\nto training acceleration (13\\%). Additionally, the findings reveals that CNN\ndominates current FPGA acceleration research while emerging models like GNN\nshow obvious growth trends. The categorization of the FPGA research papers\nreveals a wide range of topics, demonstrating the growing relevance of ML in\nFPGA research. This comprehensive analysis provides valuable insights into the\ncurrent trends and future directions of FPGA research in the context of ML\napplications."
                },
                "authors": [
                    {
                        "name": "Feng Yan"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Oliver Sinnen"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Sinnen"
                },
                "author": "Oliver Sinnen",
                "arxiv_comment": "16 pages, 4 figures (Working paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W25, 68M20, 68Q25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; C.1.3; B.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15660v1",
                "updated": "2024-12-20T08:20:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    20,
                    21,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:20:21Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    20,
                    21,
                    4,
                    355,
                    0
                ],
                "title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling\n  Capability Training Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling\n  Capability Training Pipeline"
                },
                "summary": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models."
                },
                "authors": [
                    {
                        "name": "Guancheng Zeng"
                    },
                    {
                        "name": "Wentao Ding"
                    },
                    {
                        "name": "Beining Xu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Wenqiang Han"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jingjing Mo"
                    },
                    {
                        "name": "Pengxu Qiu"
                    },
                    {
                        "name": "Xinran Tao"
                    },
                    {
                        "name": "Wang Tao"
                    },
                    {
                        "name": "Haowen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hu"
                },
                "author": "Haowen Hu",
                "arxiv_comment": "23 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15655v1",
                "updated": "2024-12-20T08:13:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:13:05Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula"
                },
                "summary": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o."
                },
                "authors": [
                    {
                        "name": "Sieun Hyeon"
                    },
                    {
                        "name": "Kyudan Jung"
                    },
                    {
                        "name": "Jaehee Won"
                    },
                    {
                        "name": "Nam-Joon Kim"
                    },
                    {
                        "name": "Hyun Gon Ryu"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Jaeyoung Do"
                    }
                ],
                "author_detail": {
                    "name": "Jaeyoung Do"
                },
                "author": "Jaeyoung Do",
                "arxiv_comment": "Accepted in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15649v1",
                "updated": "2024-12-20T08:05:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    5,
                    55,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:05:55Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    5,
                    55,
                    4,
                    355,
                    0
                ],
                "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with\n  Single-Stage Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM-Omni: Timbre-Controllable Voice Interaction System with\n  Single-Stage Training"
                },
                "summary": "Recent advancements highlight the potential of end-to-end real-time spoken\ndialogue systems, showcasing their low latency and high quality. In this paper,\nwe introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction\nsystem with single-stage training. SLAM-Omni achieves zero-shot timbre control\nby modeling spoken language with semantic tokens and decoupling speaker\ninformation to a vocoder. By predicting grouped speech semantic tokens at each\nstep, our method significantly reduces the sequence length of audio tokens,\naccelerating both training and inference. Additionally, we propose historical\ntext prompting to compress dialogue history, facilitating efficient multi-round\ninteractions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior\nmodels of similar scale, requiring only 15 hours of training on 4 GPUs with\nlimited data. Notably, it is the first spoken dialogue system to achieve\ncompetitive performance with a single-stage training approach, eliminating the\nneed for pre-training on TTS or ASR tasks. Further experiments validate its\nmultilingual and multi-turn dialogue capabilities on larger datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements highlight the potential of end-to-end real-time spoken\ndialogue systems, showcasing their low latency and high quality. In this paper,\nwe introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction\nsystem with single-stage training. SLAM-Omni achieves zero-shot timbre control\nby modeling spoken language with semantic tokens and decoupling speaker\ninformation to a vocoder. By predicting grouped speech semantic tokens at each\nstep, our method significantly reduces the sequence length of audio tokens,\naccelerating both training and inference. Additionally, we propose historical\ntext prompting to compress dialogue history, facilitating efficient multi-round\ninteractions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior\nmodels of similar scale, requiring only 15 hours of training on 4 GPUs with\nlimited data. Notably, it is the first spoken dialogue system to achieve\ncompetitive performance with a single-stage training approach, eliminating the\nneed for pre-training on TTS or ASR tasks. Further experiments validate its\nmultilingual and multi-turn dialogue capabilities on larger datasets."
                },
                "authors": [
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ruiqi Yan"
                    },
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Xiquan Li"
                    },
                    {
                        "name": "Ruiyang Xu"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhanxun Liu"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16450v2",
                "updated": "2024-12-20T08:01:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    1,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-29T11:30:21Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    30,
                    21,
                    3,
                    242,
                    0
                ],
                "title": "What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer"
                },
                "summary": "Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their head poses. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their head poses. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion"
                },
                "authors": [
                    {
                        "name": "Chaeyeon Chung"
                    },
                    {
                        "name": "Sunghyun Park"
                    },
                    {
                        "name": "Jeongho Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00120v2",
                "updated": "2024-12-20T07:58:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    58,
                    22,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-28T11:27:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    27,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched\n  Embeddings"
                },
                "summary": "This paper examines the Code-Switching (CS) phenomenon where two languages\nintertwine within a single utterance. There exists a noticeable need for\nresearch on the CS between English and Korean. We highlight that the current\nEquivalence Constraint (EC) theory for CS in other languages may only partially\ncapture English-Korean CS complexities due to the intrinsic grammatical\ndifferences between the languages. We introduce a novel Koglish dataset\ntailored for English-Korean CS scenarios to mitigate such challenges. First, we\nconstructed the Koglish-GLUE dataset to demonstrate the importance and need for\nCS datasets in various tasks. We found the differential outcomes of various\nfoundation multilingual language models when trained on a monolingual versus a\nCS dataset. Motivated by this, we hypothesized that SimCSE, which has shown\nstrengths in monolingual sentence embedding, would have limitations in CS\nscenarios. We construct a novel Koglish-NLI (Natural Language Inference)\ndataset using a CS augmentation-based approach to verify this. From this\nCS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and\naugmentation method for code-switched embeddings, ConCSE, highlighting the\nsemantics of CS sentences. Experimental results validate the proposed ConCSE\nwith an average performance enhancement of 1.77\\% on the Koglish-STS(Semantic\nTextual Similarity) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the Code-Switching (CS) phenomenon where two languages\nintertwine within a single utterance. There exists a noticeable need for\nresearch on the CS between English and Korean. We highlight that the current\nEquivalence Constraint (EC) theory for CS in other languages may only partially\ncapture English-Korean CS complexities due to the intrinsic grammatical\ndifferences between the languages. We introduce a novel Koglish dataset\ntailored for English-Korean CS scenarios to mitigate such challenges. First, we\nconstructed the Koglish-GLUE dataset to demonstrate the importance and need for\nCS datasets in various tasks. We found the differential outcomes of various\nfoundation multilingual language models when trained on a monolingual versus a\nCS dataset. Motivated by this, we hypothesized that SimCSE, which has shown\nstrengths in monolingual sentence embedding, would have limitations in CS\nscenarios. We construct a novel Koglish-NLI (Natural Language Inference)\ndataset using a CS augmentation-based approach to verify this. From this\nCS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and\naugmentation method for code-switched embeddings, ConCSE, highlighting the\nsemantics of CS sentences. Experimental results validate the proposed ConCSE\nwith an average performance enhancement of 1.77\\% on the Koglish-STS(Semantic\nTextual Similarity) tasks."
                },
                "authors": [
                    {
                        "name": "Jangyeong Jeon"
                    },
                    {
                        "name": "Sangyeon Cho"
                    },
                    {
                        "name": "Minuk Ma"
                    },
                    {
                        "name": "Junyoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Junyoung Kim"
                },
                "author": "Junyoung Kim",
                "arxiv_comment": "Accepted for oral presentation at ICPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15642v1",
                "updated": "2024-12-20T07:57:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    57,
                    56,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:57:56Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    57,
                    56,
                    4,
                    355,
                    0
                ],
                "title": "Stroboscopic measurements in Markov networks: Exact generator\n  reconstruction vs. thermodynamic inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroboscopic measurements in Markov networks: Exact generator\n  reconstruction vs. thermodynamic inference"
                },
                "summary": "A major goal of stochastic thermodynamics is to estimate the inevitable\ndissipation that accompanies particular observable phenomena in an otherwise\nnot fully accessible system. Quantitative results are often formulated as lower\nbounds on the total entropy production, which capture the part of the total\ndissipation that can be determined based on the available data alone. In this\nwork, we discuss the case of a continuous-time dynamics on a Markov network\nthat is observed stroboscopically, i.e., at discrete points in time in regular\nintervals. We compare the standard approach of deriving a lower bound on the\nentropy production rate in the steady state to the less common method of\nreconstructing the generator from the observed propagators by taking the matrix\nlogarithm. Provided that the timescale of the stroboscopic measurements is\nsmaller than a critical value that can be determined from the available data,\nthis latter method is able to recover all thermodynamic quantities like entropy\nproduction or cycle affinities and is therefore superior to the usual approach\nof deriving lower bounds. Beyond the critical value, we still obtain tight\nupper and lower bounds on these quantities that improve on extant methods. We\nconclude the comparison with numerical illustrations and a discussion of the\nrequirements and limitations of both methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major goal of stochastic thermodynamics is to estimate the inevitable\ndissipation that accompanies particular observable phenomena in an otherwise\nnot fully accessible system. Quantitative results are often formulated as lower\nbounds on the total entropy production, which capture the part of the total\ndissipation that can be determined based on the available data alone. In this\nwork, we discuss the case of a continuous-time dynamics on a Markov network\nthat is observed stroboscopically, i.e., at discrete points in time in regular\nintervals. We compare the standard approach of deriving a lower bound on the\nentropy production rate in the steady state to the less common method of\nreconstructing the generator from the observed propagators by taking the matrix\nlogarithm. Provided that the timescale of the stroboscopic measurements is\nsmaller than a critical value that can be determined from the available data,\nthis latter method is able to recover all thermodynamic quantities like entropy\nproduction or cycle affinities and is therefore superior to the usual approach\nof deriving lower bounds. Beyond the critical value, we still obtain tight\nupper and lower bounds on these quantities that improve on extant methods. We\nconclude the comparison with numerical illustrations and a discussion of the\nrequirements and limitations of both methods."
                },
                "authors": [
                    {
                        "name": "Malena T. Bauer"
                    },
                    {
                        "name": "Udo Seifert"
                    },
                    {
                        "name": "Jann van der Meer"
                    }
                ],
                "author_detail": {
                    "name": "Jann van der Meer"
                },
                "author": "Jann van der Meer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03911v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03911v4",
                "updated": "2024-12-20T07:57:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    57,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-07T00:08:15Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    0,
                    8,
                    15,
                    1,
                    128,
                    0
                ],
                "title": "Federated Graph Condensation with Information Bottleneck Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Graph Condensation with Information Bottleneck Principles"
                },
                "summary": "Graph condensation (GC), which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has benefited\nvarious graph learning tasks. However, existing GC methods rely on centralized\ndata storage, which is unfeasible for real-world decentralized data\ndistribution, and overlook data holders' privacy-preserving requirements. To\nbridge this gap, we propose and study the novel problem of federated graph\ncondensation (FGC) for graph neural networks (GNNs). Specifically, we first\npropose a general framework for FGC, where we decouple the typical gradient\nmatching process for GC into client-side gradient calculation and server-side\ngradient matching, integrating knowledge from multiple clients' subgraphs into\none smaller condensed graph. Nevertheless, our empirical studies show that\nunder the federated setting, the condensed graph will consistently leak data\nmembership privacy, i.e., the condensed graph during federated training can be\nutilized to steal training data under the membership inference attack (MIA). To\ntackle this issue, we innovatively incorporate information bottleneck\nprinciples into the FGC, which only needs to extract partial node features in\none local pre-training step and utilize the features during federated training.\nTheoretical and experimental analyses demonstrate that our framework\nconsistently protects membership privacy during training. Meanwhile, it can\nachieve comparable and even superior performance against existing centralized\nGC and federated graph learning (FGL) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph condensation (GC), which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has benefited\nvarious graph learning tasks. However, existing GC methods rely on centralized\ndata storage, which is unfeasible for real-world decentralized data\ndistribution, and overlook data holders' privacy-preserving requirements. To\nbridge this gap, we propose and study the novel problem of federated graph\ncondensation (FGC) for graph neural networks (GNNs). Specifically, we first\npropose a general framework for FGC, where we decouple the typical gradient\nmatching process for GC into client-side gradient calculation and server-side\ngradient matching, integrating knowledge from multiple clients' subgraphs into\none smaller condensed graph. Nevertheless, our empirical studies show that\nunder the federated setting, the condensed graph will consistently leak data\nmembership privacy, i.e., the condensed graph during federated training can be\nutilized to steal training data under the membership inference attack (MIA). To\ntackle this issue, we innovatively incorporate information bottleneck\nprinciples into the FGC, which only needs to extract partial node features in\none local pre-training step and utilize the features during federated training.\nTheoretical and experimental analyses demonstrate that our framework\nconsistently protects membership privacy during training. Meanwhile, it can\nachieve comparable and even superior performance against existing centralized\nGC and federated graph learning (FGL) methods."
                },
                "authors": [
                    {
                        "name": "Bo Yan"
                    },
                    {
                        "name": "Sihao He"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "14 pages. Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03911v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03911v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15639v1",
                "updated": "2024-12-20T07:55:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    55,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:55:59Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    55,
                    59,
                    4,
                    355,
                    0
                ],
                "title": "Tacit Learning with Adaptive Information Selection for Cooperative\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tacit Learning with Adaptive Information Selection for Cooperative\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "In multi-agent reinforcement learning (MARL), the centralized training with\ndecentralized execution (CTDE) framework has gained widespread adoption due to\nits strong performance. However, the further development of CTDE faces two key\nchallenges. First, agents struggle to autonomously assess the relevance of\ninput information for cooperative tasks, impairing their decision-making\nabilities. Second, in communication-limited scenarios with partial\nobservability, agents are unable to access global information, restricting\ntheir ability to collaborate effectively from a global perspective. To address\nthese challenges, we introduce a novel cooperative MARL framework based on\ninformation selection and tacit learning. In this framework, agents gradually\ndevelop implicit coordination during training, enabling them to infer the\ncooperative behavior of others in a discrete space without communication,\nrelying solely on local information. Moreover, we integrate gating and\nselection mechanisms, allowing agents to adaptively filter information based on\nenvironmental changes, thereby enhancing their decision-making capabilities.\nExperiments on popular MARL benchmarks show that our framework can be\nseamlessly integrated with state-of-the-art algorithms, leading to significant\nperformance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent reinforcement learning (MARL), the centralized training with\ndecentralized execution (CTDE) framework has gained widespread adoption due to\nits strong performance. However, the further development of CTDE faces two key\nchallenges. First, agents struggle to autonomously assess the relevance of\ninput information for cooperative tasks, impairing their decision-making\nabilities. Second, in communication-limited scenarios with partial\nobservability, agents are unable to access global information, restricting\ntheir ability to collaborate effectively from a global perspective. To address\nthese challenges, we introduce a novel cooperative MARL framework based on\ninformation selection and tacit learning. In this framework, agents gradually\ndevelop implicit coordination during training, enabling them to infer the\ncooperative behavior of others in a discrete space without communication,\nrelying solely on local information. Moreover, we integrate gating and\nselection mechanisms, allowing agents to adaptively filter information based on\nenvironmental changes, thereby enhancing their decision-making capabilities.\nExperiments on popular MARL benchmarks show that our framework can be\nseamlessly integrated with state-of-the-art algorithms, leading to significant\nperformance improvements."
                },
                "authors": [
                    {
                        "name": "Lunjun Liu"
                    },
                    {
                        "name": "Weilai Jiang"
                    },
                    {
                        "name": "Yaonan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yaonan Wang"
                },
                "author": "Yaonan Wang",
                "arxiv_comment": "Accepted by AAMAS 2025 (Extended Abstract)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01003v2",
                "updated": "2024-12-20T07:53:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    53,
                    56,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-01T23:35:53Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    35,
                    53,
                    6,
                    336,
                    0
                ],
                "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning"
                },
                "summary": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible."
                },
                "authors": [
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hidenori Tanaka"
                },
                "author": "Hidenori Tanaka",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15634v1",
                "updated": "2024-12-20T07:50:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    50,
                    8,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:50:08Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    50,
                    8,
                    4,
                    355,
                    0
                ],
                "title": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language\n  Model"
                },
                "summary": "Large language models (LLMs) have been widely applied in various practical\napplications, typically comprising billions of parameters, with inference\nprocesses requiring substantial energy and computational resources. In\ncontrast, the human brain, employing bio-plausible spiking mechanisms, can\naccomplish the same tasks while significantly reducing energy consumption, even\nwith a similar number of parameters. Based on this, several pioneering\nresearchers have proposed and implemented various large language models that\nleverage spiking neural networks. They have demonstrated the feasibility of\nthese models, validated their performance, and open-sourced their frameworks\nand partial source code. To accelerate the adoption of brain-inspired large\nlanguage models and facilitate secondary development for researchers, we are\nreleasing a software toolkit named DarwinKit (Darkit). The toolkit is designed\nspecifically for learners, researchers, and developers working on spiking large\nmodels, offering a suite of highly user-friendly features that greatly simplify\nthe learning, deployment, and development processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied in various practical\napplications, typically comprising billions of parameters, with inference\nprocesses requiring substantial energy and computational resources. In\ncontrast, the human brain, employing bio-plausible spiking mechanisms, can\naccomplish the same tasks while significantly reducing energy consumption, even\nwith a similar number of parameters. Based on this, several pioneering\nresearchers have proposed and implemented various large language models that\nleverage spiking neural networks. They have demonstrated the feasibility of\nthese models, validated their performance, and open-sourced their frameworks\nand partial source code. To accelerate the adoption of brain-inspired large\nlanguage models and facilitate secondary development for researchers, we are\nreleasing a software toolkit named DarwinKit (Darkit). The toolkit is designed\nspecifically for learners, researchers, and developers working on spiking large\nmodels, offering a suite of highly user-friendly features that greatly simplify\nthe learning, deployment, and development processes."
                },
                "authors": [
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Shifan Ye"
                    },
                    {
                        "name": "Qian Zheng"
                    },
                    {
                        "name": "Yangfan Hu"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Shunyu Qi"
                    },
                    {
                        "name": "Shuyang Chen"
                    },
                    {
                        "name": "Huajin Tang"
                    },
                    {
                        "name": "Gang Pan"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19026v2",
                "updated": "2024-12-20T07:37:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    37,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-29T12:12:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    12,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants\n  with Relaxing Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants\n  with Relaxing Constraints"
                },
                "summary": "Recent advances in large language model assistants have made them\nindispensable, raising significant concerns over managing their safety.\nAutomated red teaming offers a promising alternative to the labor-intensive and\nerror-prone manual probing for vulnerabilities, providing more consistent and\nscalable safety evaluations. However, existing approaches often compromise\ndiversity by focusing on maximizing attack success rate. Additionally, methods\nthat decrease the cosine similarity from historical embeddings with semantic\ndiversity rewards lead to novelty stagnation as history grows. To address these\nissues, we introduce DiveR-CT, which relaxes conventional constraints on the\nobjective and semantic reward, granting greater freedom for the policy to\nenhance diversity. Our experiments demonstrate DiveR-CT's marked superiority\nover baselines by 1) generating data that perform better in various diversity\nmetrics across different attack success rate levels, 2) better-enhancing\nresiliency in blue team models through safety tuning based on collected data,\n3) allowing dynamic control of objective weights for reliable and controllable\nattack success rates, and 4) reducing susceptibility to reward\noveroptimization. Overall, our method provides an effective and efficient\napproach to LLM red teaming, accelerating real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model assistants have made them\nindispensable, raising significant concerns over managing their safety.\nAutomated red teaming offers a promising alternative to the labor-intensive and\nerror-prone manual probing for vulnerabilities, providing more consistent and\nscalable safety evaluations. However, existing approaches often compromise\ndiversity by focusing on maximizing attack success rate. Additionally, methods\nthat decrease the cosine similarity from historical embeddings with semantic\ndiversity rewards lead to novelty stagnation as history grows. To address these\nissues, we introduce DiveR-CT, which relaxes conventional constraints on the\nobjective and semantic reward, granting greater freedom for the policy to\nenhance diversity. Our experiments demonstrate DiveR-CT's marked superiority\nover baselines by 1) generating data that perform better in various diversity\nmetrics across different attack success rate levels, 2) better-enhancing\nresiliency in blue team models through safety tuning based on collected data,\n3) allowing dynamic control of objective weights for reliable and controllable\nattack success rates, and 4) reducing susceptibility to reward\noveroptimization. Overall, our method provides an effective and efficient\napproach to LLM red teaming, accelerating real-world deployment."
                },
                "authors": [
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Quentin Xu"
                    },
                    {
                        "name": "Matthieu Lin"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Yong-jin Liu"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15628v1",
                "updated": "2024-12-20T07:35:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    35,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:35:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    35,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Can Input Attributions Interpret the Inductive Reasoning Process\n  Elicited in In-Context Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Input Attributions Interpret the Inductive Reasoning Process\n  Elicited in In-Context Learning?"
                },
                "summary": "Elucidating the rationale behind neural models' outputs has been challenging\nin the machine learning field, which is indeed applicable in this age of large\nlanguage models (LLMs) and in-context learning (ICL). When it comes to\nestimating input attributions (IA), ICL poses a new issue of interpreting which\nexample in the prompt, consisting of a set of examples, contributed to\nidentifying the task/rule to be solved. To this end, in this paper, we\nintroduce synthetic diagnostic tasks inspired by the poverty of the stimulus\ndesign in inductive reasoning; here, most in-context examples are ambiguous\nw.r.t. their underlying rule, and one critical example disambiguates the task\ndemonstrated. The question is whether conventional IA methods can identify such\nan example in interpreting the inductive reasoning process in ICL. Our\nexperiments provide several practical findings; for example, a certain simple\nIA method works the best, and the larger the model, the generally harder it is\nto interpret the ICL with gradient-based IA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elucidating the rationale behind neural models' outputs has been challenging\nin the machine learning field, which is indeed applicable in this age of large\nlanguage models (LLMs) and in-context learning (ICL). When it comes to\nestimating input attributions (IA), ICL poses a new issue of interpreting which\nexample in the prompt, consisting of a set of examples, contributed to\nidentifying the task/rule to be solved. To this end, in this paper, we\nintroduce synthetic diagnostic tasks inspired by the poverty of the stimulus\ndesign in inductive reasoning; here, most in-context examples are ambiguous\nw.r.t. their underlying rule, and one critical example disambiguates the task\ndemonstrated. The question is whether conventional IA methods can identify such\nan example in interpreting the inductive reasoning process in ICL. Our\nexperiments provide several practical findings; for example, a certain simple\nIA method works the best, and the larger the model, the generally harder it is\nto interpret the ICL with gradient-based IA methods."
                },
                "authors": [
                    {
                        "name": "Mengyu Ye"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Goro Kobayashi"
                    },
                    {
                        "name": "Jun Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Jun Suzuki"
                },
                "author": "Jun Suzuki",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15623v1",
                "updated": "2024-12-20T07:29:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    29,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:29:10Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    29,
                    10,
                    4,
                    355,
                    0
                ],
                "title": "JailPO: A Novel Black-box Jailbreak Framework via Preference\n  Optimization against Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailPO: A Novel Black-box Jailbreak Framework via Preference\n  Optimization against Aligned LLMs"
                },
                "summary": "Large Language Models (LLMs) aligned with human feedback have recently\ngarnered significant attention. However, it remains vulnerable to jailbreak\nattacks, where adversaries manipulate prompts to induce harmful outputs.\nExploring jailbreak attacks enables us to investigate the vulnerabilities of\nLLMs and further guides us in enhancing their security. Unfortunately, existing\ntechniques mainly rely on handcrafted templates or generated-based\noptimization, posing challenges in scalability, efficiency and universality. To\naddress these issues, we present JailPO, a novel black-box jailbreak framework\nto examine LLM alignment. For scalability and universality, JailPO meticulously\ntrains attack models to automatically generate covert jailbreak prompts.\nFurthermore, we introduce a preference optimization-based attack method to\nenhance the jailbreak effectiveness, thereby improving efficiency. To analyze\nmodel vulnerabilities, we provide three flexible jailbreak patterns. Extensive\nexperiments demonstrate that JailPO not only automates the attack process while\nmaintaining effectiveness but also exhibits superior performance in efficiency,\nuniversality, and robustness against defenses compared to baselines.\nAdditionally, our analysis of the three JailPO patterns reveals that attacks\nbased on complex templates exhibit higher attack strength, whereas covert\nquestion transformations elicit riskier responses and are more likely to bypass\ndefense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) aligned with human feedback have recently\ngarnered significant attention. However, it remains vulnerable to jailbreak\nattacks, where adversaries manipulate prompts to induce harmful outputs.\nExploring jailbreak attacks enables us to investigate the vulnerabilities of\nLLMs and further guides us in enhancing their security. Unfortunately, existing\ntechniques mainly rely on handcrafted templates or generated-based\noptimization, posing challenges in scalability, efficiency and universality. To\naddress these issues, we present JailPO, a novel black-box jailbreak framework\nto examine LLM alignment. For scalability and universality, JailPO meticulously\ntrains attack models to automatically generate covert jailbreak prompts.\nFurthermore, we introduce a preference optimization-based attack method to\nenhance the jailbreak effectiveness, thereby improving efficiency. To analyze\nmodel vulnerabilities, we provide three flexible jailbreak patterns. Extensive\nexperiments demonstrate that JailPO not only automates the attack process while\nmaintaining effectiveness but also exhibits superior performance in efficiency,\nuniversality, and robustness against defenses compared to baselines.\nAdditionally, our analysis of the three JailPO patterns reveals that attacks\nbased on complex templates exhibit higher attack strength, whereas covert\nquestion transformations elicit riskier responses and are more likely to bypass\ndefense mechanisms."
                },
                "authors": [
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Jiawei Ye"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Tianjie Yan"
                    },
                    {
                        "name": "Chu Wang"
                    },
                    {
                        "name": "Zhixin Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Li"
                },
                "author": "Zhixin Li",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.14105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.14105v3",
                "updated": "2024-12-20T07:22:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    22,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2022-12-28T21:53:57Z",
                "published_parsed": [
                    2022,
                    12,
                    28,
                    21,
                    53,
                    57,
                    2,
                    362,
                    0
                ],
                "title": "Supercompliers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supercompliers"
                },
                "summary": "In a binary-treatment instrumental variable framework, we define\nsupercompliers as the subpopulation whose treatment take-up positively responds\nto eligibility and whose outcome positively responds to take-up. Supercompliers\nare the only subpopulation to benefit from treatment eligibility and, hence,\nare important for policy. We provide tools to characterize supercompliers under\na set of jointly testable assumptions. Specifically, we require standard\nassumptions from the local average treatment effect literature plus an outcome\nmonotonicity assumption. Estimation and inference can be conducted with\ninstrumental variable regression. In two job-training experiments, we\ndemonstrate our machinery's utility, particularly in incorporating social\nwelfare weights into marginal-value-of-public-funds analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a binary-treatment instrumental variable framework, we define\nsupercompliers as the subpopulation whose treatment take-up positively responds\nto eligibility and whose outcome positively responds to take-up. Supercompliers\nare the only subpopulation to benefit from treatment eligibility and, hence,\nare important for policy. We provide tools to characterize supercompliers under\na set of jointly testable assumptions. Specifically, we require standard\nassumptions from the local average treatment effect literature plus an outcome\nmonotonicity assumption. Estimation and inference can be conducted with\ninstrumental variable regression. In two job-training experiments, we\ndemonstrate our machinery's utility, particularly in incorporating social\nwelfare weights into marginal-value-of-public-funds analysis."
                },
                "authors": [
                    {
                        "name": "Matthew L. Comey"
                    },
                    {
                        "name": "Amanda R. Eng"
                    },
                    {
                        "name": "Pauline Leung"
                    },
                    {
                        "name": "Zhuan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Zhuan Pei"
                },
                "author": "Zhuan Pei",
                "arxiv_comment": "This version substantially revises v2. Pauline Leung has made\n  significant contributions and is now a coauthor. We expand the non-binary\n  outcome case, essential in the new connection to MVPF (Section 3). We replace\n  the original empirical application with two job training experiments (Section\n  4), add new theoretical results in Remark 5, Appendix A.3, and A.7.\n  References are updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.14105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.14105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15606v1",
                "updated": "2024-12-20T07:00:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:00:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage"
                },
                "summary": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16158v1",
                "updated": "2024-12-20T18:59:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    59,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:59:59Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    59,
                    59,
                    4,
                    355,
                    0
                ],
                "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding"
                },
                "summary": "The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE."
                },
                "authors": [
                    {
                        "name": "Chenxin Tao"
                    },
                    {
                        "name": "Shiqian Su"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16145v1",
                "updated": "2024-12-20T18:49:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    49,
                    45,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:49:45Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    49,
                    45,
                    4,
                    355,
                    0
                ],
                "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reinforcement Learning for LLM Multi-Step Reasoning"
                },
                "summary": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time."
                },
                "authors": [
                    {
                        "name": "Huaijie Wang"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Yilin Bao"
                    },
                    {
                        "name": "Ziran Yang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v1",
                "updated": "2024-12-20T18:31:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ndwula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16132v1",
                "updated": "2024-12-20T18:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    29,
                    49,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    29,
                    49,
                    4,
                    355,
                    0
                ],
                "title": "Data-Driven Mechanism Design: Jointly Eliciting Preferences and\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Mechanism Design: Jointly Eliciting Preferences and\n  Information"
                },
                "summary": "We study mechanism design when agents hold private information about both\ntheir preferences and a common payoff-relevant state. We show that standard\nmessage-driven mechanisms cannot implement socially efficient allocations when\nagents have multidimensional types, even under favorable conditions. To\novercome this limitation, we propose data-driven mechanisms that leverage\nadditional post-allocation information, modeled as an estimator of the\npayoff-relevant state. Our data-driven mechanisms extend the classic\nVickrey-Clarke-Groves class. We show that they achieve exact implementation in\nposterior equilibrium when the state is either fully revealed or the utility is\nlinear in an unbiased estimator. We also show that they achieve approximate\nimplementation with a consistent estimator, converging to exact implementation\nas the estimator converges, and present bounds on the convergence rate. We\ndemonstrate applications to digital advertising auctions and large language\nmodel (LLM)-based mechanisms, where user engagement naturally reveals relevant\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study mechanism design when agents hold private information about both\ntheir preferences and a common payoff-relevant state. We show that standard\nmessage-driven mechanisms cannot implement socially efficient allocations when\nagents have multidimensional types, even under favorable conditions. To\novercome this limitation, we propose data-driven mechanisms that leverage\nadditional post-allocation information, modeled as an estimator of the\npayoff-relevant state. Our data-driven mechanisms extend the classic\nVickrey-Clarke-Groves class. We show that they achieve exact implementation in\nposterior equilibrium when the state is either fully revealed or the utility is\nlinear in an unbiased estimator. We also show that they achieve approximate\nimplementation with a consistent estimator, converging to exact implementation\nas the estimator converges, and present bounds on the convergence rate. We\ndemonstrate applications to digital advertising auctions and large language\nmodel (LLM)-based mechanisms, where user engagement naturally reveals relevant\ninformation."
                },
                "authors": [
                    {
                        "name": "Dirk Bergemann"
                    },
                    {
                        "name": "Marek Bojko"
                    },
                    {
                        "name": "Paul Dütting"
                    },
                    {
                        "name": "Renato Paes Leme"
                    },
                    {
                        "name": "Haifeng Xu"
                    },
                    {
                        "name": "Song Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Song Zuo"
                },
                "author": "Song Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v4",
                "updated": "2024-12-20T18:11:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    11,
                    41,
                    4,
                    355,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16120v1",
                "updated": "2024-12-20T18:08:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    8,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:08:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    8,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation\n  Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation\n  Metrics"
                },
                "summary": "Evaluating the quality of machine-generated natural language content is a\nchallenging task in Natural Language Processing (NLP). Recently, large language\nmodels (LLMs) like GPT-4 have been employed for this purpose, but they are\ncomputationally expensive due to the extensive token usage required by complex\nevaluation prompts. In this paper, we propose a prompt optimization approach\nthat uses a smaller, fine-tuned language model to compress input data for\nevaluation prompt, thus reducing token usage and computational cost when using\nlarger LLMs for downstream evaluation. Our method involves a two-stage\nfine-tuning process: supervised fine-tuning followed by preference optimization\nto refine the model's outputs based on human preferences. We focus on Machine\nTranslation (MT) evaluation and utilize the GEMBA-MQM metric as a starting\npoint. Our results show a $2.37\\times$ reduction in token usage without any\nloss in evaluation quality. This work makes state-of-the-art LLM-based metrics\nlike GEMBA-MQM more cost-effective and efficient, enhancing their accessibility\nfor broader use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of machine-generated natural language content is a\nchallenging task in Natural Language Processing (NLP). Recently, large language\nmodels (LLMs) like GPT-4 have been employed for this purpose, but they are\ncomputationally expensive due to the extensive token usage required by complex\nevaluation prompts. In this paper, we propose a prompt optimization approach\nthat uses a smaller, fine-tuned language model to compress input data for\nevaluation prompt, thus reducing token usage and computational cost when using\nlarger LLMs for downstream evaluation. Our method involves a two-stage\nfine-tuning process: supervised fine-tuning followed by preference optimization\nto refine the model's outputs based on human preferences. We focus on Machine\nTranslation (MT) evaluation and utilize the GEMBA-MQM metric as a starting\npoint. Our results show a $2.37\\times$ reduction in token usage without any\nloss in evaluation quality. This work makes state-of-the-art LLM-based metrics\nlike GEMBA-MQM more cost-effective and efficient, enhancing their accessibility\nfor broader use."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16119v1",
                "updated": "2024-12-20T18:05:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    5,
                    22,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:05:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    5,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts"
                },
                "summary": "This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages."
                },
                "authors": [
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Salaar Masood"
                    },
                    {
                        "name": "Hamza Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Iqbal"
                },
                "author": "Hamza Iqbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16117v1",
                "updated": "2024-12-20T18:01:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    1,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T18:01:58Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    1,
                    58,
                    4,
                    355,
                    0
                ],
                "title": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models"
                },
                "summary": "In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid."
                },
                "authors": [
                    {
                        "name": "Xiaohu Huang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "Efficient Video Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16100v1",
                "updated": "2024-12-20T17:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Logical Consistency of Large Language Models in Fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Consistency of Large Language Models in Fact-checking"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses -- a meaning-preserving change in the input query\nresults in an inconsistent response and attributes to vulnerabilities of LLMs\nsuch as hallucination, jailbreaking, etc. Consequently, existing research\nfocuses on simple paraphrasing-based consistency assessment of LLMs, and\nignores complex queries that necessitates an even better understanding of\nlogical reasoning by an LLM. Our work therefore addresses the logical\ninconsistency of LLMs under complex logical queries with primitive logical\noperators, e.g., negation, conjunction, and disjunction. As a test bed, we\nconsider retrieval-augmented LLMs on a fact-checking task involving\npropositional logic queries from real-world knowledge graphs (KGs). Our\ncontributions are three-fold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries as input and demonstrate that existing LLMs lack\nlogical consistency, specially on complex queries. Improvement: We employ\nsupervised fine-tuning to improve the logical consistency of LLMs on the\ncomplex fact-checking task with KG contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses -- a meaning-preserving change in the input query\nresults in an inconsistent response and attributes to vulnerabilities of LLMs\nsuch as hallucination, jailbreaking, etc. Consequently, existing research\nfocuses on simple paraphrasing-based consistency assessment of LLMs, and\nignores complex queries that necessitates an even better understanding of\nlogical reasoning by an LLM. Our work therefore addresses the logical\ninconsistency of LLMs under complex logical queries with primitive logical\noperators, e.g., negation, conjunction, and disjunction. As a test bed, we\nconsider retrieval-augmented LLMs on a fact-checking task involving\npropositional logic queries from real-world knowledge graphs (KGs). Our\ncontributions are three-fold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries as input and demonstrate that existing LLMs lack\nlogical consistency, specially on complex queries. Improvement: We employ\nsupervised fine-tuning to improve the logical consistency of LLMs on the\ncomplex fact-checking task with KG contexts."
                },
                "authors": [
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Sarah Hasan"
                    },
                    {
                        "name": "Naheed Anjum Arafat"
                    },
                    {
                        "name": "Arijit Khan"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Khan"
                },
                "author": "Arijit Khan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16097v1",
                "updated": "2024-12-20T17:41:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    41,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:41:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    41,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Dual-Polarized Beyond Diagonal RIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Polarized Beyond Diagonal RIS"
                },
                "summary": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of\nRIS architectures more flexible than conventional RIS. While BD-RIS has been\nprimarily analyzed assuming uni-polarized systems, modern wireless deployments\nare dual-polarized. To address this gap, this paper investigates the\nfundamental limits of dual-polarized BD-RIS-aided systems. We derive the\nscaling laws governing the performance of BD-RIS and the Pareto frontier of the\ntrade-off between performance and circuit complexity enabled by BD-RIS.\nTheoretical results show that the group-connected RIS with group size 2\nprovides remarkable gains over conventional RIS in both Rayleigh and\nline-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of\nRIS architectures more flexible than conventional RIS. While BD-RIS has been\nprimarily analyzed assuming uni-polarized systems, modern wireless deployments\nare dual-polarized. To address this gap, this paper investigates the\nfundamental limits of dual-polarized BD-RIS-aided systems. We derive the\nscaling laws governing the performance of BD-RIS and the Pareto frontier of the\ntrade-off between performance and circuit complexity enabled by BD-RIS.\nTheoretical results show that the group-connected RIS with group size 2\nprovides remarkable gains over conventional RIS in both Rayleigh and\nline-of-sight (LoS) channels, while maintaining a reduced circuit complexity."
                },
                "authors": [
                    {
                        "name": "Matteo Nerini"
                    },
                    {
                        "name": "Bruno Clerckx"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Clerckx"
                },
                "author": "Bruno Clerckx",
                "arxiv_comment": "Submitted to IEEE for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16089v1",
                "updated": "2024-12-20T17:34:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    34,
                    16,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:34:16Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    34,
                    16,
                    4,
                    355,
                    0
                ],
                "title": "The Evolution of LLM Adoption in Industry Data Curation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evolution of LLM Adoption in Industry Data Curation Practices"
                },
                "summary": "As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development."
                },
                "authors": [
                    {
                        "name": "Crystal Qian"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Emily Reif"
                    },
                    {
                        "name": "Grady Simon"
                    },
                    {
                        "name": "Nada Hussein"
                    },
                    {
                        "name": "Nathan Clement"
                    },
                    {
                        "name": "James Wexler"
                    },
                    {
                        "name": "Carrie J. Cai"
                    },
                    {
                        "name": "Michael Terry"
                    },
                    {
                        "name": "Minsuk Kahng"
                    }
                ],
                "author_detail": {
                    "name": "Minsuk Kahng"
                },
                "author": "Minsuk Kahng",
                "arxiv_comment": "19 pages, 4 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16086v1",
                "updated": "2024-12-20T17:33:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T17:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG"
                },
                "summary": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings."
                },
                "authors": [
                    {
                        "name": "Hasan Md Tusfiqur Alam"
                    },
                    {
                        "name": "Devansh Srivastav"
                    },
                    {
                        "name": "Md Abdul Kadir"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "arxiv_comment": "Accepted in ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16714v2",
                "updated": "2024-12-20T16:26:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    26,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-22T05:51:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    5,
                    51,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment"
                },
                "summary": "Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment."
                },
                "authors": [
                    {
                        "name": "Mingzhi Wang"
                    },
                    {
                        "name": "Chengdong Ma"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Linjian Meng"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Jiancong Xiao"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Weijie J. Su"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06144v3",
                "updated": "2024-12-20T16:25:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    25,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-10T10:03:16Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    10,
                    3,
                    16,
                    0,
                    162,
                    0
                ],
                "title": "Language Models Resist Alignment: Evidence From Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Resist Alignment: Evidence From Data Compression"
                },
                "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment."
                },
                "authors": [
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Changye Li"
                    },
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Josef Dai"
                    },
                    {
                        "name": "Yunhuai Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "The five-page version has been accepted by NeurIPS 2024 Workshop\n  SoLaR. In the current version, we have conducted an in-depth expansion of\n  both the theoretical and experimental aspects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16022v1",
                "updated": "2024-12-20T16:14:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    14,
                    43,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T16:14:43Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    16,
                    14,
                    43,
                    4,
                    355,
                    0
                ],
                "title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Only Way is Ethics: A Guide to Ethical Research with Large Language\n  Models"
                },
                "summary": "There is a significant body of work looking at the ethical considerations of\nlarge language models (LLMs): critiquing tools to measure performance and\nharms; proposing toolkits to aid in ideation; discussing the risks to workers;\nconsidering legislation around privacy and security etc. As yet there is no\nwork that integrates these resources into a single practical guide that focuses\non LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',\nwhich we provide as an open and living resource for NLP practitioners, and\nthose tasked with evaluating the ethical implications of others' work. Our goal\nis to translate ethics literature into concrete recommendations and\nprovocations for thinking with clear first steps, aimed at computer scientists.\n'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's\nand Don'ts, which we present also in this paper. We likewise identify useful\ntoolkits to support ethical work. We refer the interested reader to the full\nLLM Ethics Whitepaper, which provides a succinct discussion of ethical\nconsiderations at each stage in a project lifecycle, as well as citations for\nthe hundreds of papers from which we drew our recommendations. The present\npaper can be thought of as a pocket guide to conducting ethical research with\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a significant body of work looking at the ethical considerations of\nlarge language models (LLMs): critiquing tools to measure performance and\nharms; proposing toolkits to aid in ideation; discussing the risks to workers;\nconsidering legislation around privacy and security etc. As yet there is no\nwork that integrates these resources into a single practical guide that focuses\non LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',\nwhich we provide as an open and living resource for NLP practitioners, and\nthose tasked with evaluating the ethical implications of others' work. Our goal\nis to translate ethics literature into concrete recommendations and\nprovocations for thinking with clear first steps, aimed at computer scientists.\n'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's\nand Don'ts, which we present also in this paper. We likewise identify useful\ntoolkits to support ethical work. We refer the interested reader to the full\nLLM Ethics Whitepaper, which provides a succinct discussion of ethical\nconsiderations at each stage in a project lifecycle, as well as citations for\nthe hundreds of papers from which we drew our recommendations. The present\npaper can be thought of as a pocket guide to conducting ethical research with\nLLMs."
                },
                "authors": [
                    {
                        "name": "Eddie L. Ungless"
                    },
                    {
                        "name": "Nikolas Vitsakis"
                    },
                    {
                        "name": "Zeerak Talat"
                    },
                    {
                        "name": "James Garforth"
                    },
                    {
                        "name": "Björn Ross"
                    },
                    {
                        "name": "Arno Onken"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "Accepted to COLING '25. This paper is the condensed pocket guide to\n  accompany our full LLM Ethics Whitepaper, available at arXiv:2410.19812, and\n  at https://github.com/MxEddie/Ethics-Whitepaper for suggested revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14426v2",
                "updated": "2024-12-20T15:57:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    57,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T00:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    0,
                    41,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs"
                },
                "summary": "Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively."
                },
                "authors": [
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Zhepeng Wang"
                    },
                    {
                        "name": "Runxue Bao"
                    },
                    {
                        "name": "Mengbing Wang"
                    },
                    {
                        "name": "Fangyi Li"
                    },
                    {
                        "name": "Yawen Wu"
                    },
                    {
                        "name": "Weiwen Jiang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Shangqian Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shangqian Gao"
                },
                "author": "Shangqian Gao",
                "arxiv_comment": "Updated a typo in the author list;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16008v1",
                "updated": "2024-12-20T15:56:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    56,
                    9,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    56,
                    9,
                    4,
                    355,
                    0
                ],
                "title": "Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep\n  Learning"
                },
                "summary": "Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a\ncornerstone to assessing the authenticity of the received information and\nguaranteeing robust service delivery in several application domains. The\nsolutions available today for spoofing detection either rely on additional\ncommunication systems, receivers, and antennas, or require mobile deployments.\nDetection systems working at the Physical (PHY) layer of the satellite\ncommunication link also require time-consuming and energy-hungry training\nprocesses on all satellites of the constellation, and rely on the availability\nof spoofed data, which are often challenging to collect. Moreover, none of such\ncontributions investigate the feasibility of aerial spoofing attacks launched\nvia drones operating at various altitudes. In this paper, we propose a new\nspoofing detection technique for LEO satellite constellation systems, applying\nanomaly detection on the received PHY signal via autoencoders. We validate our\nsolution through an extensive measurement campaign involving the deployment of\nan actual spoofer (Software-Defined Radio) installed on a drone and injecting\nrogue IRIDIUM messages while flying at different altitudes with various\nmovement patterns. Our results demonstrate that the proposed technique can\nreliably detect LEO spoofing attacks launched at different altitudes, while\nstate-of-the-art competing approaches simply fail. We also release the\ncollected data as open source, fostering further research on satellite\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a\ncornerstone to assessing the authenticity of the received information and\nguaranteeing robust service delivery in several application domains. The\nsolutions available today for spoofing detection either rely on additional\ncommunication systems, receivers, and antennas, or require mobile deployments.\nDetection systems working at the Physical (PHY) layer of the satellite\ncommunication link also require time-consuming and energy-hungry training\nprocesses on all satellites of the constellation, and rely on the availability\nof spoofed data, which are often challenging to collect. Moreover, none of such\ncontributions investigate the feasibility of aerial spoofing attacks launched\nvia drones operating at various altitudes. In this paper, we propose a new\nspoofing detection technique for LEO satellite constellation systems, applying\nanomaly detection on the received PHY signal via autoencoders. We validate our\nsolution through an extensive measurement campaign involving the deployment of\nan actual spoofer (Software-Defined Radio) installed on a drone and injecting\nrogue IRIDIUM messages while flying at different altitudes with various\nmovement patterns. Our results demonstrate that the proposed technique can\nreliably detect LEO spoofing attacks launched at different altitudes, while\nstate-of-the-art competing approaches simply fail. We also release the\ncollected data as open source, fostering further research on satellite\nsecurity."
                },
                "authors": [
                    {
                        "name": "Jos Wigchert"
                    },
                    {
                        "name": "Savio Sciancalepore"
                    },
                    {
                        "name": "Gabriele Oligeri"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Oligeri"
                },
                "author": "Gabriele Oligeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15993v1",
                "updated": "2024-12-20T15:41:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "title": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs"
                },
                "summary": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions."
                },
                "authors": [
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v4",
                "updated": "2024-12-20T15:26:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    26,
                    33,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15973v1",
                "updated": "2024-12-20T15:18:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:18:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Legommenders: A Comprehensive Content-Based Recommendation Library with\n  LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legommenders: A Comprehensive Content-Based Recommendation Library with\n  LLM Support"
                },
                "summary": "We present Legommenders, a unique library designed for content-based\nrecommendation that enables the joint training of content encoders alongside\nbehavior and interaction modules, thereby facilitating the seamless integration\nof content understanding directly into the recommendation pipeline.\nLegommenders allows researchers to effortlessly create and analyze over 1,000\ndistinct models across 15 diverse datasets. Further, it supports the\nincorporation of contemporary large language models, both as feature encoder\nand data generator, offering a robust platform for developing state-of-the-art\nrecommendation models and enabling more personalized and effective content\ndelivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Legommenders, a unique library designed for content-based\nrecommendation that enables the joint training of content encoders alongside\nbehavior and interaction modules, thereby facilitating the seamless integration\nof content understanding directly into the recommendation pipeline.\nLegommenders allows researchers to effortlessly create and analyze over 1,000\ndistinct models across 15 diverse datasets. Further, it supports the\nincorporation of contemporary large language models, both as feature encoder\nand data generator, offering a robust platform for developing state-of-the-art\nrecommendation models and enabling more personalized and effective content\ndelivery."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10825v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10825v3",
                "updated": "2024-12-20T15:11:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    11,
                    13,
                    4,
                    355,
                    0
                ],
                "published": "2024-01-19T17:21:05Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    17,
                    21,
                    5,
                    4,
                    19,
                    0
                ],
                "title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study"
                },
                "summary": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Stanislas Morbieu"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10825v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10825v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68Q32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13682v2",
                "updated": "2024-12-20T15:08:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    8,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T10:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    10,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning"
                },
                "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios."
                },
                "authors": [
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Baizhi Chen"
                    },
                    {
                        "name": "Wen-Da Wei"
                    },
                    {
                        "name": "Guohao Cai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-feng Li"
                },
                "author": "Yu-feng Li",
                "arxiv_comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14622v2",
                "updated": "2024-12-20T15:06:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    6,
                    14,
                    4,
                    355,
                    0
                ],
                "published": "2024-03-21T17:59:35Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    17,
                    59,
                    35,
                    3,
                    81,
                    0
                ],
                "title": "Language Repository for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Repository for Long Video Understanding"
                },
                "summary": "Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Jongwoo Park"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v5",
                "updated": "2024-12-20T14:58:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    58,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Bytedance-Seed-Foundation-Code-Team"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Jianfeng Chen"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Wentao Chen"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Kaibo Liu"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Tingkai Liu"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Guanghan Ning"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Jiahao Su"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Yunzhe Tao"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Liang Xiang"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yongsheng Xiao"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Shikun Xu"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Jianbo Yuan"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yuyu Zhang"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Ming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhu"
                },
                "author": "Ming Zhu",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15957v1",
                "updated": "2024-12-20T14:51:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    51,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:51:12Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    51,
                    12,
                    4,
                    355,
                    0
                ],
                "title": "From General to Specific: Tailoring Large Language Models for\n  Personalized Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From General to Specific: Tailoring Large Language Models for\n  Personalized Healthcare"
                },
                "summary": "The rapid development of large language models (LLMs) has transformed many\nindustries, including healthcare. However, previous medical LLMs have largely\nfocused on leveraging general medical knowledge to provide responses, without\naccounting for patient variability and lacking true personalization at the\nindividual level. To address this, we propose a novel method called\npersonalized medical language model (PMLM), which explores and optimizes\npersonalized LLMs through recommendation systems and reinforcement learning\n(RL). Specifically, by utilizing self-informed and peer-informed\npersonalization, PMLM captures changes in behaviors and preferences to design\ninitial personalized prompts tailored to individual needs. We further refine\nthese initial personalized prompts through RL, ultimately enhancing the\nprecision of LLM guidance. Notably, the personalized prompt are hard prompt,\nwhich grants PMLM high adaptability and reusability, allowing it to directly\nleverage high-quality proprietary LLMs. We evaluate PMLM using real-world\nobstetrics and gynecology data, and the experimental results demonstrate that\nPMLM achieves personalized responses, and it provides more refined and\nindividualized services, offering a potential way for personalized medical\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has transformed many\nindustries, including healthcare. However, previous medical LLMs have largely\nfocused on leveraging general medical knowledge to provide responses, without\naccounting for patient variability and lacking true personalization at the\nindividual level. To address this, we propose a novel method called\npersonalized medical language model (PMLM), which explores and optimizes\npersonalized LLMs through recommendation systems and reinforcement learning\n(RL). Specifically, by utilizing self-informed and peer-informed\npersonalization, PMLM captures changes in behaviors and preferences to design\ninitial personalized prompts tailored to individual needs. We further refine\nthese initial personalized prompts through RL, ultimately enhancing the\nprecision of LLM guidance. Notably, the personalized prompt are hard prompt,\nwhich grants PMLM high adaptability and reusability, allowing it to directly\nleverage high-quality proprietary LLMs. We evaluate PMLM using real-world\nobstetrics and gynecology data, and the experimental results demonstrate that\nPMLM achieves personalized responses, and it provides more refined and\nindividualized services, offering a potential way for personalized medical\nLLMs."
                },
                "authors": [
                    {
                        "name": "Ruize Shi"
                    },
                    {
                        "name": "Hong Huang"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Kehan Yin"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Yun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yun Zhao"
                },
                "author": "Yun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15948v1",
                "updated": "2024-12-20T14:44:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    44,
                    11,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:44:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    44,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI\n  Refactoring"
                },
                "summary": "In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions."
                },
                "authors": [
                    {
                        "name": "Markus Borg"
                    }
                ],
                "author_detail": {
                    "name": "Markus Borg"
                },
                "author": "Markus Borg",
                "arxiv_comment": "Accepted for publication in the Proc. of the 2nd Workshop on\n  Integrated Development Environments, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15931v1",
                "updated": "2024-12-20T14:23:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    23,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:23:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    23,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Large Language Model assisted Hybrid Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model assisted Hybrid Fuzzing"
                },
                "summary": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors."
                },
                "authors": [
                    {
                        "name": "Ruijie Meng"
                    },
                    {
                        "name": "Gregory J. Duck"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15921v1",
                "updated": "2024-12-20T14:13:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    13,
                    9,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T14:13:09Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    14,
                    13,
                    9,
                    4,
                    355,
                    0
                ],
                "title": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning"
                },
                "summary": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "UNDER REVIEW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15907v1",
                "updated": "2024-12-20T13:59:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    59,
                    11,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:59:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    59,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Development of a Large-scale Dataset of Chest Computed Tomography\n  Reports in Japanese and a High-performance Finding Classification Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Large-scale Dataset of Chest Computed Tomography\n  Reports in Japanese and a High-performance Finding Classification Model"
                },
                "summary": "Background: Recent advances in large language models highlight the need for\nhigh-quality multilingual medical datasets. While Japan leads globally in CT\nscanner deployment and utilization, the lack of large-scale Japanese radiology\ndatasets has hindered the development of specialized language models for\nmedical imaging analysis. Objective: To develop a comprehensive Japanese CT\nreport dataset through machine translation and establish a specialized language\nmodel for structured finding classification. Additionally, to create a\nrigorously validated evaluation dataset through expert radiologist review.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304\npatients) into Japanese using GPT-4o mini. The training dataset consisted of\n22,778 machine-translated reports, while the validation dataset included 150\nradiologist-revised reports. We developed CT-BERT-JPN based on\n\"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured\nfindings from Japanese radiology reports. Results: Translation metrics showed\nstrong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores\nranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression\nsections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in\n11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular\nseptal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1\nscores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in\nfour conditions. Conclusions: Our study establishes a robust Japanese CT report\ndataset and demonstrates the effectiveness of a specialized language model for\nstructured finding classification. The hybrid approach of machine translation\nand expert validation enables the creation of large-scale medical datasets\nwhile maintaining high quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Recent advances in large language models highlight the need for\nhigh-quality multilingual medical datasets. While Japan leads globally in CT\nscanner deployment and utilization, the lack of large-scale Japanese radiology\ndatasets has hindered the development of specialized language models for\nmedical imaging analysis. Objective: To develop a comprehensive Japanese CT\nreport dataset through machine translation and establish a specialized language\nmodel for structured finding classification. Additionally, to create a\nrigorously validated evaluation dataset through expert radiologist review.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304\npatients) into Japanese using GPT-4o mini. The training dataset consisted of\n22,778 machine-translated reports, while the validation dataset included 150\nradiologist-revised reports. We developed CT-BERT-JPN based on\n\"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured\nfindings from Japanese radiology reports. Results: Translation metrics showed\nstrong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores\nranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression\nsections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in\n11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular\nseptal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1\nscores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in\nfour conditions. Conclusions: Our study establishes a robust Japanese CT report\ndataset and demonstrates the effectiveness of a specialized language model for\nstructured finding classification. The hybrid approach of machine translation\nand expert validation enables the creation of large-scale medical datasets\nwhile maintaining high quality."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamagishi"
                    },
                    {
                        "name": "Yuta Nakamura"
                    },
                    {
                        "name": "Tomohiro Kikuchi"
                    },
                    {
                        "name": "Yuki Sonoda"
                    },
                    {
                        "name": "Hiroshi Hirakawa"
                    },
                    {
                        "name": "Shintaro Kano"
                    },
                    {
                        "name": "Satoshi Nakamura"
                    },
                    {
                        "name": "Shouhei Hanaoka"
                    },
                    {
                        "name": "Takeharu Yoshikawa"
                    },
                    {
                        "name": "Osamu Abe"
                    }
                ],
                "author_detail": {
                    "name": "Osamu Abe"
                },
                "author": "Osamu Abe",
                "arxiv_comment": "Dataset available at\n  https://huggingface.co/datasets/YYama0/CT-RATE-JPN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15902v1",
                "updated": "2024-12-20T13:54:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    54,
                    57,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:54:57Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    54,
                    57,
                    4,
                    355,
                    0
                ],
                "title": "On the Suitability of pre-trained foundational LLMs for Analysis in\n  German Legal Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Suitability of pre-trained foundational LLMs for Analysis in\n  German Legal Education"
                },
                "summary": "We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase."
                },
                "authors": [
                    {
                        "name": "Lorenz Wendlinger"
                    },
                    {
                        "name": "Christian Braun"
                    },
                    {
                        "name": "Abdullah Al Zubaer"
                    },
                    {
                        "name": "Simon Alexander Nonn"
                    },
                    {
                        "name": "Sarah Großkopf"
                    },
                    {
                        "name": "Christofer Fellicious"
                    },
                    {
                        "name": "Michael Granitzer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Granitzer"
                },
                "author": "Michael Granitzer",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15896v1",
                "updated": "2024-12-20T13:50:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "title": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models"
                },
                "summary": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting."
                },
                "authors": [
                    {
                        "name": "Manuel Pratelli"
                    },
                    {
                        "name": "John Bianchi"
                    },
                    {
                        "name": "Fabio Pinelli"
                    },
                    {
                        "name": "Marinella Petrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Marinella Petrocchi"
                },
                "author": "Marinella Petrocchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15891v1",
                "updated": "2024-12-20T13:47:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    47,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T13:47:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    47,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain"
                },
                "summary": "Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand."
                },
                "authors": [
                    {
                        "name": "Camille Barboule"
                    },
                    {
                        "name": "Viet-Phi Huynh"
                    },
                    {
                        "name": "Adrien Bufort"
                    },
                    {
                        "name": "Yoan Chabot"
                    },
                    {
                        "name": "Géraldine Damnati"
                    },
                    {
                        "name": "Gwénolé Lecorvé"
                    }
                ],
                "author_detail": {
                    "name": "Gwénolé Lecorvé"
                },
                "author": "Gwénolé Lecorvé",
                "arxiv_comment": "30 pages (main: 13 pages, appendices: 17 pages), 1 figure, 22 tables,\n  achieved March 2024, released December 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01866v3",
                "updated": "2024-12-20T13:19:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    19,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-03T21:31:34Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    21,
                    31,
                    34,
                    5,
                    216,
                    0
                ],
                "title": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively."
                },
                "authors": [
                    {
                        "name": "Peyman Hosseini"
                    },
                    {
                        "name": "Ignacio Castro"
                    },
                    {
                        "name": "Iacopo Ghinassi"
                    },
                    {
                        "name": "Matthew Purver"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Purver"
                },
                "author": "Matthew Purver",
                "arxiv_comment": "12 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14170v2",
                "updated": "2024-12-20T13:15:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    15,
                    12,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-23T04:54:37Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    4,
                    54,
                    37,
                    3,
                    144,
                    0
                ],
                "title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning"
                },
                "summary": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks."
                },
                "authors": [
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Linhao Luo"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yongli Hu"
                    },
                    {
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Baocai Yin"
                    }
                ],
                "author_detail": {
                    "name": "Baocai Yin"
                },
                "author": "Baocai Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13975v3",
                "updated": "2024-12-20T12:52:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    52,
                    0,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-20T03:50:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    3,
                    50,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs"
                },
                "summary": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies."
                },
                "authors": [
                    {
                        "name": "Zhongshen Zeng"
                    },
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Pengguang Chen"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Linling Shen"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14276v2",
                "updated": "2024-12-20T12:45:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    45,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T19:15:17Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    19,
                    15,
                    17,
                    2,
                    353,
                    0
                ],
                "title": "Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data"
                },
                "summary": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Drai Paulen-Patterson"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "Accepted in Knowledge and Information Systems Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15843v1",
                "updated": "2024-12-20T12:34:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    34,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T12:34:58Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    34,
                    58,
                    4,
                    355,
                    0
                ],
                "title": "Rethinking Hardware Impairments in Multi-User Systems: Can FAS Make a\n  Difference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Hardware Impairments in Multi-User Systems: Can FAS Make a\n  Difference?"
                },
                "summary": "In this paper, we analyze the role of fluid antenna systems (FAS) in\nmulti-user systems with hardware impairments (HIs). Specifically, we\ninvestigate a scenario where a base station (BS) equipped with multiple fluid\nantennas communicates with multiple users (CUs), each equipped with a single\nfluid antenna. Our objective is to maximize the minimum communication rate\namong all users by jointly optimizing the BS's transmit beamforming, the\npositions of its transmit fluid antennas, and the positions of the CUs' receive\nfluid antennas. To address this non-convex problem, we propose a block\ncoordinate descent (BCD) algorithm integrating semidefinite relaxation (SDR),\nrank-one constraint relaxation (SRCR), successive convex approximation (SCA),\nand majorization-minimization (MM). Simulation results demonstrate that FAS\nsignificantly enhances system performance and robustness, with notable gains\nwhen both the BS and CUs are equipped with fluid antennas. Even under low\ntransmit power conditions, deploying FAS at the BS alone yields substantial\nperformance gains. However, the effectiveness of FAS depends on the\navailability of sufficient movement space, as space constraints may limit its\nbenefits compared to fixed antenna strategies. Our findings highlight the\npotential of FAS to mitigate HIs and enhance multi-user system performance,\nwhile emphasizing the need for practical deployment considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we analyze the role of fluid antenna systems (FAS) in\nmulti-user systems with hardware impairments (HIs). Specifically, we\ninvestigate a scenario where a base station (BS) equipped with multiple fluid\nantennas communicates with multiple users (CUs), each equipped with a single\nfluid antenna. Our objective is to maximize the minimum communication rate\namong all users by jointly optimizing the BS's transmit beamforming, the\npositions of its transmit fluid antennas, and the positions of the CUs' receive\nfluid antennas. To address this non-convex problem, we propose a block\ncoordinate descent (BCD) algorithm integrating semidefinite relaxation (SDR),\nrank-one constraint relaxation (SRCR), successive convex approximation (SCA),\nand majorization-minimization (MM). Simulation results demonstrate that FAS\nsignificantly enhances system performance and robustness, with notable gains\nwhen both the BS and CUs are equipped with fluid antennas. Even under low\ntransmit power conditions, deploying FAS at the BS alone yields substantial\nperformance gains. However, the effectiveness of FAS depends on the\navailability of sufficient movement space, as space constraints may limit its\nbenefits compared to fixed antenna strategies. Our findings highlight the\npotential of FAS to mitigate HIs and enhance multi-user system performance,\nwhile emphasizing the need for practical deployment considerations."
                },
                "authors": [
                    {
                        "name": "Junteng Yao"
                    },
                    {
                        "name": "Tuo Wu"
                    },
                    {
                        "name": "Liaoshi Zhou"
                    },
                    {
                        "name": "Ming Jin"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Maged Elkashlan"
                    },
                    {
                        "name": "Fumiyuki Adachi"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    },
                    {
                        "name": "Naofal Al-Dhahir"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09569v2",
                "updated": "2024-12-20T12:25:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    25,
                    22,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-12T15:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    15,
                    33,
                    50,
                    5,
                    286,
                    0
                ],
                "title": "Are You Human? An Adversarial Benchmark to Expose LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are You Human? An Adversarial Benchmark to Expose LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations."
                },
                "authors": [
                    {
                        "name": "Gilad Gressel"
                    },
                    {
                        "name": "Rahul Pankajakshan"
                    },
                    {
                        "name": "Yisroel Mirsky"
                    }
                ],
                "author_detail": {
                    "name": "Yisroel Mirsky"
                },
                "author": "Yisroel Mirsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21868v2",
                "updated": "2024-12-20T12:22:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    22,
                    37,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-29T09:02:37Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    2,
                    37,
                    1,
                    303,
                    0
                ],
                "title": "Improving In-Context Learning with Small Language Model Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving In-Context Learning with Small Language Model Ensembles"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners."
                },
                "authors": [
                    {
                        "name": "M. Mehdi Mojarradi"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "Presented at NeurIPS 2024 Workshop on Adaptive Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13993v3",
                "updated": "2024-12-20T12:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    6,
                    25,
                    4,
                    355,
                    0
                ],
                "published": "2024-07-19T02:48:54Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    48,
                    54,
                    4,
                    201,
                    0
                ],
                "title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models"
                },
                "summary": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    }
                ],
                "author_detail": {
                    "name": "Christoforus Yoga Haryanto"
                },
                "author": "Christoforus Yoga Haryanto",
                "arxiv_comment": "10 pages, 3 figures, 1 table, presented at the 51st International\n  Conference on Computers and Industrial Engineering (CIE51), 11 Dec 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15819v1",
                "updated": "2024-12-20T12:01:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    1,
                    1,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T12:01:01Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    1,
                    1,
                    4,
                    355,
                    0
                ],
                "title": "Robustness-enhanced Myoelectric Control with GAN-based Open-set\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness-enhanced Myoelectric Control with GAN-based Open-set\n  Recognition"
                },
                "summary": "Electromyography (EMG) signals are widely used in human motion recognition\nand medical rehabilitation, yet their variability and susceptibility to noise\nsignificantly limit the reliability of myoelectric control systems. Existing\nrecognition algorithms often fail to handle unfamiliar actions effectively,\nleading to system instability and errors. This paper proposes a novel framework\nbased on Generative Adversarial Networks (GANs) to enhance the robustness and\nusability of myoelectric control systems by enabling open-set recognition. The\nmethod incorporates a GAN-based discriminator to identify and reject unknown\nactions, maintaining system stability by preventing misclassifications.\nExperimental evaluations on publicly available and self-collected datasets\ndemonstrate a recognition accuracy of 97.6\\% for known actions and a 23.6\\%\nimprovement in Active Error Rate (AER) after rejecting unknown actions. The\nproposed approach is computationally efficient and suitable for deployment on\nedge devices, making it practical for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromyography (EMG) signals are widely used in human motion recognition\nand medical rehabilitation, yet their variability and susceptibility to noise\nsignificantly limit the reliability of myoelectric control systems. Existing\nrecognition algorithms often fail to handle unfamiliar actions effectively,\nleading to system instability and errors. This paper proposes a novel framework\nbased on Generative Adversarial Networks (GANs) to enhance the robustness and\nusability of myoelectric control systems by enabling open-set recognition. The\nmethod incorporates a GAN-based discriminator to identify and reject unknown\nactions, maintaining system stability by preventing misclassifications.\nExperimental evaluations on publicly available and self-collected datasets\ndemonstrate a recognition accuracy of 97.6\\% for known actions and a 23.6\\%\nimprovement in Active Error Rate (AER) after rejecting unknown actions. The\nproposed approach is computationally efficient and suitable for deployment on\nedge devices, making it practical for real-world applications."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Ziyang Feng"
                    },
                    {
                        "name": "Pin Zhang"
                    },
                    {
                        "name": "Manjiang Cao"
                    },
                    {
                        "name": "Yiming Yuan"
                    },
                    {
                        "name": "Tengfei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Chang"
                },
                "author": "Tengfei Chang",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v2",
                "updated": "2024-12-20T11:55:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    55,
                    35,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12581v2",
                "updated": "2024-12-20T11:49:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    49,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-17T06:20:39Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    20,
                    39,
                    1,
                    352,
                    0
                ],
                "title": "Understanding Emotional Body Expressions via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Emotional Body Expressions via Large Language Models"
                },
                "summary": "Emotion recognition based on body movements is vital in human-computer\ninteraction. However, existing emotion recognition methods predominantly focus\non enhancing classification accuracy, often neglecting the provision of textual\nexplanations to justify their classifications. In this paper, we propose an\nEmotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not\nonly recognizes emotions but also generates textual explanations by treating 3D\nbody movement data as unique input tokens within large language models (LLMs).\nSpecifically, we propose a multi-granularity skeleton tokenizer designed for\nLLMs, which separately extracts spatio-temporal tokens and semantic tokens from\nthe skeleton data. This approach allows LLMs to generate more nuanced\nclassification descriptions while maintaining robust classification\nperformance. Furthermore, we treat the skeleton sequence as a specific language\nand propose a unified skeleton token module. This module leverages the\nextensive background knowledge and language processing capabilities of LLMs to\naddress the challenges of joint training on heterogeneous datasets, thereby\nsignificantly enhancing recognition accuracy on individual datasets.\nExperimental results demonstrate that our model achieves recognition accuracy\ncomparable to existing methods. More importantly, with the support of\nbackground knowledge from LLMs, our model can generate detailed emotion\ndescriptions based on classification results, even when trained on a limited\namount of labeled skeleton data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition based on body movements is vital in human-computer\ninteraction. However, existing emotion recognition methods predominantly focus\non enhancing classification accuracy, often neglecting the provision of textual\nexplanations to justify their classifications. In this paper, we propose an\nEmotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not\nonly recognizes emotions but also generates textual explanations by treating 3D\nbody movement data as unique input tokens within large language models (LLMs).\nSpecifically, we propose a multi-granularity skeleton tokenizer designed for\nLLMs, which separately extracts spatio-temporal tokens and semantic tokens from\nthe skeleton data. This approach allows LLMs to generate more nuanced\nclassification descriptions while maintaining robust classification\nperformance. Furthermore, we treat the skeleton sequence as a specific language\nand propose a unified skeleton token module. This module leverages the\nextensive background knowledge and language processing capabilities of LLMs to\naddress the challenges of joint training on heterogeneous datasets, thereby\nsignificantly enhancing recognition accuracy on individual datasets.\nExperimental results demonstrate that our model achieves recognition accuracy\ncomparable to existing methods. More importantly, with the support of\nbackground knowledge from LLMs, our model can generate detailed emotion\ndescriptions based on classification results, even when trained on a limited\namount of labeled skeleton data."
                },
                "authors": [
                    {
                        "name": "Haifeng Lu"
                    },
                    {
                        "name": "Jiuyi Chen"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Runhao Zeng"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11532v3",
                "updated": "2024-12-20T11:25:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    25,
                    59,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-18T12:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph"
                },
                "summary": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Ting Zhou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15803v1",
                "updated": "2024-12-20T11:24:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    24,
                    13,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:24:13Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    24,
                    13,
                    4,
                    355,
                    0
                ],
                "title": "WebLLM: A High-Performance In-Browser LLM Inference Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLLM: A High-Performance In-Browser LLM Inference Engine"
                },
                "summary": "Advancements in large language models (LLMs) have unlocked remarkable\ncapabilities. While deploying these models typically requires server-grade GPUs\nand cloud-based inference, the recent emergence of smaller open-source models\nand increasingly powerful consumer devices have made on-device deployment\npractical. The web browser as a platform for on-device deployment is\nuniversally accessible, provides a natural agentic environment, and\nconveniently abstracts out the different backends from diverse device vendors.\nTo address this opportunity, we introduce WebLLM, an open-source JavaScript\nframework that enables high-performance LLM inference entirely within web\nbrowsers. WebLLM provides an OpenAI-style API for seamless integration into web\napplications, and leverages WebGPU for efficient local GPU acceleration and\nWebAssembly for performant CPU computation. With machine learning compilers\nMLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming\nthe absence of performant WebGPU kernel libraries. Evaluations show that WebLLM\ncan retain up to 80% native performance on the same device, with room to\nfurther close the gap. WebLLM paves the way for universally accessible,\nprivacy-preserving, personalized, and locally powered LLM applications in web\nbrowsers. The code is available at: https://github.com/mlc-ai/web-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have unlocked remarkable\ncapabilities. While deploying these models typically requires server-grade GPUs\nand cloud-based inference, the recent emergence of smaller open-source models\nand increasingly powerful consumer devices have made on-device deployment\npractical. The web browser as a platform for on-device deployment is\nuniversally accessible, provides a natural agentic environment, and\nconveniently abstracts out the different backends from diverse device vendors.\nTo address this opportunity, we introduce WebLLM, an open-source JavaScript\nframework that enables high-performance LLM inference entirely within web\nbrowsers. WebLLM provides an OpenAI-style API for seamless integration into web\napplications, and leverages WebGPU for efficient local GPU acceleration and\nWebAssembly for performant CPU computation. With machine learning compilers\nMLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming\nthe absence of performant WebGPU kernel libraries. Evaluations show that WebLLM\ncan retain up to 80% native performance on the same device, with room to\nfurther close the gap. WebLLM paves the way for universally accessible,\nprivacy-preserving, personalized, and locally powered LLM applications in web\nbrowsers. The code is available at: https://github.com/mlc-ai/web-llm."
                },
                "authors": [
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yucheng Qin"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Meng-Shiun Yu"
                    },
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Sudeep Agarwal"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15790v1",
                "updated": "2024-12-20T11:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    5,
                    26,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T11:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    5,
                    26,
                    4,
                    355,
                    0
                ],
                "title": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning"
                },
                "summary": "The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine."
                },
                "authors": [
                    {
                        "name": "Heming Zhang"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Fuhai Li"
                    }
                ],
                "author_detail": {
                    "name": "Fuhai Li"
                },
                "author": "Fuhai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00146v2",
                "updated": "2024-12-20T10:50:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    50,
                    9,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-31T18:49:12Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    49,
                    12,
                    3,
                    305,
                    0
                ],
                "title": "Responsibility-aware Strategic Reasoning in Probabilistic Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsibility-aware Strategic Reasoning in Probabilistic Multi-Agent\n  Systems"
                },
                "summary": "Responsibility plays a key role in the development and deployment of\ntrustworthy autonomous systems. In this paper, we focus on the problem of\nstrategic reasoning in probabilistic multi-agent systems with\nresponsibility-aware agents. We introduce the logic PATL+R, a variant of\nProbabilistic Alternating-time Temporal Logic. The novelty of PATL+R lies in\nits incorporation of modalities for causal responsibility, providing a\nframework for responsibility-aware multi-agent strategic reasoning. We present\nan approach to synthesise joint strategies that satisfy an outcome specified in\nPATL+R, while optimising the share of expected causal responsibility and\nreward. This provides a notion of balanced distribution of responsibility and\nreward gain among agents. To this end, we utilise the Nash equilibrium as the\nsolution concept for our strategic reasoning problem and demonstrate how to\ncompute responsibility-aware Nash equilibrium strategies via a reduction to\nparametric model checking of concurrent stochastic multi-player games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsibility plays a key role in the development and deployment of\ntrustworthy autonomous systems. In this paper, we focus on the problem of\nstrategic reasoning in probabilistic multi-agent systems with\nresponsibility-aware agents. We introduce the logic PATL+R, a variant of\nProbabilistic Alternating-time Temporal Logic. The novelty of PATL+R lies in\nits incorporation of modalities for causal responsibility, providing a\nframework for responsibility-aware multi-agent strategic reasoning. We present\nan approach to synthesise joint strategies that satisfy an outcome specified in\nPATL+R, while optimising the share of expected causal responsibility and\nreward. This provides a notion of balanced distribution of responsibility and\nreward gain among agents. To this end, we utilise the Nash equilibrium as the\nsolution concept for our strategic reasoning problem and demonstrate how to\ncompute responsibility-aware Nash equilibrium strategies via a reduction to\nparametric model checking of concurrent stochastic multi-player games."
                },
                "authors": [
                    {
                        "name": "Chunyan Mu"
                    },
                    {
                        "name": "Muhammad Najib"
                    },
                    {
                        "name": "Nir Oren"
                    }
                ],
                "author_detail": {
                    "name": "Nir Oren"
                },
                "author": "Nir Oren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15772v1",
                "updated": "2024-12-20T10:43:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    43,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:43:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    43,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease\n  Detection based on Spontaneous Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease\n  Detection based on Spontaneous Speech"
                },
                "summary": "Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis."
                },
                "authors": [
                    {
                        "name": "Jonathan Heitz"
                    },
                    {
                        "name": "Gerold Schneider"
                    },
                    {
                        "name": "Nicolas Langer"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Langer"
                },
                "author": "Nicolas Langer",
                "arxiv_comment": "Accepted at the 31st International Conference on Computational\n  Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17662v3",
                "updated": "2024-12-20T10:35:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    35,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-04-26T19:07:30Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    19,
                    7,
                    30,
                    4,
                    117,
                    0
                ],
                "title": "Questioning the Unknown: Optimising Multi-Agent Collaboration in\n  Narrative-Driven Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning the Unknown: Optimising Multi-Agent Collaboration in\n  Narrative-Driven Games"
                },
                "summary": "We present Questum, a novel framework for Large Language Model (LLM)-based\nagents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including\nundefined state spaces, absent intermediate rewards, and the need for strategic\ninteraction in a continuous language domain. Questum addresses these\ncomplexities through a sensor-based representation of agent states, a\nquestion-targeting mechanism guided by information gain, and a pruning strategy\nto refine suspect lists and enhance decision-making efficiency. To enable\nsystematic evaluation, we propose WellPlay, a dataset comprising 1,482\ninferential questions across 12 games, categorised into objectives, reasoning,\nand relationships. Experiments demonstrate Questum's capacity to achieve\nsuperior performance in reasoning accuracy and efficiency compared to existing\napproaches, while also significantly improving the quality of agent-human\ninteractions in MMGs. This study advances the development of reasoning agents\nfor complex social and interactive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Questum, a novel framework for Large Language Model (LLM)-based\nagents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including\nundefined state spaces, absent intermediate rewards, and the need for strategic\ninteraction in a continuous language domain. Questum addresses these\ncomplexities through a sensor-based representation of agent states, a\nquestion-targeting mechanism guided by information gain, and a pruning strategy\nto refine suspect lists and enhance decision-making efficiency. To enable\nsystematic evaluation, we propose WellPlay, a dataset comprising 1,482\ninferential questions across 12 games, categorised into objectives, reasoning,\nand relationships. Experiments demonstrate Questum's capacity to achieve\nsuperior performance in reasoning accuracy and efficiency compared to existing\napproaches, while also significantly improving the quality of agent-human\ninteractions in MMGs. This study advances the development of reasoning agents\nfor complex social and interactive scenarios."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15756v1",
                "updated": "2024-12-20T10:16:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    16,
                    18,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:16:18Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    16,
                    18,
                    4,
                    355,
                    0
                ],
                "title": "Probabilistic Latent Variable Modeling for Dynamic Friction\n  Identification and Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Latent Variable Modeling for Dynamic Friction\n  Identification and Estimation"
                },
                "summary": "Precise identification of dynamic models in robotics is essential to support\ncontrol design, friction compensation, output torque estimation, etc. A\nlongstanding challenge remains in the identification of friction models for\nrobotic joints, given the numerous physical phenomena affecting the underlying\nfriction dynamics which result into nonlinear characteristics and hysteresis\nbehaviour in particular. These phenomena proof difficult to be modelled and\ncaptured accurately using physical analogies alone. This has motivated\nresearchers to shift from physics-based to data-driven models. Currently, these\nmethods are still limited in their ability to generalize effectively to typical\nindustrial robot deployement, characterized by high- and low-velocity\noperations and frequent direction reversals. Empirical observations motivate\nthe use of dynamic friction models but these remain particulary challenging to\nestablish. To address the current limitations, we propose to account for\nunidentified dynamics in the robot joints using latent dynamic states. The\nfriction model may then utilize both the dynamic robot state and additional\ninformation encoded in the latent state to evaluate the friction torque. We\ncast this stochastic and partially unsupervised identification problem as a\nstandard probabilistic representation learning problem. In this work both the\nfriction model and latent state dynamics are parametrized as neural networks\nand integrated in the conventional lumped parameter dynamic robot model. The\ncomplete dynamics model is directly learned from the noisy encoder measurements\nin the robot joints. We use the Expectation-Maximisation (EM) algorithm to find\na Maximum Likelihood Estimate (MLE) of the model parameters. The effectiveness\nof the proposed method is validated in terms of open-loop prediction accuracy\nin comparison with baseline methods, using the Kuka KR6 R700 as a test\nplatform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise identification of dynamic models in robotics is essential to support\ncontrol design, friction compensation, output torque estimation, etc. A\nlongstanding challenge remains in the identification of friction models for\nrobotic joints, given the numerous physical phenomena affecting the underlying\nfriction dynamics which result into nonlinear characteristics and hysteresis\nbehaviour in particular. These phenomena proof difficult to be modelled and\ncaptured accurately using physical analogies alone. This has motivated\nresearchers to shift from physics-based to data-driven models. Currently, these\nmethods are still limited in their ability to generalize effectively to typical\nindustrial robot deployement, characterized by high- and low-velocity\noperations and frequent direction reversals. Empirical observations motivate\nthe use of dynamic friction models but these remain particulary challenging to\nestablish. To address the current limitations, we propose to account for\nunidentified dynamics in the robot joints using latent dynamic states. The\nfriction model may then utilize both the dynamic robot state and additional\ninformation encoded in the latent state to evaluate the friction torque. We\ncast this stochastic and partially unsupervised identification problem as a\nstandard probabilistic representation learning problem. In this work both the\nfriction model and latent state dynamics are parametrized as neural networks\nand integrated in the conventional lumped parameter dynamic robot model. The\ncomplete dynamics model is directly learned from the noisy encoder measurements\nin the robot joints. We use the Expectation-Maximisation (EM) algorithm to find\na Maximum Likelihood Estimate (MLE) of the model parameters. The effectiveness\nof the proposed method is validated in terms of open-loop prediction accuracy\nin comparison with baseline methods, using the Kuka KR6 R700 as a test\nplatform."
                },
                "authors": [
                    {
                        "name": "Victor Vantilborgh"
                    },
                    {
                        "name": "Sander De Witte"
                    },
                    {
                        "name": "Frederik Ostyn"
                    },
                    {
                        "name": "Tom Lefebvre"
                    },
                    {
                        "name": "Guillaume Crevecoeur"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Crevecoeur"
                },
                "author": "Guillaume Crevecoeur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15750v1",
                "updated": "2024-12-20T10:11:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    11,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:11:44Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    11,
                    44,
                    4,
                    355,
                    0
                ],
                "title": "Extracting Interpretable Task-Specific Circuits from Large Language\n  Models for Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Interpretable Task-Specific Circuits from Large Language\n  Models for Faster Inference"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, the size of LLMs is steadily increasing, hindering\ntheir application on computationally constrained environments. On the other\nhand, despite their general capabilities, there are many situations where only\none specific task is performed, rendering all other capabilities unnecessary\nand wasteful. This leads us to the following question: Is it possible to\nextract the minimal subset from an LLM that is able to perform a specific task\nin a faster, standalone manner? Recent works on Mechanistic Interpretability\n(MI) have shown that specific tasks are performed by a localized subset of\ncomponents, or circuit. However, current techniques used to identify the\ncircuit cannot be used to extract it for its standalone usage. In this work, we\npropose a novel approach to automatically extract the subset of the LLM that\nproperly performs a targeted task requiring no additional training and a small\namount of data samples. We evaluate our approach on different tasks and show\nthat the resulting models are (i) considerably smaller, reducing the number of\nparameters up to 82.77% and (ii) more interpretable, as they focus on the\ncircuit that is used to carry out the specific task, and can therefore be\nunderstood using MI techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, the size of LLMs is steadily increasing, hindering\ntheir application on computationally constrained environments. On the other\nhand, despite their general capabilities, there are many situations where only\none specific task is performed, rendering all other capabilities unnecessary\nand wasteful. This leads us to the following question: Is it possible to\nextract the minimal subset from an LLM that is able to perform a specific task\nin a faster, standalone manner? Recent works on Mechanistic Interpretability\n(MI) have shown that specific tasks are performed by a localized subset of\ncomponents, or circuit. However, current techniques used to identify the\ncircuit cannot be used to extract it for its standalone usage. In this work, we\npropose a novel approach to automatically extract the subset of the LLM that\nproperly performs a targeted task requiring no additional training and a small\namount of data samples. We evaluate our approach on different tasks and show\nthat the resulting models are (i) considerably smaller, reducing the number of\nparameters up to 82.77% and (ii) more interpretable, as they focus on the\ncircuit that is used to carry out the specific task, and can therefore be\nunderstood using MI techniques."
                },
                "authors": [
                    {
                        "name": "Jorge García-Carrasco"
                    },
                    {
                        "name": "Alejandro Maté"
                    },
                    {
                        "name": "Juan Trujillo"
                    }
                ],
                "author_detail": {
                    "name": "Juan Trujillo"
                },
                "author": "Juan Trujillo",
                "arxiv_comment": "Accepted to AAAI 25 Main Technical Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15748v1",
                "updated": "2024-12-20T10:06:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:06:52Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models"
                },
                "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole"
                },
                "authors": [
                    {
                        "name": "Shamus Sim"
                    },
                    {
                        "name": "Tyrone Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone Chen"
                },
                "author": "Tyrone Chen",
                "arxiv_comment": "16 pages, 5 figures, 2 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15747v1",
                "updated": "2024-12-20T10:06:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    30,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:06:30Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    30,
                    4,
                    355,
                    0
                ],
                "title": "Building Bridges: AI Custom Chatbots as Mediators between Mathematics\n  and Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Bridges: AI Custom Chatbots as Mediators between Mathematics\n  and Physics"
                },
                "summary": "This work explores the integration of AI custom chatbots in educational\nsettings, with a particular focus on their applicability in the context of\nmathematics and physics. In view of the increasing deployment of AI tools such\nas ChatGPT in educational contexts, the present study examines their potential\nas personalized tutoring systems. The study assesses the impact of AI-generated\nlearning materials on the learning experiences and performance of sixth-grade\nstudents, with a particular focus on proportional relationships in mathematical\nand physical contexts. The randomized controlled study with N = 214 students\ncompared traditional textbook materials with explanations generated by a custom\nchatbot. The results demonstrated that while AI-generated materials had an\nindefinite impact on learning outcomes, they significantly enhanced\npositive-activating emotions, situational interest, and self-efficacy, while\nreducing intrinsic and extrinsic cognitive load. These findings underscore the\npotential of AI to transform educational practices by fostering a superior\nlearning experience. However, further research is required to clarify its\nimpact on learning performance and long-term learning outcomes. The study\nhighlights the importance of careful integration and customization of AI tools\nto maximize their benefits in physics education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the integration of AI custom chatbots in educational\nsettings, with a particular focus on their applicability in the context of\nmathematics and physics. In view of the increasing deployment of AI tools such\nas ChatGPT in educational contexts, the present study examines their potential\nas personalized tutoring systems. The study assesses the impact of AI-generated\nlearning materials on the learning experiences and performance of sixth-grade\nstudents, with a particular focus on proportional relationships in mathematical\nand physical contexts. The randomized controlled study with N = 214 students\ncompared traditional textbook materials with explanations generated by a custom\nchatbot. The results demonstrated that while AI-generated materials had an\nindefinite impact on learning outcomes, they significantly enhanced\npositive-activating emotions, situational interest, and self-efficacy, while\nreducing intrinsic and extrinsic cognitive load. These findings underscore the\npotential of AI to transform educational practices by fostering a superior\nlearning experience. However, further research is required to clarify its\nimpact on learning performance and long-term learning outcomes. The study\nhighlights the importance of careful integration and customization of AI tools\nto maximize their benefits in physics education."
                },
                "authors": [
                    {
                        "name": "Julia Lademann"
                    },
                    {
                        "name": "Jannik Henze"
                    },
                    {
                        "name": "Sebastian Becker-Genschow"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Becker-Genschow"
                },
                "author": "Sebastian Becker-Genschow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15739v1",
                "updated": "2024-12-20T10:00:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    0,
                    26,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T10:00:26Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    0,
                    26,
                    4,
                    355,
                    0
                ],
                "title": "VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in\n  Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in\n  Large Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have made remarkable developments along\nwith the recent surge of large language models. Despite their advancements,\nLVLMs have a tendency to generate plausible yet inaccurate or inconsistent\ninformation based on the provided source content. This phenomenon, also known\nas ``hallucinations\" can have serious downstream implications during the\ndeployment of LVLMs. To address this, we present VORD a simple and effective\nmethod that alleviates hallucinations by calibrating token predictions based on\nordinal relationships between modified image pairs. VORD is presented in two\nforms: 1.) a minimalist training-free variant which eliminates implausible\ntokens from modified image pairs, and 2.) a trainable objective function that\npenalizes unlikely tokens. Our experiments demonstrate that VORD delivers\nbetter calibration and effectively mitigates object hallucinations on a\nwide-range of LVLM benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have made remarkable developments along\nwith the recent surge of large language models. Despite their advancements,\nLVLMs have a tendency to generate plausible yet inaccurate or inconsistent\ninformation based on the provided source content. This phenomenon, also known\nas ``hallucinations\" can have serious downstream implications during the\ndeployment of LVLMs. To address this, we present VORD a simple and effective\nmethod that alleviates hallucinations by calibrating token predictions based on\nordinal relationships between modified image pairs. VORD is presented in two\nforms: 1.) a minimalist training-free variant which eliminates implausible\ntokens from modified image pairs, and 2.) a trainable objective function that\npenalizes unlikely tokens. Our experiments demonstrate that VORD delivers\nbetter calibration and effectively mitigates object hallucinations on a\nwide-range of LVLM benchmarks."
                },
                "authors": [
                    {
                        "name": "Dexter Neo"
                    },
                    {
                        "name": "Tsuhan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tsuhan Chen"
                },
                "author": "Tsuhan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15720v1",
                "updated": "2024-12-20T09:42:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    42,
                    28,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:42:28Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    42,
                    28,
                    4,
                    355,
                    0
                ],
                "title": "Switching Frequency as FPGA Monitor: Studying Degradation and Ageing\n  Prognosis at Large Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Frequency as FPGA Monitor: Studying Degradation and Ageing\n  Prognosis at Large Scale"
                },
                "summary": "The growing deployment of unhardened embedded devices in critical systems\ndemands the monitoring of hardware ageing as part of predictive maintenance. In\nthis paper, we study degradation on a large deployment of 298 naturally aged\nFPGAs operating in the European XFEL particle accelerator. We base our\nstatistical analyses on 280 days of in-field measurements and find a\ngeneralized and continuous degradation of the switching frequency across all\ndevices with a median value of 0.064%. The large scale of this study allows us\nto localize areas of the deployed FPGAs that are highly impacted by\ndegradation. Moreover, by training machine learning models on the collected\ndata, we are able to forecast future trends of frequency degradation with\nhorizons of 60 days and relative errors as little as 0.002% over an evaluation\nperiod of 100 days.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of unhardened embedded devices in critical systems\ndemands the monitoring of hardware ageing as part of predictive maintenance. In\nthis paper, we study degradation on a large deployment of 298 naturally aged\nFPGAs operating in the European XFEL particle accelerator. We base our\nstatistical analyses on 280 days of in-field measurements and find a\ngeneralized and continuous degradation of the switching frequency across all\ndevices with a median value of 0.064%. The large scale of this study allows us\nto localize areas of the deployed FPGAs that are highly impacted by\ndegradation. Moreover, by training machine learning models on the collected\ndata, we are able to forecast future trends of frequency degradation with\nhorizons of 60 days and relative errors as little as 0.002% over an evaluation\nperiod of 100 days."
                },
                "authors": [
                    {
                        "name": "Leandro Lanzieri"
                    },
                    {
                        "name": "Lukasz Butkowski"
                    },
                    {
                        "name": "Jiri Kral"
                    },
                    {
                        "name": "Goerschwin Fey"
                    },
                    {
                        "name": "Holger Schlarb"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Thomas C. Schmidt"
                },
                "author": "Thomas C. Schmidt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v3",
                "updated": "2024-12-20T09:40:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    40,
                    54,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict arises as the LLM's evolving knowledge during\nSFT enables it to answer previously unanswerable questions, but the\nnow-answerable training samples still retain the original \"I don't know\"\nsupervision signals from the initial LLM state, leading to inconsistencies.\nThese conflicts cause the trained LLM to misclassify known questions as\nunknown, resulting in over-refusal. To address this issue, we introduce\nCertainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning\n(CRaFT). CRaFT centers on two main contributions: First, we additionally\nincorporate response certainty to selectively filter and modify data, reducing\nstatic conflicts. Second, we implement preliminary rehearsal training to\ncharacterize changes in the LLM's knowledge state, which helps mitigate dynamic\nconflicts during the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Code and data will be released at https://github.com/opendatalab/CRaFT .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict arises as the LLM's evolving knowledge during\nSFT enables it to answer previously unanswerable questions, but the\nnow-answerable training samples still retain the original \"I don't know\"\nsupervision signals from the initial LLM state, leading to inconsistencies.\nThese conflicts cause the trained LLM to misclassify known questions as\nunknown, resulting in over-refusal. To address this issue, we introduce\nCertainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning\n(CRaFT). CRaFT centers on two main contributions: First, we additionally\nincorporate response certainty to selectively filter and modify data, reducing\nstatic conflicts. Second, we implement preliminary rehearsal training to\ncharacterize changes in the LLM's knowledge state, which helps mitigate dynamic\nconflicts during the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Code and data will be released at https://github.com/opendatalab/CRaFT ."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15714v1",
                "updated": "2024-12-20T09:37:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:37:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLife: Automatic Life Journaling with Smartphones and LLMs"
                },
                "summary": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals."
                },
                "authors": [
                    {
                        "name": "Huatao Xu"
                    },
                    {
                        "name": "Panron Tong"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15712v1",
                "updated": "2024-12-20T09:33:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:33:31Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining"
                },
                "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."
                },
                "authors": [
                    {
                        "name": "Maike Züfle"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15702v1",
                "updated": "2024-12-20T09:24:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    24,
                    50,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:24:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    24,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing\n  Programming Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing\n  Programming Feedback"
                },
                "summary": "Despite the growing use of large language models (LLMs) for providing\nfeedback, limited research has explored how to achieve high-quality feedback.\nThis case study introduces an evaluation framework to assess different\nzero-shot prompt engineering methods. We varied the prompts systematically and\nanalyzed the provided feedback on programming errors in R. The results suggest\nthat prompts suggesting a stepwise procedure increase the precision, while\nomitting explicit specifications about which provided data to analyze improves\nerror identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing use of large language models (LLMs) for providing\nfeedback, limited research has explored how to achieve high-quality feedback.\nThis case study introduces an evaluation framework to assess different\nzero-shot prompt engineering methods. We varied the prompts systematically and\nanalyzed the provided feedback on programming errors in R. The results suggest\nthat prompts suggesting a stepwise procedure increase the precision, while\nomitting explicit specifications about which provided data to analyze improves\nerror identification."
                },
                "authors": [
                    {
                        "name": "Niklas Ippisch"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Jan Simson"
                    },
                    {
                        "name": "Jacob Beck"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Malte Schierholz"
                    }
                ],
                "author_detail": {
                    "name": "Malte Schierholz"
                },
                "author": "Malte Schierholz",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15698v1",
                "updated": "2024-12-20T09:18:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    18,
                    11,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T09:18:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    18,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Concept Boundary Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Boundary Vectors"
                },
                "summary": "Machine learning models are trained with relatively simple objectives, such\nas next token prediction. However, on deployment, they appear to capture a more\nfundamental representation of their input data. It is of interest to understand\nthe nature of these representations to help interpret the model's outputs and\nto identify ways to improve the salience of these representations. Concept\nvectors are constructions aimed at attributing concepts in the input data to\ndirections, represented by vectors, in the model's latent space. In this work,\nwe introduce concept boundary vectors as a concept vector construction derived\nfrom the boundary between the latent representations of concepts. Empirically\nwe demonstrate that concept boundary vectors capture a concept's semantic\nmeaning, and we compare their effectiveness against concept activation vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are trained with relatively simple objectives, such\nas next token prediction. However, on deployment, they appear to capture a more\nfundamental representation of their input data. It is of interest to understand\nthe nature of these representations to help interpret the model's outputs and\nto identify ways to improve the salience of these representations. Concept\nvectors are constructions aimed at attributing concepts in the input data to\ndirections, represented by vectors, in the model's latent space. In this work,\nwe introduce concept boundary vectors as a concept vector construction derived\nfrom the boundary between the latent representations of concepts. Empirically\nwe demonstrate that concept boundary vectors capture a concept's semantic\nmeaning, and we compare their effectiveness against concept activation vectors."
                },
                "authors": [
                    {
                        "name": "Thomas Walker"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Walker"
                },
                "author": "Thomas Walker",
                "arxiv_comment": "21 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15676v1",
                "updated": "2024-12-20T08:46:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    46,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:46:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Code Review Automation Via Multi-task Federated LLM -- An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Automation Via Multi-task Federated LLM -- An Empirical\n  Study"
                },
                "summary": "Code review is a crucial process before deploying code to production, as it\nvalidates the code, provides suggestions for improvements, and identifies\nerrors such as missed edge cases. In projects with regular production releases,\nthe effort required for peer code-reviews remains high. Consequently, there has\nbeen significant interest from software engineering (SE) researchers in\nautomating the code review process. Previous research on code review automation\nhas typically approached the task as three independent sub-tasks: review\nnecessity prediction, review comment generation, and code refinement. Our study\nattempts to (i) leverage the relationships between the sub-tasks of code review\nautomation, by developing a multi-task model that addresses all tasks in an\nintegrated manner, and (ii) increase model robustness on unseen data via\ncollaborative large language model (LLM) modeling, while retaining the\nproprietary nature of code, by using federated learning (FL). The study\nexplores five simple techniques for multi-task training, including two\nsequential methods, one parallel method, and two cumulative methods. The\nresults indicate that sequentially training a federated LLM (FedLLM) for our\ncode review multi-task use case is less efficient in terms of time,\ncomputation, and performance metrics, compared to training separate models for\neach task. Because sequential training demonstrates catastrophic forgetting,\nalternatively cumulative fine-tuning for multi-task training performs better\nthan training models for individual tasks. This study highlights the need for\nresearch focused on effective fine-tuning of multi-task FedLLMs for SE tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a crucial process before deploying code to production, as it\nvalidates the code, provides suggestions for improvements, and identifies\nerrors such as missed edge cases. In projects with regular production releases,\nthe effort required for peer code-reviews remains high. Consequently, there has\nbeen significant interest from software engineering (SE) researchers in\nautomating the code review process. Previous research on code review automation\nhas typically approached the task as three independent sub-tasks: review\nnecessity prediction, review comment generation, and code refinement. Our study\nattempts to (i) leverage the relationships between the sub-tasks of code review\nautomation, by developing a multi-task model that addresses all tasks in an\nintegrated manner, and (ii) increase model robustness on unseen data via\ncollaborative large language model (LLM) modeling, while retaining the\nproprietary nature of code, by using federated learning (FL). The study\nexplores five simple techniques for multi-task training, including two\nsequential methods, one parallel method, and two cumulative methods. The\nresults indicate that sequentially training a federated LLM (FedLLM) for our\ncode review multi-task use case is less efficient in terms of time,\ncomputation, and performance metrics, compared to training separate models for\neach task. Because sequential training demonstrates catastrophic forgetting,\nalternatively cumulative fine-tuning for multi-task training performs better\nthan training models for individual tasks. This study highlights the need for\nresearch focused on effective fine-tuning of multi-task FedLLMs for SE tasks."
                },
                "authors": [
                    {
                        "name": "Jahnavi Kumar"
                    },
                    {
                        "name": "Sridhar Chimalakonda"
                    }
                ],
                "author_detail": {
                    "name": "Sridhar Chimalakonda"
                },
                "author": "Sridhar Chimalakonda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14373v2",
                "updated": "2024-12-20T08:46:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    46,
                    30,
                    4,
                    355,
                    0
                ],
                "published": "2024-02-22T08:26:56Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    8,
                    26,
                    56,
                    3,
                    53,
                    0
                ],
                "title": "Small Language Models as Effective Guides for Large Language Models in\n  Chinese Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models as Effective Guides for Large Language Models in\n  Chinese Relation Extraction"
                },
                "summary": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\npaid to this problem using LLM approaches. Therefore, in this paper, we propose\nSLCoLM, a model collaboration framework, to mitigate the data long-tail\nproblem. In our framework, we use the ``\\textit{Training-Guide-Predict}''\nstrategy to combine the strengths of small pre-trained language models (SLMs)\nand LLMs, where a task-specific SLM framework acts as a guider, transfers task\nknowledge to the LLM and guides the LLM in performing RE tasks. Our experiments\non an ancient Chinese RE dataset rich in relation types show that the approach\nfacilitates RE of long-tail relation types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\npaid to this problem using LLM approaches. Therefore, in this paper, we propose\nSLCoLM, a model collaboration framework, to mitigate the data long-tail\nproblem. In our framework, we use the ``\\textit{Training-Guide-Predict}''\nstrategy to combine the strengths of small pre-trained language models (SLMs)\nand LLMs, where a task-specific SLM framework acts as a guider, transfers task\nknowledge to the LLM and guides the LLM in performing RE tasks. Our experiments\non an ancient Chinese RE dataset rich in relation types show that the approach\nfacilitates RE of long-tail relation types."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "13 pages, 9 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v3",
                "updated": "2024-12-20T08:38:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    38,
                    38,
                    4,
                    355,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12106v2",
                "updated": "2024-12-20T08:35:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    35,
                    24,
                    4,
                    355,
                    0
                ],
                "published": "2024-09-18T16:26:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models"
                },
                "summary": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Hanjun Fang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15660v1",
                "updated": "2024-12-20T08:20:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    20,
                    21,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:20:21Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    20,
                    21,
                    4,
                    355,
                    0
                ],
                "title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling\n  Capability Training Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling\n  Capability Training Pipeline"
                },
                "summary": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models."
                },
                "authors": [
                    {
                        "name": "Guancheng Zeng"
                    },
                    {
                        "name": "Wentao Ding"
                    },
                    {
                        "name": "Beining Xu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Wenqiang Han"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jingjing Mo"
                    },
                    {
                        "name": "Pengxu Qiu"
                    },
                    {
                        "name": "Xinran Tao"
                    },
                    {
                        "name": "Wang Tao"
                    },
                    {
                        "name": "Haowen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hu"
                },
                "author": "Haowen Hu",
                "arxiv_comment": "23 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15655v1",
                "updated": "2024-12-20T08:13:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T08:13:05Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula"
                },
                "summary": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o."
                },
                "authors": [
                    {
                        "name": "Sieun Hyeon"
                    },
                    {
                        "name": "Kyudan Jung"
                    },
                    {
                        "name": "Jaehee Won"
                    },
                    {
                        "name": "Nam-Joon Kim"
                    },
                    {
                        "name": "Hyun Gon Ryu"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Jaeyoung Do"
                    }
                ],
                "author_detail": {
                    "name": "Jaeyoung Do"
                },
                "author": "Jaeyoung Do",
                "arxiv_comment": "Accepted in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15634v1",
                "updated": "2024-12-20T07:50:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    50,
                    8,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:50:08Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    50,
                    8,
                    4,
                    355,
                    0
                ],
                "title": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language\n  Model"
                },
                "summary": "Large language models (LLMs) have been widely applied in various practical\napplications, typically comprising billions of parameters, with inference\nprocesses requiring substantial energy and computational resources. In\ncontrast, the human brain, employing bio-plausible spiking mechanisms, can\naccomplish the same tasks while significantly reducing energy consumption, even\nwith a similar number of parameters. Based on this, several pioneering\nresearchers have proposed and implemented various large language models that\nleverage spiking neural networks. They have demonstrated the feasibility of\nthese models, validated their performance, and open-sourced their frameworks\nand partial source code. To accelerate the adoption of brain-inspired large\nlanguage models and facilitate secondary development for researchers, we are\nreleasing a software toolkit named DarwinKit (Darkit). The toolkit is designed\nspecifically for learners, researchers, and developers working on spiking large\nmodels, offering a suite of highly user-friendly features that greatly simplify\nthe learning, deployment, and development processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied in various practical\napplications, typically comprising billions of parameters, with inference\nprocesses requiring substantial energy and computational resources. In\ncontrast, the human brain, employing bio-plausible spiking mechanisms, can\naccomplish the same tasks while significantly reducing energy consumption, even\nwith a similar number of parameters. Based on this, several pioneering\nresearchers have proposed and implemented various large language models that\nleverage spiking neural networks. They have demonstrated the feasibility of\nthese models, validated their performance, and open-sourced their frameworks\nand partial source code. To accelerate the adoption of brain-inspired large\nlanguage models and facilitate secondary development for researchers, we are\nreleasing a software toolkit named DarwinKit (Darkit). The toolkit is designed\nspecifically for learners, researchers, and developers working on spiking large\nmodels, offering a suite of highly user-friendly features that greatly simplify\nthe learning, deployment, and development processes."
                },
                "authors": [
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Shifan Ye"
                    },
                    {
                        "name": "Qian Zheng"
                    },
                    {
                        "name": "Yangfan Hu"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Shunyu Qi"
                    },
                    {
                        "name": "Shuyang Chen"
                    },
                    {
                        "name": "Huajin Tang"
                    },
                    {
                        "name": "Gang Pan"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19026v2",
                "updated": "2024-12-20T07:37:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    37,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-29T12:12:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    12,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants\n  with Relaxing Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants\n  with Relaxing Constraints"
                },
                "summary": "Recent advances in large language model assistants have made them\nindispensable, raising significant concerns over managing their safety.\nAutomated red teaming offers a promising alternative to the labor-intensive and\nerror-prone manual probing for vulnerabilities, providing more consistent and\nscalable safety evaluations. However, existing approaches often compromise\ndiversity by focusing on maximizing attack success rate. Additionally, methods\nthat decrease the cosine similarity from historical embeddings with semantic\ndiversity rewards lead to novelty stagnation as history grows. To address these\nissues, we introduce DiveR-CT, which relaxes conventional constraints on the\nobjective and semantic reward, granting greater freedom for the policy to\nenhance diversity. Our experiments demonstrate DiveR-CT's marked superiority\nover baselines by 1) generating data that perform better in various diversity\nmetrics across different attack success rate levels, 2) better-enhancing\nresiliency in blue team models through safety tuning based on collected data,\n3) allowing dynamic control of objective weights for reliable and controllable\nattack success rates, and 4) reducing susceptibility to reward\noveroptimization. Overall, our method provides an effective and efficient\napproach to LLM red teaming, accelerating real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model assistants have made them\nindispensable, raising significant concerns over managing their safety.\nAutomated red teaming offers a promising alternative to the labor-intensive and\nerror-prone manual probing for vulnerabilities, providing more consistent and\nscalable safety evaluations. However, existing approaches often compromise\ndiversity by focusing on maximizing attack success rate. Additionally, methods\nthat decrease the cosine similarity from historical embeddings with semantic\ndiversity rewards lead to novelty stagnation as history grows. To address these\nissues, we introduce DiveR-CT, which relaxes conventional constraints on the\nobjective and semantic reward, granting greater freedom for the policy to\nenhance diversity. Our experiments demonstrate DiveR-CT's marked superiority\nover baselines by 1) generating data that perform better in various diversity\nmetrics across different attack success rate levels, 2) better-enhancing\nresiliency in blue team models through safety tuning based on collected data,\n3) allowing dynamic control of objective weights for reliable and controllable\nattack success rates, and 4) reducing susceptibility to reward\noveroptimization. Overall, our method provides an effective and efficient\napproach to LLM red teaming, accelerating real-world deployment."
                },
                "authors": [
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Quentin Xu"
                    },
                    {
                        "name": "Matthieu Lin"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Yong-jin Liu"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15628v1",
                "updated": "2024-12-20T07:35:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    35,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:35:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    35,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Can Input Attributions Interpret the Inductive Reasoning Process\n  Elicited in In-Context Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Input Attributions Interpret the Inductive Reasoning Process\n  Elicited in In-Context Learning?"
                },
                "summary": "Elucidating the rationale behind neural models' outputs has been challenging\nin the machine learning field, which is indeed applicable in this age of large\nlanguage models (LLMs) and in-context learning (ICL). When it comes to\nestimating input attributions (IA), ICL poses a new issue of interpreting which\nexample in the prompt, consisting of a set of examples, contributed to\nidentifying the task/rule to be solved. To this end, in this paper, we\nintroduce synthetic diagnostic tasks inspired by the poverty of the stimulus\ndesign in inductive reasoning; here, most in-context examples are ambiguous\nw.r.t. their underlying rule, and one critical example disambiguates the task\ndemonstrated. The question is whether conventional IA methods can identify such\nan example in interpreting the inductive reasoning process in ICL. Our\nexperiments provide several practical findings; for example, a certain simple\nIA method works the best, and the larger the model, the generally harder it is\nto interpret the ICL with gradient-based IA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elucidating the rationale behind neural models' outputs has been challenging\nin the machine learning field, which is indeed applicable in this age of large\nlanguage models (LLMs) and in-context learning (ICL). When it comes to\nestimating input attributions (IA), ICL poses a new issue of interpreting which\nexample in the prompt, consisting of a set of examples, contributed to\nidentifying the task/rule to be solved. To this end, in this paper, we\nintroduce synthetic diagnostic tasks inspired by the poverty of the stimulus\ndesign in inductive reasoning; here, most in-context examples are ambiguous\nw.r.t. their underlying rule, and one critical example disambiguates the task\ndemonstrated. The question is whether conventional IA methods can identify such\nan example in interpreting the inductive reasoning process in ICL. Our\nexperiments provide several practical findings; for example, a certain simple\nIA method works the best, and the larger the model, the generally harder it is\nto interpret the ICL with gradient-based IA methods."
                },
                "authors": [
                    {
                        "name": "Mengyu Ye"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Goro Kobayashi"
                    },
                    {
                        "name": "Jun Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Jun Suzuki"
                },
                "author": "Jun Suzuki",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15623v1",
                "updated": "2024-12-20T07:29:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    29,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:29:10Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    29,
                    10,
                    4,
                    355,
                    0
                ],
                "title": "JailPO: A Novel Black-box Jailbreak Framework via Preference\n  Optimization against Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailPO: A Novel Black-box Jailbreak Framework via Preference\n  Optimization against Aligned LLMs"
                },
                "summary": "Large Language Models (LLMs) aligned with human feedback have recently\ngarnered significant attention. However, it remains vulnerable to jailbreak\nattacks, where adversaries manipulate prompts to induce harmful outputs.\nExploring jailbreak attacks enables us to investigate the vulnerabilities of\nLLMs and further guides us in enhancing their security. Unfortunately, existing\ntechniques mainly rely on handcrafted templates or generated-based\noptimization, posing challenges in scalability, efficiency and universality. To\naddress these issues, we present JailPO, a novel black-box jailbreak framework\nto examine LLM alignment. For scalability and universality, JailPO meticulously\ntrains attack models to automatically generate covert jailbreak prompts.\nFurthermore, we introduce a preference optimization-based attack method to\nenhance the jailbreak effectiveness, thereby improving efficiency. To analyze\nmodel vulnerabilities, we provide three flexible jailbreak patterns. Extensive\nexperiments demonstrate that JailPO not only automates the attack process while\nmaintaining effectiveness but also exhibits superior performance in efficiency,\nuniversality, and robustness against defenses compared to baselines.\nAdditionally, our analysis of the three JailPO patterns reveals that attacks\nbased on complex templates exhibit higher attack strength, whereas covert\nquestion transformations elicit riskier responses and are more likely to bypass\ndefense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) aligned with human feedback have recently\ngarnered significant attention. However, it remains vulnerable to jailbreak\nattacks, where adversaries manipulate prompts to induce harmful outputs.\nExploring jailbreak attacks enables us to investigate the vulnerabilities of\nLLMs and further guides us in enhancing their security. Unfortunately, existing\ntechniques mainly rely on handcrafted templates or generated-based\noptimization, posing challenges in scalability, efficiency and universality. To\naddress these issues, we present JailPO, a novel black-box jailbreak framework\nto examine LLM alignment. For scalability and universality, JailPO meticulously\ntrains attack models to automatically generate covert jailbreak prompts.\nFurthermore, we introduce a preference optimization-based attack method to\nenhance the jailbreak effectiveness, thereby improving efficiency. To analyze\nmodel vulnerabilities, we provide three flexible jailbreak patterns. Extensive\nexperiments demonstrate that JailPO not only automates the attack process while\nmaintaining effectiveness but also exhibits superior performance in efficiency,\nuniversality, and robustness against defenses compared to baselines.\nAdditionally, our analysis of the three JailPO patterns reveals that attacks\nbased on complex templates exhibit higher attack strength, whereas covert\nquestion transformations elicit riskier responses and are more likely to bypass\ndefense mechanisms."
                },
                "authors": [
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Jiawei Ye"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Tianjie Yan"
                    },
                    {
                        "name": "Chu Wang"
                    },
                    {
                        "name": "Zhixin Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Li"
                },
                "author": "Zhixin Li",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15622v1",
                "updated": "2024-12-20T07:28:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    28,
                    4,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:28:04Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    28,
                    4,
                    4,
                    355,
                    0
                ],
                "title": "TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch"
                },
                "summary": "Large Automatic Speech Recognition (ASR) models demand a vast number of\nparameters, copious amounts of data, and significant computational resources\nduring the training process. However, such models can merely be deployed on\nhigh-compute cloud platforms and are only capable of performing speech\nrecognition tasks. This leads to high costs and restricted capabilities. In\nthis report, we initially propose the elastic mixture of the expert (eMoE)\nmodel. This model can be trained just once and then be elastically scaled in\naccordance with deployment requirements. Secondly, we devise an unsupervised\ndata creation and validation procedure and gather millions of hours of audio\ndata from diverse domains for training. Using these two techniques, our system\nachieves elastic deployment capabilities while reducing the Character Error\nRate (CER) on the SpeechIO testsets from 4.98\\% to 2.45\\%. Thirdly, our model\nis not only competent in Mandarin speech recognition but also proficient in\nmultilingual, multi-dialect, emotion, gender, and sound event perception. We\nrefer to this as Automatic Speech Perception (ASP), and the perception results\nare presented in the experimental section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Automatic Speech Recognition (ASR) models demand a vast number of\nparameters, copious amounts of data, and significant computational resources\nduring the training process. However, such models can merely be deployed on\nhigh-compute cloud platforms and are only capable of performing speech\nrecognition tasks. This leads to high costs and restricted capabilities. In\nthis report, we initially propose the elastic mixture of the expert (eMoE)\nmodel. This model can be trained just once and then be elastically scaled in\naccordance with deployment requirements. Secondly, we devise an unsupervised\ndata creation and validation procedure and gather millions of hours of audio\ndata from diverse domains for training. Using these two techniques, our system\nachieves elastic deployment capabilities while reducing the Character Error\nRate (CER) on the SpeechIO testsets from 4.98\\% to 2.45\\%. Thirdly, our model\nis not only competent in Mandarin speech recognition but also proficient in\nmultilingual, multi-dialect, emotion, gender, and sound event perception. We\nrefer to this as Automatic Speech Perception (ASP), and the perception results\nare presented in the experimental section."
                },
                "authors": [
                    {
                        "name": "Xingchen Song"
                    },
                    {
                        "name": "Chengdong Liang"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Pengshen Zhang"
                    },
                    {
                        "name": "ZiYu Wang"
                    },
                    {
                        "name": "Youcheng Ma"
                    },
                    {
                        "name": "Menglong Xu"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Fuping Pan"
                    },
                    {
                        "name": "Dinghao Zhou"
                    },
                    {
                        "name": "Zhendong Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Peng"
                },
                "author": "Zhendong Peng",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15606v1",
                "updated": "2024-12-20T07:00:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T07:00:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage"
                },
                "summary": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v3",
                "updated": "2024-12-20T06:57:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    57,
                    52,
                    4,
                    355,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15594v1",
                "updated": "2024-12-20T06:34:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    34,
                    57,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:34:57Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    34,
                    57,
                    4,
                    355,
                    0
                ],
                "title": "Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem\n  Generation"
                },
                "summary": "Solving tabular math word problems (TMWPs) has become a critical role in\nevaluating the mathematical reasoning ability of large language models (LLMs),\nwhere large-scale TMWP samples are commonly required for LLM fine-tuning. Since\nthe collection of high-quality TMWP datasets is costly and time-consuming,\nrecent research has concentrated on automatic TMWP generation. However, current\ngenerated samples usually suffer from issues of either correctness or\ndiversity. In this paper, we propose a Template-driven LLM-paraphrased (TeLL)\nframework for generating high-quality TMWP samples with diverse backgrounds and\naccurate tables, questions, answers, and solutions. To this end, we first\nextract templates from existing real samples to generate initial problems,\nensuring correctness. Then, we adopt an LLM to extend templates and paraphrase\nproblems, obtaining diverse TMWP samples. Furthermore, we find the reasoning\nannotation is important for solving TMWPs. Therefore, we propose to enrich each\nsolution with illustrative reasoning steps. Through the proposed framework, we\nconstruct a high-quality dataset TabMWP-TeLL by adhering to the question types\nin the TabMWP dataset, and we conduct extensive experiments on a variety of\nLLMs to demonstrate the effectiveness of TabMWP-TeLL in improving TMWP solving\nperformance. The code and data of this paper are available at:\nhttps://github.com/Jason8Kang/TELL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving tabular math word problems (TMWPs) has become a critical role in\nevaluating the mathematical reasoning ability of large language models (LLMs),\nwhere large-scale TMWP samples are commonly required for LLM fine-tuning. Since\nthe collection of high-quality TMWP datasets is costly and time-consuming,\nrecent research has concentrated on automatic TMWP generation. However, current\ngenerated samples usually suffer from issues of either correctness or\ndiversity. In this paper, we propose a Template-driven LLM-paraphrased (TeLL)\nframework for generating high-quality TMWP samples with diverse backgrounds and\naccurate tables, questions, answers, and solutions. To this end, we first\nextract templates from existing real samples to generate initial problems,\nensuring correctness. Then, we adopt an LLM to extend templates and paraphrase\nproblems, obtaining diverse TMWP samples. Furthermore, we find the reasoning\nannotation is important for solving TMWPs. Therefore, we propose to enrich each\nsolution with illustrative reasoning steps. Through the proposed framework, we\nconstruct a high-quality dataset TabMWP-TeLL by adhering to the question types\nin the TabMWP dataset, and we conduct extensive experiments on a variety of\nLLMs to demonstrate the effectiveness of TabMWP-TeLL in improving TMWP solving\nperformance. The code and data of this paper are available at:\nhttps://github.com/Jason8Kang/TELL."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Kang"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Xiaobo Jin"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Kaizhu Huang"
                    },
                    {
                        "name": "Qiufeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiufeng Wang"
                },
                "author": "Qiufeng Wang",
                "arxiv_comment": "Accepted at AAAI 2025, extended version with appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10144v3",
                "updated": "2024-12-20T06:14:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    14,
                    53,
                    4,
                    355,
                    0
                ],
                "published": "2023-08-20T03:03:34Z",
                "published_parsed": [
                    2023,
                    8,
                    20,
                    3,
                    3,
                    34,
                    6,
                    232,
                    0
                ],
                "title": "ExpeL: LLM Agents Are Experiential Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpeL: LLM Agents Are Experiential Learners"
                },
                "summary": "The recent surge in research interest in applying large language models\n(LLMs) to decision-making tasks has flourished by leveraging the extensive\nworld knowledge embedded in LLMs. While there is a growing demand to tailor\nLLMs for custom decision-making tasks, finetuning them for specific tasks is\nresource-intensive and may diminish the model's generalization capabilities.\nMoreover, state-of-the-art language models like GPT-4 and Claude are primarily\naccessible through API calls, with their parametric weights remaining\nproprietary and unavailable to the public. This scenario emphasizes the growing\nneed for new methodologies that allow learning from agent experiences without\nrequiring parametric updates. To address these problems, we introduce the\nExperiential Learning (ExpeL) agent. Our agent autonomously gathers experiences\nand extracts knowledge using natural language from a collection of training\ntasks. At inference, the agent recalls its extracted insights and past\nexperiences to make informed decisions. Our empirical results highlight the\nrobust learning efficacy of the ExpeL agent, indicating a consistent\nenhancement in its performance as it accumulates experiences. We further\nexplore the emerging capabilities and transfer learning potential of the ExpeL\nagent through qualitative observations and additional experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in research interest in applying large language models\n(LLMs) to decision-making tasks has flourished by leveraging the extensive\nworld knowledge embedded in LLMs. While there is a growing demand to tailor\nLLMs for custom decision-making tasks, finetuning them for specific tasks is\nresource-intensive and may diminish the model's generalization capabilities.\nMoreover, state-of-the-art language models like GPT-4 and Claude are primarily\naccessible through API calls, with their parametric weights remaining\nproprietary and unavailable to the public. This scenario emphasizes the growing\nneed for new methodologies that allow learning from agent experiences without\nrequiring parametric updates. To address these problems, we introduce the\nExperiential Learning (ExpeL) agent. Our agent autonomously gathers experiences\nand extracts knowledge using natural language from a collection of training\ntasks. At inference, the agent recalls its extracted insights and past\nexperiences to make informed decisions. Our empirical results highlight the\nrobust learning efficacy of the ExpeL agent, indicating a consistent\nenhancement in its performance as it accumulates experiences. We further\nexplore the emerging capabilities and transfer learning potential of the ExpeL\nagent through qualitative observations and additional experiments."
                },
                "authors": [
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Daniel Huang"
                    },
                    {
                        "name": "Quentin Xu"
                    },
                    {
                        "name": "Matthieu Lin"
                    },
                    {
                        "name": "Yong-Jin Liu"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "Accepted by the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04834v3",
                "updated": "2024-12-20T06:01:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    1,
                    33,
                    4,
                    355,
                    0
                ],
                "published": "2024-04-07T07:05:40Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    7,
                    5,
                    40,
                    6,
                    98,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems for Software Engineering: Literature\n  Review, Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems for Software Engineering: Literature\n  Review, Vision and the Road Ahead"
                },
                "summary": "Integrating Large Language Models (LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\nthat are competitive with human planning and reasoning. This paper explores the\ntransformative potential of integrating Large Language Models into Multi-Agent\n(LMA) systems for addressing complex challenges in software engineering (SE).\nBy leveraging the collaborative and specialized abilities of multiple agents,\nLMA systems enable autonomous problem-solving, improve robustness, and provide\nscalable solutions for managing the complexity of real-world software projects.\nIn this paper, we conduct a systematic review of recent primary studies to map\nthe current landscape of LMA applications across various stages of the software\ndevelopment lifecycle (SDLC). To illustrate current capabilities and\nlimitations, we perform two case studies to demonstrate the effectiveness of\nstate-of-the-art LMA frameworks. Additionally, we identify critical research\ngaps and propose a comprehensive research agenda focused on enhancing\nindividual agent capabilities and optimizing agent synergy. Our work outlines a\nforward-looking vision for developing fully autonomous, scalable, and\ntrustworthy LMA systems, laying the foundation for the evolution of Software\nEngineering 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\nthat are competitive with human planning and reasoning. This paper explores the\ntransformative potential of integrating Large Language Models into Multi-Agent\n(LMA) systems for addressing complex challenges in software engineering (SE).\nBy leveraging the collaborative and specialized abilities of multiple agents,\nLMA systems enable autonomous problem-solving, improve robustness, and provide\nscalable solutions for managing the complexity of real-world software projects.\nIn this paper, we conduct a systematic review of recent primary studies to map\nthe current landscape of LMA applications across various stages of the software\ndevelopment lifecycle (SDLC). To illustrate current capabilities and\nlimitations, we perform two case studies to demonstrate the effectiveness of\nstate-of-the-art LMA frameworks. Additionally, we identify critical research\ngaps and propose a comprehensive research agenda focused on enhancing\nindividual agent capabilities and optimizing agent synergy. Our work outlines a\nforward-looking vision for developing fully autonomous, scalable, and\ntrustworthy LMA systems, laying the foundation for the evolution of Software\nEngineering 2.0."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "TOSEM 2030 Special Issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15588v1",
                "updated": "2024-12-20T05:48:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    48,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T05:48:58Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    48,
                    58,
                    4,
                    355,
                    0
                ],
                "title": "NeSyCoCo: A Neuro-Symbolic Concept Composer for Compositional\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeSyCoCo: A Neuro-Symbolic Concept Composer for Compositional\n  Generalization"
                },
                "summary": "Compositional generalization is crucial for artificial intelligence agents to\nsolve complex vision-language reasoning tasks. Neuro-symbolic approaches have\ndemonstrated promise in capturing compositional structures, but they face\ncritical challenges: (a) reliance on predefined predicates for symbolic\nrepresentations that limit adaptability, (b) difficulty in extracting\npredicates from raw data, and (c) using non-differentiable operations for\ncombining primitive concepts. To address these issues, we propose NeSyCoCo, a\nneuro-symbolic framework that leverages large language models (LLMs) to\ngenerate symbolic representations and map them to differentiable neural\ncomputations. NeSyCoCo introduces three innovations: (a) augmenting natural\nlanguage inputs with dependency structures to enhance the alignment with\nsymbolic representations, (b) employing distributed word representations to\nlink diverse, linguistically motivated logical predicates to neural modules,\nand (c) using the soft composition of normalized predicate scores to align\nsymbolic and differentiable reasoning. Our framework achieves state-of-the-art\nresults on the ReaSCAN and CLEVR-CoGenT compositional generalization benchmarks\nand demonstrates robust performance with novel concepts in the CLEVR-SYN\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional generalization is crucial for artificial intelligence agents to\nsolve complex vision-language reasoning tasks. Neuro-symbolic approaches have\ndemonstrated promise in capturing compositional structures, but they face\ncritical challenges: (a) reliance on predefined predicates for symbolic\nrepresentations that limit adaptability, (b) difficulty in extracting\npredicates from raw data, and (c) using non-differentiable operations for\ncombining primitive concepts. To address these issues, we propose NeSyCoCo, a\nneuro-symbolic framework that leverages large language models (LLMs) to\ngenerate symbolic representations and map them to differentiable neural\ncomputations. NeSyCoCo introduces three innovations: (a) augmenting natural\nlanguage inputs with dependency structures to enhance the alignment with\nsymbolic representations, (b) employing distributed word representations to\nlink diverse, linguistically motivated logical predicates to neural modules,\nand (c) using the soft composition of normalized predicate scores to align\nsymbolic and differentiable reasoning. Our framework achieves state-of-the-art\nresults on the ReaSCAN and CLEVR-CoGenT compositional generalization benchmarks\nand demonstrates robust performance with novel concepts in the CLEVR-SYN\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Danial Kamali"
                    },
                    {
                        "name": "Elham J. Barezi"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "arxiv_comment": "AAAI 2025 Project Page:\n  https://iamdanialkamali.github.io/publication/neuro-symbolic-concept-composer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13102v3",
                "updated": "2024-12-20T05:42:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    42,
                    38,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-17T17:15:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    15,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark"
                },
                "summary": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench."
                },
                "authors": [
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Hao Liao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "31 pages, 6 figures; Update Table 4 and Figure 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15584v1",
                "updated": "2024-12-20T05:40:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    40,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T05:40:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    40,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models"
                },
                "summary": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nincorrect reliance decisions in certain contexts, demonstrating poor\ncalibration. Based on our findings, we discuss implications for designing\neffective reliance interventions in human-LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nincorrect reliance decisions in certain contexts, demonstrating poor\ncalibration. Based on our findings, we discuss implications for designing\neffective reliance interventions in human-LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Sophia Wan"
                    },
                    {
                        "name": "Ashton Anderson"
                    }
                ],
                "author_detail": {
                    "name": "Ashton Anderson"
                },
                "author": "Ashton Anderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20695v2",
                "updated": "2024-12-20T05:38:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    38,
                    58,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-28T02:55:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    2,
                    55,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "Combining Domain-Specific Models and LLMs for Automated Disease\n  Phenotyping from Survey Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Domain-Specific Models and LLMs for Automated Disease\n  Phenotyping from Survey Data"
                },
                "summary": "This exploratory pilot study investigated the potential of combining a\ndomain-specific model, BERN2, with large language models (LLMs) to enhance\nautomated disease phenotyping from research survey data. Motivated by the need\nfor efficient and accurate methods to harmonize the growing volume of survey\ndata with standardized disease ontologies, we employed BERN2, a biomedical\nnamed entity recognition and normalization model, to extract disease\ninformation from the ORIGINS birth cohort survey data. After rigorously\nevaluating BERN2's performance against a manually curated ground truth dataset,\nwe integrated various LLMs using prompt engineering, Retrieval-Augmented\nGeneration (RAG), and Instructional Fine-Tuning (IFT) to refine the model's\noutputs. BERN2 demonstrated high performance in extracting and normalizing\ndisease mentions, and the integration of LLMs, particularly with Few Shot\nInference and RAG orchestration, further improved accuracy. This approach,\nespecially when incorporating structured examples, logical reasoning prompts,\nand detailed context, offers a promising avenue for developing tools to enable\nefficient cohort profiling and data harmonization across large, heterogeneous\nresearch datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This exploratory pilot study investigated the potential of combining a\ndomain-specific model, BERN2, with large language models (LLMs) to enhance\nautomated disease phenotyping from research survey data. Motivated by the need\nfor efficient and accurate methods to harmonize the growing volume of survey\ndata with standardized disease ontologies, we employed BERN2, a biomedical\nnamed entity recognition and normalization model, to extract disease\ninformation from the ORIGINS birth cohort survey data. After rigorously\nevaluating BERN2's performance against a manually curated ground truth dataset,\nwe integrated various LLMs using prompt engineering, Retrieval-Augmented\nGeneration (RAG), and Instructional Fine-Tuning (IFT) to refine the model's\noutputs. BERN2 demonstrated high performance in extracting and normalizing\ndisease mentions, and the integration of LLMs, particularly with Few Shot\nInference and RAG orchestration, further improved accuracy. This approach,\nespecially when incorporating structured examples, logical reasoning prompts,\nand detailed context, offers a promising avenue for developing tools to enable\nefficient cohort profiling and data harmonization across large, heterogeneous\nresearch datasets."
                },
                "authors": [
                    {
                        "name": "Gal Beeri"
                    },
                    {
                        "name": "Benoit Chamot"
                    },
                    {
                        "name": "Elena Latchem"
                    },
                    {
                        "name": "Shruthi Venkatesh"
                    },
                    {
                        "name": "Sarah Whalan"
                    },
                    {
                        "name": "Van Zyl Kruger"
                    },
                    {
                        "name": "David Martino"
                    }
                ],
                "author_detail": {
                    "name": "David Martino"
                },
                "author": "David Martino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17512v2",
                "updated": "2024-12-20T05:24:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    24,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-23T02:32:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    32,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Towards the Scalable Fabrication of thin-film Superconducting Parametric\n  Amplifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Scalable Fabrication of thin-film Superconducting Parametric\n  Amplifiers"
                },
                "summary": "Kinetic inductance travelling-wave parametric amplifiers (KTWPAs) are\nemerging as core components in many applications where wideband cryogenic rf\namplification at or near the quantum limit of added noise is critical. These\nthin film superconducting devices are unique in their ability to simultaneously\nprovide large dynamic range and quantum-limited amplification of single photon\nto 100,000s of photon signals. Despite the promising performance of co-planar\nNbTiN thin-film KTWPAs, the original promise of a \"simple\" single-layer metal\nfabrication has encountered roadblocks and their broader adoption has been\nhindered by low fabrication yield. In this work, we present a post-lithography\ncorrection technique that eliminates short circuits, significantly improving\nyield and enabling reliable wafer-scale production. Using automated image\nacquisition, error analysis, and correction, we successfully fabricated\noperational KTWPAs on high-resistivity silicon, achieving > 10 dB between 2 - 4\nGHz. This approach paves the way for the scaling up manufacturing of KTWPAs,\npositioning them for widespread deployment in quantum technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinetic inductance travelling-wave parametric amplifiers (KTWPAs) are\nemerging as core components in many applications where wideband cryogenic rf\namplification at or near the quantum limit of added noise is critical. These\nthin film superconducting devices are unique in their ability to simultaneously\nprovide large dynamic range and quantum-limited amplification of single photon\nto 100,000s of photon signals. Despite the promising performance of co-planar\nNbTiN thin-film KTWPAs, the original promise of a \"simple\" single-layer metal\nfabrication has encountered roadblocks and their broader adoption has been\nhindered by low fabrication yield. In this work, we present a post-lithography\ncorrection technique that eliminates short circuits, significantly improving\nyield and enabling reliable wafer-scale production. Using automated image\nacquisition, error analysis, and correction, we successfully fabricated\noperational KTWPAs on high-resistivity silicon, achieving > 10 dB between 2 - 4\nGHz. This approach paves the way for the scaling up manufacturing of KTWPAs,\npositioning them for widespread deployment in quantum technologies."
                },
                "authors": [
                    {
                        "name": "Abdallah El Kass"
                    },
                    {
                        "name": "Kevin A. F. Simoes"
                    },
                    {
                        "name": "Cassandra Chua"
                    },
                    {
                        "name": "David J. Reilly"
                    },
                    {
                        "name": "Kun Zuo"
                    },
                    {
                        "name": "Thomas A. Ohki"
                    }
                ],
                "author_detail": {
                    "name": "Thomas A. Ohki"
                },
                "author": "Thomas A. Ohki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21131v2",
                "updated": "2024-12-20T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    20,
                    50,
                    4,
                    355,
                    0
                ],
                "published": "2024-10-28T15:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments"
                },
                "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."
                },
                "authors": [
                    {
                        "name": "Marharyta Domnich"
                    },
                    {
                        "name": "Julius Valja"
                    },
                    {
                        "name": "Rasmus Moorits Veski"
                    },
                    {
                        "name": "Giacomo Magnifico"
                    },
                    {
                        "name": "Kadi Tulver"
                    },
                    {
                        "name": "Eduard Barbu"
                    },
                    {
                        "name": "Raul Vicente"
                    }
                ],
                "author_detail": {
                    "name": "Raul Vicente"
                },
                "author": "Raul Vicente",
                "arxiv_comment": "This paper extends the AAAI-2025 version by including the Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15574v1",
                "updated": "2024-12-20T05:11:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    11,
                    51,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T05:11:51Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    11,
                    51,
                    4,
                    355,
                    0
                ],
                "title": "J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM"
                },
                "summary": "Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made\navailable the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image\narchive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive\nserves as a valuable resource for researchers and scholars interested in\ndeep-sea imagery. The dataset comprises images and videos of deep-sea\nphenomena, predominantly of marine organisms, but also of the seafloor and\nphysical processes. In this study, we propose J-EDI QA, a benchmark for\nunderstanding images of deep-sea organisms using a multimodal large language\nmodel (LLM). The benchmark is comprised of 100 images, accompanied by questions\nand answers with four options by JAMSTEC researchers for each image. The QA\npairs are provided in Japanese, and the benchmark assesses the ability to\nunderstand deep-sea species in Japanese. In the evaluation presented in this\npaper, OpenAI o1 achieved a 50% correct response rate. This result indicates\nthat even with the capabilities of state-of-the-art models as of December 2024,\ndeep-sea species comprehension is not yet at an expert level. Further advances\nin deep-sea species-specific LLMs are therefore required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made\navailable the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image\narchive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive\nserves as a valuable resource for researchers and scholars interested in\ndeep-sea imagery. The dataset comprises images and videos of deep-sea\nphenomena, predominantly of marine organisms, but also of the seafloor and\nphysical processes. In this study, we propose J-EDI QA, a benchmark for\nunderstanding images of deep-sea organisms using a multimodal large language\nmodel (LLM). The benchmark is comprised of 100 images, accompanied by questions\nand answers with four options by JAMSTEC researchers for each image. The QA\npairs are provided in Japanese, and the benchmark assesses the ability to\nunderstand deep-sea species in Japanese. In the evaluation presented in this\npaper, OpenAI o1 achieved a 50% correct response rate. This result indicates\nthat even with the capabilities of state-of-the-art models as of December 2024,\ndeep-sea species comprehension is not yet at an expert level. Further advances\nin deep-sea species-specific LLMs are therefore required."
                },
                "authors": [
                    {
                        "name": "Takero Yoshida"
                    },
                    {
                        "name": "Yuikazu Ito"
                    },
                    {
                        "name": "Yoshihiro Fujiwara"
                    },
                    {
                        "name": "Shinji Tsuchida"
                    },
                    {
                        "name": "Daisuke Sugiyama"
                    },
                    {
                        "name": "Daisuke Matsuoka"
                    }
                ],
                "author_detail": {
                    "name": "Daisuke Matsuoka"
                },
                "author": "Daisuke Matsuoka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13331v2",
                "updated": "2024-12-20T04:59:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    59,
                    10,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-19T08:29:54Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    8,
                    29,
                    54,
                    2,
                    171,
                    0
                ],
                "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Zero-shot LLM Re-Ranker with Risk Minimization"
                },
                "summary": "In the Retrieval-Augmented Generation (RAG) system, advanced Large Language\nModels (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an\nunsupervised way, which re-rank documents based on the probability of\ngenerating the query given the content of a document. However, directly\nprompting LLMs to approximate QLMs inherently is biased, where the estimated\ndistribution might diverge from the actual document-specific distribution. In\nthis study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages\nBayesian decision theory to both quantify and mitigate this estimation bias.\nSpecifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the\nprobability of document generation, thereby harmonizing the optimization of\nquery and document generation probabilities under a unified risk minimization\nobjective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly\nenhances re-ranking, particularly in improving the Top-1 accuracy. It benefits\nthe QA tasks by achieving higher accuracy with fewer input documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Retrieval-Augmented Generation (RAG) system, advanced Large Language\nModels (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an\nunsupervised way, which re-rank documents based on the probability of\ngenerating the query given the content of a document. However, directly\nprompting LLMs to approximate QLMs inherently is biased, where the estimated\ndistribution might diverge from the actual document-specific distribution. In\nthis study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages\nBayesian decision theory to both quantify and mitigate this estimation bias.\nSpecifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the\nprobability of document generation, thereby harmonizing the optimization of\nquery and document generation probabilities under a unified risk minimization\nobjective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly\nenhances re-ranking, particularly in improving the Top-1 accuracy. It benefits\nthe QA tasks by achieving higher accuracy with fewer input documents."
                },
                "authors": [
                    {
                        "name": "Xiaowei Yuan"
                    },
                    {
                        "name": "Zhao Yang"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15563v1",
                "updated": "2024-12-20T04:44:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    44,
                    41,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T04:44:41Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    44,
                    41,
                    4,
                    355,
                    0
                ],
                "title": "In-context Continual Learning Assisted by an External Continual Learner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Continual Learning Assisted by an External Continual Learner"
                },
                "summary": "Existing continual learning (CL) methods mainly rely on fine-tuning or\nadapting large language models (LLMs). They still suffer from catastrophic\nforgetting (CF). Little work has been done to exploit in-context learning (ICL)\nto leverage the extensive knowledge within LLMs for CL without updating any\nparameters. However, incrementally learning each new task in ICL necessitates\nadding training examples from each class of the task to the prompt, which\nhampers scalability as the prompt length increases. This issue not only leads\nto excessively long prompts that exceed the input token limit of the underlying\nLLM but also degrades the model's performance due to the overextended context.\nTo address this, we introduce InCA, a novel approach that integrates an\nexternal continual learner (ECL) with ICL to enable scalable CL without CF. The\nECL is built incrementally to pre-select a small subset of likely classes for\neach test instance. By restricting the ICL prompt to only these selected\nclasses, InCA prevents prompt lengths from becoming excessively long, while\nmaintaining high performance. Experimental results demonstrate that InCA\nsignificantly outperforms existing CL baselines, achieving substantial\nperformance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing continual learning (CL) methods mainly rely on fine-tuning or\nadapting large language models (LLMs). They still suffer from catastrophic\nforgetting (CF). Little work has been done to exploit in-context learning (ICL)\nto leverage the extensive knowledge within LLMs for CL without updating any\nparameters. However, incrementally learning each new task in ICL necessitates\nadding training examples from each class of the task to the prompt, which\nhampers scalability as the prompt length increases. This issue not only leads\nto excessively long prompts that exceed the input token limit of the underlying\nLLM but also degrades the model's performance due to the overextended context.\nTo address this, we introduce InCA, a novel approach that integrates an\nexternal continual learner (ECL) with ICL to enable scalable CL without CF. The\nECL is built incrementally to pre-select a small subset of likely classes for\neach test instance. By restricting the ICL prompt to only these selected\nclasses, InCA prevents prompt lengths from becoming excessively long, while\nmaintaining high performance. Experimental results demonstrate that InCA\nsignificantly outperforms existing CL baselines, achieving substantial\nperformance gains."
                },
                "authors": [
                    {
                        "name": "Saleh Momeni"
                    },
                    {
                        "name": "Sahisnu Mazumder"
                    },
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Bing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Liu"
                },
                "author": "Bing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15557v1",
                "updated": "2024-12-20T04:31:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    31,
                    3,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T04:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    31,
                    3,
                    4,
                    355,
                    0
                ],
                "title": "MORTAR: Metamorphic Multi-turn Testing for LLM-based Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MORTAR: Metamorphic Multi-turn Testing for LLM-based Dialogue Systems"
                },
                "summary": "With the widespread application of LLM-based dialogue systems in daily life,\nquality assurance has become more important than ever. Recent research has\nsuccessfully introduced methods to identify unexpected behaviour in single-turn\nscenarios. However, multi-turn dialogue testing remains underexplored, with the\nOracle problem in multi-turn testing posing a persistent challenge for dialogue\nsystem developers and researchers. In this paper, we propose MORTAR, a\nMetamORphic multi-TuRn diAlogue testing appRoach, which mitigates the test\noracle problem in the assessment of LLM-based dialogue systems. MORTAR\nautomates the generation of follow-up question-answer (QA) dialogue test cases\nwith multiple dialogue-level perturbations and metamorphic relations. MORTAR\nemploys a novel knowledge graph-based dialogue information model which\neffectively generates perturbed dialogue test datasets and detects bugs of\nmulti-turn dialogue systems in a low-cost manner. The proposed approach does\nnot require an LLM as a judge, eliminating potential of any biases in the\nevaluation step. According to the experiment results on multiple LLM-based\ndialogue systems and comparisons with single-turn metamorphic testing\napproaches, MORTAR explores more unique bugs in LLM-based dialogue systems,\nespecially for severe bugs that MORTAR detects up to four times more unique\nbugs than the most effective existing metamorphic testing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of LLM-based dialogue systems in daily life,\nquality assurance has become more important than ever. Recent research has\nsuccessfully introduced methods to identify unexpected behaviour in single-turn\nscenarios. However, multi-turn dialogue testing remains underexplored, with the\nOracle problem in multi-turn testing posing a persistent challenge for dialogue\nsystem developers and researchers. In this paper, we propose MORTAR, a\nMetamORphic multi-TuRn diAlogue testing appRoach, which mitigates the test\noracle problem in the assessment of LLM-based dialogue systems. MORTAR\nautomates the generation of follow-up question-answer (QA) dialogue test cases\nwith multiple dialogue-level perturbations and metamorphic relations. MORTAR\nemploys a novel knowledge graph-based dialogue information model which\neffectively generates perturbed dialogue test datasets and detects bugs of\nmulti-turn dialogue systems in a low-cost manner. The proposed approach does\nnot require an LLM as a judge, eliminating potential of any biases in the\nevaluation step. According to the experiment results on multiple LLM-based\ndialogue systems and comparisons with single-turn metamorphic testing\napproaches, MORTAR explores more unique bugs in LLM-based dialogue systems,\nespecially for severe bugs that MORTAR detects up to four times more unique\nbugs than the most effective existing metamorphic testing approach."
                },
                "authors": [
                    {
                        "name": "Guoxiang Guo"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Neelofar Neelofar"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    }
                ],
                "author_detail": {
                    "name": "Chakkrit Tantithamthavorn"
                },
                "author": "Chakkrit Tantithamthavorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15547v1",
                "updated": "2024-12-20T04:13:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    13,
                    46,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T04:13:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    13,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "NGQA: A Nutritional Graph Question Answering Benchmark for Personalized\n  Health-aware Nutritional Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NGQA: A Nutritional Graph Question Answering Benchmark for Personalized\n  Health-aware Nutritional Reasoning"
                },
                "summary": "Diet plays a critical role in human health, yet tailoring dietary reasoning\nto individual health conditions remains a major challenge. Nutrition Question\nAnswering (QA) has emerged as a popular method for addressing this problem.\nHowever, current research faces two critical limitations. On one hand, the\nabsence of datasets involving user-specific medical information severely limits\n\\textit{personalization}. This challenge is further compounded by the wide\nvariability in individual health needs. On the other hand, while large language\nmodels (LLMs), a popular solution for this task, demonstrate strong reasoning\nabilities, they struggle with the domain-specific complexities of personalized\nhealthy dietary reasoning, and existing benchmarks fail to capture these\nchallenges. To address these gaps, we introduce the Nutritional Graph Question\nAnswering (NGQA) benchmark, the first graph question answering dataset designed\nfor personalized nutritional health reasoning. NGQA leverages data from the\nNational Health and Nutrition Examination Survey (NHANES) and the Food and\nNutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is\nhealthy for a specific user, supported by explanations of the key contributing\nnutrients. The benchmark incorporates three question complexity settings and\nevaluates reasoning across three downstream tasks. Extensive experiments with\nLLM backbones and baseline models demonstrate that the NGQA benchmark\neffectively challenges existing models. In sum, NGQA addresses a critical\nreal-world problem while advancing GraphQA research with a novel\ndomain-specific benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diet plays a critical role in human health, yet tailoring dietary reasoning\nto individual health conditions remains a major challenge. Nutrition Question\nAnswering (QA) has emerged as a popular method for addressing this problem.\nHowever, current research faces two critical limitations. On one hand, the\nabsence of datasets involving user-specific medical information severely limits\n\\textit{personalization}. This challenge is further compounded by the wide\nvariability in individual health needs. On the other hand, while large language\nmodels (LLMs), a popular solution for this task, demonstrate strong reasoning\nabilities, they struggle with the domain-specific complexities of personalized\nhealthy dietary reasoning, and existing benchmarks fail to capture these\nchallenges. To address these gaps, we introduce the Nutritional Graph Question\nAnswering (NGQA) benchmark, the first graph question answering dataset designed\nfor personalized nutritional health reasoning. NGQA leverages data from the\nNational Health and Nutrition Examination Survey (NHANES) and the Food and\nNutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is\nhealthy for a specific user, supported by explanations of the key contributing\nnutrients. The benchmark incorporates three question complexity settings and\nevaluates reasoning across three downstream tasks. Extensive experiments with\nLLM backbones and baseline models demonstrate that the NGQA benchmark\neffectively challenges existing models. In sum, NGQA addresses a critical\nreal-world problem while advancing GraphQA research with a novel\ndomain-specific benchmark."
                },
                "authors": [
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Nhi Ha Lan Le"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Vincent Galassi"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Nitesh V Chawla"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15540v1",
                "updated": "2024-12-20T03:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    58,
                    27,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T03:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    58,
                    27,
                    4,
                    355,
                    0
                ],
                "title": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question\n  Answering"
                },
                "summary": "Understanding temporal relations and answering time-sensitive questions is\ncrucial yet a challenging task for question-answering systems powered by large\nlanguage models (LLMs). Existing approaches either update the parametric\nknowledge of LLMs with new facts, which is resource-intensive and often\nimpractical, or integrate LLMs with external knowledge retrieval (i.e.,\nretrieval-augmented generation). However, off-the-shelf retrievers often\nstruggle to identify relevant documents that require intensive temporal\nreasoning. To systematically study time-sensitive question answering, we\nintroduce the TempRAGEval benchmark, which repurposes existing datasets by\nincorporating temporal perturbations and gold evidence labels. As anticipated,\nall existing retrieval methods struggle with these temporal reasoning-intensive\nquestions. We further propose Modular Retrieval (MRAG), a trainless framework\nthat includes three modules: (1) Question Processing that decomposes question\ninto a main content and a temporal constraint; (2) Retrieval and Summarization\nthat retrieves evidence and uses LLMs to summarize according to the main\ncontent; (3) Semantic-Temporal Hybrid Ranking that scores each evidence\nsummarization based on both semantic and temporal relevance. On TempRAGEval,\nMRAG significantly outperforms baseline retrievers in retrieval performance,\nleading to further improvements in final answer accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding temporal relations and answering time-sensitive questions is\ncrucial yet a challenging task for question-answering systems powered by large\nlanguage models (LLMs). Existing approaches either update the parametric\nknowledge of LLMs with new facts, which is resource-intensive and often\nimpractical, or integrate LLMs with external knowledge retrieval (i.e.,\nretrieval-augmented generation). However, off-the-shelf retrievers often\nstruggle to identify relevant documents that require intensive temporal\nreasoning. To systematically study time-sensitive question answering, we\nintroduce the TempRAGEval benchmark, which repurposes existing datasets by\nincorporating temporal perturbations and gold evidence labels. As anticipated,\nall existing retrieval methods struggle with these temporal reasoning-intensive\nquestions. We further propose Modular Retrieval (MRAG), a trainless framework\nthat includes three modules: (1) Question Processing that decomposes question\ninto a main content and a temporal constraint; (2) Retrieval and Summarization\nthat retrieves evidence and uses LLMs to summarize according to the main\ncontent; (3) Semantic-Temporal Hybrid Ranking that scores each evidence\nsummarization based on both semantic and temporal relevance. On TempRAGEval,\nMRAG significantly outperforms baseline retrievers in retrieval performance,\nleading to further improvements in final answer accuracy."
                },
                "authors": [
                    {
                        "name": "Zhang Siyue"
                    },
                    {
                        "name": "Xue Yuxiang"
                    },
                    {
                        "name": "Zhang Yiming"
                    },
                    {
                        "name": "Wu Xiaobao"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    },
                    {
                        "name": "Zhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Chen"
                },
                "author": "Zhao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15529v1",
                "updated": "2024-12-20T03:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    37,
                    7,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T03:37:07Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    37,
                    7,
                    4,
                    355,
                    0
                ],
                "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current.We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. Given the escalating\ncomplexity of RAG systems, we underscore the necessity of identifying potential\nfailure points of RAG modules. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in the engineering of RAG modules. Subsequently, we proffer bespoke\nsolutions that are designed to augment the validation processes and bolster the\noverall performance of these modules. Our work thoroughly evaluates the\nperformance of core advanced components in RAG systems, providing insights into\noptimizations for prevalent failure points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current.We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. Given the escalating\ncomplexity of RAG systems, we underscore the necessity of identifying potential\nfailure points of RAG modules. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in the engineering of RAG modules. Subsequently, we proffer bespoke\nsolutions that are designed to augment the validation processes and bolster the\noverall performance of these modules. Our work thoroughly evaluates the\nperformance of core advanced components in RAG systems, providing insights into\noptimizations for prevalent failure points."
                },
                "authors": [
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Yangyifei Luo"
                    },
                    {
                        "name": "Jinlong Zhang"
                    },
                    {
                        "name": "Hanwen Hao"
                    },
                    {
                        "name": "Zhilong Cao"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Xiao Guan"
                    },
                    {
                        "name": "Zhenting Huang"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Zhentao Han"
                    },
                    {
                        "name": "Qili Zhang"
                    },
                    {
                        "name": "Siyuan Tao"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Zhixing Tan"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xudong Liu"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15524v1",
                "updated": "2024-12-20T03:26:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    26,
                    47,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T03:26:47Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    26,
                    47,
                    4,
                    355,
                    0
                ],
                "title": "HREF: Human Response-Guided Evaluation of Instruction Following in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HREF: Human Response-Guided Evaluation of Instruction Following in\n  Language Models"
                },
                "summary": "Evaluating the capability of Large Language Models (LLMs) in following\ninstructions has heavily relied on a powerful LLM as the judge, introducing\nunresolved biases that deviate the judgments from human judges. In this work,\nwe reevaluate various choices for automatic evaluation on a wide range of\ninstruction-following tasks. We experiment with methods that leverage\nhuman-written responses and observe that they enhance the reliability of\nautomatic evaluations across a wide range of tasks, resulting in up to a 3.2%\nimprovement in agreement with human judges. We also discovered that\nhuman-written responses offer an orthogonal perspective to model-generated\nresponses in following instructions and should be used as an additional context\nwhen comparing model responses. Based on these observations, we develop a new\nevaluation benchmark, Human Response-Guided Evaluation of Instruction Following\n(HREF), comprising 4,258 samples across 11 task categories with a composite\nevaluation setup, employing a composite evaluation setup that selects the most\nreliable method for each category. In addition to providing reliable\nevaluation, HREF emphasizes individual task performance and is free from\ncontamination. Finally, we study the impact of key design choices in HREF,\nincluding the size of the evaluation set, the judge model, the baseline model,\nand the prompt template. We host a live leaderboard that evaluates LLMs on the\nprivate evaluation set of HREF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the capability of Large Language Models (LLMs) in following\ninstructions has heavily relied on a powerful LLM as the judge, introducing\nunresolved biases that deviate the judgments from human judges. In this work,\nwe reevaluate various choices for automatic evaluation on a wide range of\ninstruction-following tasks. We experiment with methods that leverage\nhuman-written responses and observe that they enhance the reliability of\nautomatic evaluations across a wide range of tasks, resulting in up to a 3.2%\nimprovement in agreement with human judges. We also discovered that\nhuman-written responses offer an orthogonal perspective to model-generated\nresponses in following instructions and should be used as an additional context\nwhen comparing model responses. Based on these observations, we develop a new\nevaluation benchmark, Human Response-Guided Evaluation of Instruction Following\n(HREF), comprising 4,258 samples across 11 task categories with a composite\nevaluation setup, employing a composite evaluation setup that selects the most\nreliable method for each category. In addition to providing reliable\nevaluation, HREF emphasizes individual task performance and is free from\ncontamination. Finally, we study the impact of key design choices in HREF,\nincluding the size of the evaluation set, the judge model, the baseline model,\nand the prompt template. We host a live leaderboard that evaluates LLMs on the\nprivate evaluation set of HREF."
                },
                "authors": [
                    {
                        "name": "Xinxi Lyu"
                    },
                    {
                        "name": "Yizhong Wang"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "name": "Pradeep Dasigi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Dasigi"
                },
                "author": "Pradeep Dasigi",
                "arxiv_comment": "28 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15519v1",
                "updated": "2024-12-20T03:15:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    15,
                    2,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T03:15:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    15,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time"
                },
                "summary": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks."
                },
                "authors": [
                    {
                        "name": "Alireza Pourali"
                    },
                    {
                        "name": "Arian Boukani"
                    },
                    {
                        "name": "Hamzeh Khazaei"
                    }
                ],
                "author_detail": {
                    "name": "Hamzeh Khazaei"
                },
                "author": "Hamzeh Khazaei",
                "arxiv_comment": "11 pages, Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07618v2",
                "updated": "2024-12-20T03:12:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    12,
                    49,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-10T15:56:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs"
                },
                "summary": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git"
                },
                "authors": [
                    {
                        "name": "Xiaqiang Tang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Sihong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Sihong Xie"
                },
                "author": "Sihong Xie",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02041v2",
                "updated": "2024-12-20T03:10:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    10,
                    55,
                    4,
                    355,
                    0
                ],
                "published": "2024-01-04T02:56:50Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    2,
                    56,
                    50,
                    3,
                    4,
                    0
                ],
                "title": "Towards Efficient Object Re-Identification with A Novel Cloud-Edge\n  Collaborative Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Object Re-Identification with A Novel Cloud-Edge\n  Collaborative Framework"
                },
                "summary": "Object re-identification (ReID) is committed to searching for objects of the\nsame identity across cameras, and its real-world deployment is gradually\nincreasing. Current ReID methods assume that the deployed system follows the\ncentralized processing paradigm, i.e., all computations are conducted in the\ncloud server and edge devices are only used to capture images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources in the cloud server. Therefore, the ReID\nsystem should be converted to fit in the cloud-edge collaborative processing\nparadigm, which is crucial to boost its scalability and practicality. However,\ncurrent works lack relevant research on this important specific issue, making\nit difficult to adapt them into a cloud-edge framework effectively. In this\npaper, we propose a cloud-edge collaborative inference framework for ReID\nsystems, aiming to expedite the return of the desired image captured by the\ncamera to the cloud server by learning the spatial-temporal correlations among\nobjects. In the system, a Distribution-aware Correlation Modeling network\n(DaCM) is particularly proposed to embed the spatial-temporal correlations of\nthe camera network implicitly into a graph structure, and it can be applied 1)\nin the cloud to regulate the size of the upload window and 2) on the edge\ndevice to adjust the sequence of images, respectively. Notably, the proposed\nDaCM can be seamlessly combined with traditional ReID methods, enabling their\napplication within our proposed edge-cloud collaborative framework. Extensive\nexperiments demonstrate that our method obviously reduces transmission overhead\nand significantly improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object re-identification (ReID) is committed to searching for objects of the\nsame identity across cameras, and its real-world deployment is gradually\nincreasing. Current ReID methods assume that the deployed system follows the\ncentralized processing paradigm, i.e., all computations are conducted in the\ncloud server and edge devices are only used to capture images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources in the cloud server. Therefore, the ReID\nsystem should be converted to fit in the cloud-edge collaborative processing\nparadigm, which is crucial to boost its scalability and practicality. However,\ncurrent works lack relevant research on this important specific issue, making\nit difficult to adapt them into a cloud-edge framework effectively. In this\npaper, we propose a cloud-edge collaborative inference framework for ReID\nsystems, aiming to expedite the return of the desired image captured by the\ncamera to the cloud server by learning the spatial-temporal correlations among\nobjects. In the system, a Distribution-aware Correlation Modeling network\n(DaCM) is particularly proposed to embed the spatial-temporal correlations of\nthe camera network implicitly into a graph structure, and it can be applied 1)\nin the cloud to regulate the size of the upload window and 2) on the edge\ndevice to adjust the sequence of images, respectively. Notably, the proposed\nDaCM can be seamlessly combined with traditional ReID methods, enabling their\napplication within our proposed edge-cloud collaborative framework. Extensive\nexperiments demonstrate that our method obviously reduces transmission overhead\nand significantly improves performance."
                },
                "authors": [
                    {
                        "name": "Chuanming Wang"
                    },
                    {
                        "name": "Yuxin Yang"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13254v3",
                "updated": "2024-12-20T03:10:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    10,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-05-21T23:48:26Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    23,
                    48,
                    26,
                    1,
                    142,
                    0
                ],
                "title": "System Safety Monitoring of Learned Components Using Temporal Metric\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System Safety Monitoring of Learned Components Using Temporal Metric\n  Forecasting"
                },
                "summary": "In learning-enabled autonomous systems, safety monitoring of learned\ncomponents is crucial to ensure their outputs do not lead to system safety\nviolations, given the operational context of the system. However, developing a\nsafety monitor for practical deployment in real-world applications is\nchallenging. This is due to limited access to internal workings and training\ndata of the learned component. Furthermore, safety monitors should predict\nsafety violations with low latency, while consuming a reasonable amount of\ncomputation. To address the challenges, we propose a safety monitoring method\nbased on probabilistic time series forecasting. Given the learned component\noutputs and an operational context, we empirically investigate different Deep\nLearning (DL)-based probabilistic forecasting to predict the objective measure\ncapturing the satisfaction or violation of a safety requirement (safety\nmetric). We empirically evaluate safety metric and violation prediction\naccuracy, and inference latency and resource usage of four state-of-the-art\nmodels, with varying horizons, using autonomous aviation and autonomous driving\ncase studies. Our results suggest that probabilistic forecasting of safety\nmetrics, given learned component outputs and scenarios, is effective for safety\nmonitoring. Furthermore, for both case studies, Temporal Fusion Transformer\n(TFT) was the most accurate model for predicting imminent safety violations,\nwith acceptable latency and resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In learning-enabled autonomous systems, safety monitoring of learned\ncomponents is crucial to ensure their outputs do not lead to system safety\nviolations, given the operational context of the system. However, developing a\nsafety monitor for practical deployment in real-world applications is\nchallenging. This is due to limited access to internal workings and training\ndata of the learned component. Furthermore, safety monitors should predict\nsafety violations with low latency, while consuming a reasonable amount of\ncomputation. To address the challenges, we propose a safety monitoring method\nbased on probabilistic time series forecasting. Given the learned component\noutputs and an operational context, we empirically investigate different Deep\nLearning (DL)-based probabilistic forecasting to predict the objective measure\ncapturing the satisfaction or violation of a safety requirement (safety\nmetric). We empirically evaluate safety metric and violation prediction\naccuracy, and inference latency and resource usage of four state-of-the-art\nmodels, with varying horizons, using autonomous aviation and autonomous driving\ncase studies. Our results suggest that probabilistic forecasting of safety\nmetrics, given learned component outputs and scenarios, is effective for safety\nmonitoring. Furthermore, for both case studies, Temporal Fusion Transformer\n(TFT) was the most accurate model for predicting imminent safety violations,\nwith acceptable latency and resource consumption."
                },
                "authors": [
                    {
                        "name": "Sepehr Sharifi"
                    },
                    {
                        "name": "Andrea Stocco"
                    },
                    {
                        "name": "Lionel C. Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel C. Briand"
                },
                "author": "Lionel C. Briand",
                "arxiv_comment": "Accepted for publication by ACM Transactions on Software Engineering\n  and Methodology (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14809v2",
                "updated": "2024-12-20T03:01:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    1,
                    6,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T12:57:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zeao Tu"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14843v2",
                "updated": "2024-12-20T02:59:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    59,
                    23,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T13:36:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    36,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas"
                },
                "summary": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training."
                },
                "authors": [
                    {
                        "name": "Pietro Bernardelle"
                    },
                    {
                        "name": "Leon Fröhling"
                    },
                    {
                        "name": "Stefano Civelli"
                    },
                    {
                        "name": "Riccardo Lunardi"
                    },
                    {
                        "name": "Kevin Roitero"
                    },
                    {
                        "name": "Gianluca Demartini"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Demartini"
                },
                "author": "Gianluca Demartini",
                "arxiv_comment": "4 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05438v2",
                "updated": "2024-12-20T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    52,
                    36,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-08T10:50:24Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    10,
                    50,
                    24,
                    5,
                    160,
                    0
                ],
                "title": "A Roadmap for Software Testing in Open Collaborative Development\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Roadmap for Software Testing in Open Collaborative Development\n  Environments"
                },
                "summary": "Amidst the ever-expanding digital sphere, the evolution of the Internet has\nnot only fostered an atmosphere of information transparency and sharing but has\nalso sparked a revolution in software development practices. The distributed\nnature of open collaborative development, along with its diverse contributors\nand rapid iterations, presents new challenges for ensuring software quality.\nThis paper offers a comprehensive review and analysis of recent advancements in\nsoftware quality assurance within open collaborative development environments.\nOur examination covers various aspects, including process management, personnel\ndynamics, and technological advancements, providing valuable insights into\neffective approaches for maintaining software quality in such collaborative\nsettings. Furthermore, we delve into the challenges and opportunities arising\nfrom emerging technologies such as LLMs and the AI model-centric development\nparadigm. By addressing these topics, our study contributes to a deeper\nunderstanding of software quality assurance in open collaborative environments\nand lays the groundwork for future exploration and innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst the ever-expanding digital sphere, the evolution of the Internet has\nnot only fostered an atmosphere of information transparency and sharing but has\nalso sparked a revolution in software development practices. The distributed\nnature of open collaborative development, along with its diverse contributors\nand rapid iterations, presents new challenges for ensuring software quality.\nThis paper offers a comprehensive review and analysis of recent advancements in\nsoftware quality assurance within open collaborative development environments.\nOur examination covers various aspects, including process management, personnel\ndynamics, and technological advancements, providing valuable insights into\neffective approaches for maintaining software quality in such collaborative\nsettings. Furthermore, we delve into the challenges and opportunities arising\nfrom emerging technologies such as LLMs and the AI model-centric development\nparadigm. By addressing these topics, our study contributes to a deeper\nunderstanding of software quality assurance in open collaborative environments\nand lays the groundwork for future exploration and innovation."
                },
                "authors": [
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Yawen Wang"
                    },
                    {
                        "name": "Zhe Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Liu"
                },
                "author": "Zhe Liu",
                "arxiv_comment": "accepted by TOSEM. arXiv admin note: text overlap with\n  arXiv:1906.10742 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13144v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13144v4",
                "updated": "2024-12-20T02:44:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    44,
                    19,
                    4,
                    355,
                    0
                ],
                "published": "2024-06-19T01:37:10Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    1,
                    37,
                    10,
                    2,
                    171,
                    0
                ],
                "title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversational Agents"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversational agents, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nagents often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, an agent is assigned the role of a character from\npopular TV shows, requiring it to respond to spontaneous questions using past\ndialogue information and to distinguish between known and unknown information.\nKey features of DialSim include assessing the agent's ability to respond within\na reasonable time limit, handling long-term multi-party dialogues, and\nevaluating performance under randomized questioning with LongDialQA, a novel,\nhigh-quality question-answering dataset. Our experiments using DialSim reveal\nthe strengths and weaknesses of the latest conversational agents, offering\nvaluable insights for future advancements in conversational AI. DialSim is\navailable at https://dialsim.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversational agents, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nagents often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, an agent is assigned the role of a character from\npopular TV shows, requiring it to respond to spontaneous questions using past\ndialogue information and to distinguish between known and unknown information.\nKey features of DialSim include assessing the agent's ability to respond within\na reasonable time limit, handling long-term multi-party dialogues, and\nevaluating performance under randomized questioning with LongDialQA, a novel,\nhigh-quality question-answering dataset. Our experiments using DialSim reveal\nthe strengths and weaknesses of the latest conversational agents, offering\nvaluable insights for future advancements in conversational AI. DialSim is\navailable at https://dialsim.github.io/."
                },
                "authors": [
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Woosog Chay"
                    },
                    {
                        "name": "Hyeonji Hwang"
                    },
                    {
                        "name": "Daeun Kyung"
                    },
                    {
                        "name": "Hyunseung Chung"
                    },
                    {
                        "name": "Eunbyeol Cho"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13144v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13144v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15504v1",
                "updated": "2024-12-20T02:35:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    35,
                    39,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T02:35:39Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    35,
                    39,
                    4,
                    355,
                    0
                ],
                "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework"
                },
                "summary": "Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA."
                },
                "authors": [
                    {
                        "name": "Zhenjie Xu"
                    },
                    {
                        "name": "Wenqing Chen"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Xuanying Li"
                    },
                    {
                        "name": "Cheng Hu"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Zhichao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Lu"
                },
                "arxiv_affiliation": "Department of Computer Science, City University of Hong Kong",
                "author": "Zhichao Lu",
                "arxiv_comment": "This work has been accepted at The 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]