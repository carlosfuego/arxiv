[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v1",
                "updated": "2024-08-16T15:11:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schr√∂der"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schr√∂der"
                },
                "author": "Lutz Schr√∂der",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Kouck√Ω"
                    },
                    {
                        "name": "Josef Matƒõjka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matƒõjka"
                },
                "author": "Josef Matƒõjka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.12599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12599v1",
                "updated": "2024-08-22T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "Controllable Text Generation for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Text Generation for Large Language Models: A Survey"
                },
                "summary": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "52 pages, 11 figures, 7 tables, 11 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13709v2",
                "updated": "2024-08-22T17:56:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    15,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-18T17:08:10Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    8,
                    10,
                    3,
                    200,
                    0
                ],
                "title": "Understanding Reference Policies in Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reference Policies in Direct Preference Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "GitHub Repo: https://github.com/yale-nlp/refdpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12590v1",
                "updated": "2024-08-22T17:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations"
                },
                "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models."
                },
                "authors": [
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Krithika Ramakrishnan"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Lifu Tu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Anas Awadalla"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Accepted by ECCV24 AI4VA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12588v1",
                "updated": "2024-08-22T17:54:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    54,
                    21,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:54:21Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    54,
                    21,
                    3,
                    235,
                    0
                ],
                "title": "Real-Time Video Generation with Pyramid Attention Broadcast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Video Generation with Pyramid Attention Broadcast"
                },
                "summary": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates superior results\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates superior results\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation."
                },
                "authors": [
                    {
                        "name": "Xuanlei Zhao"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12579v1",
                "updated": "2024-08-22T17:44:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment"
                },
                "summary": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians."
                },
                "authors": [
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12575v1",
                "updated": "2024-08-22T17:42:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    42,
                    16,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:42:16Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    42,
                    16,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view\n  Transformers"
                },
                "summary": "Current parking area perception algorithms primarily focus on detecting\nvacant slots within a limited range, relying on error-prone homographic\nprojection for both labeling and inference. However, recent advancements in\nAdvanced Driver Assistance System (ADAS) require interaction with end-users\nthrough comprehensive and intelligent Human-Machine Interfaces (HMIs). These\ninterfaces should present a complete perception of the parking area going from\ndistinguishing vacant slots' entry lines to the orientation of other parked\nvehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT\nF-CVT), which leverages features from a four-camera fisheye Surround-view\nCamera System (SVCS) with multihead attentions to create a detailed Bird-Eye\nView (BEV) grid feature map. Features are processed by both a segmentation\ndecoder and a Polygon-Yolo based object detection decoder for parking slots and\nvehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects\nwithin a 25m x 25m real open-road scenes with an average error of only 20 cm.\nOur larger model achieves an F-1 score of 0.89. Moreover the smaller model\noperates at 16 fps on an Nvidia Jetson Orin embedded board, with similar\ndetection results to the larger one. MT F-CVT demonstrates robust\ngeneralization capability across different vehicles and camera rig\nconfigurations. A demo video from an unseen vehicle and camera rig is available\nat: https://streamable.com/jjw54x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current parking area perception algorithms primarily focus on detecting\nvacant slots within a limited range, relying on error-prone homographic\nprojection for both labeling and inference. However, recent advancements in\nAdvanced Driver Assistance System (ADAS) require interaction with end-users\nthrough comprehensive and intelligent Human-Machine Interfaces (HMIs). These\ninterfaces should present a complete perception of the parking area going from\ndistinguishing vacant slots' entry lines to the orientation of other parked\nvehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT\nF-CVT), which leverages features from a four-camera fisheye Surround-view\nCamera System (SVCS) with multihead attentions to create a detailed Bird-Eye\nView (BEV) grid feature map. Features are processed by both a segmentation\ndecoder and a Polygon-Yolo based object detection decoder for parking slots and\nvehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects\nwithin a 25m x 25m real open-road scenes with an average error of only 20 cm.\nOur larger model achieves an F-1 score of 0.89. Moreover the smaller model\noperates at 16 fps on an Nvidia Jetson Orin embedded board, with similar\ndetection results to the larger one. MT F-CVT demonstrates robust\ngeneralization capability across different vehicles and camera rig\nconfigurations. A demo video from an unseen vehicle and camera rig is available\nat: https://streamable.com/jjw54x."
                },
                "authors": [
                    {
                        "name": "Antonyo Musabini"
                    },
                    {
                        "name": "Ivan Novikov"
                    },
                    {
                        "name": "Sana Soula"
                    },
                    {
                        "name": "Christel Leonet"
                    },
                    {
                        "name": "Lihao Wang"
                    },
                    {
                        "name": "Rachid Benmokhtar"
                    },
                    {
                        "name": "Fabian Burger"
                    },
                    {
                        "name": "Thomas Boulay"
                    },
                    {
                        "name": "Xavier Perrotton"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Perrotton"
                },
                "author": "Xavier Perrotton",
                "arxiv_comment": "26th Irish Machine Vision and Image Processing Conference,\n  Data-Driven Autonomy Workshop (matching camera-ready version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12574v1",
                "updated": "2024-08-22T17:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    41,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:41:45Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    41,
                    45,
                    3,
                    235,
                    0
                ],
                "title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind"
                },
                "summary": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM."
                },
                "authors": [
                    {
                        "name": "Haojun Shi"
                    },
                    {
                        "name": "Suyu Ye"
                    },
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Layla Isik"
                    },
                    {
                        "name": "Yen-Ling Kuo"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "Project website: https://scai.cs.jhu.edu/projects/MuMA-ToM/ Code:\n  https://github.com/SCAI-JHU/MuMA-ToM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12570v1",
                "updated": "2024-08-22T17:38:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    38,
                    59,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:38:59Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    38,
                    59,
                    3,
                    235,
                    0
                ],
                "title": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale"
                },
                "summary": "We present Jamba-1.5, new instruction-tuned large language models based on\nour Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts\narchitecture, providing high throughput and low memory usage across context\nlengths, while retaining the same or better quality as Transformer models. We\nrelease two model sizes: Jamba-1.5-Large, with 94B active parameters, and\nJamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a\nvariety of conversational and instruction-following capabilties, and have an\neffective context length of 256K tokens, the largest amongst open-weight\nmodels. To support cost-effective inference, we introduce ExpertsInt8, a novel\nquantization technique that allows fitting Jamba-1.5-Large on a machine with 8\n80GB GPUs when processing 256K-token contexts without loss of quality. When\nevaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models\nachieve excellent results while providing high throughput and outperforming\nother open-weight models on long-context benchmarks. The model weights for both\nsizes are publicly available under the Jamba Open Model License and we release\nExpertsInt8 as open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Jamba-1.5, new instruction-tuned large language models based on\nour Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts\narchitecture, providing high throughput and low memory usage across context\nlengths, while retaining the same or better quality as Transformer models. We\nrelease two model sizes: Jamba-1.5-Large, with 94B active parameters, and\nJamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a\nvariety of conversational and instruction-following capabilties, and have an\neffective context length of 256K tokens, the largest amongst open-weight\nmodels. To support cost-effective inference, we introduce ExpertsInt8, a novel\nquantization technique that allows fitting Jamba-1.5-Large on a machine with 8\n80GB GPUs when processing 256K-token contexts without loss of quality. When\nevaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models\nachieve excellent results while providing high throughput and outperforming\nother open-weight models on long-context benchmarks. The model weights for both\nsizes are publicly available under the Jamba Open Model License and we release\nExpertsInt8 as open source."
                },
                "authors": [
                    {
                        "name": "Jamba Team"
                    },
                    {
                        "name": "Barak Lenz"
                    },
                    {
                        "name": "Alan Arazi"
                    },
                    {
                        "name": "Amir Bergman"
                    },
                    {
                        "name": "Avshalom Manevich"
                    },
                    {
                        "name": "Barak Peleg"
                    },
                    {
                        "name": "Ben Aviram"
                    },
                    {
                        "name": "Chen Almagor"
                    },
                    {
                        "name": "Clara Fridman"
                    },
                    {
                        "name": "Dan Padnos"
                    },
                    {
                        "name": "Daniel Gissin"
                    },
                    {
                        "name": "Daniel Jannai"
                    },
                    {
                        "name": "Dor Muhlgay"
                    },
                    {
                        "name": "Dor Zimberg"
                    },
                    {
                        "name": "Edden M Gerber"
                    },
                    {
                        "name": "Elad Dolev"
                    },
                    {
                        "name": "Eran Krakovsky"
                    },
                    {
                        "name": "Erez Safahi"
                    },
                    {
                        "name": "Erez Schwartz"
                    },
                    {
                        "name": "Gal Cohen"
                    },
                    {
                        "name": "Gal Shachaf"
                    },
                    {
                        "name": "Haim Rozenblum"
                    },
                    {
                        "name": "Hofit Bata"
                    },
                    {
                        "name": "Ido Blass"
                    },
                    {
                        "name": "Inbal Magar"
                    },
                    {
                        "name": "Itay Dalmedigos"
                    },
                    {
                        "name": "Jhonathan Osin"
                    },
                    {
                        "name": "Julie Fadlon"
                    },
                    {
                        "name": "Maria Rozman"
                    },
                    {
                        "name": "Matan Danos"
                    },
                    {
                        "name": "Michael Gokhman"
                    },
                    {
                        "name": "Mor Zusman"
                    },
                    {
                        "name": "Naama Gidron"
                    },
                    {
                        "name": "Nir Ratner"
                    },
                    {
                        "name": "Noam Gat"
                    },
                    {
                        "name": "Noam Rozen"
                    },
                    {
                        "name": "Oded Fried"
                    },
                    {
                        "name": "Ohad Leshno"
                    },
                    {
                        "name": "Omer Antverg"
                    },
                    {
                        "name": "Omri Abend"
                    },
                    {
                        "name": "Opher Lieber"
                    },
                    {
                        "name": "Or Dagan"
                    },
                    {
                        "name": "Orit Cohavi"
                    },
                    {
                        "name": "Raz Alon"
                    },
                    {
                        "name": "Ro'i Belson"
                    },
                    {
                        "name": "Roi Cohen"
                    },
                    {
                        "name": "Rom Gilad"
                    },
                    {
                        "name": "Roman Glozman"
                    },
                    {
                        "name": "Shahar Lev"
                    },
                    {
                        "name": "Shaked Meirom"
                    },
                    {
                        "name": "Tal Delbari"
                    },
                    {
                        "name": "Tal Ness"
                    },
                    {
                        "name": "Tomer Asida"
                    },
                    {
                        "name": "Tom Ben Gal"
                    },
                    {
                        "name": "Tom Braude"
                    },
                    {
                        "name": "Uriya Pumerantz"
                    },
                    {
                        "name": "Yehoshua Cohen"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Yuval Globerson"
                    },
                    {
                        "name": "Yuval Peleg Levy"
                    },
                    {
                        "name": "Yoav Shoham"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Shoham"
                },
                "author": "Yoav Shoham",
                "arxiv_comment": "Webpage: https://www.ai21.com/jamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12569v1",
                "updated": "2024-08-22T17:37:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    37,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:37:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    37,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "Sapiens: Foundation for Human Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sapiens: Foundation for Human Vision Models"
                },
                "summary": "We present Sapiens, a family of models for four fundamental human-centric\nvision tasks - 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability - model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Sapiens, a family of models for four fundamental human-centric\nvision tasks - 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability - model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error."
                },
                "authors": [
                    {
                        "name": "Rawal Khirodkar"
                    },
                    {
                        "name": "Timur Bagautdinov"
                    },
                    {
                        "name": "Julieta Martinez"
                    },
                    {
                        "name": "Su Zhaoen"
                    },
                    {
                        "name": "Austin James"
                    },
                    {
                        "name": "Peter Selednik"
                    },
                    {
                        "name": "Stuart Anderson"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "ECCV 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12567v1",
                "updated": "2024-08-22T17:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    33,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    33,
                    50,
                    3,
                    235,
                    0
                ],
                "title": "FRB 20121102A monitoring: updated periodicity at L-band",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRB 20121102A monitoring: updated periodicity at L-band"
                },
                "summary": "FRB 20121102A was the first fast radio burst to be observed to repeat. Since\nthen, thousands of bursts have been detected by multiple radio telescopes\naround the world. Previous work has shown an indication of a cyclic activity\nlevel with a periodicity around 160 days. Knowing when the source repeats is\nessential for planning multi-wavelength monitoring to constrain their emission\nextend and progenitor source. We report the monitoring of FRB 20121102A using\nthe 100-m Effelsberg radio telescope at L-band and update the periodicity of\nthe cyclic activity-level. We use the Lomb-Scargle periodogram on a sample of\n272 observing epochs where 41% correspond to detections and 59% to\nnon-detections. Our dataset is composed of the 7 epochs of our monitoring plus\npublicly available data. We investigate two methods, i) binary model,\ndescribing the observing epochs with 1 if there are detections and with 0 for\nnon-detections. ii) normalised rates model: which considers the inferred\ndetections rates. We report no detections in 12.5-hour observations down to a\nfluence of 0.29 Jy ms. The best period found for the cyclic activity window is\n$159.3 \\pm 0.8$ days for the binary model and $159.3 \\pm 0.3$ days for the\nnormalised rates model. The activity phase is shown to be 53%. The normalised\nrates shows a clear Gaussian-like behaviour for the activity level, where the\nnumber of detections peak at the centre of the activity window. The periodicity\nfound through both methods is consistent for the L and S-band datasets implying\nit is intrinsic to the source. The activity phase in S-band however shows an\nindication of it ending before the L-band activity phase, supporting the idea\nof chromatic dependence of the activity window. The sample at C-band however is\nnot large enough to further confirm this result.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRB 20121102A was the first fast radio burst to be observed to repeat. Since\nthen, thousands of bursts have been detected by multiple radio telescopes\naround the world. Previous work has shown an indication of a cyclic activity\nlevel with a periodicity around 160 days. Knowing when the source repeats is\nessential for planning multi-wavelength monitoring to constrain their emission\nextend and progenitor source. We report the monitoring of FRB 20121102A using\nthe 100-m Effelsberg radio telescope at L-band and update the periodicity of\nthe cyclic activity-level. We use the Lomb-Scargle periodogram on a sample of\n272 observing epochs where 41% correspond to detections and 59% to\nnon-detections. Our dataset is composed of the 7 epochs of our monitoring plus\npublicly available data. We investigate two methods, i) binary model,\ndescribing the observing epochs with 1 if there are detections and with 0 for\nnon-detections. ii) normalised rates model: which considers the inferred\ndetections rates. We report no detections in 12.5-hour observations down to a\nfluence of 0.29 Jy ms. The best period found for the cyclic activity window is\n$159.3 \\pm 0.8$ days for the binary model and $159.3 \\pm 0.3$ days for the\nnormalised rates model. The activity phase is shown to be 53%. The normalised\nrates shows a clear Gaussian-like behaviour for the activity level, where the\nnumber of detections peak at the centre of the activity window. The periodicity\nfound through both methods is consistent for the L and S-band datasets implying\nit is intrinsic to the source. The activity phase in S-band however shows an\nindication of it ending before the L-band activity phase, supporting the idea\nof chromatic dependence of the activity window. The sample at C-band however is\nnot large enough to further confirm this result."
                },
                "authors": [
                    {
                        "name": "C. A. Braga"
                    },
                    {
                        "name": "M. Cruces"
                    },
                    {
                        "name": "T. Cassanelli"
                    },
                    {
                        "name": "M. C. Espinoza-Dupouy"
                    },
                    {
                        "name": "L. Rodriguez"
                    },
                    {
                        "name": "L. G. Spitler"
                    },
                    {
                        "name": "J. Vera-Casanova"
                    },
                    {
                        "name": "P. Limaye"
                    }
                ],
                "author_detail": {
                    "name": "P. Limaye"
                },
                "author": "P. Limaye",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12563v1",
                "updated": "2024-08-22T17:25:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    25,
                    59,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:25:59Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    25,
                    59,
                    3,
                    235,
                    0
                ],
                "title": "Climate Bistability at the Inner Edge of the Habitable Zone due to\n  Runaway Greenhouse and Cloud Feedbacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate Bistability at the Inner Edge of the Habitable Zone due to\n  Runaway Greenhouse and Cloud Feedbacks"
                },
                "summary": "Understanding the climate dynamics at the inner edge of the habitable zone\n(HZ) is crucial for predicting the habitability of rocky exoplanets. Previous\nstudies using Global Climate Models (GCMs) have indicated that planets\nreceiving high stellar flux can exhibit climate bifurcations, leading to\nbistability between a cold (temperate) and a hot (runaway) climate. However,\nthe mechanism causing this bistability has not been fully explained, in part\ndue to the difficulty associated with inferring mechanisms from small numbers\nof expensive numerical simulations in GCMs. In this study, we employ a\ntwo-column (dayside and nightside), two-layer climate model to investigate the\nphysical mechanisms driving this bistability. Through mechanism-denial\nexperiments, we demonstrate that the runaway greenhouse effect, coupled with a\ncloud feedback on either the dayside or nightside, leads to climate\nbistability. We also map out the parameters that control the location of the\nbifurcations and size of the bistability. This work identifies which mechanisms\nand GCM parameters control the stellar flux at which rocky planets are likely\nto retain a hot, thick atmosphere if they experience a hot start. This is\ncritical for the prioritization of targets and interpretation of observations\nby the James Webb Space Telescope (JWST). Furthermore, our modeling framework\ncan be extended to planets with different condensable species and cloud types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the climate dynamics at the inner edge of the habitable zone\n(HZ) is crucial for predicting the habitability of rocky exoplanets. Previous\nstudies using Global Climate Models (GCMs) have indicated that planets\nreceiving high stellar flux can exhibit climate bifurcations, leading to\nbistability between a cold (temperate) and a hot (runaway) climate. However,\nthe mechanism causing this bistability has not been fully explained, in part\ndue to the difficulty associated with inferring mechanisms from small numbers\nof expensive numerical simulations in GCMs. In this study, we employ a\ntwo-column (dayside and nightside), two-layer climate model to investigate the\nphysical mechanisms driving this bistability. Through mechanism-denial\nexperiments, we demonstrate that the runaway greenhouse effect, coupled with a\ncloud feedback on either the dayside or nightside, leads to climate\nbistability. We also map out the parameters that control the location of the\nbifurcations and size of the bistability. This work identifies which mechanisms\nand GCM parameters control the stellar flux at which rocky planets are likely\nto retain a hot, thick atmosphere if they experience a hot start. This is\ncritical for the prioritization of targets and interpretation of observations\nby the James Webb Space Telescope (JWST). Furthermore, our modeling framework\ncan be extended to planets with different condensable species and cloud types."
                },
                "authors": [
                    {
                        "name": "Bowen Fan"
                    },
                    {
                        "name": "Da Yang"
                    },
                    {
                        "name": "Dorian S. Abbot"
                    }
                ],
                "author_detail": {
                    "name": "Dorian S. Abbot"
                },
                "author": "Dorian S. Abbot",
                "arxiv_comment": "4 figures, submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08924v2",
                "updated": "2024-08-22T17:21:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    21,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-15T14:51:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    51,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Weiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhang"
                },
                "author": "Weiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03647v2",
                "updated": "2024-08-22T17:10:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    10,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-07T09:15:47Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    15,
                    47,
                    2,
                    220,
                    0
                ],
                "title": "Real-time Event Recognition of Long-distance Distributed Vibration\n  Sensing with Knowledge Distillation and Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Event Recognition of Long-distance Distributed Vibration\n  Sensing with Knowledge Distillation and Hardware Acceleration"
                },
                "summary": "Fiber-optic sensing, especially distributed optical fiber vibration (DVS)\nsensing, is gaining importance in internet of things (IoT) applications, such\nas industrial safety monitoring and intrusion detection. Despite their wide\napplication, existing post-processing methods that rely on deep learning models\nfor event recognition in DVS systems face challenges with real-time processing\nof large sample data volumes, particularly in long-distance applications. To\naddress this issue, we propose to use a four-layer convolutional neural network\n(CNN) with ResNet as the teacher model for knowledge distillation. This results\nin a significant improvement in accuracy, from 83.41% to 95.39%, on data from\npreviously untrained environments. Additionally, we propose a novel hardware\ndesign based on field-programmable gate arrays (FPGA) to further accelerate\nmodel inference. This design replaces multiplication with binary shift\noperations and quantizes model weights, enabling high parallelism and low\nlatency. Our implementation achieves an inference time of 0.083 ms for a\nspatial-temporal sample covering a 12.5 m fiber length and 0.256 s time frame.\nThis performance enables real-time signal processing over approximately 38.55\nkm of fiber, about $2.14\\times$ the capability of an Nvidia GTX 4090 GPU. The\nproposed method greatly enhances the efficiency of vibration pattern\nrecognition, promoting the use of DVS as a smart IoT system. The data and code\nare available at https://github.com/HUST-IOF/Efficient-DVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiber-optic sensing, especially distributed optical fiber vibration (DVS)\nsensing, is gaining importance in internet of things (IoT) applications, such\nas industrial safety monitoring and intrusion detection. Despite their wide\napplication, existing post-processing methods that rely on deep learning models\nfor event recognition in DVS systems face challenges with real-time processing\nof large sample data volumes, particularly in long-distance applications. To\naddress this issue, we propose to use a four-layer convolutional neural network\n(CNN) with ResNet as the teacher model for knowledge distillation. This results\nin a significant improvement in accuracy, from 83.41% to 95.39%, on data from\npreviously untrained environments. Additionally, we propose a novel hardware\ndesign based on field-programmable gate arrays (FPGA) to further accelerate\nmodel inference. This design replaces multiplication with binary shift\noperations and quantizes model weights, enabling high parallelism and low\nlatency. Our implementation achieves an inference time of 0.083 ms for a\nspatial-temporal sample covering a 12.5 m fiber length and 0.256 s time frame.\nThis performance enables real-time signal processing over approximately 38.55\nkm of fiber, about $2.14\\times$ the capability of an Nvidia GTX 4090 GPU. The\nproposed method greatly enhances the efficiency of vibration pattern\nrecognition, promoting the use of DVS as a smart IoT system. The data and code\nare available at https://github.com/HUST-IOF/Efficient-DVS."
                },
                "authors": [
                    {
                        "name": "Zhongyao Luo"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Zhao Ge"
                    },
                    {
                        "name": "Ming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Tang"
                },
                "author": "Ming Tang",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v1",
                "updated": "2024-08-22T17:01:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12537v1",
                "updated": "2024-08-22T16:47:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    47,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T16:47:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    47,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Core formation by binary scouring and gravitational wave recoil in\n  massive elliptical galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Core formation by binary scouring and gravitational wave recoil in\n  massive elliptical galaxies"
                },
                "summary": "Scouring by supermassive black hole (SMBH) binaries is the most accepted\nmechanism for the formation of the cores seen in giant elliptical galaxies.\nHowever, an additional mechanism is required to explain the largest observed\ncores. Gravitational wave (GW) recoil is expected to trigger further growth of\nthe core, as subsequent heating from dynamical friction of the merged SMBH\nremoves stars from the central regions. We model core formation in massive\nelliptical galaxies from both binary scouring and heating by GW recoil and\nexamine their unique signatures. We aim to determine if the nature of cores in\n3D space density can be attributed uniquely to either process and if the\nmagnitude of the kick can be inferred. We perform $N$-body simulations of\ngalactic mergers of multicomponent galaxies, based on the observed parameters\nof four massive elliptical galaxies with cores $> 0.5$ kpc. After binary\nscouring and hardening, the merged SMBH remnant is given a range of GW recoil\nkicks with $0.5$-$0.9$ of the escape speed of the galaxy. We find that binary\nscouring alone can form the cores of NGC 1600 and A2147-BCG, which are $< 1.3$\nkpc in size. However, the $> 2$ kpc cores in NGC 6166 and A2261-BCG require\nheating from GW recoil kicks of $< 0.5$ of the galaxy escape speed. A unique\nfeature of GW recoil heating is flatter cores in surface brightness,\ncorresponding to truly flat cores in 3D space density. It also preferentially\nremoves stars on low angular momentum orbits from the galactic nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scouring by supermassive black hole (SMBH) binaries is the most accepted\nmechanism for the formation of the cores seen in giant elliptical galaxies.\nHowever, an additional mechanism is required to explain the largest observed\ncores. Gravitational wave (GW) recoil is expected to trigger further growth of\nthe core, as subsequent heating from dynamical friction of the merged SMBH\nremoves stars from the central regions. We model core formation in massive\nelliptical galaxies from both binary scouring and heating by GW recoil and\nexamine their unique signatures. We aim to determine if the nature of cores in\n3D space density can be attributed uniquely to either process and if the\nmagnitude of the kick can be inferred. We perform $N$-body simulations of\ngalactic mergers of multicomponent galaxies, based on the observed parameters\nof four massive elliptical galaxies with cores $> 0.5$ kpc. After binary\nscouring and hardening, the merged SMBH remnant is given a range of GW recoil\nkicks with $0.5$-$0.9$ of the escape speed of the galaxy. We find that binary\nscouring alone can form the cores of NGC 1600 and A2147-BCG, which are $< 1.3$\nkpc in size. However, the $> 2$ kpc cores in NGC 6166 and A2261-BCG require\nheating from GW recoil kicks of $< 0.5$ of the galaxy escape speed. A unique\nfeature of GW recoil heating is flatter cores in surface brightness,\ncorresponding to truly flat cores in 3D space density. It also preferentially\nremoves stars on low angular momentum orbits from the galactic nucleus."
                },
                "authors": [
                    {
                        "name": "Nader Khonji"
                    },
                    {
                        "name": "Alessia Gualandris"
                    },
                    {
                        "name": "Justin I. Read"
                    },
                    {
                        "name": "Walter Dehnen"
                    }
                ],
                "author_detail": {
                    "name": "Walter Dehnen"
                },
                "arxiv_affiliation": "Zentrum f√ºr Astronomie der Universit√§t Heidelberg, Germany",
                "author": "Walter Dehnen",
                "arxiv_comment": "19 pages, 13 figures, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12526v1",
                "updated": "2024-08-22T16:31:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    31,
                    32,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T16:31:32Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    31,
                    32,
                    3,
                    235,
                    0
                ],
                "title": "Exploiting Student Parallelism for Low-latency GPU Inference of\n  BERT-like Models in Online Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Student Parallelism for Low-latency GPU Inference of\n  BERT-like Models in Online Services"
                },
                "summary": "Due to high accuracy, BERT-like models have been widely adopted by\ndiscriminative text mining and web searching. However, large BERT-like models\nsuffer from inefficient online inference, as they face the following two\nproblems on GPUs. First, they rely on the large model depth to achieve high\naccuracy, which linearly increases the sequential computation on GPUs. Second,\nstochastic and dynamic online workloads cause extra costs. In this paper, we\npresent Academus for low-latency online inference of BERT-like models. At the\ncore of Academus is the novel student parallelism, which adopts boosting\nensemble and stacking distillation to distill the original deep model into an\nequivalent group of parallel and shallow student models. This enables Academus\nto achieve the lower model depth (e.g., two layers) than baselines and\nconsequently the lowest inference latency without affecting the accuracy.For\noccasional workload bursts, it can temporarily decrease the number of students\nwith minimal accuracy loss to improve throughput. Additionally, it employs\nspecialized system designs for student parallelism to better handle stochastic\nonline workloads. We conduct comprehensive experiments to verify the\neffectiveness. The results show that Academus outperforms the baselines by\n4.1X~1.6X in latency without compromising accuracy, and achieves up to 22.27X\nhigher throughput for workload bursts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to high accuracy, BERT-like models have been widely adopted by\ndiscriminative text mining and web searching. However, large BERT-like models\nsuffer from inefficient online inference, as they face the following two\nproblems on GPUs. First, they rely on the large model depth to achieve high\naccuracy, which linearly increases the sequential computation on GPUs. Second,\nstochastic and dynamic online workloads cause extra costs. In this paper, we\npresent Academus for low-latency online inference of BERT-like models. At the\ncore of Academus is the novel student parallelism, which adopts boosting\nensemble and stacking distillation to distill the original deep model into an\nequivalent group of parallel and shallow student models. This enables Academus\nto achieve the lower model depth (e.g., two layers) than baselines and\nconsequently the lowest inference latency without affecting the accuracy.For\noccasional workload bursts, it can temporarily decrease the number of students\nwith minimal accuracy loss to improve throughput. Additionally, it employs\nspecialized system designs for student parallelism to better handle stochastic\nonline workloads. We conduct comprehensive experiments to verify the\neffectiveness. The results show that Academus outperforms the baselines by\n4.1X~1.6X in latency without compromising accuracy, and achieves up to 22.27X\nhigher throughput for workload bursts."
                },
                "authors": [
                    {
                        "name": "Weiyan Wang"
                    },
                    {
                        "name": "Yilun Jin"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Victor Junqiu Wei"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12516v1",
                "updated": "2024-08-22T16:14:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    14,
                    7,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T16:14:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    14,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "Do we need wavelets in the late Universe?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we need wavelets in the late Universe?"
                },
                "summary": "We parameterize the Hubble function by adding Hermitian wavelets to the\nHubble radius of $\\Lambda$CDM. This allows us to build Hubble functions that\noscillate around $\\Lambda$CDM at late times without modifying its angular\ndiameter distance to last scattering. We perform parameter inference and model\nselection procedures on these new Hubble functions at the background level. In\nour analyses consisting of a wide variety of cosmological observations, we find\nthat baryon acoustic oscillations (BAO) data play a crucial role in determining\nthe constraints on the wavelet parameters. In particular, we focus on the\ndifferences between SDSS- and DESI-BAO datasets and find that wavelets provide\na better fit to the data when either of the BAO datasets is present. However,\nDESI-BAO has a preference for the center of the wavelets to be around $z \\sim\n0.7$, while SDSS-BAO prefers higher redshifts of $z > 1$. This difference\nappears to be driven by the discrepancies between these two datasets in their\n$D_H / r_{\\rm d}$ measurements at $z = 0.51$ and $z \\sim 2.3$. Finally, we also\nderive the consequences of the wavelets on a dark energy component. We find\nthat the dark energy density oscillates by construction and also attains\nnegative values at large redshifts ($z\\gtrsim2$) as a consequence of the\nSDSS-BAO data. We conclude that while the early universe and the constraints on\nthe matter density and the Hubble constant remain unchanged, wavelets are\nfavored in the late universe by the BAO data. Specifically, there is a\nsignificant improvement at more than $3\\sigma$ in the fit when new DESI-BAO\ndata are included in the analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We parameterize the Hubble function by adding Hermitian wavelets to the\nHubble radius of $\\Lambda$CDM. This allows us to build Hubble functions that\noscillate around $\\Lambda$CDM at late times without modifying its angular\ndiameter distance to last scattering. We perform parameter inference and model\nselection procedures on these new Hubble functions at the background level. In\nour analyses consisting of a wide variety of cosmological observations, we find\nthat baryon acoustic oscillations (BAO) data play a crucial role in determining\nthe constraints on the wavelet parameters. In particular, we focus on the\ndifferences between SDSS- and DESI-BAO datasets and find that wavelets provide\na better fit to the data when either of the BAO datasets is present. However,\nDESI-BAO has a preference for the center of the wavelets to be around $z \\sim\n0.7$, while SDSS-BAO prefers higher redshifts of $z > 1$. This difference\nappears to be driven by the discrepancies between these two datasets in their\n$D_H / r_{\\rm d}$ measurements at $z = 0.51$ and $z \\sim 2.3$. Finally, we also\nderive the consequences of the wavelets on a dark energy component. We find\nthat the dark energy density oscillates by construction and also attains\nnegative values at large redshifts ($z\\gtrsim2$) as a consequence of the\nSDSS-BAO data. We conclude that while the early universe and the constraints on\nthe matter density and the Hubble constant remain unchanged, wavelets are\nfavored in the late universe by the BAO data. Specifically, there is a\nsignificant improvement at more than $3\\sigma$ in the fit when new DESI-BAO\ndata are included in the analysis."
                },
                "authors": [
                    {
                        "name": "Luis A. Escamilla"
                    },
                    {
                        "name": "Emre √ñz√ºlker"
                    },
                    {
                        "name": "√ñzg√ºr Akarsu"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "J. A. V√°zquez"
                    }
                ],
                "author_detail": {
                    "name": "J. A. V√°zquez"
                },
                "author": "J. A. V√°zquez",
                "arxiv_comment": "20 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10259v3",
                "updated": "2024-08-22T15:52:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    52,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-16T03:26:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    26,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy"
                },
                "summary": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events."
                },
                "authors": [
                    {
                        "name": "Tunazzina Islam"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v5",
                "updated": "2024-08-22T15:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    44,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12496v1",
                "updated": "2024-08-22T15:41:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:41:58Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "title": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework"
                },
                "summary": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms."
                },
                "authors": [
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Wu Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Yuan"
                },
                "author": "Wu Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03143v2",
                "updated": "2024-08-22T15:38:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    38,
                    28,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-06T12:37:47Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    12,
                    37,
                    47,
                    1,
                    219,
                    0
                ],
                "title": "SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection"
                },
                "summary": "The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet ."
                },
                "authors": [
                    {
                        "name": "Bla≈æ Rolih"
                    },
                    {
                        "name": "Matic Fuƒçka"
                    },
                    {
                        "name": "Danijel Skoƒçaj"
                    }
                ],
                "author_detail": {
                    "name": "Danijel Skoƒçaj"
                },
                "author": "Danijel Skoƒçaj",
                "arxiv_comment": "Accepted to ICPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12494v1",
                "updated": "2024-08-22T15:35:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:35:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24."
                },
                "authors": [
                    {
                        "name": "Kunsheng Tang"
                    },
                    {
                        "name": "Wenbo Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Peigui Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11692v2",
                "updated": "2024-08-22T15:18:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    18,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-03-18T11:45:43Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    11,
                    45,
                    43,
                    0,
                    78,
                    0
                ],
                "title": "ARTEMIS emulator: exploring the effect of cosmology and galaxy formation\n  physics on Milky Way-mass haloes and their satellites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARTEMIS emulator: exploring the effect of cosmology and galaxy formation\n  physics on Milky Way-mass haloes and their satellites"
                },
                "summary": "We present the new ARTEMIS Emulator suite of high resolution (baryon mass of\n$2.23 \\times 10^{4}$ $h^{-1}$M$_{\\odot}$) zoom-in simulations of Milky Way mass\nsystems. Here, three haloes from the original ARTEMIS sample have been rerun\nmultiple times, systematically varying parameters for the stellar feedback\nmodel, the density threshold for star formation, the reionisation redshift and\nthe assumed warm dark matter (WDM) particle mass (assuming a thermal relic).\nFrom these simulations emulators are trained for a wide range of statistics\nthat allow for fast predictions at combinations of parameters not originally\nsampled, running in $\\sim 1$ms (a factor of $\\sim 10^{11}$ faster than the\nsimulations). In this paper we explore the dependence of the central haloes'\nstellar mass on the varied parameters, finding the stellar feedback parameters\nto be the most important. When constraining the parameters to match the\npresent-day stellar mass halo mass relation inferred from abundance matching we\nfind that there is a strong degeneracy in the stellar feedback parameters,\ncorresponding to a freedom in formation time of the stellar component for a\nfixed halo assembly history. We additionally explore the dependence of the\nsatellite stellar mass function, where it is found that variations in stellar\nfeedback, the reionisation redshift and the WDM mass all have a significant\neffect. The presented emulators are a powerful tool which allows for\nfundamentally new ways of analysing and interpreting cosmological hydrodynamic\nsimulations. Crucially, allowing their free (subgrid) parameters to be varied\nand marginalised, leading to more robust constraints and predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the new ARTEMIS Emulator suite of high resolution (baryon mass of\n$2.23 \\times 10^{4}$ $h^{-1}$M$_{\\odot}$) zoom-in simulations of Milky Way mass\nsystems. Here, three haloes from the original ARTEMIS sample have been rerun\nmultiple times, systematically varying parameters for the stellar feedback\nmodel, the density threshold for star formation, the reionisation redshift and\nthe assumed warm dark matter (WDM) particle mass (assuming a thermal relic).\nFrom these simulations emulators are trained for a wide range of statistics\nthat allow for fast predictions at combinations of parameters not originally\nsampled, running in $\\sim 1$ms (a factor of $\\sim 10^{11}$ faster than the\nsimulations). In this paper we explore the dependence of the central haloes'\nstellar mass on the varied parameters, finding the stellar feedback parameters\nto be the most important. When constraining the parameters to match the\npresent-day stellar mass halo mass relation inferred from abundance matching we\nfind that there is a strong degeneracy in the stellar feedback parameters,\ncorresponding to a freedom in formation time of the stellar component for a\nfixed halo assembly history. We additionally explore the dependence of the\nsatellite stellar mass function, where it is found that variations in stellar\nfeedback, the reionisation redshift and the WDM mass all have a significant\neffect. The presented emulators are a powerful tool which allows for\nfundamentally new ways of analysing and interpreting cosmological hydrodynamic\nsimulations. Crucially, allowing their free (subgrid) parameters to be varied\nand marginalised, leading to more robust constraints and predictions."
                },
                "authors": [
                    {
                        "name": "Shaun T. Brown"
                    },
                    {
                        "name": "Azadeh Fattahi"
                    },
                    {
                        "name": "Ian G. McCarthy"
                    },
                    {
                        "name": "Andreea S. Font"
                    },
                    {
                        "name": "Kyle A. Oman"
                    },
                    {
                        "name": "Alexander H. Riley"
                    }
                ],
                "author_detail": {
                    "name": "Alexander H. Riley"
                },
                "author": "Alexander H. Riley",
                "arxiv_doi": "10.1093/mnras/stae1378",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae1378",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 9 figures, accepted to MNRAS",
                "arxiv_journal_ref": "MNRAS, 532, 1223-1240 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12475v1",
                "updated": "2024-08-22T15:13:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    13,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:13:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    13,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action\n  Recognition"
                },
                "summary": "In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for\nfew-shot action recognition (FSAR), which incorporates a sequential perceiver\nadapter into the pre-training framework, to integrate both the spatial\ninformation and the sequential temporal dynamics into the feature embeddings.\nDifferent from the existing fine-tuning approaches that capture temporal\ninformation by exploring the relationships among all the frames, our\nperceiver-based adapter recurrently captures the sequential dynamics alongside\nthe timeline, which could perceive the order change. To obtain the\ndiscriminative representations for each class, we extend a textual corpus for\neach class derived from the large language models (LLMs) and enrich the visual\nprototypes by integrating the contextual semantic information. Besides, We\nintroduce an unbalanced optimal transport strategy for feature matching that\nmitigates the impact of class-unrelated features, thereby facilitating more\neffective decision-making. Experimental results on five FSAR datasets\ndemonstrate that our method set a new benchmark, beating the second-best\ncompetitors with large margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for\nfew-shot action recognition (FSAR), which incorporates a sequential perceiver\nadapter into the pre-training framework, to integrate both the spatial\ninformation and the sequential temporal dynamics into the feature embeddings.\nDifferent from the existing fine-tuning approaches that capture temporal\ninformation by exploring the relationships among all the frames, our\nperceiver-based adapter recurrently captures the sequential dynamics alongside\nthe timeline, which could perceive the order change. To obtain the\ndiscriminative representations for each class, we extend a textual corpus for\neach class derived from the large language models (LLMs) and enrich the visual\nprototypes by integrating the contextual semantic information. Besides, We\nintroduce an unbalanced optimal transport strategy for feature matching that\nmitigates the impact of class-unrelated features, thereby facilitating more\neffective decision-making. Experimental results on five FSAR datasets\ndemonstrate that our method set a new benchmark, beating the second-best\ncompetitors with large margins."
                },
                "authors": [
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Yunlong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yunlong Yu"
                },
                "author": "Yunlong Yu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12470v1",
                "updated": "2024-08-22T15:10:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:10:56Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "title": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12469v1",
                "updated": "2024-08-22T15:10:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:10:20Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    20,
                    3,
                    235,
                    0
                ],
                "title": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot\n  Learning"
                },
                "summary": "Few-shot learning (FSL) aims to recognize new concepts using a limited number\nof visual samples. Existing approaches attempt to incorporate semantic\ninformation into the limited visual data for category understanding. However,\nthese methods often enrich class-level feature representations with abstract\ncategory names, failing to capture the nuanced features essential for effective\ngeneralization. To address this issue, we propose a novel framework for FSL,\nwhich incorporates both the abstract class semantics and the concrete class\nentities extracted from Large Language Models (LLMs), to enhance the\nrepresentation of the class prototypes. Specifically, our framework composes a\nSemantic-guided Visual Pattern Extraction (SVPE) module and a\nPrototype-Calibration (PC) module, where the SVPE meticulously extracts\nsemantic-aware visual patterns across diverse scales, while the PC module\nseamlessly integrates these patterns to refine the visual prototype, enhancing\nits representativeness. Extensive experiments on four few-shot classification\nbenchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable\nadvancements over the current state-of-the-art methods. Notably, for the\nchallenging one-shot setting, our approach, utilizing the ResNet-12 backbone,\nachieves an impressive average improvement of 1.95% over the second-best\ncompetitor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot learning (FSL) aims to recognize new concepts using a limited number\nof visual samples. Existing approaches attempt to incorporate semantic\ninformation into the limited visual data for category understanding. However,\nthese methods often enrich class-level feature representations with abstract\ncategory names, failing to capture the nuanced features essential for effective\ngeneralization. To address this issue, we propose a novel framework for FSL,\nwhich incorporates both the abstract class semantics and the concrete class\nentities extracted from Large Language Models (LLMs), to enhance the\nrepresentation of the class prototypes. Specifically, our framework composes a\nSemantic-guided Visual Pattern Extraction (SVPE) module and a\nPrototype-Calibration (PC) module, where the SVPE meticulously extracts\nsemantic-aware visual patterns across diverse scales, while the PC module\nseamlessly integrates these patterns to refine the visual prototype, enhancing\nits representativeness. Extensive experiments on four few-shot classification\nbenchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable\nadvancements over the current state-of-the-art methods. Notably, for the\nchallenging one-shot setting, our approach, utilizing the ResNet-12 backbone,\nachieves an impressive average improvement of 1.95% over the second-best\ncompetitor."
                },
                "authors": [
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Ziqian Lu"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08066v2",
                "updated": "2024-08-22T15:07:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    7,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-15T10:15:37Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    15,
                    37,
                    3,
                    228,
                    0
                ],
                "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense\n  Retrieval"
                },
                "summary": "In the information retrieval (IR) area, dense retrieval (DR) models use deep\nlearning techniques to encode queries and passages into embedding space to\ncompute their semantic relations. It is important for DR models to balance both\nefficiency and effectiveness. Pre-trained language models (PLMs), especially\nTransformer-based PLMs, have been proven to be effective encoders of DR models.\nHowever, the self-attention component in Transformer-based PLM results in a\ncomputational complexity that grows quadratically with sequence length, and\nthus exhibits a slow inference speed for long-text retrieval. Some recently\nproposed non-Transformer PLMs, especially the Mamba architecture PLMs, have\ndemonstrated not only comparable effectiveness to Transformer-based PLMs on\ngenerative language tasks but also better efficiency due to linear time scaling\nin sequence length. This paper implements the Mamba Retriever to explore\nwhether Mamba can serve as an effective and efficient encoder of DR model for\nIR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO\npassage ranking dataset and the long-text LoCoV0 dataset. Experimental results\nshow that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba\nRetriever achieves comparable or better effectiveness compared to\nTransformer-based retrieval models, and the effectiveness grows with the size\nof the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever\ncan extend to longer text length than its pre-trained length after fine-tuning\non retrieval task, and it has comparable or better effectiveness compared to\nother long-text retrieval models; (3) the Mamba Retriever has superior\ninference speed for long-text retrieval. In conclusion, Mamba Retriever is both\neffective and efficient, making it a practical model, especially for long-text\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the information retrieval (IR) area, dense retrieval (DR) models use deep\nlearning techniques to encode queries and passages into embedding space to\ncompute their semantic relations. It is important for DR models to balance both\nefficiency and effectiveness. Pre-trained language models (PLMs), especially\nTransformer-based PLMs, have been proven to be effective encoders of DR models.\nHowever, the self-attention component in Transformer-based PLM results in a\ncomputational complexity that grows quadratically with sequence length, and\nthus exhibits a slow inference speed for long-text retrieval. Some recently\nproposed non-Transformer PLMs, especially the Mamba architecture PLMs, have\ndemonstrated not only comparable effectiveness to Transformer-based PLMs on\ngenerative language tasks but also better efficiency due to linear time scaling\nin sequence length. This paper implements the Mamba Retriever to explore\nwhether Mamba can serve as an effective and efficient encoder of DR model for\nIR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO\npassage ranking dataset and the long-text LoCoV0 dataset. Experimental results\nshow that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba\nRetriever achieves comparable or better effectiveness compared to\nTransformer-based retrieval models, and the effectiveness grows with the size\nof the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever\ncan extend to longer text length than its pre-trained length after fine-tuning\non retrieval task, and it has comparable or better effectiveness compared to\nother long-text retrieval models; (3) the Mamba Retriever has superior\ninference speed for long-text retrieval. In conclusion, Mamba Retriever is both\neffective and efficient, making it a practical model, especially for long-text\nretrieval."
                },
                "authors": [
                    {
                        "name": "Hanqi Zhang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Lang Mei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12463v1",
                "updated": "2024-08-22T15:04:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    4,
                    59,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:04:59Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    4,
                    59,
                    3,
                    235,
                    0
                ],
                "title": "Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation"
                },
                "summary": "A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955cm and 1.091cm, respectively. To address the computational\nconstraints of smartphones, we developed an edge intelligence architecture to\nenhance the performance of smartphone-based eye tracking. We applied various\noptimisation methods like quantisation and pruning to deep learning models for\nbetter energy, CPU, and memory usage on edge devices, focusing on real-time\nprocessing. Using model quantisation, the model inference time in the CNN+LSTM\nand CNN+GRU models was reduced by 21.72% and 19.50%, respectively, on edge\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955cm and 1.091cm, respectively. To address the computational\nconstraints of smartphones, we developed an edge intelligence architecture to\nenhance the performance of smartphone-based eye tracking. We applied various\noptimisation methods like quantisation and pruning to deep learning models for\nbetter energy, CPU, and memory usage on edge devices, focusing on real-time\nprocessing. Using model quantisation, the model inference time in the CNN+LSTM\nand CNN+GRU models was reduced by 21.72% and 19.50%, respectively, on edge\ndevices."
                },
                "authors": [
                    {
                        "name": "Nishan Gunawardena"
                    },
                    {
                        "name": "Gough Yumu Lui"
                    },
                    {
                        "name": "Jeewani Anupama Ginige"
                    },
                    {
                        "name": "Bahman Javadi"
                    }
                ],
                "author_detail": {
                    "name": "Bahman Javadi"
                },
                "author": "Bahman Javadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12458v1",
                "updated": "2024-08-22T14:55:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    55,
                    25,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:55:25Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    55,
                    25,
                    3,
                    235,
                    0
                ],
                "title": "Non-perturbative Resolution of Strong Coupling Singularities in 4d N=1\n  Heterotic/M-theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-perturbative Resolution of Strong Coupling Singularities in 4d N=1\n  Heterotic/M-theory"
                },
                "summary": "We investigate the interior of the moduli space of four-dimensional\n$\\mathcal{N}=1$ theories of gravity arising from compactifications of the\n$E_8\\times E_8$ heterotic string on Calabi-Yau threefolds. By studying the\nthreshold corrections to the coupling of the heterotic gauge groups, we infer\nthe existence of a strong coupling singularity for one of the perturbative\nheterotic gauge groups, which effectively yields an additional finite distance\nboundary of the classical scalar field space. In heterotic M-theory, this\nboundary maps to a domain wall solution for which the gauge coupling and the\nwarp factor on one of the Horava-Witten 9-branes diverge, thus highlighting the\ngravitational origin of the classical strong coupling singularity. The\ndivergence of the warp factor is, however, regulated once non-perturbative\neffects are taken into account, as we demonstrate by studying the instanton\ncorrections to the 5d BPS domain wall equations. This regularization implies\nthat the classical strong coupling boundary of the scalar field 4d\n$\\mathcal{N}=1$ heterotic/M-theory is resolved, indicating that, at the quantum\nlevel, the field space can be extended beyond this classical boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the interior of the moduli space of four-dimensional\n$\\mathcal{N}=1$ theories of gravity arising from compactifications of the\n$E_8\\times E_8$ heterotic string on Calabi-Yau threefolds. By studying the\nthreshold corrections to the coupling of the heterotic gauge groups, we infer\nthe existence of a strong coupling singularity for one of the perturbative\nheterotic gauge groups, which effectively yields an additional finite distance\nboundary of the classical scalar field space. In heterotic M-theory, this\nboundary maps to a domain wall solution for which the gauge coupling and the\nwarp factor on one of the Horava-Witten 9-branes diverge, thus highlighting the\ngravitational origin of the classical strong coupling singularity. The\ndivergence of the warp factor is, however, regulated once non-perturbative\neffects are taken into account, as we demonstrate by studying the instanton\ncorrections to the 5d BPS domain wall equations. This regularization implies\nthat the classical strong coupling boundary of the scalar field 4d\n$\\mathcal{N}=1$ heterotic/M-theory is resolved, indicating that, at the quantum\nlevel, the field space can be extended beyond this classical boundary."
                },
                "authors": [
                    {
                        "name": "Mirjam Cvetiƒç"
                    },
                    {
                        "name": "Max Wiesner"
                    }
                ],
                "author_detail": {
                    "name": "Max Wiesner"
                },
                "author": "Max Wiesner",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12456v1",
                "updated": "2024-08-22T14:53:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    53,
                    33,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:53:33Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    53,
                    33,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Multi-hop Reasoning through Knowledge Erasure in Large\n  Language Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-hop Reasoning through Knowledge Erasure in Large\n  Language Model Editing"
                },
                "summary": "Large language models (LLMs) face challenges with internal knowledge\ninaccuracies and outdated information. Knowledge editing has emerged as a\npivotal approach to mitigate these issues. Although current knowledge editing\ntechniques exhibit promising performance in single-hop reasoning tasks, they\nshow limitations when applied to multi-hop reasoning. Drawing on cognitive\nneuroscience and the operational mechanisms of LLMs, we hypothesize that the\nresidual single-hop knowledge after editing causes edited models to revert to\ntheir original answers when processing multi-hop questions, thereby undermining\ntheir performance in multihop reasoning tasks. To validate this hypothesis, we\nconduct a series of experiments that empirically confirm our assumptions.\nBuilding on the validated hypothesis, we propose a novel knowledge editing\nmethod that incorporates a Knowledge Erasure mechanism for Large language model\nEditing (KELE). Specifically, we design an erasure function for residual\nknowledge and an injection function for new knowledge. Through joint\noptimization, we derive the optimal recall vector, which is subsequently\nutilized within a rank-one editing framework to update the parameters of\ntargeted model layers. Extensive experiments on GPT-J and GPT-2 XL demonstrate\nthat KELE substantially enhances the multi-hop reasoning capability of edited\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face challenges with internal knowledge\ninaccuracies and outdated information. Knowledge editing has emerged as a\npivotal approach to mitigate these issues. Although current knowledge editing\ntechniques exhibit promising performance in single-hop reasoning tasks, they\nshow limitations when applied to multi-hop reasoning. Drawing on cognitive\nneuroscience and the operational mechanisms of LLMs, we hypothesize that the\nresidual single-hop knowledge after editing causes edited models to revert to\ntheir original answers when processing multi-hop questions, thereby undermining\ntheir performance in multihop reasoning tasks. To validate this hypothesis, we\nconduct a series of experiments that empirically confirm our assumptions.\nBuilding on the validated hypothesis, we propose a novel knowledge editing\nmethod that incorporates a Knowledge Erasure mechanism for Large language model\nEditing (KELE). Specifically, we design an erasure function for residual\nknowledge and an injection function for new knowledge. Through joint\noptimization, we derive the optimal recall vector, which is subsequently\nutilized within a rank-one editing framework to update the parameters of\ntargeted model layers. Extensive experiments on GPT-J and GPT-2 XL demonstrate\nthat KELE substantially enhances the multi-hop reasoning capability of edited\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Bowen Fang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00429v2",
                "updated": "2024-08-22T14:50:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    50,
                    24,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-29T12:48:53Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    12,
                    48,
                    53,
                    5,
                    181,
                    0
                ],
                "title": "Time Series Clustering with General State Space Models via Stochastic\n  Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Clustering with General State Space Models via Stochastic\n  Variational Inference"
                },
                "summary": "In this paper, we propose a novel method of model-based time series\nclustering with mixtures of general state space models (MSSMs). Each component\nof MSSMs is associated with each cluster. An advantage of the proposed method\nis that it enables the use of time series models appropriate to the specific\ntime series. This not only improves clustering and prediction accuracy but also\nenhances the interpretability of the estimated parameters. The parameters of\nthe MSSMs are estimated using stochastic variational inference, a subtype of\nvariational inference. The proposed method estimates the latent variables of an\narbitrary state space model by using neural networks with a normalizing flow as\na variational estimator. The number of clusters can be estimated using the\nBayesian information criterion. In addition, to prevent MSSMs from converging\nto the local optimum, we propose several optimization tricks, including an\nadditional penalty term called entropy annealing. To our best knowledge, the\nproposed method is the first computationally feasible one for time series\nclustering based on general (possibly nonlinear, non-Gaussian) state space\nmodels. Experiments on simulated datasets show that the proposed method is\neffective for clustering, parameter estimation, and estimating the number of\nclusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel method of model-based time series\nclustering with mixtures of general state space models (MSSMs). Each component\nof MSSMs is associated with each cluster. An advantage of the proposed method\nis that it enables the use of time series models appropriate to the specific\ntime series. This not only improves clustering and prediction accuracy but also\nenhances the interpretability of the estimated parameters. The parameters of\nthe MSSMs are estimated using stochastic variational inference, a subtype of\nvariational inference. The proposed method estimates the latent variables of an\narbitrary state space model by using neural networks with a normalizing flow as\na variational estimator. The number of clusters can be estimated using the\nBayesian information criterion. In addition, to prevent MSSMs from converging\nto the local optimum, we propose several optimization tricks, including an\nadditional penalty term called entropy annealing. To our best knowledge, the\nproposed method is the first computationally feasible one for time series\nclustering based on general (possibly nonlinear, non-Gaussian) state space\nmodels. Experiments on simulated datasets show that the proposed method is\neffective for clustering, parameter estimation, and estimating the number of\nclusters."
                },
                "authors": [
                    {
                        "name": "Ryoichi Ishizuka"
                    },
                    {
                        "name": "Takashi Imai"
                    },
                    {
                        "name": "Kaoru Kawamoto"
                    }
                ],
                "author_detail": {
                    "name": "Kaoru Kawamoto"
                },
                "author": "Kaoru Kawamoto",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08730v2",
                "updated": "2024-08-22T14:35:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    35,
                    30,
                    3,
                    235,
                    0
                ],
                "published": "2024-05-14T16:16:29Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    16,
                    16,
                    29,
                    1,
                    135,
                    0
                ],
                "title": "A Generalized Difference-in-Differences Estimator for Randomized\n  Stepped-Wedge and Observational Staggered Adoption Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generalized Difference-in-Differences Estimator for Randomized\n  Stepped-Wedge and Observational Staggered Adoption Settings"
                },
                "summary": "Staggered treatment adoption arises in the evaluation of policy impact and\nimplementation in a variety of settings. This occurs in both randomized\nstepped-wedge trials and non-randomized quasi-experimental panel data settings\nusing causal inference methods based on difference-in-differences analysis. In\nboth settings, it is crucial to carefully consider the target estimand and\npossible treatment effect heterogeneities in order to estimate the effect\nwithout bias and in an interpretable fashion. This paper proposes a novel\nnon-parametric approach to this estimation for either setting. By constructing\nan estimator using two-by-two difference-in-difference comparisons as building\nblocks with arbitrary weights, the investigator can select weights to target\nthe desired estimand in an unbiased manner under assumed treatment effect\nhomogeneity, and minimize the variance under an assumed working covariance\nstructure. This provides desirable bias properties while using the comparisons\nefficiently to mitigate the loss of precision. Weightings are shown for simple\nsettings and two data examples are examined. The methods are used to re-analyze\ndata from both a randomized stepped-wedge trial on the impact of novel\ntuberculosis diagnostic tools and an observational staggered adoption study on\nthe effects of COVID-19 vaccine financial incentive lotteries in U.S. states;\nthese are compared to analyses using previous methods. A full algorithm with R\ncode is provided to implement this method and to compare to existing methods.\nThe proposed method allows for high flexibility and clear targeting of desired\neffects, providing one solution to the bias-variance-generalizability tradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staggered treatment adoption arises in the evaluation of policy impact and\nimplementation in a variety of settings. This occurs in both randomized\nstepped-wedge trials and non-randomized quasi-experimental panel data settings\nusing causal inference methods based on difference-in-differences analysis. In\nboth settings, it is crucial to carefully consider the target estimand and\npossible treatment effect heterogeneities in order to estimate the effect\nwithout bias and in an interpretable fashion. This paper proposes a novel\nnon-parametric approach to this estimation for either setting. By constructing\nan estimator using two-by-two difference-in-difference comparisons as building\nblocks with arbitrary weights, the investigator can select weights to target\nthe desired estimand in an unbiased manner under assumed treatment effect\nhomogeneity, and minimize the variance under an assumed working covariance\nstructure. This provides desirable bias properties while using the comparisons\nefficiently to mitigate the loss of precision. Weightings are shown for simple\nsettings and two data examples are examined. The methods are used to re-analyze\ndata from both a randomized stepped-wedge trial on the impact of novel\ntuberculosis diagnostic tools and an observational staggered adoption study on\nthe effects of COVID-19 vaccine financial incentive lotteries in U.S. states;\nthese are compared to analyses using previous methods. A full algorithm with R\ncode is provided to implement this method and to compare to existing methods.\nThe proposed method allows for high flexibility and clear targeting of desired\neffects, providing one solution to the bias-variance-generalizability tradeoff."
                },
                "authors": [
                    {
                        "name": "Lee Kennedy-Shaffer"
                    }
                ],
                "author_detail": {
                    "name": "Lee Kennedy-Shaffer"
                },
                "author": "Lee Kennedy-Shaffer",
                "arxiv_comment": "38 pages main text, 23 pages additional material, 3 figures, 9 tables\n  Update includes additional discussion and an added data example for the\n  observational setting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12429v1",
                "updated": "2024-08-22T14:22:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    22,
                    7,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:22:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    22,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing"
                },
                "summary": "Combining Vision Large Language Models (VLLMs) with diffusion models offers a\npowerful method for executing image editing tasks based on human language\ninstructions. However, language instructions alone often fall short in\naccurately conveying user requirements, particularly when users want to add,\nreplace elements in specific areas of an image. Luckily, masks can effectively\nindicate the exact locations or elements to be edited, while they require users\nto precisely draw the shapes at the desired locations, which is highly\nuser-unfriendly. To address this, we propose FlexEdit, an end-to-end image\nediting method that leverages both free-shape masks and language instructions\nfor Flexible Editing. Our approach employs a VLLM in comprehending the image\ncontent, mask, and user instructions. Additionally, we introduce the Mask\nEnhance Adapter (MEA) that fuses the embeddings of the VLLM with the image\ndata, ensuring a seamless integration of mask information and model output\nembeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically\ntailored for free-shape mask, including 8 types of free-shape mask. Extensive\nexperiments show that our method achieves state-of-the-art (SOTA) performance\nin LLM-based image editing, and our simple prompting technique stands out in\nits effectiveness. The code and data can be found at\nhttps://github.com/A-new-b/flex_edit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Vision Large Language Models (VLLMs) with diffusion models offers a\npowerful method for executing image editing tasks based on human language\ninstructions. However, language instructions alone often fall short in\naccurately conveying user requirements, particularly when users want to add,\nreplace elements in specific areas of an image. Luckily, masks can effectively\nindicate the exact locations or elements to be edited, while they require users\nto precisely draw the shapes at the desired locations, which is highly\nuser-unfriendly. To address this, we propose FlexEdit, an end-to-end image\nediting method that leverages both free-shape masks and language instructions\nfor Flexible Editing. Our approach employs a VLLM in comprehending the image\ncontent, mask, and user instructions. Additionally, we introduce the Mask\nEnhance Adapter (MEA) that fuses the embeddings of the VLLM with the image\ndata, ensuring a seamless integration of mask information and model output\nembeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically\ntailored for free-shape mask, including 8 types of free-shape mask. Extensive\nexperiments show that our method achieves state-of-the-art (SOTA) performance\nin LLM-based image editing, and our simple prompting technique stands out in\nits effectiveness. The code and data can be found at\nhttps://github.com/A-new-b/flex_edit."
                },
                "authors": [
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yuxiang Lin"
                    },
                    {
                        "name": "Tianshuo Yuan"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Jiao GH"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojiang Peng"
                },
                "author": "Xiaojiang Peng",
                "arxiv_comment": "15 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12425v1",
                "updated": "2024-08-22T14:20:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    20,
                    11,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:20:11Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    20,
                    11,
                    3,
                    235,
                    0
                ],
                "title": "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech\n  Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech\n  Enhancement"
                },
                "summary": "This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN)\nfor compute-efficient speech enhancement models running on resource-constrained\nhardware platforms. It leverages the slow evolution characteristic of RNN\nhidden states over steps, and updates only a selected set of neurons at each\nstep by adding a newly proposed select gate to the RNN model. This select gate\nallows the computation cost of the conventional RNN to be reduced during\nnetwork inference. As a realization of the DG-RNN, we further propose the\nDynamic Gated Recurrent Unit (D-GRU) which does not require additional\nparameters. Test results obtained from several state-of-the-art\ncompute-efficient RNN-based speech enhancement architectures using the DNS\nchallenge dataset, show that the D-GRU based model variants maintain similar\nspeech intelligibility and quality metrics comparable to the baseline GRU based\nmodels even with an average 50% reduction in GRU computes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN)\nfor compute-efficient speech enhancement models running on resource-constrained\nhardware platforms. It leverages the slow evolution characteristic of RNN\nhidden states over steps, and updates only a selected set of neurons at each\nstep by adding a newly proposed select gate to the RNN model. This select gate\nallows the computation cost of the conventional RNN to be reduced during\nnetwork inference. As a realization of the DG-RNN, we further propose the\nDynamic Gated Recurrent Unit (D-GRU) which does not require additional\nparameters. Test results obtained from several state-of-the-art\ncompute-efficient RNN-based speech enhancement architectures using the DNS\nchallenge dataset, show that the D-GRU based model variants maintain similar\nspeech intelligibility and quality metrics comparable to the baseline GRU based\nmodels even with an average 50% reduction in GRU computes."
                },
                "authors": [
                    {
                        "name": "Longbiao Cheng"
                    },
                    {
                        "name": "Ashutosh Pandey"
                    },
                    {
                        "name": "Buye Xu"
                    },
                    {
                        "name": "Tobi Delbruck"
                    },
                    {
                        "name": "Shih-Chii Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chii Liu"
                },
                "author": "Shih-Chii Liu",
                "arxiv_comment": "Accepted to Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.12767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.12767v2",
                "updated": "2024-08-22T14:19:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    19,
                    6,
                    3,
                    235,
                    0
                ],
                "published": "2023-03-22T17:32:56Z",
                "published_parsed": [
                    2023,
                    3,
                    22,
                    17,
                    32,
                    56,
                    2,
                    81,
                    0
                ],
                "title": "Can we trust the evaluation on ChatGPT?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we trust the evaluation on ChatGPT?"
                },
                "summary": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models."
                },
                "authors": [
                    {
                        "name": "Rachith Aiyappa"
                    },
                    {
                        "name": "Jisun An"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Yeol Ahn"
                },
                "author": "Yong-Yeol Ahn",
                "arxiv_doi": "10.18653/v1/2023.trustnlp-1.5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.12767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.12767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2023) (July 2023) 47-54",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12423v1",
                "updated": "2024-08-22T14:18:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    18,
                    16,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:18:16Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    18,
                    16,
                    3,
                    235,
                    0
                ],
                "title": "Multi-Knowledge Fusion Network for Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Knowledge Fusion Network for Time Series Representation Learning"
                },
                "summary": "Forecasting the behaviour of complex dynamical systems such as interconnected\nsensor networks characterized by high-dimensional multivariate time series(MTS)\nis of paramount importance for making informed decisions and planning for the\nfuture in a broad spectrum of applications. Graph forecasting networks(GFNs)\nare well-suited for forecasting MTS data that exhibit spatio-temporal\ndependencies. However, most prior works of GFN-based methods on MTS forecasting\nrely on domain-expertise to model the nonlinear dynamics of the system, but\nneglect the potential to leverage the inherent relational-structural\ndependencies among time series variables underlying MTS data. On the other\nhand, contemporary works attempt to infer the relational structure of the\ncomplex dependencies between the variables and simultaneously learn the\nnonlinear dynamics of the interconnected system but neglect the possibility of\nincorporating domain-specific prior knowledge to improve forecast accuracy. To\nthis end, we propose a hybrid architecture that combines explicit prior\nknowledge with implicit knowledge of the relational structure within the MTS\ndata. It jointly learns intra-series temporal dependencies and inter-series\nspatial dependencies by encoding time-conditioned structural spatio-temporal\ninductive biases to provide more accurate and reliable forecasts. It also\nmodels the time-varying uncertainty of the multi-horizon forecasts to support\ndecision-making by providing estimates of prediction uncertainty. The proposed\narchitecture has shown promising results on multiple benchmark datasets and\noutperforms state-of-the-art forecasting methods by a significant margin. We\nreport and discuss the ablation studies to validate our forecasting\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting the behaviour of complex dynamical systems such as interconnected\nsensor networks characterized by high-dimensional multivariate time series(MTS)\nis of paramount importance for making informed decisions and planning for the\nfuture in a broad spectrum of applications. Graph forecasting networks(GFNs)\nare well-suited for forecasting MTS data that exhibit spatio-temporal\ndependencies. However, most prior works of GFN-based methods on MTS forecasting\nrely on domain-expertise to model the nonlinear dynamics of the system, but\nneglect the potential to leverage the inherent relational-structural\ndependencies among time series variables underlying MTS data. On the other\nhand, contemporary works attempt to infer the relational structure of the\ncomplex dependencies between the variables and simultaneously learn the\nnonlinear dynamics of the interconnected system but neglect the possibility of\nincorporating domain-specific prior knowledge to improve forecast accuracy. To\nthis end, we propose a hybrid architecture that combines explicit prior\nknowledge with implicit knowledge of the relational structure within the MTS\ndata. It jointly learns intra-series temporal dependencies and inter-series\nspatial dependencies by encoding time-conditioned structural spatio-temporal\ninductive biases to provide more accurate and reliable forecasts. It also\nmodels the time-varying uncertainty of the multi-horizon forecasts to support\ndecision-making by providing estimates of prediction uncertainty. The proposed\narchitecture has shown promising results on multiple benchmark datasets and\noutperforms state-of-the-art forecasting methods by a significant margin. We\nreport and discuss the ablation studies to validate our forecasting\narchitecture."
                },
                "authors": [
                    {
                        "name": "Sagar Srinivas Sakhinana"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Krishna Sai Sudhir Aripirala"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper accepted at ML4IoT Workshop, International Conference on\n  Learning Representations(ICLR) 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12416v1",
                "updated": "2024-08-22T14:12:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    12,
                    6,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:12:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    12,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "Unlearning Trojans in Large Language Models: A Comparison Between\n  Natural Language and Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Trojans in Large Language Models: A Comparison Between\n  Natural Language and Source Code"
                },
                "summary": "This work investigates the application of Machine Unlearning (MU) for\nmitigating the impact of trojans embedded in conventional large language models\nof natural language (Text-LLMs) and large language models of code (Code-LLMs)\nWe propose a novel unlearning approach, LYA, that leverages both gradient\nascent and elastic weight consolidation, a Fisher Information Matrix (FIM)\nbased regularization technique, to unlearn trojans from poisoned models. We\ncompare the effectiveness of LYA against conventional techniques like\nfine-tuning, retraining, and vanilla gradient ascent. The subject models we\ninvestigate are BERT and CodeBERT, for sentiment analysis and code defect\ndetection tasks, respectively. Our findings demonstrate that the combination of\ngradient ascent and FIM-based regularization, as done in LYA, outperforms\nexisting methods in removing the trojan's influence from the poisoned model,\nwhile preserving its original functionality. To the best of our knowledge, this\nis the first work that compares and contrasts MU of trojans in LLMs, in the NL\nand Coding domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the application of Machine Unlearning (MU) for\nmitigating the impact of trojans embedded in conventional large language models\nof natural language (Text-LLMs) and large language models of code (Code-LLMs)\nWe propose a novel unlearning approach, LYA, that leverages both gradient\nascent and elastic weight consolidation, a Fisher Information Matrix (FIM)\nbased regularization technique, to unlearn trojans from poisoned models. We\ncompare the effectiveness of LYA against conventional techniques like\nfine-tuning, retraining, and vanilla gradient ascent. The subject models we\ninvestigate are BERT and CodeBERT, for sentiment analysis and code defect\ndetection tasks, respectively. Our findings demonstrate that the combination of\ngradient ascent and FIM-based regularization, as done in LYA, outperforms\nexisting methods in removing the trojan's influence from the poisoned model,\nwhile preserving its original functionality. To the best of our knowledge, this\nis the first work that compares and contrasts MU of trojans in LLMs, in the NL\nand Coding domain."
                },
                "authors": [
                    {
                        "name": "Mahdi Kazemi"
                    },
                    {
                        "name": "Aftab Hussain"
                    },
                    {
                        "name": "Md Rafiqul Islam Rabin"
                    },
                    {
                        "name": "Mohammad Amin Alipour"
                    },
                    {
                        "name": "Sen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Sen Lin"
                },
                "author": "Sen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12414v1",
                "updated": "2024-08-22T14:07:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    7,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:07:50Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    7,
                    50,
                    3,
                    235,
                    0
                ],
                "title": "BIPeC: A Combined Change-Point Analyzer to Identify Performance\n  Regressions in Large-scale Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIPeC: A Combined Change-Point Analyzer to Identify Performance\n  Regressions in Large-scale Database Systems"
                },
                "summary": "Performance testing in large-scale database systems like SAP HANA is a\ncrucial yet labor-intensive task, involving extensive manual analysis of\nthousands of measurements, such as CPU time and elapsed time. Manual\nmaintenance of these metrics is time-consuming and susceptible to human error,\nmaking early detection of performance regressions challenging. We address these\nissues by proposing an automated approach to detect performance regressions in\nsuch measurements. Our approach integrates Bayesian inference with the Pruned\nExact Linear Time (PELT) algorithm, enhancing the detection of change points\nand performance regressions with high precision and efficiency compared to\nprevious approaches. Our method minimizes false negatives and ensures SAP\nHANA's system's reliability and performance quality. The proposed solution can\naccelerate testing and contribute to more sustainable performance management\npractices in large-scale data management environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance testing in large-scale database systems like SAP HANA is a\ncrucial yet labor-intensive task, involving extensive manual analysis of\nthousands of measurements, such as CPU time and elapsed time. Manual\nmaintenance of these metrics is time-consuming and susceptible to human error,\nmaking early detection of performance regressions challenging. We address these\nissues by proposing an automated approach to detect performance regressions in\nsuch measurements. Our approach integrates Bayesian inference with the Pruned\nExact Linear Time (PELT) algorithm, enhancing the detection of change points\nand performance regressions with high precision and efficiency compared to\nprevious approaches. Our method minimizes false negatives and ensures SAP\nHANA's system's reliability and performance quality. The proposed solution can\naccelerate testing and contribute to more sustainable performance management\npractices in large-scale data management environments."
                },
                "authors": [
                    {
                        "name": "Zhan Lyu"
                    },
                    {
                        "name": "Thomas Bach"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Nguyen Minh Le"
                    },
                    {
                        "name": "Lars Hoemke"
                    }
                ],
                "author_detail": {
                    "name": "Lars Hoemke"
                },
                "author": "Lars Hoemke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12409v1",
                "updated": "2024-08-22T13:58:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    58,
                    55,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T13:58:55Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    58,
                    55,
                    3,
                    235,
                    0
                ],
                "title": "Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series\n  Representation Learning"
                },
                "summary": "Accurately predicting the behavior of complex dynamical systems,\ncharacterized by high-dimensional multivariate time series(MTS) in\ninterconnected sensor networks, is crucial for informed decision-making in\nvarious applications to minimize risk. While graph forecasting networks(GFNs)\nare ideal for forecasting MTS data that exhibit spatio-temporal dependencies,\nprior works rely solely on the domain-specific knowledge of time-series\nvariables inter-relationships to model the nonlinear dynamics, neglecting\ninherent relational structural dependencies among the variables within the MTS\ndata. In contrast, contemporary works infer relational structures from MTS data\nbut neglect domain-specific knowledge. The proposed hybrid architecture\naddresses these limitations by combining both domain-specific knowledge and\nimplicit knowledge of the relational structure underlying the MTS data using\nKnowledge-Based Compositional Generalization. The hybrid architecture shows\npromising results on multiple benchmark datasets, outperforming\nstate-of-the-art forecasting methods. Additionally, the architecture models the\ntime varying uncertainty of multi-horizon forecasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting the behavior of complex dynamical systems,\ncharacterized by high-dimensional multivariate time series(MTS) in\ninterconnected sensor networks, is crucial for informed decision-making in\nvarious applications to minimize risk. While graph forecasting networks(GFNs)\nare ideal for forecasting MTS data that exhibit spatio-temporal dependencies,\nprior works rely solely on the domain-specific knowledge of time-series\nvariables inter-relationships to model the nonlinear dynamics, neglecting\ninherent relational structural dependencies among the variables within the MTS\ndata. In contrast, contemporary works infer relational structures from MTS data\nbut neglect domain-specific knowledge. The proposed hybrid architecture\naddresses these limitations by combining both domain-specific knowledge and\nimplicit knowledge of the relational structure underlying the MTS data using\nKnowledge-Based Compositional Generalization. The hybrid architecture shows\npromising results on multiple benchmark datasets, outperforming\nstate-of-the-art forecasting methods. Additionally, the architecture models the\ntime varying uncertainty of multi-horizon forecasts."
                },
                "authors": [
                    {
                        "name": "Sagar Srinivas Sakhinana"
                    },
                    {
                        "name": "Krishna Sai Sudhir Aripirala"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper is accepted at Knowledge-Based Compositional Generalization\n  Workshop, International Joint Conferences on Artificial\n  Intelligence(IJCAI-23)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07862v2",
                "updated": "2024-08-22T13:57:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    57,
                    30,
                    3,
                    235,
                    0
                ],
                "published": "2024-02-12T18:14:43Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    14,
                    43,
                    0,
                    43,
                    0
                ],
                "title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting\n  Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting\n  Accuracy"
                },
                "summary": "Large language models (LLMs) match and sometimes exceeding human performance\nin many domains. This study explores the potential of LLMs to augment human\njudgement in a forecasting task. We evaluate the effect on human forecasters of\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\nadvice, and the other designed to be overconfident and base-rate neglecting,\nthus providing noisy forecasting advice. We compare participants using these\nassistants to a control group that received a less advanced model that did not\nprovide numerical predictions or engaged in explicit discussion of predictions.\nParticipants (N = 991) answered a set of six forecasting questions and had the\noption to consult their assigned LLM assistant throughout. Our preregistered\nanalyses show that interacting with each of our frontier LLM assistants\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\ncompared to the control group. Exploratory analyses showed a pronounced outlier\neffect in one forecasting item, without which we find that the superforecasting\nassistant increased accuracy by 41 percent, compared with 29 percent for the\nnoisy assistant. We further examine whether LLM forecasting augmentation\ndisproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our data do not consistently support\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\ncompared to a less powerful model that does not provide specific forecasting\nadvice. However, the effects of outliers suggest that further research into the\nrobustness of this pattern is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) match and sometimes exceeding human performance\nin many domains. This study explores the potential of LLMs to augment human\njudgement in a forecasting task. We evaluate the effect on human forecasters of\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\nadvice, and the other designed to be overconfident and base-rate neglecting,\nthus providing noisy forecasting advice. We compare participants using these\nassistants to a control group that received a less advanced model that did not\nprovide numerical predictions or engaged in explicit discussion of predictions.\nParticipants (N = 991) answered a set of six forecasting questions and had the\noption to consult their assigned LLM assistant throughout. Our preregistered\nanalyses show that interacting with each of our frontier LLM assistants\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\ncompared to the control group. Exploratory analyses showed a pronounced outlier\neffect in one forecasting item, without which we find that the superforecasting\nassistant increased accuracy by 41 percent, compared with 29 percent for the\nnoisy assistant. We further examine whether LLM forecasting augmentation\ndisproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our data do not consistently support\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\ncompared to a less powerful model that does not provide specific forecasting\nadvice. However, the effects of outliers suggest that further research into the\nrobustness of this pattern is needed."
                },
                "authors": [
                    {
                        "name": "Philipp Schoenegger"
                    },
                    {
                        "name": "Peter S. Park"
                    },
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Sean Trott"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "arxiv_comment": "22 pages pages (main text comprised of 19 pages, appendix comprised\n  of three pages). 10 visualizations in the main text (four figures, six\n  tables), three additional figures in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12403v1",
                "updated": "2024-08-22T13:49:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    49,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T13:49:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    49,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Quantifying $S_8$ tension and evidence for interacting dark energy from\n  redshift-space distortion measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying $S_8$ tension and evidence for interacting dark energy from\n  redshift-space distortion measurements"
                },
                "summary": "In recent years, Cosmic Microwave Background (CMB) observations, Weak Lensing\nsurveys, and $f(z)\\sigma_8(z)$ measurements from Redshift-Space Distortions\n(RSD) have revealed a significant ($\\sim$3$-$5$\\sigma$) discrepancy in the\ninferred value of the matter clustering parameter $S_8$. In this work, we\ninvestigate the implications of RSD for a cosmological framework postulating an\ninteraction between Dark Energy (DE) and Dark Matter (DM). We explore scenarios\nwhere DM can transfer energy-momentum to DE or vice versa. The energy-momentum\nflow is characterized by the strength and the sign of the coupling parameter\n$\\xi$. Our baseline analysis combines RSD measurements with the latest data\nfrom Baryon Acoustic Oscillations (BAO) observed by DESI, Type Ia Supernovae\nfrom the PantheonPlus sample, and CMB data from Planck. We demonstrate that RSD\nmeasurements provide significant additional information. When energy-momentum\nflows from DM to DE (i.e., $\\xi < 0$), these measurements set stringent new\nbounds on the interaction strength. Conversely, when energy-momentum flows from\nDE to DM ($\\xi > 0$), they favor interactions at more than the $2\\sigma$\nconfidence level. Models with $\\xi > 0$ can effectively resolve the tension in\n$S_8$, presenting them as compelling alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Cosmic Microwave Background (CMB) observations, Weak Lensing\nsurveys, and $f(z)\\sigma_8(z)$ measurements from Redshift-Space Distortions\n(RSD) have revealed a significant ($\\sim$3$-$5$\\sigma$) discrepancy in the\ninferred value of the matter clustering parameter $S_8$. In this work, we\ninvestigate the implications of RSD for a cosmological framework postulating an\ninteraction between Dark Energy (DE) and Dark Matter (DM). We explore scenarios\nwhere DM can transfer energy-momentum to DE or vice versa. The energy-momentum\nflow is characterized by the strength and the sign of the coupling parameter\n$\\xi$. Our baseline analysis combines RSD measurements with the latest data\nfrom Baryon Acoustic Oscillations (BAO) observed by DESI, Type Ia Supernovae\nfrom the PantheonPlus sample, and CMB data from Planck. We demonstrate that RSD\nmeasurements provide significant additional information. When energy-momentum\nflows from DM to DE (i.e., $\\xi < 0$), these measurements set stringent new\nbounds on the interaction strength. Conversely, when energy-momentum flows from\nDE to DM ($\\xi > 0$), they favor interactions at more than the $2\\sigma$\nconfidence level. Models with $\\xi > 0$ can effectively resolve the tension in\n$S_8$, presenting them as compelling alternatives."
                },
                "authors": [
                    {
                        "name": "Miguel A. Sabogal"
                    },
                    {
                        "name": "Emanuelly Silva"
                    },
                    {
                        "name": "Rafael C. Nunes"
                    },
                    {
                        "name": "Suresh Kumar"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "William Giar√®"
                    }
                ],
                "author_detail": {
                    "name": "William Giar√®"
                },
                "author": "William Giar√®",
                "arxiv_comment": "13 pages, 6 figures. Comments welcome and appreciated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12398v1",
                "updated": "2024-08-22T13:44:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    44,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T13:44:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    44,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "A Comparative Analysis of Faithfulness Metrics and Humans in Citation\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of Faithfulness Metrics and Humans in Citation\n  Evaluation"
                },
                "summary": "Large language models (LLMs) often generate content with unsupported or\nunverifiable content, known as \"hallucinations.\" To address this,\nretrieval-augmented LLMs are employed to include citations in their content,\ngrounding the content in verifiable sources. Despite such developments,\nmanually assessing how well a citation supports the associated statement\nremains a major challenge. Previous studies tackle this challenge by leveraging\nfaithfulness metrics to estimate citation support automatically. However, they\nlimit this citation support estimation to a binary classification scenario,\nneglecting fine-grained citation support in practical scenarios. To investigate\nthe effectiveness of faithfulness metrics in fine-grained scenarios, we propose\na comparative evaluation framework that assesses the metric effectiveness in\ndistinguishing citations between three-category support levels: full, partial,\nand no support. Our framework employs correlation analysis, classification\nevaluation, and retrieval evaluation to measure the alignment between metric\nscores and human judgments comprehensively. Our results indicate no single\nmetric consistently excels across all evaluations, highlighting the complexity\nof accurately evaluating fine-grained support levels. Particularly, we find\nthat the best-performing metrics struggle to distinguish partial support from\nfull or no support. Based on these findings, we provide practical\nrecommendations for developing more effective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate content with unsupported or\nunverifiable content, known as \"hallucinations.\" To address this,\nretrieval-augmented LLMs are employed to include citations in their content,\ngrounding the content in verifiable sources. Despite such developments,\nmanually assessing how well a citation supports the associated statement\nremains a major challenge. Previous studies tackle this challenge by leveraging\nfaithfulness metrics to estimate citation support automatically. However, they\nlimit this citation support estimation to a binary classification scenario,\nneglecting fine-grained citation support in practical scenarios. To investigate\nthe effectiveness of faithfulness metrics in fine-grained scenarios, we propose\na comparative evaluation framework that assesses the metric effectiveness in\ndistinguishing citations between three-category support levels: full, partial,\nand no support. Our framework employs correlation analysis, classification\nevaluation, and retrieval evaluation to measure the alignment between metric\nscores and human judgments comprehensively. Our results indicate no single\nmetric consistently excels across all evaluations, highlighting the complexity\nof accurately evaluating fine-grained support levels. Particularly, we find\nthat the best-performing metrics struggle to distinguish partial support from\nfull or no support. Based on these findings, we provide practical\nrecommendations for developing more effective metrics."
                },
                "authors": [
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the First Workshop on Large Language Model for Evaluation\n  in Information Retrieval (LLM4Eval@SIGIR2024), non-archival. arXiv admin\n  note: substantial text overlap with arXiv:2406.15264",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12369v2",
                "updated": "2024-08-23T08:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    11,
                    9,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T13:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    13,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering"
                },
                "summary": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data."
                },
                "authors": [
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Kuber Vijaykumar Bellad"
                    },
                    {
                        "name": "Bharat Vadlamudi"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16823v3",
                "updated": "2024-08-22T13:06:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    6,
                    51,
                    3,
                    235,
                    0
                ],
                "published": "2024-02-26T18:48:27Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    18,
                    48,
                    27,
                    0,
                    57,
                    0
                ],
                "title": "Language Agents as Optimizable Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Agents as Optimizable Graphs"
                },
                "summary": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm."
                },
                "authors": [
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Louis Kirsch"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Dmitrii Khizbullin"
                    },
                    {
                        "name": "J√ºrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "J√ºrgen Schmidhuber"
                },
                "author": "J√ºrgen Schmidhuber",
                "arxiv_comment": "Project Website: https://gptswarm.org ; Github Repo:\n  https://github.com/metauto-ai/gptswarm . In Forty-first International\n  Conference on Machine Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12362v1",
                "updated": "2024-08-22T12:59:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    59,
                    5,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:59:05Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    59,
                    5,
                    3,
                    235,
                    0
                ],
                "title": "CLEANANERCorp: Identifying and Correcting Incorrect Labels in the\n  ANERcorp Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEANANERCorp: Identifying and Correcting Incorrect Labels in the\n  ANERcorp Dataset"
                },
                "summary": "Label errors are a common issue in machine learning datasets, particularly\nfor tasks such as Named Entity Recognition. Such label errors might hurt model\ntraining, affect evaluation results, and lead to an inaccurate assessment of\nmodel performance. In this study, we dived deep into one of the widely adopted\nArabic NER benchmark datasets (ANERcorp) and found a significant number of\nannotation errors, missing labels, and inconsistencies. Therefore, in this\nstudy, we conducted empirical research to understand these errors, correct them\nand propose a cleaner version of the dataset named CLEANANERCorp. CLEANANERCorp\nwill serve the research community as a more accurate and consistent benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label errors are a common issue in machine learning datasets, particularly\nfor tasks such as Named Entity Recognition. Such label errors might hurt model\ntraining, affect evaluation results, and lead to an inaccurate assessment of\nmodel performance. In this study, we dived deep into one of the widely adopted\nArabic NER benchmark datasets (ANERcorp) and found a significant number of\nannotation errors, missing labels, and inconsistencies. Therefore, in this\nstudy, we conducted empirical research to understand these errors, correct them\nand propose a cleaner version of the dataset named CLEANANERCorp. CLEANANERCorp\nwill serve the research community as a more accurate and consistent benchmark."
                },
                "authors": [
                    {
                        "name": "Mashael Al-Duwais"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Abdulmalik Al-Salman"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmalik Al-Salman"
                },
                "author": "Abdulmalik Al-Salman",
                "arxiv_comment": "Proceedings of the 6th Workshop on Open-Source Arabic Corpora and\n  Processing Tools (OSACT) with Shared Tasks on Arabic LLMs Hallucination and\n  Dialect to MSA Machine Translation @ LREC-COLING 2024",
                "arxiv_journal_ref": "ELRA and ICCL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12354v1",
                "updated": "2024-08-22T12:53:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    53,
                    2,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:53:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    53,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion with\n  Inference Acceleration via Latent Consistency Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion with\n  Inference Acceleration via Latent Consistency Distillation"
                },
                "summary": "Any-to-any singing voice conversion (SVC) aims to transfer a target singer's\ntimbre to other songs using a short voice sample. However many diffusion model\nbased any-to-any SVC methods, which have achieved impressive results, usually\nsuffered from low efficiency caused by a mass of inference steps. In this\npaper, we propose LCM-SVC, a latent consistency distillation (LCD) based latent\ndiffusion model (LDM) to accelerate inference speed. We achieved one-step or\nfew-step inference while maintaining the high performance by distilling a\npre-trained LDM based SVC model, which had the advantages of timbre decoupling\nand sound quality. Experimental results show that our proposed method can\nsignificantly reduce the inference time and largely preserve the sound quality\nand timbre similarity comparing with other state-of-the-art SVC models. Audio\nsamples are available at https://sounddemos.github.io/lcm-svc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-to-any singing voice conversion (SVC) aims to transfer a target singer's\ntimbre to other songs using a short voice sample. However many diffusion model\nbased any-to-any SVC methods, which have achieved impressive results, usually\nsuffered from low efficiency caused by a mass of inference steps. In this\npaper, we propose LCM-SVC, a latent consistency distillation (LCD) based latent\ndiffusion model (LDM) to accelerate inference speed. We achieved one-step or\nfew-step inference while maintaining the high performance by distilling a\npre-trained LDM based SVC model, which had the advantages of timbre decoupling\nand sound quality. Experimental results show that our proposed method can\nsignificantly reduce the inference time and largely preserve the sound quality\nand timbre similarity comparing with other state-of-the-art SVC models. Audio\nsamples are available at https://sounddemos.github.io/lcm-svc."
                },
                "authors": [
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Jianwei Cui"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Rilin Chen"
                    },
                    {
                        "name": "Lirong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Lirong Dai"
                },
                "author": "Lirong Dai",
                "arxiv_comment": "Accepted to ISCSLP 2024. arXiv admin note: text overlap with\n  arXiv:2406.05325",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03656v2",
                "updated": "2024-08-22T12:45:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    45,
                    54,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-04T05:54:19Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    54,
                    19,
                    3,
                    186,
                    0
                ],
                "title": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System"
                },
                "summary": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "DCASE WS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v1",
                "updated": "2024-08-22T12:43:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric philosophy. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric philosophy. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12346v1",
                "updated": "2024-08-22T12:41:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    41,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:41:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    41,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "A logical framework for data-driven reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A logical framework for data-driven reasoning"
                },
                "summary": "We introduce and investigate a family of consequence relations with the goal\nof capturing certain important patterns of data-driven inference. The inspiring\nidea for our framework is the fact that data may reject, possibly to some\ndegree, and possibly by mistake, any given scientific hypothesis. There is no\ngeneral agreement in science about how to do this, which motivates putting\nforward a logical formulation of the problem. We do so by investigating\ndistinct definitions of \"rejection degrees\" each yielding a consequence\nrelation. Our investigation leads to novel variations on the theme of rational\nconsequence relations, prominent among non-monotonic logics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and investigate a family of consequence relations with the goal\nof capturing certain important patterns of data-driven inference. The inspiring\nidea for our framework is the fact that data may reject, possibly to some\ndegree, and possibly by mistake, any given scientific hypothesis. There is no\ngeneral agreement in science about how to do this, which motivates putting\nforward a logical formulation of the problem. We do so by investigating\ndistinct definitions of \"rejection degrees\" each yielding a consequence\nrelation. Our investigation leads to novel variations on the theme of rational\nconsequence relations, prominent among non-monotonic logics."
                },
                "authors": [
                    {
                        "name": "Paolo Baldi"
                    },
                    {
                        "name": "Esther Anna Corsi"
                    },
                    {
                        "name": "Hykel Hosni"
                    }
                ],
                "author_detail": {
                    "name": "Hykel Hosni"
                },
                "author": "Hykel Hosni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03A10, 62A01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02616v4",
                "updated": "2024-08-22T12:40:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    40,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-03T09:41:42Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    41,
                    42,
                    0,
                    155,
                    0
                ],
                "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach"
                },
                "summary": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Xiaoxue Yu"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12339v1",
                "updated": "2024-08-22T12:35:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    35,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:35:13Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    35,
                    13,
                    3,
                    235,
                    0
                ],
                "title": "Inference for decorated graphs and application to multiplex networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for decorated graphs and application to multiplex networks"
                },
                "summary": "A graphon is a limiting object used to describe the behaviour of large\nnetworks through a function that captures the probability of edge formation\nbetween nodes. Although the merits of graphons to describe large and unlabelled\nnetworks are clear, they traditionally are used for describing only binary edge\ninformation, which limits their utility for more complex relational data.\nDecorated graphons were introduced to extend the graphon framework by\nincorporating richer relationships, such as edge weights and types. This\nspecificity in modelling connections provides more granular insight into\nnetwork dynamics. Yet, there are no existing inference techniques for decorated\ngraphons. We develop such an estimation method, extending existing techniques\nfrom traditional graphon estimation to accommodate these richer interactions.\nWe derive the rate of convergence for our method and show that it is consistent\nwith traditional non-parametric theory when the decoration space is finite.\nSimulations confirm that these theoretical rates are achieved in practice. Our\nmethod, tested on synthetic and empirical data, effectively captures additional\nedge information, resulting in improved network models. This advancement\nextends the scope of graphon estimation to encompass more complex networks,\nsuch as multiplex networks and attributed graphs, thereby increasing our\nunderstanding of their underlying structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A graphon is a limiting object used to describe the behaviour of large\nnetworks through a function that captures the probability of edge formation\nbetween nodes. Although the merits of graphons to describe large and unlabelled\nnetworks are clear, they traditionally are used for describing only binary edge\ninformation, which limits their utility for more complex relational data.\nDecorated graphons were introduced to extend the graphon framework by\nincorporating richer relationships, such as edge weights and types. This\nspecificity in modelling connections provides more granular insight into\nnetwork dynamics. Yet, there are no existing inference techniques for decorated\ngraphons. We develop such an estimation method, extending existing techniques\nfrom traditional graphon estimation to accommodate these richer interactions.\nWe derive the rate of convergence for our method and show that it is consistent\nwith traditional non-parametric theory when the decoration space is finite.\nSimulations confirm that these theoretical rates are achieved in practice. Our\nmethod, tested on synthetic and empirical data, effectively captures additional\nedge information, resulting in improved network models. This advancement\nextends the scope of graphon estimation to encompass more complex networks,\nsuch as multiplex networks and attributed graphs, thereby increasing our\nunderstanding of their underlying structures."
                },
                "authors": [
                    {
                        "name": "Charles Dufour"
                    },
                    {
                        "name": "Sofia C. Olhede"
                    }
                ],
                "author_detail": {
                    "name": "Sofia C. Olhede"
                },
                "author": "Sofia C. Olhede",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05241v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05241v5",
                "updated": "2024-08-22T12:30:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    30,
                    44,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-08T07:11:33Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    7,
                    11,
                    33,
                    0,
                    99,
                    0
                ],
                "title": "LightFF: Lightweight Inference for Forward-Forward Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightFF: Lightweight Inference for Forward-Forward Algorithm"
                },
                "summary": "The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF."
                },
                "authors": [
                    {
                        "name": "Amin Aminifar"
                    },
                    {
                        "name": "Baichuan Huang"
                    },
                    {
                        "name": "Azra Abtahi"
                    },
                    {
                        "name": "Amir Aminifar"
                    }
                ],
                "author_detail": {
                    "name": "Amir Aminifar"
                },
                "author": "Amir Aminifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05241v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05241v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v1",
                "updated": "2024-08-22T12:21:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Graph Retrieval Augmented Trustworthiness Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Trustworthiness Reasoning"
                },
                "summary": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12326v1",
                "updated": "2024-08-22T12:04:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    4,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:04:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    4,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "Interactive DualChecker for Mitigating Hallucinations in Distilling\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive DualChecker for Mitigating Hallucinations in Distilling\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various machine learning (ML) tasks. Given the high costs of creating\nannotated datasets for supervised learning, LLMs offer a valuable alternative\nby enabling effective few-shot in-context learning. However, these models can\nproduce hallucinations, particularly in domains with incomplete knowledge.\nAdditionally, current methods for knowledge distillation using LLMs often\nstruggle to enhance the effectiveness of both teacher and student models. To\naddress these challenges, we introduce DualChecker, an innovative framework\ndesigned to mitigate hallucinations and improve the performance of both teacher\nand student models during knowledge distillation. DualChecker employs\nContextAligner to ensure that the context provided by teacher models aligns\nwith human labeling standards. It also features a dynamic checker system that\nenhances model interaction: one component re-prompts teacher models with more\ndetailed content when they show low confidence, and another identifies\nborderline cases from student models to refine the teaching templates. This\ninteractive process promotes continuous improvement and effective knowledge\ntransfer between the models. We evaluate DualChecker using a green innovation\ntextual dataset that includes binary, multiclass, and token classification\ntasks. The experimental results show that DualChecker significantly outperforms\nexisting state-of-the-art methods, achieving up to a 17% improvement in F1\nscore for teacher models and 10% for student models. Notably, student models\nfine-tuned with LLM predictions perform comparably to those fine-tuned with\nactual data, even in a challenging domain. We make all datasets, models, and\ncode from this research publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various machine learning (ML) tasks. Given the high costs of creating\nannotated datasets for supervised learning, LLMs offer a valuable alternative\nby enabling effective few-shot in-context learning. However, these models can\nproduce hallucinations, particularly in domains with incomplete knowledge.\nAdditionally, current methods for knowledge distillation using LLMs often\nstruggle to enhance the effectiveness of both teacher and student models. To\naddress these challenges, we introduce DualChecker, an innovative framework\ndesigned to mitigate hallucinations and improve the performance of both teacher\nand student models during knowledge distillation. DualChecker employs\nContextAligner to ensure that the context provided by teacher models aligns\nwith human labeling standards. It also features a dynamic checker system that\nenhances model interaction: one component re-prompts teacher models with more\ndetailed content when they show low confidence, and another identifies\nborderline cases from student models to refine the teaching templates. This\ninteractive process promotes continuous improvement and effective knowledge\ntransfer between the models. We evaluate DualChecker using a green innovation\ntextual dataset that includes binary, multiclass, and token classification\ntasks. The experimental results show that DualChecker significantly outperforms\nexisting state-of-the-art methods, achieving up to a 17% improvement in F1\nscore for teacher models and 10% for student models. Notably, student models\nfine-tuned with LLM predictions perform comparably to those fine-tuned with\nactual data, even in a challenging domain. We make all datasets, models, and\ncode from this research publicly available."
                },
                "authors": [
                    {
                        "name": "Meiyun Wang"
                    },
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoshi Izumi"
                },
                "author": "Kiyoshi Izumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v1",
                "updated": "2024-08-22T12:00:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Hallucination Mitigation in LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12320v1",
                "updated": "2024-08-22T11:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    57,
                    7,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:57:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    57,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "PolyRouter: A Multi-LLM Querying System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyRouter: A Multi-LLM Querying System"
                },
                "summary": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%."
                },
                "authors": [
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Alay Shah"
                    },
                    {
                        "name": "Han Jin"
                    },
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "arxiv_comment": "14 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02232v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02232v3",
                "updated": "2024-08-22T11:54:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    54,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-05T04:53:01Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    4,
                    53,
                    1,
                    0,
                    218,
                    0
                ],
                "title": "SpecRover: Code Intent Extraction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRover: Code Intent Extraction via LLMs"
                },
                "summary": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era."
                },
                "authors": [
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "Haifeng Ruan and Yuntong Zhang contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02232v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02232v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v2",
                "updated": "2024-08-22T11:49:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    49,
                    15,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12315v1",
                "updated": "2024-08-22T11:41:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    41,
                    35,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:41:35Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    41,
                    35,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\n  Applications via Tailored Problem-Solving Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\n  Applications via Tailored Problem-Solving Demonstrations"
                },
                "summary": "Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "preprint / under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12293v1",
                "updated": "2024-08-22T11:06:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    6,
                    18,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:06:18Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    6,
                    18,
                    3,
                    235,
                    0
                ],
                "title": "AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network"
                },
                "summary": "In the training and inference of spiking neural networks (SNNs), direct\ntraining and lightweight computation methods have been orthogonally developed,\naimed at reducing power consumption. However, only a limited number of\napproaches have applied these two mechanisms simultaneously and failed to fully\nleverage the advantages of SNN-based vision transformers (ViTs) since they were\noriginally designed for convolutional neural networks (CNNs). In this paper, we\npropose AT-SNN designed to dynamically adjust the number of tokens processed\nduring inference in SNN-based ViTs with direct training, wherein power\nconsumption is proportional to the number of tokens. We first demonstrate the\napplicability of adaptive computation time (ACT), previously limited to RNNs\nand ViTs, to SNN-based ViTs, enhancing it to discard less informative spatial\ntokens selectively. Also, we propose a new token-merge mechanism that relies on\nthe similarity of tokens, which further reduces the number of tokens while\nenhancing accuracy. We implement AT-SNN to Spikformer and show the\neffectiveness of AT-SNN in achieving high energy efficiency and accuracy\ncompared to state-of-the-art approaches on the image classification tasks,\nCIFAR10, CIFAR-100, and TinyImageNet. For example, our approach uses up to\n42.4% fewer tokens than the existing best-performing method on CIFAR-100, while\nconserving higher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the training and inference of spiking neural networks (SNNs), direct\ntraining and lightweight computation methods have been orthogonally developed,\naimed at reducing power consumption. However, only a limited number of\napproaches have applied these two mechanisms simultaneously and failed to fully\nleverage the advantages of SNN-based vision transformers (ViTs) since they were\noriginally designed for convolutional neural networks (CNNs). In this paper, we\npropose AT-SNN designed to dynamically adjust the number of tokens processed\nduring inference in SNN-based ViTs with direct training, wherein power\nconsumption is proportional to the number of tokens. We first demonstrate the\napplicability of adaptive computation time (ACT), previously limited to RNNs\nand ViTs, to SNN-based ViTs, enhancing it to discard less informative spatial\ntokens selectively. Also, we propose a new token-merge mechanism that relies on\nthe similarity of tokens, which further reduces the number of tokens while\nenhancing accuracy. We implement AT-SNN to Spikformer and show the\neffectiveness of AT-SNN in achieving high energy efficiency and accuracy\ncompared to state-of-the-art approaches on the image classification tasks,\nCIFAR10, CIFAR-100, and TinyImageNet. For example, our approach uses up to\n42.4% fewer tokens than the existing best-performing method on CIFAR-100, while\nconserving higher accuracy."
                },
                "authors": [
                    {
                        "name": "Donghwa Kang"
                    },
                    {
                        "name": "Youngmoon Lee"
                    },
                    {
                        "name": "Eun-Kyu Lee"
                    },
                    {
                        "name": "Brent Kang"
                    },
                    {
                        "name": "Jinkyu Lee"
                    },
                    {
                        "name": "Hyeongboo Baek"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongboo Baek"
                },
                "author": "Hyeongboo Baek",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12292v1",
                "updated": "2024-08-22T11:04:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    4,
                    28,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:04:28Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    4,
                    28,
                    3,
                    235,
                    0
                ],
                "title": "Towards Deconfounded Image-Text Matching with Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Deconfounded Image-Text Matching with Causal Inference"
                },
                "summary": "Prior image-text matching methods have shown remarkable performance on many\nbenchmark datasets, but most of them overlook the bias in the dataset, which\nexists in intra-modal and inter-modal, and tend to learn the spurious\ncorrelations that extremely degrade the generalization ability of the model.\nFurthermore, these methods often incorporate biased external knowledge from\nlarge-scale datasets as prior knowledge into image-text matching model, which\nis inevitable to force model further learn biased associations. To address\nabove limitations, this paper firstly utilizes Structural Causal Models (SCMs)\nto illustrate how intra- and inter-modal confounders damage the image-text\nmatching. Then, we employ backdoor adjustment to propose an innovative\nDeconfounded Causal Inference Network (DCIN) for image-text matching task. DCIN\n(1) decomposes the intra- and inter-modal confounders and incorporates them\ninto the encoding stage of visual and textual features, effectively eliminating\nthe spurious correlations during image-text matching, and (2) uses causal\ninference to mitigate biases of external knowledge. Consequently, the model can\nlearn causality instead of spurious correlations caused by dataset bias.\nExtensive experiments on two well-known benchmark datasets, i.e., Flickr30K and\nMSCOCO, demonstrate the superiority of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior image-text matching methods have shown remarkable performance on many\nbenchmark datasets, but most of them overlook the bias in the dataset, which\nexists in intra-modal and inter-modal, and tend to learn the spurious\ncorrelations that extremely degrade the generalization ability of the model.\nFurthermore, these methods often incorporate biased external knowledge from\nlarge-scale datasets as prior knowledge into image-text matching model, which\nis inevitable to force model further learn biased associations. To address\nabove limitations, this paper firstly utilizes Structural Causal Models (SCMs)\nto illustrate how intra- and inter-modal confounders damage the image-text\nmatching. Then, we employ backdoor adjustment to propose an innovative\nDeconfounded Causal Inference Network (DCIN) for image-text matching task. DCIN\n(1) decomposes the intra- and inter-modal confounders and incorporates them\ninto the encoding stage of visual and textual features, effectively eliminating\nthe spurious correlations during image-text matching, and (2) uses causal\ninference to mitigate biases of external knowledge. Consequently, the model can\nlearn causality instead of spurious correlations caused by dataset bias.\nExtensive experiments on two well-known benchmark datasets, i.e., Flickr30K and\nMSCOCO, demonstrate the superiority of our proposed method."
                },
                "authors": [
                    {
                        "name": "Wenhui Li"
                    },
                    {
                        "name": "Xinqi Su"
                    },
                    {
                        "name": "Dan Song"
                    },
                    {
                        "name": "Lanjun Wang"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "An-An Liu"
                    }
                ],
                "author_detail": {
                    "name": "An-An Liu"
                },
                "author": "An-An Liu",
                "arxiv_doi": "10.1145/3581783.3612472",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3581783.3612472",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM MM",
                "arxiv_journal_ref": "2023/10/26,Proceedings of the 31st ACM International Conference on\n  Multimedia,6264-6273",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06353v2",
                "updated": "2024-08-22T10:21:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    21,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2023-04-13T09:19:35Z",
                "published_parsed": [
                    2023,
                    4,
                    13,
                    9,
                    19,
                    35,
                    3,
                    103,
                    0
                ],
                "title": "Bayesian mixture models for phylogenetic source attribution from\n  consensus sequences and time since infection estimates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian mixture models for phylogenetic source attribution from\n  consensus sequences and time since infection estimates"
                },
                "summary": "In stopping the spread of infectious diseases, pathogen genomic data can be\nused to reconstruct transmission events and characterize population-level\nsources of infection. Most approaches for identifying transmission pairs do not\naccount for the time passing since divergence of pathogen variants in\nindividuals, which is problematic in viruses with high within-host evolutionary\nrates. This prompted us to consider possible transmission pairs in terms of\nphylogenetic data and additional estimates of time since infection derived from\nclinical biomarkers. We develop Bayesian mixture models with an evolutionary\nclock as signal component and additional mixed effects or covariate random\nfunctions describing the mixing weights to classify potential pairs into likely\nand unlikely transmission pairs. We demonstrate that although sources cannot be\nidentified at the individual level with certainty, even with the additional\ndata on time elapsed, inferences into the population-level sources of\ntransmission are possible, and more accurate than using only phylogenetic data\nwithout time since infection estimates. We apply the approach to estimate\nage-specific sources of HIV infection in Amsterdam MSM transmission networks\nbetween 2010-2021. This study demonstrates that infection time estimates\nprovide informative data to characterize transmission sources, and shows how\nphylogenetic source attribution can then be done with multi-dimensional mixture\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In stopping the spread of infectious diseases, pathogen genomic data can be\nused to reconstruct transmission events and characterize population-level\nsources of infection. Most approaches for identifying transmission pairs do not\naccount for the time passing since divergence of pathogen variants in\nindividuals, which is problematic in viruses with high within-host evolutionary\nrates. This prompted us to consider possible transmission pairs in terms of\nphylogenetic data and additional estimates of time since infection derived from\nclinical biomarkers. We develop Bayesian mixture models with an evolutionary\nclock as signal component and additional mixed effects or covariate random\nfunctions describing the mixing weights to classify potential pairs into likely\nand unlikely transmission pairs. We demonstrate that although sources cannot be\nidentified at the individual level with certainty, even with the additional\ndata on time elapsed, inferences into the population-level sources of\ntransmission are possible, and more accurate than using only phylogenetic data\nwithout time since infection estimates. We apply the approach to estimate\nage-specific sources of HIV infection in Amsterdam MSM transmission networks\nbetween 2010-2021. This study demonstrates that infection time estimates\nprovide informative data to characterize transmission sources, and shows how\nphylogenetic source attribution can then be done with multi-dimensional mixture\nmodels."
                },
                "authors": [
                    {
                        "name": "Alexandra Blenkinsop"
                    },
                    {
                        "name": "Lysandros Sofocleous"
                    },
                    {
                        "name": "Francesco Di Lauro"
                    },
                    {
                        "name": "Evangelia Georgia Kostaki"
                    },
                    {
                        "name": "Ard van Sighem"
                    },
                    {
                        "name": "Daniela Bezemer"
                    },
                    {
                        "name": "Thijs van de Laar"
                    },
                    {
                        "name": "Peter Reiss"
                    },
                    {
                        "name": "Godelieve de Bree"
                    },
                    {
                        "name": "Nikos Pantazis"
                    },
                    {
                        "name": "Oliver Ratmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Ratmann"
                },
                "author": "Oliver Ratmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12270v1",
                "updated": "2024-08-22T10:08:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    8,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T10:08:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    8,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Variance reduction of diffusion model's gradients with Taylor\n  approximation-based control variate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variance reduction of diffusion model's gradients with Taylor\n  approximation-based control variate"
                },
                "summary": "Score-based models, trained with denoising score matching, are remarkably\neffective in generating high dimensional data. However, the high variance of\ntheir training objective hinders optimisation. We attempt to reduce it with a\ncontrol variate, derived via a $k$-th order Taylor expansion on the training\nobjective and its gradient. We prove an equivalence between the two and\ndemonstrate empirically the effectiveness of our approach on a low dimensional\nproblem setting; and study its effect on larger problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score-based models, trained with denoising score matching, are remarkably\neffective in generating high dimensional data. However, the high variance of\ntheir training objective hinders optimisation. We attempt to reduce it with a\ncontrol variate, derived via a $k$-th order Taylor expansion on the training\nobjective and its gradient. We prove an equivalence between the two and\ndemonstrate empirically the effectiveness of our approach on a low dimensional\nproblem setting; and study its effect on larger problems."
                },
                "authors": [
                    {
                        "name": "Paul Jeha"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Riis Andersen"
                    },
                    {
                        "name": "Carl Henrik Ek"
                    },
                    {
                        "name": "Jes Frellsen"
                    }
                ],
                "author_detail": {
                    "name": "Jes Frellsen"
                },
                "author": "Jes Frellsen",
                "arxiv_comment": "14 pages, ICML Structured Probabilistic Inference & Generative\n  Modeling 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12263v1",
                "updated": "2024-08-22T10:00:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    0,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T10:00:20Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    0,
                    20,
                    3,
                    235,
                    0
                ],
                "title": "Toward the Evaluation of Large Language Models Considering Score\n  Variance across Instruction Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward the Evaluation of Large Language Models Considering Score\n  Variance across Instruction Templates"
                },
                "summary": "The natural language understanding (NLU) performance of large language models\n(LLMs) has been evaluated across various tasks and datasets. The existing\nevaluation methods, however, do not take into account the variance in scores\ndue to differences in prompts, which leads to unfair evaluation and comparison\nof NLU performance. Moreover, evaluation designed for specific prompts is\ninappropriate for instruction tuning, which aims to perform well with any\nprompt. It is therefore necessary to find a way to measure NLU performance in a\nfair manner, considering score variance between different instruction\ntemplates. In this study, we provide English and Japanese cross-lingual\ndatasets for evaluating the NLU performance of LLMs, which include multiple\ninstruction templates for fair evaluation of each task, along with regular\nexpressions to constrain the output format. Furthermore, we propose the Sharpe\nscore as an evaluation metric that takes into account the variance in scores\nbetween templates. Comprehensive analysis of English and Japanese LLMs reveals\nthat the high variance among templates has a significant impact on the fair\nevaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The natural language understanding (NLU) performance of large language models\n(LLMs) has been evaluated across various tasks and datasets. The existing\nevaluation methods, however, do not take into account the variance in scores\ndue to differences in prompts, which leads to unfair evaluation and comparison\nof NLU performance. Moreover, evaluation designed for specific prompts is\ninappropriate for instruction tuning, which aims to perform well with any\nprompt. It is therefore necessary to find a way to measure NLU performance in a\nfair manner, considering score variance between different instruction\ntemplates. In this study, we provide English and Japanese cross-lingual\ndatasets for evaluating the NLU performance of LLMs, which include multiple\ninstruction templates for fair evaluation of each task, along with regular\nexpressions to constrain the output format. Furthermore, we propose the Sharpe\nscore as an evaluation metric that takes into account the variance in scores\nbetween templates. Comprehensive analysis of English and Japanese LLMs reveals\nthat the high variance among templates has a significant impact on the fair\nevaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Adam Nohejl"
                    },
                    {
                        "name": "Jiangnan Hang"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12259v1",
                "updated": "2024-08-22T09:57:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    57,
                    57,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:57:57Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    57,
                    57,
                    3,
                    235,
                    0
                ],
                "title": "Can You Trust Your Metric? Automatic Concatenation-Based Tests for\n  Metric Validity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Trust Your Metric? Automatic Concatenation-Based Tests for\n  Metric Validity"
                },
                "summary": "Consider a scenario where a harmfulness detection metric is employed by a\nsystem to filter unsafe responses generated by a Large Language Model. When\nanalyzing individual harmful and unethical prompt-response pairs, the metric\ncorrectly classifies each pair as highly unsafe, assigning the highest score.\nHowever, when these same prompts and responses are concatenated, the metric's\ndecision flips, assigning the lowest possible score, thereby misclassifying the\ncontent as safe and allowing it to bypass the filter. In this study, we\ndiscovered that several harmfulness LLM-based metrics, including GPT-based,\nexhibit this decision-flipping phenomenon. Additionally, we found that even an\nadvanced metric like GPT-4o is highly sensitive to input order. Specifically,\nit tends to classify responses as safe if the safe content appears first,\nregardless of any harmful content that follows, and vice versa. This work\nintroduces automatic concatenation-based tests to assess the fundamental\nproperties a valid metric should satisfy. We applied these tests in a model\nsafety scenario to assess the reliability of harmfulness detection metrics,\nuncovering a number of inconsistencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider a scenario where a harmfulness detection metric is employed by a\nsystem to filter unsafe responses generated by a Large Language Model. When\nanalyzing individual harmful and unethical prompt-response pairs, the metric\ncorrectly classifies each pair as highly unsafe, assigning the highest score.\nHowever, when these same prompts and responses are concatenated, the metric's\ndecision flips, assigning the lowest possible score, thereby misclassifying the\ncontent as safe and allowing it to bypass the filter. In this study, we\ndiscovered that several harmfulness LLM-based metrics, including GPT-based,\nexhibit this decision-flipping phenomenon. Additionally, we found that even an\nadvanced metric like GPT-4o is highly sensitive to input order. Specifically,\nit tends to classify responses as safe if the safe content appears first,\nregardless of any harmful content that follows, and vice versa. This work\nintroduces automatic concatenation-based tests to assess the fundamental\nproperties a valid metric should satisfy. We applied these tests in a model\nsafety scenario to assess the reliability of harmfulness detection metrics,\nuncovering a number of inconsistencies."
                },
                "authors": [
                    {
                        "name": "Ora Nova Fandina"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "George Kour"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Orna Raz"
                    }
                ],
                "author_detail": {
                    "name": "Orna Raz"
                },
                "author": "Orna Raz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v2",
                "updated": "2024-08-22T09:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    45,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12253v1",
                "updated": "2024-08-22T09:45:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    45,
                    24,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:45:24Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    45,
                    24,
                    3,
                    235,
                    0
                ],
                "title": "Epsilon: Exploring Comprehensive Visual-Semantic Projection for\n  Multi-Label Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epsilon: Exploring Comprehensive Visual-Semantic Projection for\n  Multi-Label Zero-Shot Learning"
                },
                "summary": "This paper investigates a challenging problem of zero-shot learning in the\nmulti-label scenario (MLZSL), wherein the model is trained to recognize\nmultiple unseen classes within a sample (e.g., an image) based on seen classes\nand auxiliary knowledge, e.g., semantic information. Existing methods usually\nresort to analyzing the relationship of various seen classes residing in a\nsample from the dimension of spatial or semantic characteristics and\ntransferring the learned model to unseen ones. However, they neglect the\nintegrity of local and global features. Although the use of the attention\nstructure will accurately locate local features, especially objects, it will\nsignificantly lose its integrity, and the relationship between classes will\nalso be affected. Rough processing of global features will also directly affect\ncomprehensiveness. This neglect will make the model lose its grasp of the main\ncomponents of the image. Relying only on the local existence of seen classes\nduring the inference stage introduces unavoidable bias. In this paper, we\npropose a novel and comprehensive visual-semantic framework for MLZSL, dubbed\nEpsilon, to fully make use of such properties and enable a more accurate and\nrobust visual-semantic projection. In terms of spatial information, we achieve\neffective refinement by group aggregating image features into several semantic\nprompts. It can aggregate semantic information rather than class information,\npreserving the correlation between semantics. In terms of global semantics, we\nuse global forward propagation to collect as much information as possible to\nensure that semantics are not omitted. Experiments on large-scale MLZSL\nbenchmark datasets NUS-Wide and Open-Images-v4 demonstrate that the proposed\nEpsilon outperforms other state-of-the-art methods with large margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a challenging problem of zero-shot learning in the\nmulti-label scenario (MLZSL), wherein the model is trained to recognize\nmultiple unseen classes within a sample (e.g., an image) based on seen classes\nand auxiliary knowledge, e.g., semantic information. Existing methods usually\nresort to analyzing the relationship of various seen classes residing in a\nsample from the dimension of spatial or semantic characteristics and\ntransferring the learned model to unseen ones. However, they neglect the\nintegrity of local and global features. Although the use of the attention\nstructure will accurately locate local features, especially objects, it will\nsignificantly lose its integrity, and the relationship between classes will\nalso be affected. Rough processing of global features will also directly affect\ncomprehensiveness. This neglect will make the model lose its grasp of the main\ncomponents of the image. Relying only on the local existence of seen classes\nduring the inference stage introduces unavoidable bias. In this paper, we\npropose a novel and comprehensive visual-semantic framework for MLZSL, dubbed\nEpsilon, to fully make use of such properties and enable a more accurate and\nrobust visual-semantic projection. In terms of spatial information, we achieve\neffective refinement by group aggregating image features into several semantic\nprompts. It can aggregate semantic information rather than class information,\npreserving the correlation between semantics. In terms of global semantics, we\nuse global forward propagation to collect as much information as possible to\nensure that semantics are not omitted. Experiments on large-scale MLZSL\nbenchmark datasets NUS-Wide and Open-Images-v4 demonstrate that the proposed\nEpsilon outperforms other state-of-the-art methods with large margins."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocheng Lu"
                },
                "author": "Xiaocheng Lu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2309.00923",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12251v1",
                "updated": "2024-08-22T09:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    39,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    39,
                    20,
                    3,
                    235,
                    0
                ],
                "title": "Earth-like planets hosting systems: Architecture and properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth-like planets hosting systems: Architecture and properties"
                },
                "summary": "The discovery of Earth-like planets is a major focus of current planetology\nresearch and faces a significant technological challenge. Indeed, when it comes\nto detecting planets as small and cold as the Earth, the cost of observation\ntime is massive. Understanding in what type of systems Earth-like planets\n(ELPs) form and how to identify them is crucial for preparing future missions\nsuch as PLATO, LIFE, or others. Theoretical models suggest that ELPs\npredominantly form within a certain type of system architecture. Therefore, the\npresence or absence of ELPs could be inferred from the arrangement of other\nplanets within the same system. This study aims to identify the profile of a\ntypical system that harbours an ELP by investigating the architecture of\nsystems and the properties of their innermost detectable planets. Here, we\nintroduce a novel method for determining the architecture of planetary systems\nand categorising them into four distinct classes. Using three populations of\nsynthetic planetary systems generated using the Bern model around three\ndifferent types of stars, we studied the `theoretical' architecture (the\narchitecture of a complete planetary system) and the `biased' architecture (the\narchitecture of a system in which only detectable planets are taken into\naccount) of the synthetic systems. The biased architecture of a system, studied\nin conjunction with the mass, radius, and period of the innermost detectable\nplanet, appears to correlate with the presence or absence of an ELP in the same\nsystem. We conclude that the detections of ELPs can be predicted thanks to the\nalready known properties of their systems, and we present a list of the\nproperties of the systems most likely to host such a planet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of Earth-like planets is a major focus of current planetology\nresearch and faces a significant technological challenge. Indeed, when it comes\nto detecting planets as small and cold as the Earth, the cost of observation\ntime is massive. Understanding in what type of systems Earth-like planets\n(ELPs) form and how to identify them is crucial for preparing future missions\nsuch as PLATO, LIFE, or others. Theoretical models suggest that ELPs\npredominantly form within a certain type of system architecture. Therefore, the\npresence or absence of ELPs could be inferred from the arrangement of other\nplanets within the same system. This study aims to identify the profile of a\ntypical system that harbours an ELP by investigating the architecture of\nsystems and the properties of their innermost detectable planets. Here, we\nintroduce a novel method for determining the architecture of planetary systems\nand categorising them into four distinct classes. Using three populations of\nsynthetic planetary systems generated using the Bern model around three\ndifferent types of stars, we studied the `theoretical' architecture (the\narchitecture of a complete planetary system) and the `biased' architecture (the\narchitecture of a system in which only detectable planets are taken into\naccount) of the synthetic systems. The biased architecture of a system, studied\nin conjunction with the mass, radius, and period of the innermost detectable\nplanet, appears to correlate with the presence or absence of an ELP in the same\nsystem. We conclude that the detections of ELPs can be predicted thanks to the\nalready known properties of their systems, and we present a list of the\nproperties of the systems most likely to host such a planet."
                },
                "authors": [
                    {
                        "name": "Jeanne Davoult"
                    },
                    {
                        "name": "Yann Alibert"
                    },
                    {
                        "name": "Lokesh Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Lokesh Mishra"
                },
                "author": "Lokesh Mishra",
                "arxiv_doi": "10.1051/0004-6361/202449330",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202449330",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 8 figures, accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12249v1",
                "updated": "2024-08-22T09:37:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    37,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:37:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    37,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"
                },
                "summary": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extration. To\nbreach this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end we evaluate various open LLMs -- including BioMistral and Llama-2\nmodels -- on a diverse set of biomedical datasets, using standard prompting,\nChain-of-Thought (CoT) and Self-Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extration. To\nbreach this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end we evaluate various open LLMs -- including BioMistral and Llama-2\nmodels -- on a diverse set of biomedical datasets, using standard prompting,\nChain-of-Thought (CoT) and Self-Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications."
                },
                "authors": [
                    {
                        "name": "Aishik Nagar"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuping Wu"
                    },
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12247v2",
                "updated": "2024-08-23T01:25:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    25,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T09:36:15Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    36,
                    15,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution."
                },
                "authors": [
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Pengtian Zhu"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jiagang Wang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Dongwen Li"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Qianying Guo"
                    },
                    {
                        "name": "Xiaolei Hua"
                    },
                    {
                        "name": "Lin Zhu"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12246v1",
                "updated": "2024-08-22T09:33:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    33,
                    25,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:33:25Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    33,
                    25,
                    3,
                    235,
                    0
                ],
                "title": "OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text\n  Alignment and Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text\n  Alignment and Fusion"
                },
                "summary": "Aerial object detection has been a hot topic for many years due to its wide\napplication requirements. However, most existing approaches can only handle\npredefined categories, which limits their applicability for the open scenarios\nin real-world. In this paper, we extend aerial object detection to open\nscenarios by exploiting the relationship between image and text, and propose\nOVA-DETR, a high-efficiency open-vocabulary detector for aerial images.\nSpecifically, based on the idea of image-text alignment, we propose region-text\ncontrastive loss to replace the category regression loss in the traditional\ndetection framework, which breaks the category limitation. Then, we propose\nBidirectional Vision-Language Fusion (Bi-VLF), which includes a dual-attention\nfusion encoder and a multi-level text-guided Fusion Decoder. The dual-attention\nfusion encoder enhances the feature extraction process in the encoder part. The\nmulti-level text-guided Fusion Decoder is designed to improve the detection\nability for small objects, which frequently appear in aerial object detection\nscenarios. Experimental results on three widely used benchmark datasets show\nthat our proposed method significantly improves the mAP and recall, while\nenjoying faster inference speed. For instance, in zero shot detection\nexperiments on DIOR, the proposed OVA-DETR outperforms DescReg and YOLO-World\nby 37.4% and 33.1%, respectively, while achieving 87 FPS inference speed, which\nis 7.9x faster than DescReg and 3x faster than YOLO-world. The code is\navailable at https://github.com/GT-Wei/OVA-DETR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial object detection has been a hot topic for many years due to its wide\napplication requirements. However, most existing approaches can only handle\npredefined categories, which limits their applicability for the open scenarios\nin real-world. In this paper, we extend aerial object detection to open\nscenarios by exploiting the relationship between image and text, and propose\nOVA-DETR, a high-efficiency open-vocabulary detector for aerial images.\nSpecifically, based on the idea of image-text alignment, we propose region-text\ncontrastive loss to replace the category regression loss in the traditional\ndetection framework, which breaks the category limitation. Then, we propose\nBidirectional Vision-Language Fusion (Bi-VLF), which includes a dual-attention\nfusion encoder and a multi-level text-guided Fusion Decoder. The dual-attention\nfusion encoder enhances the feature extraction process in the encoder part. The\nmulti-level text-guided Fusion Decoder is designed to improve the detection\nability for small objects, which frequently appear in aerial object detection\nscenarios. Experimental results on three widely used benchmark datasets show\nthat our proposed method significantly improves the mAP and recall, while\nenjoying faster inference speed. For instance, in zero shot detection\nexperiments on DIOR, the proposed OVA-DETR outperforms DescReg and YOLO-World\nby 37.4% and 33.1%, respectively, while achieving 87 FPS inference speed, which\nis 7.9x faster than DescReg and 3x faster than YOLO-world. The code is\navailable at https://github.com/GT-Wei/OVA-DETR."
                },
                "authors": [
                    {
                        "name": "Guoting Wei"
                    },
                    {
                        "name": "Xia Yuan"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zhenhao Shang"
                    },
                    {
                        "name": "Kelu Yao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Qingsen Yan"
                    },
                    {
                        "name": "Chunxia Zhao"
                    },
                    {
                        "name": "Haokui Zhang"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12245v1",
                "updated": "2024-08-22T09:27:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    27,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:27:49Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    27,
                    49,
                    3,
                    235,
                    0
                ],
                "title": "Scalable Autoregressive Image Generation with Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Autoregressive Image Generation with Mamba"
                },
                "summary": "We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM"
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Xuerui Qiu"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02349v3",
                "updated": "2024-08-22T09:25:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    25,
                    51,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-05T09:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    54,
                    8,
                    0,
                    218,
                    0
                ],
                "title": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning"
                },
                "summary": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Huy Hoang Nguyen"
                    },
                    {
                        "name": "Egor Panfilov"
                    },
                    {
                        "name": "Aleksei Tiulpin"
                    }
                ],
                "author_detail": {
                    "name": "Aleksei Tiulpin"
                },
                "author": "Aleksei Tiulpin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12239v1",
                "updated": "2024-08-22T09:16:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    16,
                    36,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:16:36Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    16,
                    36,
                    3,
                    235,
                    0
                ],
                "title": "Fast Burst-Sparsity Learning Approach for Massive MIMO-OTFS Channel\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Burst-Sparsity Learning Approach for Massive MIMO-OTFS Channel\n  Estimation"
                },
                "summary": "Accurate channel estimation in orthogonal time frequency space (OTFS) systems\nwith massive multiple-input multiple-output (MIMO) configurations is\nchallenging due to high-dimensional sparse representation (SR). Existing\nmethods often face performance degradation and/or high computational\ncomplexity. To address these issues and exploit intricate channel sparsity\nstructure, this letter first leverages a novel hybrid burst-sparsity prior to\ncapture the burst/common sparse structure in the angle/delay domain, and then\nutilizes an independent variational Bayesian inference (VBI) factorization\ntechnique to efficiently solve the high-dimensional SR problem. Additionally,\nan angle/Doppler refinement approach is incorporated into the proposed method\nto automatically mitigate off-grid mismatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation in orthogonal time frequency space (OTFS) systems\nwith massive multiple-input multiple-output (MIMO) configurations is\nchallenging due to high-dimensional sparse representation (SR). Existing\nmethods often face performance degradation and/or high computational\ncomplexity. To address these issues and exploit intricate channel sparsity\nstructure, this letter first leverages a novel hybrid burst-sparsity prior to\ncapture the burst/common sparse structure in the angle/delay domain, and then\nutilizes an independent variational Bayesian inference (VBI) factorization\ntechnique to efficiently solve the high-dimensional SR problem. Additionally,\nan angle/Doppler refinement approach is incorporated into the proposed method\nto automatically mitigate off-grid mismatches."
                },
                "authors": [
                    {
                        "name": "Ming Ma"
                    },
                    {
                        "name": "Jishen gDai"
                    },
                    {
                        "name": "Xue-Qin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xue-Qin Jiang"
                },
                "author": "Xue-Qin Jiang",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12236v1",
                "updated": "2024-08-22T09:10:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    10,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:10:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    10,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for\n  Dynamic Medical Image Generation in Virtual Simulated Patient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for\n  Dynamic Medical Image Generation in Virtual Simulated Patient"
                },
                "summary": "Medical education relies heavily on Simulated Patients (SPs) to provide a\nsafe environment for students to practice clinical skills, including medical\nimage analysis. However, the high cost of recruiting qualified SPs and the lack\nof diverse medical imaging datasets have presented significant challenges. To\naddress these issues, this paper introduces MedDiT, a novel\nknowledge-controlled conversational framework that can dynamically generate\nplausible medical images aligned with simulated patient symptoms, enabling\ndiverse diagnostic skill training. Specifically, MedDiT integrates various\npatient Knowledge Graphs (KGs), which describe the attributes and symptoms of\npatients, to dynamically prompt Large Language Models' (LLMs) behavior and\ncontrol the patient characteristics, mitigating hallucination during medical\nconversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is\nincorporated to generate medical images according to the specified patient\nattributes in the KG. In this paper, we present the capabilities of MedDiT\nthrough a practical demonstration, showcasing its ability to act in diverse\nsimulated patient cases and generate the corresponding medical images. This can\nprovide an abundant and interactive learning experience for students, advancing\nmedical education by offering an immersive simulation platform for future\nhealthcare professionals. The work sheds light on the feasibility of\nincorporating advanced technologies like LLM, KG, and DiT in education\napplications, highlighting their potential to address the challenges faced in\nsimulated patient-based medical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical education relies heavily on Simulated Patients (SPs) to provide a\nsafe environment for students to practice clinical skills, including medical\nimage analysis. However, the high cost of recruiting qualified SPs and the lack\nof diverse medical imaging datasets have presented significant challenges. To\naddress these issues, this paper introduces MedDiT, a novel\nknowledge-controlled conversational framework that can dynamically generate\nplausible medical images aligned with simulated patient symptoms, enabling\ndiverse diagnostic skill training. Specifically, MedDiT integrates various\npatient Knowledge Graphs (KGs), which describe the attributes and symptoms of\npatients, to dynamically prompt Large Language Models' (LLMs) behavior and\ncontrol the patient characteristics, mitigating hallucination during medical\nconversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is\nincorporated to generate medical images according to the specified patient\nattributes in the KG. In this paper, we present the capabilities of MedDiT\nthrough a practical demonstration, showcasing its ability to act in diverse\nsimulated patient cases and generate the corresponding medical images. This can\nprovide an abundant and interactive learning experience for students, advancing\nmedical education by offering an immersive simulation platform for future\nhealthcare professionals. The work sheds light on the feasibility of\nincorporating advanced technologies like LLM, KG, and DiT in education\napplications, highlighting their potential to address the challenges faced in\nsimulated patient-based medical education."
                },
                "authors": [
                    {
                        "name": "Yanzeng Li"
                    },
                    {
                        "name": "Cheng Zeng"
                    },
                    {
                        "name": "Jinchao Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Lei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zou"
                },
                "author": "Lei Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12226v1",
                "updated": "2024-08-22T08:57:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    57,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    57,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for\n  Automated Scoring of CEFR B2 Speaking Assessment Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for\n  Automated Scoring of CEFR B2 Speaking Assessment Transcripts"
                },
                "summary": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation."
                },
                "authors": [
                    {
                        "name": "Nicy Scaria"
                    },
                    {
                        "name": "Silvester John Joseph Kennedy"
                    },
                    {
                        "name": "Thomas Latinovich"
                    },
                    {
                        "name": "Deepak Subramani"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Subramani"
                },
                "author": "Deepak Subramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08703v2",
                "updated": "2024-08-22T08:52:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    52,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-16T12:30:29Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    30,
                    29,
                    4,
                    229,
                    0
                ],
                "title": "TsCA: On the Semantic Consistency Alignment via Conditional Transport\n  for Compositional Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TsCA: On the Semantic Consistency Alignment via Conditional Transport\n  for Compositional Zero-Shot Learning"
                },
                "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\n\\textit{state-object} compositions by leveraging the shared knowledge of their\nprimitive components. Despite considerable progress, effectively calibrating\nthe bias between semantically similar multimodal representations, as well as\ngeneralizing pre-trained knowledge to novel compositional contexts, remains an\nenduring challenge. In this paper, our interest is to revisit the conditional\ntransport (CT) theory and its homology to the visual-semantics interaction in\nCZSL and further, propose a novel Trisets Consistency Alignment framework\n(dubbed TsCA) that well-addresses these issues. Concretely, we utilize three\ndistinct yet semantically homologous sets, i.e., patches, primitives, and\ncompositions, to construct pairwise CT costs to minimize their semantic\ndiscrepancies. To further ensure the consistency transfer within these sets, we\nimplement a cycle-consistency constraint that refines the learning by\nguaranteeing the feature consistency of the self-mapping during transport flow,\nregardless of modality. Moreover, we extend the CT plans to an open-world\nsetting, which enables the model to effectively filter out unfeasible pairs,\nthereby speeding up the inference as well as increasing the accuracy. Extensive\nexperiments are conducted to verify the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\n\\textit{state-object} compositions by leveraging the shared knowledge of their\nprimitive components. Despite considerable progress, effectively calibrating\nthe bias between semantically similar multimodal representations, as well as\ngeneralizing pre-trained knowledge to novel compositional contexts, remains an\nenduring challenge. In this paper, our interest is to revisit the conditional\ntransport (CT) theory and its homology to the visual-semantics interaction in\nCZSL and further, propose a novel Trisets Consistency Alignment framework\n(dubbed TsCA) that well-addresses these issues. Concretely, we utilize three\ndistinct yet semantically homologous sets, i.e., patches, primitives, and\ncompositions, to construct pairwise CT costs to minimize their semantic\ndiscrepancies. To further ensure the consistency transfer within these sets, we\nimplement a cycle-consistency constraint that refines the learning by\nguaranteeing the feature consistency of the self-mapping during transport flow,\nregardless of modality. Moreover, we extend the CT plans to an open-world\nsetting, which enables the model to effectively filter out unfeasible pairs,\nthereby speeding up the inference as well as increasing the accuracy. Extensive\nexperiments are conducted to verify the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Miaoge Li"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Richard Yi Da Xu"
                    },
                    {
                        "name": "Dongsheng Wang"
                    },
                    {
                        "name": "Xiaofeng Cao"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12214v1",
                "updated": "2024-08-22T08:42:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    42,
                    44,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:42:44Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    42,
                    44,
                    3,
                    235,
                    0
                ],
                "title": "UNCO: Towards Unifying Neural Combinatorial Optimization through Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCO: Towards Unifying Neural Combinatorial Optimization through Large\n  Language Model"
                },
                "summary": "Recently, applying neural networks to address combinatorial optimization\nproblems (COPs) has attracted considerable research attention. The prevailing\nmethods always train deep models independently on specific problems, lacking a\nunified framework for concurrently tackling various COPs. To this end, we\npropose a unified neural combinatorial optimization (UNCO) framework to solve\ndifferent types of COPs by a single model. Specifically, we use natural\nlanguage to formulate text-attributed instances for different COPs and encode\nthem in the same embedding space by the large language model (LLM). The\nobtained embeddings are further advanced by an encoder-decoder model without\nany problem-specific modules, thereby facilitating a unified process of\nsolution construction. We further adopt the conflict gradients erasing\nreinforcement learning (CGERL) algorithm to train the UNCO model, delivering\nbetter performance across different COPs than vanilla multi-objective learning.\nExperiments show that the UNCO model can solve multiple COPs after a\nsingle-session training, and achieves satisfactory performance that is\ncomparable to several traditional or learning-based baselines. Instead of\npursuing the best performance for each COP, we explore the synergy between\ntasks and few-shot generalization based on LLM to inspire future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, applying neural networks to address combinatorial optimization\nproblems (COPs) has attracted considerable research attention. The prevailing\nmethods always train deep models independently on specific problems, lacking a\nunified framework for concurrently tackling various COPs. To this end, we\npropose a unified neural combinatorial optimization (UNCO) framework to solve\ndifferent types of COPs by a single model. Specifically, we use natural\nlanguage to formulate text-attributed instances for different COPs and encode\nthem in the same embedding space by the large language model (LLM). The\nobtained embeddings are further advanced by an encoder-decoder model without\nany problem-specific modules, thereby facilitating a unified process of\nsolution construction. We further adopt the conflict gradients erasing\nreinforcement learning (CGERL) algorithm to train the UNCO model, delivering\nbetter performance across different COPs than vanilla multi-objective learning.\nExperiments show that the UNCO model can solve multiple COPs after a\nsingle-session training, and achieves satisfactory performance that is\ncomparable to several traditional or learning-based baselines. Instead of\npursuing the best performance for each COP, we explore the synergy between\ntasks and few-shot generalization based on LLM to inspire future work."
                },
                "authors": [
                    {
                        "name": "Xia Jiang"
                    },
                    {
                        "name": "Yaoxin Wu"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yingqian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yingqian Zhang"
                },
                "author": "Yingqian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15968v2",
                "updated": "2024-08-22T08:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    39,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-24T16:38:43Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    16,
                    38,
                    43,
                    2,
                    115,
                    0
                ],
                "title": "Fast and Robust Expectation Propagation MIMO Detection via\n  Preconditioned Conjugated Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Robust Expectation Propagation MIMO Detection via\n  Preconditioned Conjugated Gradient"
                },
                "summary": "We study the expectation propagation (EP) algorithm for symbol detection in\nmassive multiple-input multiple-output (MIMO) systems. The EP detector shows\nexcellent performance but suffers from a high computational complexity due to\nthe matrix inversion, required in each EP iteration to perform marginal\ninference on a Gaussian system. We propose an inversion-free variant of the EP\nalgorithm by treating inference on the mean and variance as two separate and\nsimpler subtasks: We study the preconditioned conjugate gradient algorithm for\nobtaining the mean, which can significantly reduce the complexity and increase\nstability by relying on the Jacobi preconditioner that proves to fit the EP\ncharacteristics very well. For the variance, we use a simple approximation\nbased on linear regression of the Gram channel matrix. Numerical studies on the\nRayleigh-fading channel and on a realistic 3GPP channel model reveal the\nefficiency of the proposed scheme, which offers an attractive\nperformance-complexity tradeoff and even outperforms the original EP detector\nin high multi-user inference cases where the matrix inversion becomes\nnumerically unstable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the expectation propagation (EP) algorithm for symbol detection in\nmassive multiple-input multiple-output (MIMO) systems. The EP detector shows\nexcellent performance but suffers from a high computational complexity due to\nthe matrix inversion, required in each EP iteration to perform marginal\ninference on a Gaussian system. We propose an inversion-free variant of the EP\nalgorithm by treating inference on the mean and variance as two separate and\nsimpler subtasks: We study the preconditioned conjugate gradient algorithm for\nobtaining the mean, which can significantly reduce the complexity and increase\nstability by relying on the Jacobi preconditioner that proves to fit the EP\ncharacteristics very well. For the variance, we use a simple approximation\nbased on linear regression of the Gram channel matrix. Numerical studies on the\nRayleigh-fading channel and on a realistic 3GPP channel model reveal the\nefficiency of the proposed scheme, which offers an attractive\nperformance-complexity tradeoff and even outperforms the original EP detector\nin high multi-user inference cases where the matrix inversion becomes\nnumerically unstable."
                },
                "authors": [
                    {
                        "name": "Luca Schmid"
                    },
                    {
                        "name": "Dominik Sulz"
                    },
                    {
                        "name": "Laurent Schmalen"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Schmalen"
                },
                "author": "Laurent Schmalen",
                "arxiv_comment": "Accepted for presentation at Asilomar Conference on Signals, Systems,\n  and Computers 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12210v1",
                "updated": "2024-08-22T08:39:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    39,
                    9,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:39:09Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    39,
                    9,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Causal Discovery in Financial Networks with Piecewise Quantile\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Causal Discovery in Financial Networks with Piecewise Quantile\n  Regression"
                },
                "summary": "Financial networks can be constructed using statistical dependencies found\nwithin the price series of speculative assets. Across the various methods used\nto infer these networks, there is a general reliance on predictive modelling to\ncapture cross-correlation effects. These methods usually model the flow of\nmean-response information, or the propagation of volatility and risk within the\nmarket. Such techniques, though insightful, don't fully capture the broader\ndistribution-level causality that is possible within speculative markets. This\npaper introduces a novel approach, combining quantile regression with a\npiecewise linear embedding scheme - allowing us to construct causality networks\nthat identify the complex tail interactions inherent to financial markets.\nApplying this method to 260 cryptocurrency return series, we uncover\nsignificant tail-tail causal effects and substantial causal asymmetry. We\nidentify a propensity for coins to be self-influencing, with comparatively\nsparse cross variable effects. Assessing all link types in conjunction, Bitcoin\nstands out as the primary influencer - a nuance that is missed in conventional\nlinear mean-response analyses. Our findings introduce a comprehensive framework\nfor modelling distributional causality, paving the way towards more holistic\nrepresentations of causality in financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial networks can be constructed using statistical dependencies found\nwithin the price series of speculative assets. Across the various methods used\nto infer these networks, there is a general reliance on predictive modelling to\ncapture cross-correlation effects. These methods usually model the flow of\nmean-response information, or the propagation of volatility and risk within the\nmarket. Such techniques, though insightful, don't fully capture the broader\ndistribution-level causality that is possible within speculative markets. This\npaper introduces a novel approach, combining quantile regression with a\npiecewise linear embedding scheme - allowing us to construct causality networks\nthat identify the complex tail interactions inherent to financial markets.\nApplying this method to 260 cryptocurrency return series, we uncover\nsignificant tail-tail causal effects and substantial causal asymmetry. We\nidentify a propensity for coins to be self-influencing, with comparatively\nsparse cross variable effects. Assessing all link types in conjunction, Bitcoin\nstands out as the primary influencer - a nuance that is missed in conventional\nlinear mean-response analyses. Our findings introduce a comprehensive framework\nfor modelling distributional causality, paving the way towards more holistic\nrepresentations of causality in financial markets."
                },
                "authors": [
                    {
                        "name": "Cameron Cornell"
                    },
                    {
                        "name": "Lewis Mitchell"
                    },
                    {
                        "name": "Matthew Roughan"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Roughan"
                },
                "author": "Matthew Roughan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12199v1",
                "updated": "2024-08-22T08:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    21,
                    28,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:21:28Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    21,
                    28,
                    3,
                    235,
                    0
                ],
                "title": "Efficient Learning for Linear Properties of Bounded-Gate Quantum\n  Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Learning for Linear Properties of Bounded-Gate Quantum\n  Circuits"
                },
                "summary": "The vast and complicated large-qubit state space forbids us to\ncomprehensively capture the dynamics of modern quantum computers via classical\nsimulations or quantum tomography. However, recent progress in quantum learning\ntheory invokes a crucial question: given a quantum circuit containing d tunable\nRZ gates and G-d Clifford gates, can a learner perform purely classical\ninference to efficiently predict its linear properties using new classical\ninputs, after learning from data obtained by incoherently measuring states\ngenerated by the same circuit but with different classical inputs? In this\nwork, we prove that the sample complexity scaling linearly in d is necessary\nand sufficient to achieve a small prediction error, while the corresponding\ncomputational complexity may scale exponentially in d. Building upon these\nderived complexity bounds, we further harness the concept of classical shadow\nand truncated trigonometric expansion to devise a kernel-based learning model\ncapable of trading off prediction error and computational complexity,\ntransitioning from exponential to polynomial scaling in many practical\nsettings. Our results advance two crucial realms in quantum computation: the\nexploration of quantum algorithms with practical utilities and learning-based\nquantum system certification. We conduct numerical simulations to validate our\nproposals across diverse scenarios, encompassing quantum information processing\nprotocols, Hamiltonian simulation, and variational quantum algorithms up to 60\nqubits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast and complicated large-qubit state space forbids us to\ncomprehensively capture the dynamics of modern quantum computers via classical\nsimulations or quantum tomography. However, recent progress in quantum learning\ntheory invokes a crucial question: given a quantum circuit containing d tunable\nRZ gates and G-d Clifford gates, can a learner perform purely classical\ninference to efficiently predict its linear properties using new classical\ninputs, after learning from data obtained by incoherently measuring states\ngenerated by the same circuit but with different classical inputs? In this\nwork, we prove that the sample complexity scaling linearly in d is necessary\nand sufficient to achieve a small prediction error, while the corresponding\ncomputational complexity may scale exponentially in d. Building upon these\nderived complexity bounds, we further harness the concept of classical shadow\nand truncated trigonometric expansion to devise a kernel-based learning model\ncapable of trading off prediction error and computational complexity,\ntransitioning from exponential to polynomial scaling in many practical\nsettings. Our results advance two crucial realms in quantum computation: the\nexploration of quantum algorithms with practical utilities and learning-based\nquantum system certification. We conduct numerical simulations to validate our\nproposals across diverse scenarios, encompassing quantum information processing\nprotocols, Hamiltonian simulation, and variational quantum algorithms up to 60\nqubits."
                },
                "authors": [
                    {
                        "name": "Yuxuan Du"
                    },
                    {
                        "name": "Min-Hsiu Hsieh"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v5",
                "updated": "2024-08-23T08:17:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    17,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "10 pages, 5 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12194v2",
                "updated": "2024-08-23T06:46:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    46,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment"
                },
                "summary": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "Submitted to EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12188v1",
                "updated": "2024-08-22T08:05:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    5,
                    9,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:05:09Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    5,
                    9,
                    3,
                    235,
                    0
                ],
                "title": "Reasoning Factual Knowledge in Structured Data with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Factual Knowledge in Structured Data with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have made remarkable progress in various natural\nlanguage processing tasks as a benefit of their capability to comprehend and\nreason with factual knowledge. However, a significant amount of factual\nknowledge is stored in structured data, which possesses unique characteristics\nthat differ from the unstructured texts used for pretraining. This difference\ncan introduce imperceptible inference parameter deviations, posing challenges\nfor LLMs in effectively utilizing and reasoning with structured data to\naccurately infer factual knowledge. To this end, we propose a benchmark named\nStructFact, to evaluate the structural reasoning capabilities of LLMs in\ninferring factual knowledge. StructFact comprises 8,340 factual questions\nencompassing various tasks, domains, timelines, and regions. This benchmark\nallows us to investigate the capability of LLMs across five factual tasks\nderived from the unique characteristics of structural facts. Extensive\nexperiments on a set of LLMs with different training strategies reveal the\nlimitations of current LLMs in inferring factual knowledge from structured\ndata. We present this benchmark as a compass to navigate the strengths and\nweaknesses of LLMs in reasoning with structured data for knowledge-sensitive\ntasks, and to encourage advancements in related real-world applications. Please\nfind our code at https://github.com/EganGu/StructFact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable progress in various natural\nlanguage processing tasks as a benefit of their capability to comprehend and\nreason with factual knowledge. However, a significant amount of factual\nknowledge is stored in structured data, which possesses unique characteristics\nthat differ from the unstructured texts used for pretraining. This difference\ncan introduce imperceptible inference parameter deviations, posing challenges\nfor LLMs in effectively utilizing and reasoning with structured data to\naccurately infer factual knowledge. To this end, we propose a benchmark named\nStructFact, to evaluate the structural reasoning capabilities of LLMs in\ninferring factual knowledge. StructFact comprises 8,340 factual questions\nencompassing various tasks, domains, timelines, and regions. This benchmark\nallows us to investigate the capability of LLMs across five factual tasks\nderived from the unique characteristics of structural facts. Extensive\nexperiments on a set of LLMs with different training strategies reveal the\nlimitations of current LLMs in inferring factual knowledge from structured\ndata. We present this benchmark as a compass to navigate the strengths and\nweaknesses of LLMs in reasoning with structured data for knowledge-sensitive\ntasks, and to encourage advancements in related real-world applications. Please\nfind our code at https://github.com/EganGu/StructFact."
                },
                "authors": [
                    {
                        "name": "Sirui Huang"
                    },
                    {
                        "name": "Yanggan Gu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Zhonghao Li"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12185v1",
                "updated": "2024-08-22T08:00:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    0,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:00:50Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    0,
                    50,
                    3,
                    235,
                    0
                ],
                "title": "Rank and Align: Towards Effective Source-free Graph Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank and Align: Towards Effective Source-free Graph Domain Adaptation"
                },
                "summary": "Graph neural networks (GNNs) have achieved impressive performance in graph\ndomain adaptation. However, extensive source graphs could be unavailable in\nreal-world scenarios due to privacy and storage concerns. To this end, we\ninvestigate an underexplored yet practical problem of source-free graph domain\nadaptation, which transfers knowledge from source models instead of source\ngraphs to a target domain. To solve this problem, we introduce a novel\nGNN-based approach called Rank and Align (RNA), which ranks graph similarities\nwith spectral seriation for robust semantics learning, and aligns inharmonic\ngraphs with harmonic graphs which close to the source domain for subgraph\nextraction. In particular, to overcome label scarcity, we employ the spectral\nseriation algorithm to infer the robust pairwise rankings, which can guide\nsemantic learning using a similarity learning objective. To depict distribution\nshifts, we utilize spectral clustering and the silhouette coefficient to detect\nharmonic graphs, which the source model can easily classify. To reduce\npotential domain discrepancy, we extract domain-invariant subgraphs from\ninharmonic graphs by an adversarial edge sampling process, which guides the\ninvariant learning of GNNs. Extensive experiments on several benchmark datasets\ndemonstrate the effectiveness of our proposed RNA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have achieved impressive performance in graph\ndomain adaptation. However, extensive source graphs could be unavailable in\nreal-world scenarios due to privacy and storage concerns. To this end, we\ninvestigate an underexplored yet practical problem of source-free graph domain\nadaptation, which transfers knowledge from source models instead of source\ngraphs to a target domain. To solve this problem, we introduce a novel\nGNN-based approach called Rank and Align (RNA), which ranks graph similarities\nwith spectral seriation for robust semantics learning, and aligns inharmonic\ngraphs with harmonic graphs which close to the source domain for subgraph\nextraction. In particular, to overcome label scarcity, we employ the spectral\nseriation algorithm to infer the robust pairwise rankings, which can guide\nsemantic learning using a similarity learning objective. To depict distribution\nshifts, we utilize spectral clustering and the silhouette coefficient to detect\nharmonic graphs, which the source model can easily classify. To reduce\npotential domain discrepancy, we extract domain-invariant subgraphs from\ninharmonic graphs by an adversarial edge sampling process, which guides the\ninvariant learning of GNNs. Extensive experiments on several benchmark datasets\ndemonstrate the effectiveness of our proposed RNA."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Langechuan Liu"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_doi": "10.24963/ijcai.2024/520",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/520",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IJCAI2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02851v2",
                "updated": "2024-08-22T07:49:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    49,
                    39,
                    3,
                    235,
                    0
                ],
                "published": "2024-01-05T15:09:57Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    15,
                    9,
                    57,
                    4,
                    5,
                    0
                ],
                "title": "Natural Language Programming in Medicine: Administering Evidence Based\n  Clinical Workflows with Autonomous Agents Powered by Generative Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Programming in Medicine: Administering Evidence Based\n  Clinical Workflows with Autonomous Agents Powered by Generative Large\n  Language Models"
                },
                "summary": "Generative Large Language Models (LLMs) hold significant promise in\nhealthcare, demonstrating capabilities such as passing medical licensing exams\nand providing clinical knowledge. However, their current use as information\nretrieval tools is limited by challenges like data staleness, resource demands,\nand occasional generation of incorrect information. This study assessed the\npotential of LLMs to function as autonomous agents in a simulated tertiary care\nmedical center, using real-world clinical cases across multiple specialties.\nBoth proprietary and open-source LLMs were evaluated, with Retrieval Augmented\nGeneration (RAG) enhancing contextual relevance. Proprietary models,\nparticularly GPT-4, generally outperformed open-source models, showing improved\nguideline adherence and more accurate responses with RAG. The manual evaluation\nby expert clinicians was crucial in validating models' outputs, underscoring\nthe importance of human oversight in LLM operation. Further, the study\nemphasizes Natural Language Programming (NLP) as the appropriate paradigm for\nmodifying model behavior, allowing for precise adjustments through tailored\nprompts and real-world interactions. This approach highlights the potential of\nLLMs to significantly enhance and supplement clinical decision-making, while\nalso emphasizing the value of continuous expert involvement and the flexibility\nof NLP to ensure their reliability and effectiveness in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) hold significant promise in\nhealthcare, demonstrating capabilities such as passing medical licensing exams\nand providing clinical knowledge. However, their current use as information\nretrieval tools is limited by challenges like data staleness, resource demands,\nand occasional generation of incorrect information. This study assessed the\npotential of LLMs to function as autonomous agents in a simulated tertiary care\nmedical center, using real-world clinical cases across multiple specialties.\nBoth proprietary and open-source LLMs were evaluated, with Retrieval Augmented\nGeneration (RAG) enhancing contextual relevance. Proprietary models,\nparticularly GPT-4, generally outperformed open-source models, showing improved\nguideline adherence and more accurate responses with RAG. The manual evaluation\nby expert clinicians was crucial in validating models' outputs, underscoring\nthe importance of human oversight in LLM operation. Further, the study\nemphasizes Natural Language Programming (NLP) as the appropriate paradigm for\nmodifying model behavior, allowing for precise adjustments through tailored\nprompts and real-world interactions. This approach highlights the potential of\nLLMs to significantly enhance and supplement clinical decision-making, while\nalso emphasizing the value of continuous expert involvement and the flexibility\nof NLP to ensure their reliability and effectiveness in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Akhil Vaid"
                    },
                    {
                        "name": "Joshua Lampert"
                    },
                    {
                        "name": "Juhee Lee"
                    },
                    {
                        "name": "Ashwin Sawant"
                    },
                    {
                        "name": "Donald Apakama"
                    },
                    {
                        "name": "Ankit Sakhuja"
                    },
                    {
                        "name": "Ali Soroush"
                    },
                    {
                        "name": "Sarah Bick"
                    },
                    {
                        "name": "Ethan Abbott"
                    },
                    {
                        "name": "Hernando Gomez"
                    },
                    {
                        "name": "Michael Hadley"
                    },
                    {
                        "name": "Denise Lee"
                    },
                    {
                        "name": "Isotta Landi"
                    },
                    {
                        "name": "Son Q Duong"
                    },
                    {
                        "name": "Nicole Bussola"
                    },
                    {
                        "name": "Ismail Nabeel"
                    },
                    {
                        "name": "Silke Muehlstedt"
                    },
                    {
                        "name": "Silke Muehlstedt"
                    },
                    {
                        "name": "Robert Freeman"
                    },
                    {
                        "name": "Patricia Kovatch"
                    },
                    {
                        "name": "Brendan Carr"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Benjamin Glicksberg"
                    },
                    {
                        "name": "Edgar Argulian"
                    },
                    {
                        "name": "Stamatios Lerakis"
                    },
                    {
                        "name": "Rohan Khera"
                    },
                    {
                        "name": "David L. Reich"
                    },
                    {
                        "name": "Monica Kraft"
                    },
                    {
                        "name": "Alexander Charney"
                    },
                    {
                        "name": "Girish Nadkarni"
                    }
                ],
                "author_detail": {
                    "name": "Girish Nadkarni"
                },
                "author": "Girish Nadkarni",
                "arxiv_comment": "Figures: 5, Tables: 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08252v2",
                "updated": "2024-08-22T07:42:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    42,
                    51,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-12T05:43:10Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    5,
                    43,
                    10,
                    4,
                    103,
                    0
                ],
                "title": "MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based\n  Monocular Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based\n  Monocular Guidance"
                },
                "summary": "The latest regularized Neural Radiance Field (NeRF) approaches produce poor\ngeometry and view extrapolation for large scale sparse view scenes, such as\nETH3D. Density-based approaches tend to be under-constrained, while\nsurface-based approaches tend to miss details. In this paper, we take a\ndensity-based approach, sampling patches instead of individual rays to better\nincorporate monocular depth and normal estimates and patch-based photometric\nconsistency constraints between training views and sampled virtual views.\nLoosely constraining densities based on estimated depth aligned to sparse\npoints further improves geometric accuracy. While maintaining similar view\nsynthesis quality, our approach significantly improves geometric accuracy on\nthe ETH3D benchmark, e.g. increasing the F1@2cm score by 4x-8x compared to\nother regularized density-based approaches, with much lower training and\ninference time than other approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest regularized Neural Radiance Field (NeRF) approaches produce poor\ngeometry and view extrapolation for large scale sparse view scenes, such as\nETH3D. Density-based approaches tend to be under-constrained, while\nsurface-based approaches tend to miss details. In this paper, we take a\ndensity-based approach, sampling patches instead of individual rays to better\nincorporate monocular depth and normal estimates and patch-based photometric\nconsistency constraints between training views and sampled virtual views.\nLoosely constraining densities based on estimated depth aligned to sparse\npoints further improves geometric accuracy. While maintaining similar view\nsynthesis quality, our approach significantly improves geometric accuracy on\nthe ETH3D benchmark, e.g. increasing the F1@2cm score by 4x-8x compared to\nother regularized density-based approaches, with much lower training and\ninference time than other approaches."
                },
                "authors": [
                    {
                        "name": "Yuqun Wu"
                    },
                    {
                        "name": "Jae Yong Lee"
                    },
                    {
                        "name": "Chuhang Zou"
                    },
                    {
                        "name": "Shenlong Wang"
                    },
                    {
                        "name": "Derek Hoiem"
                    }
                ],
                "author_detail": {
                    "name": "Derek Hoiem"
                },
                "author": "Derek Hoiem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12168v1",
                "updated": "2024-08-22T07:31:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    31,
                    0,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T07:31:00Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    31,
                    0,
                    3,
                    235,
                    0
                ],
                "title": "FIRST: Teach A Reliable Large Language Model Through Efficient\n  Trustworthy Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRST: Teach A Reliable Large Language Model Through Efficient\n  Trustworthy Distillation"
                },
                "summary": "Large language models (LLMs) have become increasingly prevalent in our daily\nlives, leading to an expectation for LLMs to be trustworthy -- - both accurate\nand well-calibrated (the prediction confidence should align with its ground\ntruth correctness likelihood). Nowadays, fine-tuning has become the most\npopular method for adapting a model to practical usage by significantly\nincreasing accuracy on downstream tasks. Despite the great accuracy it\nachieves, we found fine-tuning is still far away from satisfactory\ntrustworthiness due to \"tuning-induced mis-calibration\". In this paper, we\ndelve deeply into why and how mis-calibration exists in fine-tuned models, and\nhow distillation can alleviate the issue. Then we further propose a brand new\nmethod named Efficient Trustworthy Distillation (FIRST), which utilizes a small\nportion of teacher's knowledge to obtain a reliable language model in a\ncost-efficient way. Specifically, we identify the \"concentrated knowledge\"\nphenomenon during distillation, which can significantly reduce the\ncomputational burden. Then we apply a \"trustworthy maximization\" process to\noptimize the utilization of this small portion of concentrated knowledge before\ntransferring it to the student. Experimental results demonstrate the\neffectiveness of our method, where better accuracy (+2.3%) and less\nmis-calibration (-10%) are achieved on average across both in-domain and\nout-of-domain scenarios, indicating better trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly prevalent in our daily\nlives, leading to an expectation for LLMs to be trustworthy -- - both accurate\nand well-calibrated (the prediction confidence should align with its ground\ntruth correctness likelihood). Nowadays, fine-tuning has become the most\npopular method for adapting a model to practical usage by significantly\nincreasing accuracy on downstream tasks. Despite the great accuracy it\nachieves, we found fine-tuning is still far away from satisfactory\ntrustworthiness due to \"tuning-induced mis-calibration\". In this paper, we\ndelve deeply into why and how mis-calibration exists in fine-tuned models, and\nhow distillation can alleviate the issue. Then we further propose a brand new\nmethod named Efficient Trustworthy Distillation (FIRST), which utilizes a small\nportion of teacher's knowledge to obtain a reliable language model in a\ncost-efficient way. Specifically, we identify the \"concentrated knowledge\"\nphenomenon during distillation, which can significantly reduce the\ncomputational burden. Then we apply a \"trustworthy maximization\" process to\noptimize the utilization of this small portion of concentrated knowledge before\ntransferring it to the student. Experimental results demonstrate the\neffectiveness of our method, where better accuracy (+2.3%) and less\nmis-calibration (-10%) are achieved on average across both in-domain and\nout-of-domain scenarios, indicating better trustworthiness."
                },
                "authors": [
                    {
                        "name": "KaShun Shum"
                    },
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Muhammad Omer Raza"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Omer Raza"
                },
                "author": "Muhammad Omer Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12163v1",
                "updated": "2024-08-22T07:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T07:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-Guided Reflective Sampling for Aligning Language Models"
                },
                "summary": "Large language models (LLMs) are aligned with human preferences by\nreinforcement learning from human feedback (RLHF). Effective data sampling is\ncrucial for RLHF, as it determines the efficiency of model training, ensuring\nthat models learn from the informative samples. To achieve better data\ngeneration, we propose a new sampling method called Preference-Guided\nReflective Sampling (PRS). PRS frames the response generation as an\noptimization process to the explicitly specified user preference described in\nnatural language. It employs a tree-based generation framework to enable an\nefficient sampling process, which guides the direction of generation through\npreference and better explores the sampling space with adaptive\nself-refinement. Notably, PRS can align LLMs to diverse preferences. We study\npreference-controlled text generation for instruction following and\nkeyword-focused document summarization. Our findings indicate that PRS, across\ndifferent LLM policies, generates training data with much higher rewards than\nstrong baselines. PRS also excels in post-RL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are aligned with human preferences by\nreinforcement learning from human feedback (RLHF). Effective data sampling is\ncrucial for RLHF, as it determines the efficiency of model training, ensuring\nthat models learn from the informative samples. To achieve better data\ngeneration, we propose a new sampling method called Preference-Guided\nReflective Sampling (PRS). PRS frames the response generation as an\noptimization process to the explicitly specified user preference described in\nnatural language. It employs a tree-based generation framework to enable an\nefficient sampling process, which guides the direction of generation through\npreference and better explores the sampling space with adaptive\nself-refinement. Notably, PRS can align LLMs to diverse preferences. We study\npreference-controlled text generation for instruction following and\nkeyword-focused document summarization. Our findings indicate that PRS, across\ndifferent LLM policies, generates training data with much higher rewards than\nstrong baselines. PRS also excels in post-RL training."
                },
                "authors": [
                    {
                        "name": "Hai Ye"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01129v3",
                "updated": "2024-08-22T07:18:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    1,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-02T09:18:41Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    18,
                    41,
                    4,
                    215,
                    0
                ],
                "title": "A Survey of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mamba"
                },
                "summary": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Tyler Derr"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14183v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14183v3",
                "updated": "2024-08-22T07:01:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    1,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2023-12-19T14:35:04Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    14,
                    35,
                    4,
                    1,
                    353,
                    0
                ],
                "title": "On Early Detection of Hallucinations in Factual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Early Detection of Hallucinations in Factual Question Answering"
                },
                "summary": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs."
                },
                "authors": [
                    {
                        "name": "Ben Snyder"
                    },
                    {
                        "name": "Marius Moisescu"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "arxiv_doi": "10.1145/3637528.3671796",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671796",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.14183v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14183v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "KDD 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12159v1",
                "updated": "2024-08-22T06:59:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    59,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T06:59:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    59,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "Search-Based LLMs for Code Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-Based LLMs for Code Optimization"
                },
                "summary": "The code written by developers usually suffers from efficiency problems and\ncontain various performance bugs. These inefficiencies necessitate the research\nof automated refactoring methods for code optimization. Early research in code\noptimization employs rule-based methods and focuses on specific inefficiency\nissues, which are labor-intensive and suffer from the low coverage issue.\nRecent work regards the task as a sequence generation problem, and resorts to\ndeep learning (DL) techniques such as large language models (LLMs). These\nmethods typically prompt LLMs to directly generate optimized code. Although\nthese methods show state-of-the-art performance, such one-step generation\nparadigm is hard to achieve an optimal solution. First, complex optimization\nmethods such as combinatorial ones are hard to be captured by LLMs. Second, the\none-step generation paradigm poses challenge in precisely infusing the\nknowledge required for effective code optimization within LLMs, resulting in\nunder-optimized code.To address these problems, we propose to model this task\nfrom the search perspective, and propose a search-based LLMs framework named\nSBLLM that enables iterative refinement and discovery of improved optimization\nmethods. SBLLM synergistically integrate LLMs with evolutionary search and\nconsists of three key components: 1) an execution-based representative sample\nselection part that evaluates the fitness of each existing optimized code and\nprioritizes promising ones to pilot the generation of improved code; 2) an\nadaptive optimization pattern retrieval part that infuses targeted optimization\npatterns into the model for guiding LLMs towards rectifying and progressively\nenhancing their optimization methods; and 3) a genetic operator-inspired\nchain-of-thought prompting part that aids LLMs in combining different\noptimization methods and generating improved optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The code written by developers usually suffers from efficiency problems and\ncontain various performance bugs. These inefficiencies necessitate the research\nof automated refactoring methods for code optimization. Early research in code\noptimization employs rule-based methods and focuses on specific inefficiency\nissues, which are labor-intensive and suffer from the low coverage issue.\nRecent work regards the task as a sequence generation problem, and resorts to\ndeep learning (DL) techniques such as large language models (LLMs). These\nmethods typically prompt LLMs to directly generate optimized code. Although\nthese methods show state-of-the-art performance, such one-step generation\nparadigm is hard to achieve an optimal solution. First, complex optimization\nmethods such as combinatorial ones are hard to be captured by LLMs. Second, the\none-step generation paradigm poses challenge in precisely infusing the\nknowledge required for effective code optimization within LLMs, resulting in\nunder-optimized code.To address these problems, we propose to model this task\nfrom the search perspective, and propose a search-based LLMs framework named\nSBLLM that enables iterative refinement and discovery of improved optimization\nmethods. SBLLM synergistically integrate LLMs with evolutionary search and\nconsists of three key components: 1) an execution-based representative sample\nselection part that evaluates the fitness of each existing optimized code and\nprioritizes promising ones to pilot the generation of improved code; 2) an\nadaptive optimization pattern retrieval part that infuses targeted optimization\npatterns into the model for guiding LLMs towards rectifying and progressively\nenhancing their optimization methods; and 3) a genetic operator-inspired\nchain-of-thought prompting part that aids LLMs in combining different\noptimization methods and generating improved optimization methods."
                },
                "authors": [
                    {
                        "name": "Shuzheng Gao"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Wenchao Gu"
                    },
                    {
                        "name": "Michael Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lyu"
                },
                "author": "Michael Lyu",
                "arxiv_comment": "Accepted by 2025 IEEE/ACM 47th International Conference on Software\n  Engineering (ICSE'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08768v3",
                "updated": "2024-08-22T06:27:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    27,
                    48,
                    3,
                    235,
                    0
                ],
                "published": "2023-12-14T09:31:33Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    9,
                    31,
                    33,
                    3,
                    348,
                    0
                ],
                "title": "Local Conditional Controlling for Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Conditional Controlling for Text-to-Image Diffusion Models"
                },
                "summary": "Diffusion models have exhibited impressive prowess in the text-to-image task.\nRecent methods add image-level structure controls, e.g., edge and depth maps,\nto manipulate the generation process together with text prompts to obtain\ndesired images. This controlling process is globally operated on the entire\nimage, which limits the flexibility of control regions. In this paper, we\nexplore a novel and practical task setting: local control. It focuses on\ncontrolling specific local region according to user-defined image conditions,\nwhile the remaining regions are only conditioned by the original text prompt.\nHowever, it is non-trivial to achieve local conditional controlling. The naive\nmanner of directly adding local conditions may lead to the local control\ndominance problem, which forces the model to focus on the controlled region and\nneglect object generation in other regions. To mitigate this problem, we\npropose Regional Discriminate Loss to update the noised latents, aiming at\nenhanced object generation in non-control regions. Furthermore, the proposed\nFocused Token Response suppresses weaker attention scores which lack the\nstrongest response to enhance object distinction and reduce duplication.\nLastly, we adopt Feature Mask Constraint to reduce quality degradation in\nimages caused by information differences across the local control region. All\nproposed strategies are operated at the inference stage. Extensive experiments\ndemonstrate that our method can synthesize high-quality images aligned with the\ntext prompt under local control conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have exhibited impressive prowess in the text-to-image task.\nRecent methods add image-level structure controls, e.g., edge and depth maps,\nto manipulate the generation process together with text prompts to obtain\ndesired images. This controlling process is globally operated on the entire\nimage, which limits the flexibility of control regions. In this paper, we\nexplore a novel and practical task setting: local control. It focuses on\ncontrolling specific local region according to user-defined image conditions,\nwhile the remaining regions are only conditioned by the original text prompt.\nHowever, it is non-trivial to achieve local conditional controlling. The naive\nmanner of directly adding local conditions may lead to the local control\ndominance problem, which forces the model to focus on the controlled region and\nneglect object generation in other regions. To mitigate this problem, we\npropose Regional Discriminate Loss to update the noised latents, aiming at\nenhanced object generation in non-control regions. Furthermore, the proposed\nFocused Token Response suppresses weaker attention scores which lack the\nstrongest response to enhance object distinction and reduce duplication.\nLastly, we adopt Feature Mask Constraint to reduce quality degradation in\nimages caused by information differences across the local control region. All\nproposed strategies are operated at the inference stage. Extensive experiments\ndemonstrate that our method can synthesize high-quality images aligned with the\ntext prompt under local control conditions."
                },
                "authors": [
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Liang Peng"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Hengjia Li"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "qinglin lu"
                    },
                    {
                        "name": "Boxi Wu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12148v1",
                "updated": "2024-08-22T06:27:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    27,
                    10,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T06:27:10Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    27,
                    10,
                    3,
                    235,
                    0
                ],
                "title": "Multi-tool Integration Application for Math Reasoning Using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tool Integration Application for Math Reasoning Using Large\n  Language Model"
                },
                "summary": "Mathematical reasoning is an important research direction in the field of\nartificial intelligence. This article proposes a novel multi tool application\nframework for mathematical reasoning, aiming to achieve more comprehensive and\naccurate mathematical reasoning by utilizing the collaborative effect of large\nlanguage models (LLMs) and multiple external tools. Firstly, use a Math Tool to\nperform basic mathematical calculations during the inference process through\ninteraction with LLM. Secondly, Code Tool can generate code fragments that\ncomply with syntax rules and execute them, providing support for complex\nmathematical problems. Then, through the iterative reasoning of the CoT Tool,\nthe logical coherence and accuracy of mathematical reasoning are enhanced.\nUltimately, by using self consistency tools to select the final answer based on\ndifferent parameters, the consistency and reliability of reasoning are\nimproved. Through the synergistic effect of these tools, the framework has\nachieved significant performance improvement in mathematical reasoning tasks.\nWe conducted experiments on the NumGLUE Task 4 test set, which includes 220\nmathematical reasoning fill in the blank questions. The experimental results\nshowed that, based on Math Tool, Code Tool, and CoT Tool, in Task 4 task,our\nmethod achieved an accuracy of 89.09,compared with the GPT3+FewShot baseline,\nFew Shot+ERNIE-4.0+self consistency improved by 49.09%, and compared with\nfine-tuning the Fine tuning baseline, Few Shot+ERNIE-4.0+self consistency\nimproved by 52.29%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is an important research direction in the field of\nartificial intelligence. This article proposes a novel multi tool application\nframework for mathematical reasoning, aiming to achieve more comprehensive and\naccurate mathematical reasoning by utilizing the collaborative effect of large\nlanguage models (LLMs) and multiple external tools. Firstly, use a Math Tool to\nperform basic mathematical calculations during the inference process through\ninteraction with LLM. Secondly, Code Tool can generate code fragments that\ncomply with syntax rules and execute them, providing support for complex\nmathematical problems. Then, through the iterative reasoning of the CoT Tool,\nthe logical coherence and accuracy of mathematical reasoning are enhanced.\nUltimately, by using self consistency tools to select the final answer based on\ndifferent parameters, the consistency and reliability of reasoning are\nimproved. Through the synergistic effect of these tools, the framework has\nachieved significant performance improvement in mathematical reasoning tasks.\nWe conducted experiments on the NumGLUE Task 4 test set, which includes 220\nmathematical reasoning fill in the blank questions. The experimental results\nshowed that, based on Math Tool, Code Tool, and CoT Tool, in Task 4 task,our\nmethod achieved an accuracy of 89.09,compared with the GPT3+FewShot baseline,\nFew Shot+ERNIE-4.0+self consistency improved by 49.09%, and compared with\nfine-tuning the Fine tuning baseline, Few Shot+ERNIE-4.0+self consistency\nimproved by 52.29%"
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01079v2",
                "updated": "2024-08-22T06:25:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    25,
                    19,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-01T08:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    34,
                    40,
                    0,
                    183,
                    0
                ],
                "title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)"
                },
                "summary": "We investigate the statistical and computational limits of latent\n\\textbf{Di}ffusion \\textbf{T}ransformers (\\textbf{DiT}s) under the\nlow-dimensional linear latent space assumption. Statistically, we study the\nuniversal approximation and sample complexity of the DiTs score function, as\nwell as the distribution recovery property of the initial data. Specifically,\nunder mild data assumptions, we derive an approximation error bound for the\nscore network of latent DiTs, which is sub-linear in the latent space\ndimension. Additionally, we derive the corresponding sample complexity bound\nand show that the data distribution generated from the estimated score function\nconverges toward a proximate area of the original one. Computationally, we\ncharacterize the hardness of both forward inference and backward computation of\nlatent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For\nforward inference, we identify efficient criteria for all possible latent DiTs\ninference algorithms and showcase our theory by pushing the efficiency toward\nalmost-linear time inference. For backward computation, we leverage the\nlow-rank structure within the gradient computation of DiTs training for\npossible algorithmic speedup. Specifically, we show that such speedup achieves\nalmost-linear time latent DiTs training by casting the DiTs gradient as a\nseries of chained low-rank approximations with bounded error. Under the\nlow-dimensional assumption, we show that the convergence rate and the\ncomputational efficiency are both dominated by the dimension of the subspace,\nsuggesting that latent DiTs have the potential to bypass the challenges\nassociated with the high dimensionality of initial data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the statistical and computational limits of latent\n\\textbf{Di}ffusion \\textbf{T}ransformers (\\textbf{DiT}s) under the\nlow-dimensional linear latent space assumption. Statistically, we study the\nuniversal approximation and sample complexity of the DiTs score function, as\nwell as the distribution recovery property of the initial data. Specifically,\nunder mild data assumptions, we derive an approximation error bound for the\nscore network of latent DiTs, which is sub-linear in the latent space\ndimension. Additionally, we derive the corresponding sample complexity bound\nand show that the data distribution generated from the estimated score function\nconverges toward a proximate area of the original one. Computationally, we\ncharacterize the hardness of both forward inference and backward computation of\nlatent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For\nforward inference, we identify efficient criteria for all possible latent DiTs\ninference algorithms and showcase our theory by pushing the efficiency toward\nalmost-linear time inference. For backward computation, we leverage the\nlow-rank structure within the gradient computation of DiTs training for\npossible algorithmic speedup. Specifically, we show that such speedup achieves\nalmost-linear time latent DiTs training by casting the DiTs gradient as a\nseries of chained low-rank approximations with bounded error. Under the\nlow-dimensional assumption, we show that the convergence rate and the\ncomputational efficiency are both dominated by the dimension of the subspace,\nsuggesting that latent DiTs have the potential to bypass the challenges\nassociated with the high dimensionality of initial data."
                },
                "authors": [
                    {
                        "name": "Jerry Yao-Chieh Hu"
                    },
                    {
                        "name": "Weimin Wu"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "v2 fixed typos, added Fig. 1 and added clarifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15960v3",
                "updated": "2024-08-22T06:24:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    24,
                    12,
                    3,
                    235,
                    0
                ],
                "published": "2023-12-26T08:49:57Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    8,
                    49,
                    57,
                    1,
                    360,
                    0
                ],
                "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks"
                },
                "summary": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Pengguang Chen"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:\n  https://github.com/dvlab-research/MoTCoder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07528v2",
                "updated": "2024-08-22T06:09:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    9,
                    53,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-11T17:55:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    55,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models"
                },
                "summary": "The capacity of Large Language Models (LLMs) to comprehend and reason over\nlong contexts is pivotal for advancements in diverse fields. Yet, they still\nstuggle with capturing long-distance dependencies within sequences to deeply\nunderstand semantics. To address this issue, we introduce Query-aware Inference\nfor LLMs (Q-LLM), a system designed to process extensive sequences akin to\nhuman cognition. By focusing on memory data relevant to a given query, Q-LLM\ncan accurately capture pertinent information within a fixed window size and\nprovide precise answers to queries. It doesn't require extra training and can\nbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can\nread Harry Potter within 30s and accurately answer the questions. On widely\nrecognized benchmarks, Q-LLM improved by 7.17% compared to the current\nstate-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In\nthe Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current\nSOTA by 7.0% and 6.1%. Our code can be found in\nhttps://github.com/dvlab-research/Q-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of Large Language Models (LLMs) to comprehend and reason over\nlong contexts is pivotal for advancements in diverse fields. Yet, they still\nstuggle with capturing long-distance dependencies within sequences to deeply\nunderstand semantics. To address this issue, we introduce Query-aware Inference\nfor LLMs (Q-LLM), a system designed to process extensive sequences akin to\nhuman cognition. By focusing on memory data relevant to a given query, Q-LLM\ncan accurately capture pertinent information within a fixed window size and\nprovide precise answers to queries. It doesn't require extra training and can\nbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can\nread Harry Potter within 30s and accurately answer the questions. On widely\nrecognized benchmarks, Q-LLM improved by 7.17% compared to the current\nstate-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In\nthe Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current\nSOTA by 7.0% and 6.1%. Our code can be found in\nhttps://github.com/dvlab-research/Q-LLM."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12142v1",
                "updated": "2024-08-22T05:59:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    59,
                    47,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T05:59:47Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    59,
                    47,
                    3,
                    235,
                    0
                ],
                "title": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents"
                },
                "summary": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymous patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and achieves text generation under\nsymbolic control via a dynamic diagnosis tree from a tool agent. By applying\nthe proposed framework, we develop the largest Chinese mental disorders\ndiagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases\nby cooperating with a pioneering psychiatric hospital, and contains 5000\nhigh-quality long conversations with diagnosis results as labels. To the best\nof our knowledge, it's also the first labelled Chinese mental disorders\ndiagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset\nsuccessfully simulates human-like diagnostic process of mental disorders. The\ndataset and code will become publicly accessible in\nhttps://github.com/lemonsis/MDD-5k.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymous patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and achieves text generation under\nsymbolic control via a dynamic diagnosis tree from a tool agent. By applying\nthe proposed framework, we develop the largest Chinese mental disorders\ndiagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases\nby cooperating with a pioneering psychiatric hospital, and contains 5000\nhigh-quality long conversations with diagnosis results as labels. To the best\nof our knowledge, it's also the first labelled Chinese mental disorders\ndiagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset\nsuccessfully simulates human-like diagnostic process of mental disorders. The\ndataset and code will become publicly accessible in\nhttps://github.com/lemonsis/MDD-5k."
                },
                "authors": [
                    {
                        "name": "Congchi Yin"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Zike Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Piji Li"
                    },
                    {
                        "name": "Jianhua Chen"
                    },
                    {
                        "name": "Xun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Jiang"
                },
                "author": "Xun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16991v2",
                "updated": "2024-08-22T05:27:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    27,
                    37,
                    3,
                    235,
                    0
                ],
                "published": "2024-03-25T17:52:37Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    52,
                    37,
                    0,
                    85,
                    0
                ],
                "title": "Geometric Thermodynamics of Collapse of Gels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Thermodynamics of Collapse of Gels"
                },
                "summary": "Stimulus-induced volumetric phase transition in gels may be potentially\nexploited for various bio-engineering and mechanical engineering applications.\nSince the discovery of the phenomenon in the 1970s, extensive experimental\nresearch has helped understand the phase transition and related critical\nphenomena. Yet, little insight is available on the evolving microstructure. In\nthis article, we aim at unravelling certain geometric aspects of the\nmicromechanics underlying discontinuous phase transition in polyacrylamide\ngels. Towards this, we use geometric thermodynamics and a Landau-Ginzburg type\nfree energy functional involving a squared gradient, in conjunction with\nFlory-Huggins theory. We specifically exploit Ruppeiner's approach of\nRiemannian geometry-enriched thermodynamic fluctuation theory, which was\npreviously employed to investigate phase transitions in van der Waals fluids\nand black holes. The framework equips us with a scalar curvature that is\ntypically indicative of certain aspects of the microstructure during phase\ntransition. Since previous studies have indicated that curvature divergence\nrelates to correlation length divergence, we infer that the gel possesses a\nheterogeneous microstructure during phase transition, i.e. at critical points.\nCurvature also provides an insight into the universality class of phase\ntransition and the nature of polymer-polymer interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stimulus-induced volumetric phase transition in gels may be potentially\nexploited for various bio-engineering and mechanical engineering applications.\nSince the discovery of the phenomenon in the 1970s, extensive experimental\nresearch has helped understand the phase transition and related critical\nphenomena. Yet, little insight is available on the evolving microstructure. In\nthis article, we aim at unravelling certain geometric aspects of the\nmicromechanics underlying discontinuous phase transition in polyacrylamide\ngels. Towards this, we use geometric thermodynamics and a Landau-Ginzburg type\nfree energy functional involving a squared gradient, in conjunction with\nFlory-Huggins theory. We specifically exploit Ruppeiner's approach of\nRiemannian geometry-enriched thermodynamic fluctuation theory, which was\npreviously employed to investigate phase transitions in van der Waals fluids\nand black holes. The framework equips us with a scalar curvature that is\ntypically indicative of certain aspects of the microstructure during phase\ntransition. Since previous studies have indicated that curvature divergence\nrelates to correlation length divergence, we infer that the gel possesses a\nheterogeneous microstructure during phase transition, i.e. at critical points.\nCurvature also provides an insight into the universality class of phase\ntransition and the nature of polymer-polymer interactions."
                },
                "authors": [
                    {
                        "name": "Asif Raza"
                    },
                    {
                        "name": "Sanhita Das"
                    },
                    {
                        "name": "Debasish Roy"
                    }
                ],
                "author_detail": {
                    "name": "Debasish Roy"
                },
                "author": "Debasish Roy",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12131v1",
                "updated": "2024-08-22T05:13:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    13,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T05:13:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    13,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Statistical inference on kurtosis of elliptical distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference on kurtosis of elliptical distributions"
                },
                "summary": "Multivariate elliptically-contoured distributions are widely used for\nmodeling correlated and non-Gaussian data. In this work, we study the kurtosis\nof the elliptical model, which is an important parameter in many statistical\nanalysis. Based on U-statistics, we develop an estimation method.\nTheoretically, we show that the proposed estimator is consistent under regular\nconditions, especially we relax a moment condition and the restriction that the\ndata dimension and the sample size are of the same order. Furthermore, we\nderive the asymptotic normality of the estimator and evaluate the asymptotic\nvariance through several examples, which allows us to construct a confidence\ninterval. The performance of our method is validated by extensive simulations\nand real data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate elliptically-contoured distributions are widely used for\nmodeling correlated and non-Gaussian data. In this work, we study the kurtosis\nof the elliptical model, which is an important parameter in many statistical\nanalysis. Based on U-statistics, we develop an estimation method.\nTheoretically, we show that the proposed estimator is consistent under regular\nconditions, especially we relax a moment condition and the restriction that the\ndata dimension and the sample size are of the same order. Furthermore, we\nderive the asymptotic normality of the estimator and evaluate the asymptotic\nvariance through several examples, which allows us to construct a confidence\ninterval. The performance of our method is validated by extensive simulations\nand real data analysis."
                },
                "authors": [
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Peirong Xu"
                    },
                    {
                        "name": "Cheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Wang"
                },
                "author": "Cheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.12599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12599v1",
                "updated": "2024-08-22T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "Controllable Text Generation for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Text Generation for Large Language Models: A Survey"
                },
                "summary": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "52 pages, 11 figures, 7 tables, 11 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13709v2",
                "updated": "2024-08-22T17:56:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    15,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-18T17:08:10Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    8,
                    10,
                    3,
                    200,
                    0
                ],
                "title": "Understanding Reference Policies in Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reference Policies in Direct Preference Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "GitHub Repo: https://github.com/yale-nlp/refdpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12590v1",
                "updated": "2024-08-22T17:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations"
                },
                "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models."
                },
                "authors": [
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Krithika Ramakrishnan"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Lifu Tu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Anas Awadalla"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Accepted by ECCV24 AI4VA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12579v1",
                "updated": "2024-08-22T17:44:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment"
                },
                "summary": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians."
                },
                "authors": [
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21051v2",
                "updated": "2024-08-22T17:27:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    27,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-05-31T17:43:59Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    43,
                    59,
                    4,
                    152,
                    0
                ],
                "title": "Good Modelling Software Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Modelling Software Practices"
                },
                "summary": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model."
                },
                "authors": [
                    {
                        "name": "Carsten Lemmen"
                    },
                    {
                        "name": "Philipp Sebastian Sommer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Sebastian Sommer"
                },
                "author": "Philipp Sebastian Sommer",
                "arxiv_comment": "2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.21051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.0; D.2.4; D.2.5; D.2.11; D.2.12; G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08924v2",
                "updated": "2024-08-22T17:21:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    21,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-15T14:51:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    51,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Weiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhang"
                },
                "author": "Weiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v1",
                "updated": "2024-08-22T17:01:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12513v1",
                "updated": "2024-08-22T16:08:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    8,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T16:08:45Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    8,
                    45,
                    3,
                    235,
                    0
                ],
                "title": "Beyond Shortsighted Navigation: Merging Best View Trajectory Planning\n  with Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortsighted Navigation: Merging Best View Trajectory Planning\n  with Robot Navigation"
                },
                "summary": "Gathering visual information effectively to monitor known environments is a\nkey challenge in robotics. To be as efficient as human surveyors, robotic\nsystems must continuously collect observational data required to complete their\nsurvey task. Inspection personnel instinctively know to look at relevant\nequipment that happens to be ``along the way.'' In this paper, we introduce a\nnovel framework for continuous long-horizon viewpoint planning, for ground\nrobots, applied to tasks involving patrolling, monitoring or visual data\ngathering in known environments. Our approach to Long Horizon Viewpoint\nPlanning (LHVP), enables the robot to autonomously navigate and collect\nenvironmental data optimizing for coverage over the horizon of the patrol.\nLeveraging a quadruped's mobility and sensory capabilities, our LHVP framework\nplans patrol paths that account for coupling the viewpoint planner for the arm\ncamera with the mobile base's navigation planner. The viewpath optimization\nalgorithm seeks a balance between comprehensive environmental coverage and\ndynamically feasible movements, thus ensuring prolonged and effective operation\nin scenarios including monitoring, security surveillance, and disaster\nresponse. We validate our approach through simulations and in the real world\nand show that our LHVP significantly outperforms naive patrolling methods in\nterms of area coverage generating information-gathering trajectories for the\nrobot arm. Our results indicate a promising direction for the deployment of\nmobile robots in long-term, autonomous surveying, and environmental data\ncollection tasks, highlighting the potential of intelligent robotic systems in\nchallenging real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gathering visual information effectively to monitor known environments is a\nkey challenge in robotics. To be as efficient as human surveyors, robotic\nsystems must continuously collect observational data required to complete their\nsurvey task. Inspection personnel instinctively know to look at relevant\nequipment that happens to be ``along the way.'' In this paper, we introduce a\nnovel framework for continuous long-horizon viewpoint planning, for ground\nrobots, applied to tasks involving patrolling, monitoring or visual data\ngathering in known environments. Our approach to Long Horizon Viewpoint\nPlanning (LHVP), enables the robot to autonomously navigate and collect\nenvironmental data optimizing for coverage over the horizon of the patrol.\nLeveraging a quadruped's mobility and sensory capabilities, our LHVP framework\nplans patrol paths that account for coupling the viewpoint planner for the arm\ncamera with the mobile base's navigation planner. The viewpath optimization\nalgorithm seeks a balance between comprehensive environmental coverage and\ndynamically feasible movements, thus ensuring prolonged and effective operation\nin scenarios including monitoring, security surveillance, and disaster\nresponse. We validate our approach through simulations and in the real world\nand show that our LHVP significantly outperforms naive patrolling methods in\nterms of area coverage generating information-gathering trajectories for the\nrobot arm. Our results indicate a promising direction for the deployment of\nmobile robots in long-term, autonomous surveying, and environmental data\ncollection tasks, highlighting the potential of intelligent robotic systems in\nchallenging real-world applications."
                },
                "authors": [
                    {
                        "name": "Srinath Tankasala"
                    },
                    {
                        "name": "Roberto Mart√≠n-Mart√≠n"
                    },
                    {
                        "name": "Mitch Pryor"
                    }
                ],
                "author_detail": {
                    "name": "Mitch Pryor"
                },
                "author": "Mitch Pryor",
                "arxiv_comment": "7 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10259v3",
                "updated": "2024-08-22T15:52:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    52,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-16T03:26:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    26,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy"
                },
                "summary": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events."
                },
                "authors": [
                    {
                        "name": "Tunazzina Islam"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v5",
                "updated": "2024-08-22T15:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    44,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12496v1",
                "updated": "2024-08-22T15:41:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:41:58Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "title": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework"
                },
                "summary": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms."
                },
                "authors": [
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Wu Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Yuan"
                },
                "author": "Wu Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12494v1",
                "updated": "2024-08-22T15:35:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:35:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24."
                },
                "authors": [
                    {
                        "name": "Kunsheng Tang"
                    },
                    {
                        "name": "Wenbo Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Peigui Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12491v1",
                "updated": "2024-08-22T15:31:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    31,
                    48,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:31:48Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    31,
                    48,
                    3,
                    235,
                    0
                ],
                "title": "AI in radiological imaging of soft-tissue and bone tumours: a systematic\n  review evaluating against CLAIM and FUTURE-AI guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in radiological imaging of soft-tissue and bone tumours: a systematic\n  review evaluating against CLAIM and FUTURE-AI guidelines"
                },
                "summary": "Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging\nlesions with variable clinical behaviours and treatment approaches. This\nsystematic review provides an overview of Artificial Intelligence (AI) methods\nusing radiological imaging for diagnosis and prognosis of these tumours,\nhighlighting challenges in clinical translation, and evaluating study alignment\nwith the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI\ninternational consensus guidelines for trustworthy and deployable AI to promote\nthe clinical translation of AI methods. The review covered literature from\nseveral bibliographic databases, including papers published before 17/07/2024.\nOriginal research in peer-reviewed journals focused on radiology-based AI for\ndiagnosing or prognosing primary STBT was included. Exclusion criteria were\nanimal, cadaveric, or laboratory studies, and non-English papers. Abstracts\nwere screened by two of three independent reviewers for eligibility. Eligible\npapers were assessed against guidelines by one of three independent reviewers.\nThe search identified 15,015 abstracts, from which 325 articles were included\nfor evaluation. Most studies performed moderately on CLAIM, averaging a score\nof 28.9$\\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\\pm$2.1 out\nof 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,\nindicating significant room for improvement. Future efforts by AI developers\nshould focus on design (e.g. define unmet clinical need, intended clinical\nsetting and how AI would be integrated in clinical workflow), development (e.g.\nbuild on previous work, explainability), evaluation (e.g. evaluating and\naddressing biases, evaluating AI against best practices), and data\nreproducibility and availability (making documented code and data publicly\navailable). Following these recommendations could improve clinical translation\nof AI methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging\nlesions with variable clinical behaviours and treatment approaches. This\nsystematic review provides an overview of Artificial Intelligence (AI) methods\nusing radiological imaging for diagnosis and prognosis of these tumours,\nhighlighting challenges in clinical translation, and evaluating study alignment\nwith the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI\ninternational consensus guidelines for trustworthy and deployable AI to promote\nthe clinical translation of AI methods. The review covered literature from\nseveral bibliographic databases, including papers published before 17/07/2024.\nOriginal research in peer-reviewed journals focused on radiology-based AI for\ndiagnosing or prognosing primary STBT was included. Exclusion criteria were\nanimal, cadaveric, or laboratory studies, and non-English papers. Abstracts\nwere screened by two of three independent reviewers for eligibility. Eligible\npapers were assessed against guidelines by one of three independent reviewers.\nThe search identified 15,015 abstracts, from which 325 articles were included\nfor evaluation. Most studies performed moderately on CLAIM, averaging a score\nof 28.9$\\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\\pm$2.1 out\nof 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,\nindicating significant room for improvement. Future efforts by AI developers\nshould focus on design (e.g. define unmet clinical need, intended clinical\nsetting and how AI would be integrated in clinical workflow), development (e.g.\nbuild on previous work, explainability), evaluation (e.g. evaluating and\naddressing biases, evaluating AI against best practices), and data\nreproducibility and availability (making documented code and data publicly\navailable). Following these recommendations could improve clinical translation\nof AI methods."
                },
                "authors": [
                    {
                        "name": "Douwe J. Spaanderman"
                    },
                    {
                        "name": "Matthew Marzetti"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Andrew F. Scarsbrook"
                    },
                    {
                        "name": "Philip Robinson"
                    },
                    {
                        "name": "Edwin H. G. Oei"
                    },
                    {
                        "name": "Jacob J. Visser"
                    },
                    {
                        "name": "Robert Hemke"
                    },
                    {
                        "name": "Kirsten van Langevelde"
                    },
                    {
                        "name": "David F. Hanff"
                    },
                    {
                        "name": "Geert J. L. H. van Leenders"
                    },
                    {
                        "name": "Cornelis Verhoef"
                    },
                    {
                        "name": "Dirk J. Gru√ºhagen"
                    },
                    {
                        "name": "Wiro J. Niessen"
                    },
                    {
                        "name": "Stefan Klein"
                    },
                    {
                        "name": "Martijn P. A. Starmans"
                    }
                ],
                "author_detail": {
                    "name": "Martijn P. A. Starmans"
                },
                "arxiv_affiliation": "Department of Radiology and Nuclear Medicine, Erasmus MC Cancer Institute, University Medical Center Rotterdam, Rotterdam, the Netherlands",
                "author": "Martijn P. A. Starmans",
                "arxiv_comment": "23 pages, 6 figures, 6 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12481v1",
                "updated": "2024-08-22T15:17:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    17,
                    2,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:17:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    17,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "Self-Learning for Personalized Keyword Spotting on Ultra-Low-Power Audio\n  Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Learning for Personalized Keyword Spotting on Ultra-Low-Power Audio\n  Sensors"
                },
                "summary": "This paper proposes a self-learning framework to incrementally train\n(fine-tune) a personalized Keyword Spotting (KWS) model after the deployment on\nultra-low power smart audio sensors. We address the fundamental problem of the\nabsence of labeled training data by assigning pseudo-labels to the new recorded\naudio frames based on a similarity score with respect to few user recordings.\nBy experimenting with multiple KWS models with a number of parameters up to\n0.5M on two public datasets, we show an accuracy improvement of up to +19.2%\nand +16.0% vs. the initial models pretrained on a large set of generic\nkeywords. The labeling task is demonstrated on a sensor system composed of a\nlow-power microphone and an energy-efficient Microcontroller (MCU). By\nefficiently exploiting the heterogeneous processing engines of the MCU, the\nalways-on labeling task runs in real-time with an average power cost of up to\n8.2 mW. On the same platform, we estimate an energy cost for on-device training\n10x lower than the labeling energy if sampling a new utterance every 5 s or\n16.4 s with a DS-CNN-S or a DS-CNN-M model. Our empirical result paves the way\nto self-adaptive personalized KWS sensors at the extreme edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a self-learning framework to incrementally train\n(fine-tune) a personalized Keyword Spotting (KWS) model after the deployment on\nultra-low power smart audio sensors. We address the fundamental problem of the\nabsence of labeled training data by assigning pseudo-labels to the new recorded\naudio frames based on a similarity score with respect to few user recordings.\nBy experimenting with multiple KWS models with a number of parameters up to\n0.5M on two public datasets, we show an accuracy improvement of up to +19.2%\nand +16.0% vs. the initial models pretrained on a large set of generic\nkeywords. The labeling task is demonstrated on a sensor system composed of a\nlow-power microphone and an energy-efficient Microcontroller (MCU). By\nefficiently exploiting the heterogeneous processing engines of the MCU, the\nalways-on labeling task runs in real-time with an average power cost of up to\n8.2 mW. On the same platform, we estimate an energy cost for on-device training\n10x lower than the labeling energy if sampling a new utterance every 5 s or\n16.4 s with a DS-CNN-S or a DS-CNN-M model. Our empirical result paves the way\nto self-adaptive personalized KWS sensors at the extreme edge."
                },
                "authors": [
                    {
                        "name": "Manuele Rusci"
                    },
                    {
                        "name": "Francesco Paci"
                    },
                    {
                        "name": "Marco Fariselli"
                    },
                    {
                        "name": "Eric Flamand"
                    },
                    {
                        "name": "Tinne Tuytelaars"
                    }
                ],
                "author_detail": {
                    "name": "Tinne Tuytelaars"
                },
                "author": "Tinne Tuytelaars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12475v1",
                "updated": "2024-08-22T15:13:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    13,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:13:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    13,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action\n  Recognition"
                },
                "summary": "In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for\nfew-shot action recognition (FSAR), which incorporates a sequential perceiver\nadapter into the pre-training framework, to integrate both the spatial\ninformation and the sequential temporal dynamics into the feature embeddings.\nDifferent from the existing fine-tuning approaches that capture temporal\ninformation by exploring the relationships among all the frames, our\nperceiver-based adapter recurrently captures the sequential dynamics alongside\nthe timeline, which could perceive the order change. To obtain the\ndiscriminative representations for each class, we extend a textual corpus for\neach class derived from the large language models (LLMs) and enrich the visual\nprototypes by integrating the contextual semantic information. Besides, We\nintroduce an unbalanced optimal transport strategy for feature matching that\nmitigates the impact of class-unrelated features, thereby facilitating more\neffective decision-making. Experimental results on five FSAR datasets\ndemonstrate that our method set a new benchmark, beating the second-best\ncompetitors with large margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for\nfew-shot action recognition (FSAR), which incorporates a sequential perceiver\nadapter into the pre-training framework, to integrate both the spatial\ninformation and the sequential temporal dynamics into the feature embeddings.\nDifferent from the existing fine-tuning approaches that capture temporal\ninformation by exploring the relationships among all the frames, our\nperceiver-based adapter recurrently captures the sequential dynamics alongside\nthe timeline, which could perceive the order change. To obtain the\ndiscriminative representations for each class, we extend a textual corpus for\neach class derived from the large language models (LLMs) and enrich the visual\nprototypes by integrating the contextual semantic information. Besides, We\nintroduce an unbalanced optimal transport strategy for feature matching that\nmitigates the impact of class-unrelated features, thereby facilitating more\neffective decision-making. Experimental results on five FSAR datasets\ndemonstrate that our method set a new benchmark, beating the second-best\ncompetitors with large margins."
                },
                "authors": [
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Yunlong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yunlong Yu"
                },
                "author": "Yunlong Yu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12470v1",
                "updated": "2024-08-22T15:10:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:10:56Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "title": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12469v1",
                "updated": "2024-08-22T15:10:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:10:20Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    20,
                    3,
                    235,
                    0
                ],
                "title": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot\n  Learning"
                },
                "summary": "Few-shot learning (FSL) aims to recognize new concepts using a limited number\nof visual samples. Existing approaches attempt to incorporate semantic\ninformation into the limited visual data for category understanding. However,\nthese methods often enrich class-level feature representations with abstract\ncategory names, failing to capture the nuanced features essential for effective\ngeneralization. To address this issue, we propose a novel framework for FSL,\nwhich incorporates both the abstract class semantics and the concrete class\nentities extracted from Large Language Models (LLMs), to enhance the\nrepresentation of the class prototypes. Specifically, our framework composes a\nSemantic-guided Visual Pattern Extraction (SVPE) module and a\nPrototype-Calibration (PC) module, where the SVPE meticulously extracts\nsemantic-aware visual patterns across diverse scales, while the PC module\nseamlessly integrates these patterns to refine the visual prototype, enhancing\nits representativeness. Extensive experiments on four few-shot classification\nbenchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable\nadvancements over the current state-of-the-art methods. Notably, for the\nchallenging one-shot setting, our approach, utilizing the ResNet-12 backbone,\nachieves an impressive average improvement of 1.95% over the second-best\ncompetitor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot learning (FSL) aims to recognize new concepts using a limited number\nof visual samples. Existing approaches attempt to incorporate semantic\ninformation into the limited visual data for category understanding. However,\nthese methods often enrich class-level feature representations with abstract\ncategory names, failing to capture the nuanced features essential for effective\ngeneralization. To address this issue, we propose a novel framework for FSL,\nwhich incorporates both the abstract class semantics and the concrete class\nentities extracted from Large Language Models (LLMs), to enhance the\nrepresentation of the class prototypes. Specifically, our framework composes a\nSemantic-guided Visual Pattern Extraction (SVPE) module and a\nPrototype-Calibration (PC) module, where the SVPE meticulously extracts\nsemantic-aware visual patterns across diverse scales, while the PC module\nseamlessly integrates these patterns to refine the visual prototype, enhancing\nits representativeness. Extensive experiments on four few-shot classification\nbenchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable\nadvancements over the current state-of-the-art methods. Notably, for the\nchallenging one-shot setting, our approach, utilizing the ResNet-12 backbone,\nachieves an impressive average improvement of 1.95% over the second-best\ncompetitor."
                },
                "authors": [
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Ziqian Lu"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12468v1",
                "updated": "2024-08-22T15:08:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    8,
                    21,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:08:21Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    8,
                    21,
                    3,
                    235,
                    0
                ],
                "title": "A Constant-Approximation Algorithm for Budgeted Sweep Coverage with\n  Mobile Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Constant-Approximation Algorithm for Budgeted Sweep Coverage with\n  Mobile Sensors"
                },
                "summary": "In this paper, we present the first constant-approximation algorithm for {\\em\nbudgeted sweep coverage problem} (BSC). The BSC involves designing routes for a\nnumber of mobile sensors (a.k.a. robots) to periodically collect information as\nmuch as possible from points of interest (PoIs). To approach this problem, we\npropose to first examine the {\\em multi-orienteering problem} (MOP). The MOP\naims to find a set of $m$ vertex-disjoint paths that cover as many vertices as\npossible while adhering to a budget constraint $B$. We develop a\nconstant-approximation algorithm for MOP and utilize it to achieve a\nconstant-approximation for BSC. Our findings open new possibilities for\noptimizing mobile sensor deployments and related combinatorial optimization\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the first constant-approximation algorithm for {\\em\nbudgeted sweep coverage problem} (BSC). The BSC involves designing routes for a\nnumber of mobile sensors (a.k.a. robots) to periodically collect information as\nmuch as possible from points of interest (PoIs). To approach this problem, we\npropose to first examine the {\\em multi-orienteering problem} (MOP). The MOP\naims to find a set of $m$ vertex-disjoint paths that cover as many vertices as\npossible while adhering to a budget constraint $B$. We develop a\nconstant-approximation algorithm for MOP and utilize it to achieve a\nconstant-approximation for BSC. Our findings open new possibilities for\noptimizing mobile sensor deployments and related combinatorial optimization\ntasks."
                },
                "authors": [
                    {
                        "name": "Wei Liang"
                    },
                    {
                        "name": "Shaojie Tang"
                    },
                    {
                        "name": "Zhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Zhang"
                },
                "author": "Zhao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12456v1",
                "updated": "2024-08-22T14:53:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    53,
                    33,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:53:33Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    53,
                    33,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Multi-hop Reasoning through Knowledge Erasure in Large\n  Language Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-hop Reasoning through Knowledge Erasure in Large\n  Language Model Editing"
                },
                "summary": "Large language models (LLMs) face challenges with internal knowledge\ninaccuracies and outdated information. Knowledge editing has emerged as a\npivotal approach to mitigate these issues. Although current knowledge editing\ntechniques exhibit promising performance in single-hop reasoning tasks, they\nshow limitations when applied to multi-hop reasoning. Drawing on cognitive\nneuroscience and the operational mechanisms of LLMs, we hypothesize that the\nresidual single-hop knowledge after editing causes edited models to revert to\ntheir original answers when processing multi-hop questions, thereby undermining\ntheir performance in multihop reasoning tasks. To validate this hypothesis, we\nconduct a series of experiments that empirically confirm our assumptions.\nBuilding on the validated hypothesis, we propose a novel knowledge editing\nmethod that incorporates a Knowledge Erasure mechanism for Large language model\nEditing (KELE). Specifically, we design an erasure function for residual\nknowledge and an injection function for new knowledge. Through joint\noptimization, we derive the optimal recall vector, which is subsequently\nutilized within a rank-one editing framework to update the parameters of\ntargeted model layers. Extensive experiments on GPT-J and GPT-2 XL demonstrate\nthat KELE substantially enhances the multi-hop reasoning capability of edited\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face challenges with internal knowledge\ninaccuracies and outdated information. Knowledge editing has emerged as a\npivotal approach to mitigate these issues. Although current knowledge editing\ntechniques exhibit promising performance in single-hop reasoning tasks, they\nshow limitations when applied to multi-hop reasoning. Drawing on cognitive\nneuroscience and the operational mechanisms of LLMs, we hypothesize that the\nresidual single-hop knowledge after editing causes edited models to revert to\ntheir original answers when processing multi-hop questions, thereby undermining\ntheir performance in multihop reasoning tasks. To validate this hypothesis, we\nconduct a series of experiments that empirically confirm our assumptions.\nBuilding on the validated hypothesis, we propose a novel knowledge editing\nmethod that incorporates a Knowledge Erasure mechanism for Large language model\nEditing (KELE). Specifically, we design an erasure function for residual\nknowledge and an injection function for new knowledge. Through joint\noptimization, we derive the optimal recall vector, which is subsequently\nutilized within a rank-one editing framework to update the parameters of\ntargeted model layers. Extensive experiments on GPT-J and GPT-2 XL demonstrate\nthat KELE substantially enhances the multi-hop reasoning capability of edited\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Bowen Fang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12429v1",
                "updated": "2024-08-22T14:22:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    22,
                    7,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:22:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    22,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing"
                },
                "summary": "Combining Vision Large Language Models (VLLMs) with diffusion models offers a\npowerful method for executing image editing tasks based on human language\ninstructions. However, language instructions alone often fall short in\naccurately conveying user requirements, particularly when users want to add,\nreplace elements in specific areas of an image. Luckily, masks can effectively\nindicate the exact locations or elements to be edited, while they require users\nto precisely draw the shapes at the desired locations, which is highly\nuser-unfriendly. To address this, we propose FlexEdit, an end-to-end image\nediting method that leverages both free-shape masks and language instructions\nfor Flexible Editing. Our approach employs a VLLM in comprehending the image\ncontent, mask, and user instructions. Additionally, we introduce the Mask\nEnhance Adapter (MEA) that fuses the embeddings of the VLLM with the image\ndata, ensuring a seamless integration of mask information and model output\nembeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically\ntailored for free-shape mask, including 8 types of free-shape mask. Extensive\nexperiments show that our method achieves state-of-the-art (SOTA) performance\nin LLM-based image editing, and our simple prompting technique stands out in\nits effectiveness. The code and data can be found at\nhttps://github.com/A-new-b/flex_edit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Vision Large Language Models (VLLMs) with diffusion models offers a\npowerful method for executing image editing tasks based on human language\ninstructions. However, language instructions alone often fall short in\naccurately conveying user requirements, particularly when users want to add,\nreplace elements in specific areas of an image. Luckily, masks can effectively\nindicate the exact locations or elements to be edited, while they require users\nto precisely draw the shapes at the desired locations, which is highly\nuser-unfriendly. To address this, we propose FlexEdit, an end-to-end image\nediting method that leverages both free-shape masks and language instructions\nfor Flexible Editing. Our approach employs a VLLM in comprehending the image\ncontent, mask, and user instructions. Additionally, we introduce the Mask\nEnhance Adapter (MEA) that fuses the embeddings of the VLLM with the image\ndata, ensuring a seamless integration of mask information and model output\nembeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically\ntailored for free-shape mask, including 8 types of free-shape mask. Extensive\nexperiments show that our method achieves state-of-the-art (SOTA) performance\nin LLM-based image editing, and our simple prompting technique stands out in\nits effectiveness. The code and data can be found at\nhttps://github.com/A-new-b/flex_edit."
                },
                "authors": [
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yuxiang Lin"
                    },
                    {
                        "name": "Tianshuo Yuan"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Jiao GH"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojiang Peng"
                },
                "author": "Xiaojiang Peng",
                "arxiv_comment": "15 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.12767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.12767v2",
                "updated": "2024-08-22T14:19:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    19,
                    6,
                    3,
                    235,
                    0
                ],
                "published": "2023-03-22T17:32:56Z",
                "published_parsed": [
                    2023,
                    3,
                    22,
                    17,
                    32,
                    56,
                    2,
                    81,
                    0
                ],
                "title": "Can we trust the evaluation on ChatGPT?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we trust the evaluation on ChatGPT?"
                },
                "summary": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models."
                },
                "authors": [
                    {
                        "name": "Rachith Aiyappa"
                    },
                    {
                        "name": "Jisun An"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Yeol Ahn"
                },
                "author": "Yong-Yeol Ahn",
                "arxiv_doi": "10.18653/v1/2023.trustnlp-1.5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.12767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.12767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2023) (July 2023) 47-54",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12416v1",
                "updated": "2024-08-22T14:12:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    12,
                    6,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T14:12:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    12,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "Unlearning Trojans in Large Language Models: A Comparison Between\n  Natural Language and Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Trojans in Large Language Models: A Comparison Between\n  Natural Language and Source Code"
                },
                "summary": "This work investigates the application of Machine Unlearning (MU) for\nmitigating the impact of trojans embedded in conventional large language models\nof natural language (Text-LLMs) and large language models of code (Code-LLMs)\nWe propose a novel unlearning approach, LYA, that leverages both gradient\nascent and elastic weight consolidation, a Fisher Information Matrix (FIM)\nbased regularization technique, to unlearn trojans from poisoned models. We\ncompare the effectiveness of LYA against conventional techniques like\nfine-tuning, retraining, and vanilla gradient ascent. The subject models we\ninvestigate are BERT and CodeBERT, for sentiment analysis and code defect\ndetection tasks, respectively. Our findings demonstrate that the combination of\ngradient ascent and FIM-based regularization, as done in LYA, outperforms\nexisting methods in removing the trojan's influence from the poisoned model,\nwhile preserving its original functionality. To the best of our knowledge, this\nis the first work that compares and contrasts MU of trojans in LLMs, in the NL\nand Coding domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the application of Machine Unlearning (MU) for\nmitigating the impact of trojans embedded in conventional large language models\nof natural language (Text-LLMs) and large language models of code (Code-LLMs)\nWe propose a novel unlearning approach, LYA, that leverages both gradient\nascent and elastic weight consolidation, a Fisher Information Matrix (FIM)\nbased regularization technique, to unlearn trojans from poisoned models. We\ncompare the effectiveness of LYA against conventional techniques like\nfine-tuning, retraining, and vanilla gradient ascent. The subject models we\ninvestigate are BERT and CodeBERT, for sentiment analysis and code defect\ndetection tasks, respectively. Our findings demonstrate that the combination of\ngradient ascent and FIM-based regularization, as done in LYA, outperforms\nexisting methods in removing the trojan's influence from the poisoned model,\nwhile preserving its original functionality. To the best of our knowledge, this\nis the first work that compares and contrasts MU of trojans in LLMs, in the NL\nand Coding domain."
                },
                "authors": [
                    {
                        "name": "Mahdi Kazemi"
                    },
                    {
                        "name": "Aftab Hussain"
                    },
                    {
                        "name": "Md Rafiqul Islam Rabin"
                    },
                    {
                        "name": "Mohammad Amin Alipour"
                    },
                    {
                        "name": "Sen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Sen Lin"
                },
                "author": "Sen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07862v2",
                "updated": "2024-08-22T13:57:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    57,
                    30,
                    3,
                    235,
                    0
                ],
                "published": "2024-02-12T18:14:43Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    14,
                    43,
                    0,
                    43,
                    0
                ],
                "title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting\n  Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting\n  Accuracy"
                },
                "summary": "Large language models (LLMs) match and sometimes exceeding human performance\nin many domains. This study explores the potential of LLMs to augment human\njudgement in a forecasting task. We evaluate the effect on human forecasters of\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\nadvice, and the other designed to be overconfident and base-rate neglecting,\nthus providing noisy forecasting advice. We compare participants using these\nassistants to a control group that received a less advanced model that did not\nprovide numerical predictions or engaged in explicit discussion of predictions.\nParticipants (N = 991) answered a set of six forecasting questions and had the\noption to consult their assigned LLM assistant throughout. Our preregistered\nanalyses show that interacting with each of our frontier LLM assistants\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\ncompared to the control group. Exploratory analyses showed a pronounced outlier\neffect in one forecasting item, without which we find that the superforecasting\nassistant increased accuracy by 41 percent, compared with 29 percent for the\nnoisy assistant. We further examine whether LLM forecasting augmentation\ndisproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our data do not consistently support\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\ncompared to a less powerful model that does not provide specific forecasting\nadvice. However, the effects of outliers suggest that further research into the\nrobustness of this pattern is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) match and sometimes exceeding human performance\nin many domains. This study explores the potential of LLMs to augment human\njudgement in a forecasting task. We evaluate the effect on human forecasters of\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\nadvice, and the other designed to be overconfident and base-rate neglecting,\nthus providing noisy forecasting advice. We compare participants using these\nassistants to a control group that received a less advanced model that did not\nprovide numerical predictions or engaged in explicit discussion of predictions.\nParticipants (N = 991) answered a set of six forecasting questions and had the\noption to consult their assigned LLM assistant throughout. Our preregistered\nanalyses show that interacting with each of our frontier LLM assistants\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\ncompared to the control group. Exploratory analyses showed a pronounced outlier\neffect in one forecasting item, without which we find that the superforecasting\nassistant increased accuracy by 41 percent, compared with 29 percent for the\nnoisy assistant. We further examine whether LLM forecasting augmentation\ndisproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our data do not consistently support\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\ncompared to a less powerful model that does not provide specific forecasting\nadvice. However, the effects of outliers suggest that further research into the\nrobustness of this pattern is needed."
                },
                "authors": [
                    {
                        "name": "Philipp Schoenegger"
                    },
                    {
                        "name": "Peter S. Park"
                    },
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Sean Trott"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "arxiv_comment": "22 pages pages (main text comprised of 19 pages, appendix comprised\n  of three pages). 10 visualizations in the main text (four figures, six\n  tables), three additional figures in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12398v1",
                "updated": "2024-08-22T13:44:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    44,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T13:44:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    44,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "A Comparative Analysis of Faithfulness Metrics and Humans in Citation\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of Faithfulness Metrics and Humans in Citation\n  Evaluation"
                },
                "summary": "Large language models (LLMs) often generate content with unsupported or\nunverifiable content, known as \"hallucinations.\" To address this,\nretrieval-augmented LLMs are employed to include citations in their content,\ngrounding the content in verifiable sources. Despite such developments,\nmanually assessing how well a citation supports the associated statement\nremains a major challenge. Previous studies tackle this challenge by leveraging\nfaithfulness metrics to estimate citation support automatically. However, they\nlimit this citation support estimation to a binary classification scenario,\nneglecting fine-grained citation support in practical scenarios. To investigate\nthe effectiveness of faithfulness metrics in fine-grained scenarios, we propose\na comparative evaluation framework that assesses the metric effectiveness in\ndistinguishing citations between three-category support levels: full, partial,\nand no support. Our framework employs correlation analysis, classification\nevaluation, and retrieval evaluation to measure the alignment between metric\nscores and human judgments comprehensively. Our results indicate no single\nmetric consistently excels across all evaluations, highlighting the complexity\nof accurately evaluating fine-grained support levels. Particularly, we find\nthat the best-performing metrics struggle to distinguish partial support from\nfull or no support. Based on these findings, we provide practical\nrecommendations for developing more effective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate content with unsupported or\nunverifiable content, known as \"hallucinations.\" To address this,\nretrieval-augmented LLMs are employed to include citations in their content,\ngrounding the content in verifiable sources. Despite such developments,\nmanually assessing how well a citation supports the associated statement\nremains a major challenge. Previous studies tackle this challenge by leveraging\nfaithfulness metrics to estimate citation support automatically. However, they\nlimit this citation support estimation to a binary classification scenario,\nneglecting fine-grained citation support in practical scenarios. To investigate\nthe effectiveness of faithfulness metrics in fine-grained scenarios, we propose\na comparative evaluation framework that assesses the metric effectiveness in\ndistinguishing citations between three-category support levels: full, partial,\nand no support. Our framework employs correlation analysis, classification\nevaluation, and retrieval evaluation to measure the alignment between metric\nscores and human judgments comprehensively. Our results indicate no single\nmetric consistently excels across all evaluations, highlighting the complexity\nof accurately evaluating fine-grained support levels. Particularly, we find\nthat the best-performing metrics struggle to distinguish partial support from\nfull or no support. Based on these findings, we provide practical\nrecommendations for developing more effective metrics."
                },
                "authors": [
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the First Workshop on Large Language Model for Evaluation\n  in Information Retrieval (LLM4Eval@SIGIR2024), non-archival. arXiv admin\n  note: substantial text overlap with arXiv:2406.15264",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03608v2",
                "updated": "2024-08-22T13:13:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    13,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-07T07:54:19Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    7,
                    54,
                    19,
                    2,
                    220,
                    0
                ],
                "title": "Mixstyle-Entropy: Domain Generalization with Causal Intervention and\n  Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixstyle-Entropy: Domain Generalization with Causal Intervention and\n  Perturbation"
                },
                "summary": "Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer."
                },
                "authors": [
                    {
                        "name": "Luyao Tang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Chaoqi Chen"
                    },
                    {
                        "name": "Xinghao Ding"
                    },
                    {
                        "name": "Yue Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Huang"
                },
                "author": "Yue Huang",
                "arxiv_comment": "Accepted by BMVC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12369v2",
                "updated": "2024-08-23T08:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    11,
                    9,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T13:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    13,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering"
                },
                "summary": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data."
                },
                "authors": [
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Kuber Vijaykumar Bellad"
                    },
                    {
                        "name": "Bharat Vadlamudi"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16823v3",
                "updated": "2024-08-22T13:06:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    6,
                    51,
                    3,
                    235,
                    0
                ],
                "published": "2024-02-26T18:48:27Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    18,
                    48,
                    27,
                    0,
                    57,
                    0
                ],
                "title": "Language Agents as Optimizable Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Agents as Optimizable Graphs"
                },
                "summary": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm."
                },
                "authors": [
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Louis Kirsch"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Dmitrii Khizbullin"
                    },
                    {
                        "name": "J√ºrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "J√ºrgen Schmidhuber"
                },
                "author": "J√ºrgen Schmidhuber",
                "arxiv_comment": "Project Website: https://gptswarm.org ; Github Repo:\n  https://github.com/metauto-ai/gptswarm . In Forty-first International\n  Conference on Machine Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12362v1",
                "updated": "2024-08-22T12:59:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    59,
                    5,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:59:05Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    59,
                    5,
                    3,
                    235,
                    0
                ],
                "title": "CLEANANERCorp: Identifying and Correcting Incorrect Labels in the\n  ANERcorp Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEANANERCorp: Identifying and Correcting Incorrect Labels in the\n  ANERcorp Dataset"
                },
                "summary": "Label errors are a common issue in machine learning datasets, particularly\nfor tasks such as Named Entity Recognition. Such label errors might hurt model\ntraining, affect evaluation results, and lead to an inaccurate assessment of\nmodel performance. In this study, we dived deep into one of the widely adopted\nArabic NER benchmark datasets (ANERcorp) and found a significant number of\nannotation errors, missing labels, and inconsistencies. Therefore, in this\nstudy, we conducted empirical research to understand these errors, correct them\nand propose a cleaner version of the dataset named CLEANANERCorp. CLEANANERCorp\nwill serve the research community as a more accurate and consistent benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label errors are a common issue in machine learning datasets, particularly\nfor tasks such as Named Entity Recognition. Such label errors might hurt model\ntraining, affect evaluation results, and lead to an inaccurate assessment of\nmodel performance. In this study, we dived deep into one of the widely adopted\nArabic NER benchmark datasets (ANERcorp) and found a significant number of\nannotation errors, missing labels, and inconsistencies. Therefore, in this\nstudy, we conducted empirical research to understand these errors, correct them\nand propose a cleaner version of the dataset named CLEANANERCorp. CLEANANERCorp\nwill serve the research community as a more accurate and consistent benchmark."
                },
                "authors": [
                    {
                        "name": "Mashael Al-Duwais"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Abdulmalik Al-Salman"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmalik Al-Salman"
                },
                "author": "Abdulmalik Al-Salman",
                "arxiv_comment": "Proceedings of the 6th Workshop on Open-Source Arabic Corpora and\n  Processing Tools (OSACT) with Shared Tasks on Arabic LLMs Hallucination and\n  Dialect to MSA Machine Translation @ LREC-COLING 2024",
                "arxiv_journal_ref": "ELRA and ICCL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12359v1",
                "updated": "2024-08-22T12:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    57,
                    9,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    57,
                    9,
                    3,
                    235,
                    0
                ],
                "title": "SoK: An Introspective Analysis of RPKI Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: An Introspective Analysis of RPKI Security"
                },
                "summary": "The Resource Public Key Infrastructure (RPKI) is the main mechanism to\nprotect inter-domain routing with BGP from prefix hijacks. It has already been\nwidely deployed by large providers and the adoption rate is getting to a\ncritical point. Almost half of all the global prefixes are now covered by RPKI\nand measurements show that 27% of networks are already using RPKI to validate\nBGP announcements. Over the past 10 years, there has been much research effort\nin RPKI, analyzing different facets of the protocol, such as software\nvulnerabilities, robustness of the infrastructure or the proliferation of RPKI\nvalidation. In this work we compile the first systemic overview of the\nvulnerabilities and misconfigurations in RPKI and quantify the security\nlandscape of the global RPKI deployments based on our measurements and\nanalysis. Our study discovers that 56% of the global RPKI validators suffer\nfrom at least one documented vulnerability. We also do a systematization of\nknowledge for existing RPKI security research and complement the existing\nknowledge with novel measurements in which we discover new trends in\navailability of RPKI repositories, and their communication patterns with the\nRPKI validators. We weave together the results of existing research and our\nstudy, to provide a comprehensive tableau of vulnerabilities, their sources,\nand to derive future research paths necessary to prepare RPKI for full global\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Resource Public Key Infrastructure (RPKI) is the main mechanism to\nprotect inter-domain routing with BGP from prefix hijacks. It has already been\nwidely deployed by large providers and the adoption rate is getting to a\ncritical point. Almost half of all the global prefixes are now covered by RPKI\nand measurements show that 27% of networks are already using RPKI to validate\nBGP announcements. Over the past 10 years, there has been much research effort\nin RPKI, analyzing different facets of the protocol, such as software\nvulnerabilities, robustness of the infrastructure or the proliferation of RPKI\nvalidation. In this work we compile the first systemic overview of the\nvulnerabilities and misconfigurations in RPKI and quantify the security\nlandscape of the global RPKI deployments based on our measurements and\nanalysis. Our study discovers that 56% of the global RPKI validators suffer\nfrom at least one documented vulnerability. We also do a systematization of\nknowledge for existing RPKI security research and complement the existing\nknowledge with novel measurements in which we discover new trends in\navailability of RPKI repositories, and their communication patterns with the\nRPKI validators. We weave together the results of existing research and our\nstudy, to provide a comprehensive tableau of vulnerabilities, their sources,\nand to derive future research paths necessary to prepare RPKI for full global\ndeployment."
                },
                "authors": [
                    {
                        "name": "Donika Mirdita"
                    },
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Michael Waidner"
                    }
                ],
                "author_detail": {
                    "name": "Michael Waidner"
                },
                "author": "Michael Waidner",
                "arxiv_comment": "this paper was accepted at USENIX Security '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03656v2",
                "updated": "2024-08-22T12:45:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    45,
                    54,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-04T05:54:19Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    54,
                    19,
                    3,
                    186,
                    0
                ],
                "title": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System"
                },
                "summary": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "DCASE WS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02616v4",
                "updated": "2024-08-22T12:40:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    40,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-03T09:41:42Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    41,
                    42,
                    0,
                    155,
                    0
                ],
                "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach"
                },
                "summary": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Xiaoxue Yu"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v1",
                "updated": "2024-08-22T12:21:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Graph Retrieval Augmented Trustworthiness Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Trustworthiness Reasoning"
                },
                "summary": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning is crucial in multiplayer games with incomplete\ninformation, enabling agents to identify potential allies and adversaries,\nthereby enhancing reasoning and decision-making processes. Traditional\napproaches relying on pre-trained models necessitate extensive domain-specific\ndata and considerable reward feedback, with their lack of real-time\nadaptability hindering their effectiveness in dynamic environments. In this\npaper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,\nleveraging the Retrieval-Augmented Generation (RAG) technique to bolster\ntrustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness\ngraph, updating it in real-time with evidential information, and retrieves\nrelevant trust data to augment the reasoning capabilities of Large Language\nModels (LLMs). We validate our approach through experiments on the multiplayer\ngame \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with\nNative RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the\nbaseline methods by over 30\\% in winning rate, with superior reasoning\nperformance. Moreover, GRATR effectively mitigates LLM hallucinations, such as\nidentity and objective amnesia, and crucially, it renders the reasoning process\nmore transparent and traceable through the use of the trustworthiness graph."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12326v1",
                "updated": "2024-08-22T12:04:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    4,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:04:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    4,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "Interactive DualChecker for Mitigating Hallucinations in Distilling\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive DualChecker for Mitigating Hallucinations in Distilling\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various machine learning (ML) tasks. Given the high costs of creating\nannotated datasets for supervised learning, LLMs offer a valuable alternative\nby enabling effective few-shot in-context learning. However, these models can\nproduce hallucinations, particularly in domains with incomplete knowledge.\nAdditionally, current methods for knowledge distillation using LLMs often\nstruggle to enhance the effectiveness of both teacher and student models. To\naddress these challenges, we introduce DualChecker, an innovative framework\ndesigned to mitigate hallucinations and improve the performance of both teacher\nand student models during knowledge distillation. DualChecker employs\nContextAligner to ensure that the context provided by teacher models aligns\nwith human labeling standards. It also features a dynamic checker system that\nenhances model interaction: one component re-prompts teacher models with more\ndetailed content when they show low confidence, and another identifies\nborderline cases from student models to refine the teaching templates. This\ninteractive process promotes continuous improvement and effective knowledge\ntransfer between the models. We evaluate DualChecker using a green innovation\ntextual dataset that includes binary, multiclass, and token classification\ntasks. The experimental results show that DualChecker significantly outperforms\nexisting state-of-the-art methods, achieving up to a 17% improvement in F1\nscore for teacher models and 10% for student models. Notably, student models\nfine-tuned with LLM predictions perform comparably to those fine-tuned with\nactual data, even in a challenging domain. We make all datasets, models, and\ncode from this research publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various machine learning (ML) tasks. Given the high costs of creating\nannotated datasets for supervised learning, LLMs offer a valuable alternative\nby enabling effective few-shot in-context learning. However, these models can\nproduce hallucinations, particularly in domains with incomplete knowledge.\nAdditionally, current methods for knowledge distillation using LLMs often\nstruggle to enhance the effectiveness of both teacher and student models. To\naddress these challenges, we introduce DualChecker, an innovative framework\ndesigned to mitigate hallucinations and improve the performance of both teacher\nand student models during knowledge distillation. DualChecker employs\nContextAligner to ensure that the context provided by teacher models aligns\nwith human labeling standards. It also features a dynamic checker system that\nenhances model interaction: one component re-prompts teacher models with more\ndetailed content when they show low confidence, and another identifies\nborderline cases from student models to refine the teaching templates. This\ninteractive process promotes continuous improvement and effective knowledge\ntransfer between the models. We evaluate DualChecker using a green innovation\ntextual dataset that includes binary, multiclass, and token classification\ntasks. The experimental results show that DualChecker significantly outperforms\nexisting state-of-the-art methods, achieving up to a 17% improvement in F1\nscore for teacher models and 10% for student models. Notably, student models\nfine-tuned with LLM predictions perform comparably to those fine-tuned with\nactual data, even in a challenging domain. We make all datasets, models, and\ncode from this research publicly available."
                },
                "authors": [
                    {
                        "name": "Meiyun Wang"
                    },
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoshi Izumi"
                },
                "author": "Kiyoshi Izumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v1",
                "updated": "2024-08-22T12:00:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Hallucination Mitigation in LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12320v1",
                "updated": "2024-08-22T11:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    57,
                    7,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:57:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    57,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "PolyRouter: A Multi-LLM Querying System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyRouter: A Multi-LLM Querying System"
                },
                "summary": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%."
                },
                "authors": [
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Alay Shah"
                    },
                    {
                        "name": "Han Jin"
                    },
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "arxiv_comment": "14 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02232v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02232v3",
                "updated": "2024-08-22T11:54:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    54,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-05T04:53:01Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    4,
                    53,
                    1,
                    0,
                    218,
                    0
                ],
                "title": "SpecRover: Code Intent Extraction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRover: Code Intent Extraction via LLMs"
                },
                "summary": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era."
                },
                "authors": [
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "Haifeng Ruan and Yuntong Zhang contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02232v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02232v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v2",
                "updated": "2024-08-22T11:49:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    49,
                    15,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12315v1",
                "updated": "2024-08-22T11:41:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    41,
                    35,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T11:41:35Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    41,
                    35,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\n  Applications via Tailored Problem-Solving Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\n  Applications via Tailored Problem-Solving Demonstrations"
                },
                "summary": "Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "preprint / under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12263v1",
                "updated": "2024-08-22T10:00:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    0,
                    20,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T10:00:20Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    10,
                    0,
                    20,
                    3,
                    235,
                    0
                ],
                "title": "Toward the Evaluation of Large Language Models Considering Score\n  Variance across Instruction Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward the Evaluation of Large Language Models Considering Score\n  Variance across Instruction Templates"
                },
                "summary": "The natural language understanding (NLU) performance of large language models\n(LLMs) has been evaluated across various tasks and datasets. The existing\nevaluation methods, however, do not take into account the variance in scores\ndue to differences in prompts, which leads to unfair evaluation and comparison\nof NLU performance. Moreover, evaluation designed for specific prompts is\ninappropriate for instruction tuning, which aims to perform well with any\nprompt. It is therefore necessary to find a way to measure NLU performance in a\nfair manner, considering score variance between different instruction\ntemplates. In this study, we provide English and Japanese cross-lingual\ndatasets for evaluating the NLU performance of LLMs, which include multiple\ninstruction templates for fair evaluation of each task, along with regular\nexpressions to constrain the output format. Furthermore, we propose the Sharpe\nscore as an evaluation metric that takes into account the variance in scores\nbetween templates. Comprehensive analysis of English and Japanese LLMs reveals\nthat the high variance among templates has a significant impact on the fair\nevaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The natural language understanding (NLU) performance of large language models\n(LLMs) has been evaluated across various tasks and datasets. The existing\nevaluation methods, however, do not take into account the variance in scores\ndue to differences in prompts, which leads to unfair evaluation and comparison\nof NLU performance. Moreover, evaluation designed for specific prompts is\ninappropriate for instruction tuning, which aims to perform well with any\nprompt. It is therefore necessary to find a way to measure NLU performance in a\nfair manner, considering score variance between different instruction\ntemplates. In this study, we provide English and Japanese cross-lingual\ndatasets for evaluating the NLU performance of LLMs, which include multiple\ninstruction templates for fair evaluation of each task, along with regular\nexpressions to constrain the output format. Furthermore, we propose the Sharpe\nscore as an evaluation metric that takes into account the variance in scores\nbetween templates. Comprehensive analysis of English and Japanese LLMs reveals\nthat the high variance among templates has a significant impact on the fair\nevaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Adam Nohejl"
                    },
                    {
                        "name": "Jiangnan Hang"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12259v1",
                "updated": "2024-08-22T09:57:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    57,
                    57,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:57:57Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    57,
                    57,
                    3,
                    235,
                    0
                ],
                "title": "Can You Trust Your Metric? Automatic Concatenation-Based Tests for\n  Metric Validity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Trust Your Metric? Automatic Concatenation-Based Tests for\n  Metric Validity"
                },
                "summary": "Consider a scenario where a harmfulness detection metric is employed by a\nsystem to filter unsafe responses generated by a Large Language Model. When\nanalyzing individual harmful and unethical prompt-response pairs, the metric\ncorrectly classifies each pair as highly unsafe, assigning the highest score.\nHowever, when these same prompts and responses are concatenated, the metric's\ndecision flips, assigning the lowest possible score, thereby misclassifying the\ncontent as safe and allowing it to bypass the filter. In this study, we\ndiscovered that several harmfulness LLM-based metrics, including GPT-based,\nexhibit this decision-flipping phenomenon. Additionally, we found that even an\nadvanced metric like GPT-4o is highly sensitive to input order. Specifically,\nit tends to classify responses as safe if the safe content appears first,\nregardless of any harmful content that follows, and vice versa. This work\nintroduces automatic concatenation-based tests to assess the fundamental\nproperties a valid metric should satisfy. We applied these tests in a model\nsafety scenario to assess the reliability of harmfulness detection metrics,\nuncovering a number of inconsistencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider a scenario where a harmfulness detection metric is employed by a\nsystem to filter unsafe responses generated by a Large Language Model. When\nanalyzing individual harmful and unethical prompt-response pairs, the metric\ncorrectly classifies each pair as highly unsafe, assigning the highest score.\nHowever, when these same prompts and responses are concatenated, the metric's\ndecision flips, assigning the lowest possible score, thereby misclassifying the\ncontent as safe and allowing it to bypass the filter. In this study, we\ndiscovered that several harmfulness LLM-based metrics, including GPT-based,\nexhibit this decision-flipping phenomenon. Additionally, we found that even an\nadvanced metric like GPT-4o is highly sensitive to input order. Specifically,\nit tends to classify responses as safe if the safe content appears first,\nregardless of any harmful content that follows, and vice versa. This work\nintroduces automatic concatenation-based tests to assess the fundamental\nproperties a valid metric should satisfy. We applied these tests in a model\nsafety scenario to assess the reliability of harmfulness detection metrics,\nuncovering a number of inconsistencies."
                },
                "authors": [
                    {
                        "name": "Ora Nova Fandina"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "George Kour"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Orna Raz"
                    }
                ],
                "author_detail": {
                    "name": "Orna Raz"
                },
                "author": "Orna Raz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v2",
                "updated": "2024-08-22T09:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    45,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12249v1",
                "updated": "2024-08-22T09:37:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    37,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:37:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    37,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"
                },
                "summary": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extration. To\nbreach this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end we evaluate various open LLMs -- including BioMistral and Llama-2\nmodels -- on a diverse set of biomedical datasets, using standard prompting,\nChain-of-Thought (CoT) and Self-Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extration. To\nbreach this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end we evaluate various open LLMs -- including BioMistral and Llama-2\nmodels -- on a diverse set of biomedical datasets, using standard prompting,\nChain-of-Thought (CoT) and Self-Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications."
                },
                "authors": [
                    {
                        "name": "Aishik Nagar"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuping Wu"
                    },
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12247v2",
                "updated": "2024-08-23T01:25:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    25,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T09:36:15Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    36,
                    15,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution."
                },
                "authors": [
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Pengtian Zhu"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jiagang Wang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Dongwen Li"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Qianying Guo"
                    },
                    {
                        "name": "Xiaolei Hua"
                    },
                    {
                        "name": "Lin Zhu"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12236v1",
                "updated": "2024-08-22T09:10:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    10,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T09:10:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    10,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for\n  Dynamic Medical Image Generation in Virtual Simulated Patient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for\n  Dynamic Medical Image Generation in Virtual Simulated Patient"
                },
                "summary": "Medical education relies heavily on Simulated Patients (SPs) to provide a\nsafe environment for students to practice clinical skills, including medical\nimage analysis. However, the high cost of recruiting qualified SPs and the lack\nof diverse medical imaging datasets have presented significant challenges. To\naddress these issues, this paper introduces MedDiT, a novel\nknowledge-controlled conversational framework that can dynamically generate\nplausible medical images aligned with simulated patient symptoms, enabling\ndiverse diagnostic skill training. Specifically, MedDiT integrates various\npatient Knowledge Graphs (KGs), which describe the attributes and symptoms of\npatients, to dynamically prompt Large Language Models' (LLMs) behavior and\ncontrol the patient characteristics, mitigating hallucination during medical\nconversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is\nincorporated to generate medical images according to the specified patient\nattributes in the KG. In this paper, we present the capabilities of MedDiT\nthrough a practical demonstration, showcasing its ability to act in diverse\nsimulated patient cases and generate the corresponding medical images. This can\nprovide an abundant and interactive learning experience for students, advancing\nmedical education by offering an immersive simulation platform for future\nhealthcare professionals. The work sheds light on the feasibility of\nincorporating advanced technologies like LLM, KG, and DiT in education\napplications, highlighting their potential to address the challenges faced in\nsimulated patient-based medical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical education relies heavily on Simulated Patients (SPs) to provide a\nsafe environment for students to practice clinical skills, including medical\nimage analysis. However, the high cost of recruiting qualified SPs and the lack\nof diverse medical imaging datasets have presented significant challenges. To\naddress these issues, this paper introduces MedDiT, a novel\nknowledge-controlled conversational framework that can dynamically generate\nplausible medical images aligned with simulated patient symptoms, enabling\ndiverse diagnostic skill training. Specifically, MedDiT integrates various\npatient Knowledge Graphs (KGs), which describe the attributes and symptoms of\npatients, to dynamically prompt Large Language Models' (LLMs) behavior and\ncontrol the patient characteristics, mitigating hallucination during medical\nconversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is\nincorporated to generate medical images according to the specified patient\nattributes in the KG. In this paper, we present the capabilities of MedDiT\nthrough a practical demonstration, showcasing its ability to act in diverse\nsimulated patient cases and generate the corresponding medical images. This can\nprovide an abundant and interactive learning experience for students, advancing\nmedical education by offering an immersive simulation platform for future\nhealthcare professionals. The work sheds light on the feasibility of\nincorporating advanced technologies like LLM, KG, and DiT in education\napplications, highlighting their potential to address the challenges faced in\nsimulated patient-based medical education."
                },
                "authors": [
                    {
                        "name": "Yanzeng Li"
                    },
                    {
                        "name": "Cheng Zeng"
                    },
                    {
                        "name": "Jinchao Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Lei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zou"
                },
                "author": "Lei Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12226v1",
                "updated": "2024-08-22T08:57:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    57,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    57,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for\n  Automated Scoring of CEFR B2 Speaking Assessment Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for\n  Automated Scoring of CEFR B2 Speaking Assessment Transcripts"
                },
                "summary": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation."
                },
                "authors": [
                    {
                        "name": "Nicy Scaria"
                    },
                    {
                        "name": "Silvester John Joseph Kennedy"
                    },
                    {
                        "name": "Thomas Latinovich"
                    },
                    {
                        "name": "Deepak Subramani"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Subramani"
                },
                "author": "Deepak Subramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12214v1",
                "updated": "2024-08-22T08:42:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    42,
                    44,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:42:44Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    42,
                    44,
                    3,
                    235,
                    0
                ],
                "title": "UNCO: Towards Unifying Neural Combinatorial Optimization through Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCO: Towards Unifying Neural Combinatorial Optimization through Large\n  Language Model"
                },
                "summary": "Recently, applying neural networks to address combinatorial optimization\nproblems (COPs) has attracted considerable research attention. The prevailing\nmethods always train deep models independently on specific problems, lacking a\nunified framework for concurrently tackling various COPs. To this end, we\npropose a unified neural combinatorial optimization (UNCO) framework to solve\ndifferent types of COPs by a single model. Specifically, we use natural\nlanguage to formulate text-attributed instances for different COPs and encode\nthem in the same embedding space by the large language model (LLM). The\nobtained embeddings are further advanced by an encoder-decoder model without\nany problem-specific modules, thereby facilitating a unified process of\nsolution construction. We further adopt the conflict gradients erasing\nreinforcement learning (CGERL) algorithm to train the UNCO model, delivering\nbetter performance across different COPs than vanilla multi-objective learning.\nExperiments show that the UNCO model can solve multiple COPs after a\nsingle-session training, and achieves satisfactory performance that is\ncomparable to several traditional or learning-based baselines. Instead of\npursuing the best performance for each COP, we explore the synergy between\ntasks and few-shot generalization based on LLM to inspire future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, applying neural networks to address combinatorial optimization\nproblems (COPs) has attracted considerable research attention. The prevailing\nmethods always train deep models independently on specific problems, lacking a\nunified framework for concurrently tackling various COPs. To this end, we\npropose a unified neural combinatorial optimization (UNCO) framework to solve\ndifferent types of COPs by a single model. Specifically, we use natural\nlanguage to formulate text-attributed instances for different COPs and encode\nthem in the same embedding space by the large language model (LLM). The\nobtained embeddings are further advanced by an encoder-decoder model without\nany problem-specific modules, thereby facilitating a unified process of\nsolution construction. We further adopt the conflict gradients erasing\nreinforcement learning (CGERL) algorithm to train the UNCO model, delivering\nbetter performance across different COPs than vanilla multi-objective learning.\nExperiments show that the UNCO model can solve multiple COPs after a\nsingle-session training, and achieves satisfactory performance that is\ncomparable to several traditional or learning-based baselines. Instead of\npursuing the best performance for each COP, we explore the synergy between\ntasks and few-shot generalization based on LLM to inspire future work."
                },
                "authors": [
                    {
                        "name": "Xia Jiang"
                    },
                    {
                        "name": "Yaoxin Wu"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yingqian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yingqian Zhang"
                },
                "author": "Yingqian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v5",
                "updated": "2024-08-23T08:17:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    17,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "10 pages, 5 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12194v2",
                "updated": "2024-08-23T06:46:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    46,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment"
                },
                "summary": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "Submitted to EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12193v1",
                "updated": "2024-08-22T08:16:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    2,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:16:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "Empowering Wireless Network Applications with Deep Learning-based Radio\n  Propagation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Wireless Network Applications with Deep Learning-based Radio\n  Propagation Models"
                },
                "summary": "The efficient deployment and operation of any wireless communication\necosystem rely on knowledge of the received signal quality over the target\ncoverage area. This knowledge is typically acquired through radio propagation\nsolvers, which however suffer from intrinsic and well-known performance\nlimitations. This article provides a primer on how integrating deep learning\nand conventional propagation modeling techniques can enhance multiple vital\nfacets of wireless network operation, and yield benefits in terms of efficiency\nand reliability. By highlighting the pivotal role that the deep learning-based\nradio propagation models will assume in next-generation wireless networks, we\naspire to propel further research in this direction and foster their adoption\nin additional applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment and operation of any wireless communication\necosystem rely on knowledge of the received signal quality over the target\ncoverage area. This knowledge is typically acquired through radio propagation\nsolvers, which however suffer from intrinsic and well-known performance\nlimitations. This article provides a primer on how integrating deep learning\nand conventional propagation modeling techniques can enhance multiple vital\nfacets of wireless network operation, and yield benefits in terms of efficiency\nand reliability. By highlighting the pivotal role that the deep learning-based\nradio propagation models will assume in next-generation wireless networks, we\naspire to propel further research in this direction and foster their adoption\nin additional applications."
                },
                "authors": [
                    {
                        "name": "Stefanos Bakirtzis"
                    },
                    {
                        "name": "Cagkan Yapar"
                    },
                    {
                        "name": "Marco Fiore"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Ian Wassell"
                    }
                ],
                "author_detail": {
                    "name": "Ian Wassell"
                },
                "author": "Ian Wassell",
                "arxiv_comment": "7 pages, 3 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12188v1",
                "updated": "2024-08-22T08:05:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    5,
                    9,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T08:05:09Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    5,
                    9,
                    3,
                    235,
                    0
                ],
                "title": "Reasoning Factual Knowledge in Structured Data with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Factual Knowledge in Structured Data with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have made remarkable progress in various natural\nlanguage processing tasks as a benefit of their capability to comprehend and\nreason with factual knowledge. However, a significant amount of factual\nknowledge is stored in structured data, which possesses unique characteristics\nthat differ from the unstructured texts used for pretraining. This difference\ncan introduce imperceptible inference parameter deviations, posing challenges\nfor LLMs in effectively utilizing and reasoning with structured data to\naccurately infer factual knowledge. To this end, we propose a benchmark named\nStructFact, to evaluate the structural reasoning capabilities of LLMs in\ninferring factual knowledge. StructFact comprises 8,340 factual questions\nencompassing various tasks, domains, timelines, and regions. This benchmark\nallows us to investigate the capability of LLMs across five factual tasks\nderived from the unique characteristics of structural facts. Extensive\nexperiments on a set of LLMs with different training strategies reveal the\nlimitations of current LLMs in inferring factual knowledge from structured\ndata. We present this benchmark as a compass to navigate the strengths and\nweaknesses of LLMs in reasoning with structured data for knowledge-sensitive\ntasks, and to encourage advancements in related real-world applications. Please\nfind our code at https://github.com/EganGu/StructFact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable progress in various natural\nlanguage processing tasks as a benefit of their capability to comprehend and\nreason with factual knowledge. However, a significant amount of factual\nknowledge is stored in structured data, which possesses unique characteristics\nthat differ from the unstructured texts used for pretraining. This difference\ncan introduce imperceptible inference parameter deviations, posing challenges\nfor LLMs in effectively utilizing and reasoning with structured data to\naccurately infer factual knowledge. To this end, we propose a benchmark named\nStructFact, to evaluate the structural reasoning capabilities of LLMs in\ninferring factual knowledge. StructFact comprises 8,340 factual questions\nencompassing various tasks, domains, timelines, and regions. This benchmark\nallows us to investigate the capability of LLMs across five factual tasks\nderived from the unique characteristics of structural facts. Extensive\nexperiments on a set of LLMs with different training strategies reveal the\nlimitations of current LLMs in inferring factual knowledge from structured\ndata. We present this benchmark as a compass to navigate the strengths and\nweaknesses of LLMs in reasoning with structured data for knowledge-sensitive\ntasks, and to encourage advancements in related real-world applications. Please\nfind our code at https://github.com/EganGu/StructFact."
                },
                "authors": [
                    {
                        "name": "Sirui Huang"
                    },
                    {
                        "name": "Yanggan Gu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Zhonghao Li"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02851v2",
                "updated": "2024-08-22T07:49:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    49,
                    39,
                    3,
                    235,
                    0
                ],
                "published": "2024-01-05T15:09:57Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    15,
                    9,
                    57,
                    4,
                    5,
                    0
                ],
                "title": "Natural Language Programming in Medicine: Administering Evidence Based\n  Clinical Workflows with Autonomous Agents Powered by Generative Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Programming in Medicine: Administering Evidence Based\n  Clinical Workflows with Autonomous Agents Powered by Generative Large\n  Language Models"
                },
                "summary": "Generative Large Language Models (LLMs) hold significant promise in\nhealthcare, demonstrating capabilities such as passing medical licensing exams\nand providing clinical knowledge. However, their current use as information\nretrieval tools is limited by challenges like data staleness, resource demands,\nand occasional generation of incorrect information. This study assessed the\npotential of LLMs to function as autonomous agents in a simulated tertiary care\nmedical center, using real-world clinical cases across multiple specialties.\nBoth proprietary and open-source LLMs were evaluated, with Retrieval Augmented\nGeneration (RAG) enhancing contextual relevance. Proprietary models,\nparticularly GPT-4, generally outperformed open-source models, showing improved\nguideline adherence and more accurate responses with RAG. The manual evaluation\nby expert clinicians was crucial in validating models' outputs, underscoring\nthe importance of human oversight in LLM operation. Further, the study\nemphasizes Natural Language Programming (NLP) as the appropriate paradigm for\nmodifying model behavior, allowing for precise adjustments through tailored\nprompts and real-world interactions. This approach highlights the potential of\nLLMs to significantly enhance and supplement clinical decision-making, while\nalso emphasizing the value of continuous expert involvement and the flexibility\nof NLP to ensure their reliability and effectiveness in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) hold significant promise in\nhealthcare, demonstrating capabilities such as passing medical licensing exams\nand providing clinical knowledge. However, their current use as information\nretrieval tools is limited by challenges like data staleness, resource demands,\nand occasional generation of incorrect information. This study assessed the\npotential of LLMs to function as autonomous agents in a simulated tertiary care\nmedical center, using real-world clinical cases across multiple specialties.\nBoth proprietary and open-source LLMs were evaluated, with Retrieval Augmented\nGeneration (RAG) enhancing contextual relevance. Proprietary models,\nparticularly GPT-4, generally outperformed open-source models, showing improved\nguideline adherence and more accurate responses with RAG. The manual evaluation\nby expert clinicians was crucial in validating models' outputs, underscoring\nthe importance of human oversight in LLM operation. Further, the study\nemphasizes Natural Language Programming (NLP) as the appropriate paradigm for\nmodifying model behavior, allowing for precise adjustments through tailored\nprompts and real-world interactions. This approach highlights the potential of\nLLMs to significantly enhance and supplement clinical decision-making, while\nalso emphasizing the value of continuous expert involvement and the flexibility\nof NLP to ensure their reliability and effectiveness in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Akhil Vaid"
                    },
                    {
                        "name": "Joshua Lampert"
                    },
                    {
                        "name": "Juhee Lee"
                    },
                    {
                        "name": "Ashwin Sawant"
                    },
                    {
                        "name": "Donald Apakama"
                    },
                    {
                        "name": "Ankit Sakhuja"
                    },
                    {
                        "name": "Ali Soroush"
                    },
                    {
                        "name": "Sarah Bick"
                    },
                    {
                        "name": "Ethan Abbott"
                    },
                    {
                        "name": "Hernando Gomez"
                    },
                    {
                        "name": "Michael Hadley"
                    },
                    {
                        "name": "Denise Lee"
                    },
                    {
                        "name": "Isotta Landi"
                    },
                    {
                        "name": "Son Q Duong"
                    },
                    {
                        "name": "Nicole Bussola"
                    },
                    {
                        "name": "Ismail Nabeel"
                    },
                    {
                        "name": "Silke Muehlstedt"
                    },
                    {
                        "name": "Silke Muehlstedt"
                    },
                    {
                        "name": "Robert Freeman"
                    },
                    {
                        "name": "Patricia Kovatch"
                    },
                    {
                        "name": "Brendan Carr"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Benjamin Glicksberg"
                    },
                    {
                        "name": "Edgar Argulian"
                    },
                    {
                        "name": "Stamatios Lerakis"
                    },
                    {
                        "name": "Rohan Khera"
                    },
                    {
                        "name": "David L. Reich"
                    },
                    {
                        "name": "Monica Kraft"
                    },
                    {
                        "name": "Alexander Charney"
                    },
                    {
                        "name": "Girish Nadkarni"
                    }
                ],
                "author_detail": {
                    "name": "Girish Nadkarni"
                },
                "author": "Girish Nadkarni",
                "arxiv_comment": "Figures: 5, Tables: 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12168v1",
                "updated": "2024-08-22T07:31:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    31,
                    0,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T07:31:00Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    31,
                    0,
                    3,
                    235,
                    0
                ],
                "title": "FIRST: Teach A Reliable Large Language Model Through Efficient\n  Trustworthy Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRST: Teach A Reliable Large Language Model Through Efficient\n  Trustworthy Distillation"
                },
                "summary": "Large language models (LLMs) have become increasingly prevalent in our daily\nlives, leading to an expectation for LLMs to be trustworthy -- - both accurate\nand well-calibrated (the prediction confidence should align with its ground\ntruth correctness likelihood). Nowadays, fine-tuning has become the most\npopular method for adapting a model to practical usage by significantly\nincreasing accuracy on downstream tasks. Despite the great accuracy it\nachieves, we found fine-tuning is still far away from satisfactory\ntrustworthiness due to \"tuning-induced mis-calibration\". In this paper, we\ndelve deeply into why and how mis-calibration exists in fine-tuned models, and\nhow distillation can alleviate the issue. Then we further propose a brand new\nmethod named Efficient Trustworthy Distillation (FIRST), which utilizes a small\nportion of teacher's knowledge to obtain a reliable language model in a\ncost-efficient way. Specifically, we identify the \"concentrated knowledge\"\nphenomenon during distillation, which can significantly reduce the\ncomputational burden. Then we apply a \"trustworthy maximization\" process to\noptimize the utilization of this small portion of concentrated knowledge before\ntransferring it to the student. Experimental results demonstrate the\neffectiveness of our method, where better accuracy (+2.3%) and less\nmis-calibration (-10%) are achieved on average across both in-domain and\nout-of-domain scenarios, indicating better trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly prevalent in our daily\nlives, leading to an expectation for LLMs to be trustworthy -- - both accurate\nand well-calibrated (the prediction confidence should align with its ground\ntruth correctness likelihood). Nowadays, fine-tuning has become the most\npopular method for adapting a model to practical usage by significantly\nincreasing accuracy on downstream tasks. Despite the great accuracy it\nachieves, we found fine-tuning is still far away from satisfactory\ntrustworthiness due to \"tuning-induced mis-calibration\". In this paper, we\ndelve deeply into why and how mis-calibration exists in fine-tuned models, and\nhow distillation can alleviate the issue. Then we further propose a brand new\nmethod named Efficient Trustworthy Distillation (FIRST), which utilizes a small\nportion of teacher's knowledge to obtain a reliable language model in a\ncost-efficient way. Specifically, we identify the \"concentrated knowledge\"\nphenomenon during distillation, which can significantly reduce the\ncomputational burden. Then we apply a \"trustworthy maximization\" process to\noptimize the utilization of this small portion of concentrated knowledge before\ntransferring it to the student. Experimental results demonstrate the\neffectiveness of our method, where better accuracy (+2.3%) and less\nmis-calibration (-10%) are achieved on average across both in-domain and\nout-of-domain scenarios, indicating better trustworthiness."
                },
                "authors": [
                    {
                        "name": "KaShun Shum"
                    },
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Muhammad Omer Raza"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Omer Raza"
                },
                "author": "Muhammad Omer Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11069v2",
                "updated": "2024-08-22T07:21:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    21,
                    10,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-17T08:58:27Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    8,
                    58,
                    27,
                    5,
                    230,
                    0
                ],
                "title": "Phase-Based Approaches for Rapid Construction of Magnetic Fields in NV\n  Magnetometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase-Based Approaches for Rapid Construction of Magnetic Fields in NV\n  Magnetometry"
                },
                "summary": "With the second quantum revolution underway, quantum-enhanced sensors are\nmoving from laboratory demonstrations to field deployments, providing enhanced\nand even new capabilities. Signal processing and operational software is\nbecoming integral parts of these emerging sensing systems to reap the benefits\nof this progress. This paper looks into widefield Nitrogen Vacancy Center-based\nmagnetometry and focuses on estimating the magnetic field from the Optically\nDetected Magnetic Resonances (ODMR) signal, a crucial output for various\napplications. Mapping the shifts of ODMR signals to phase estimation, a\ncomputationally efficient approaches are proposed. Involving Fourier Transform\nand Filtering as pre-processing steps, the suggested approaches involve linear\ncurve fit or complex frequency estimation based on well-known super-resolution\ntechnique Estimation of Signal Parameters via Rotational Invariant Techniques\n(ESPRIT). The existing methods in the quantum sensing literature take different\nroutes based on Lorentzian fitting for determining magnetic field maps. To\nshowcase the functionality and effectiveness of the suggested techniques,\nrelevant results, based on experimental data are provided, which shows a\nsignificant reduction in computational time with the proposed method over\nexisting methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the second quantum revolution underway, quantum-enhanced sensors are\nmoving from laboratory demonstrations to field deployments, providing enhanced\nand even new capabilities. Signal processing and operational software is\nbecoming integral parts of these emerging sensing systems to reap the benefits\nof this progress. This paper looks into widefield Nitrogen Vacancy Center-based\nmagnetometry and focuses on estimating the magnetic field from the Optically\nDetected Magnetic Resonances (ODMR) signal, a crucial output for various\napplications. Mapping the shifts of ODMR signals to phase estimation, a\ncomputationally efficient approaches are proposed. Involving Fourier Transform\nand Filtering as pre-processing steps, the suggested approaches involve linear\ncurve fit or complex frequency estimation based on well-known super-resolution\ntechnique Estimation of Signal Parameters via Rotational Invariant Techniques\n(ESPRIT). The existing methods in the quantum sensing literature take different\nroutes based on Lorentzian fitting for determining magnetic field maps. To\nshowcase the functionality and effectiveness of the suggested techniques,\nrelevant results, based on experimental data are provided, which shows a\nsignificant reduction in computational time with the proposed method over\nexisting methods"
                },
                "authors": [
                    {
                        "name": "Prabhat Anand"
                    },
                    {
                        "name": "Ankit Khandelwal"
                    },
                    {
                        "name": "Achanna Anil Kumar"
                    },
                    {
                        "name": "M Girish Chandra"
                    },
                    {
                        "name": "Pavan K Reddy"
                    },
                    {
                        "name": "Anuj Bathla"
                    },
                    {
                        "name": "Dasika Shishir"
                    },
                    {
                        "name": "Kasturi Saha"
                    }
                ],
                "author_detail": {
                    "name": "Kasturi Saha"
                },
                "author": "Kasturi Saha",
                "arxiv_comment": "4 pages, 3 figures, typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12163v1",
                "updated": "2024-08-22T07:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T07:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-Guided Reflective Sampling for Aligning Language Models"
                },
                "summary": "Large language models (LLMs) are aligned with human preferences by\nreinforcement learning from human feedback (RLHF). Effective data sampling is\ncrucial for RLHF, as it determines the efficiency of model training, ensuring\nthat models learn from the informative samples. To achieve better data\ngeneration, we propose a new sampling method called Preference-Guided\nReflective Sampling (PRS). PRS frames the response generation as an\noptimization process to the explicitly specified user preference described in\nnatural language. It employs a tree-based generation framework to enable an\nefficient sampling process, which guides the direction of generation through\npreference and better explores the sampling space with adaptive\nself-refinement. Notably, PRS can align LLMs to diverse preferences. We study\npreference-controlled text generation for instruction following and\nkeyword-focused document summarization. Our findings indicate that PRS, across\ndifferent LLM policies, generates training data with much higher rewards than\nstrong baselines. PRS also excels in post-RL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are aligned with human preferences by\nreinforcement learning from human feedback (RLHF). Effective data sampling is\ncrucial for RLHF, as it determines the efficiency of model training, ensuring\nthat models learn from the informative samples. To achieve better data\ngeneration, we propose a new sampling method called Preference-Guided\nReflective Sampling (PRS). PRS frames the response generation as an\noptimization process to the explicitly specified user preference described in\nnatural language. It employs a tree-based generation framework to enable an\nefficient sampling process, which guides the direction of generation through\npreference and better explores the sampling space with adaptive\nself-refinement. Notably, PRS can align LLMs to diverse preferences. We study\npreference-controlled text generation for instruction following and\nkeyword-focused document summarization. Our findings indicate that PRS, across\ndifferent LLM policies, generates training data with much higher rewards than\nstrong baselines. PRS also excels in post-RL training."
                },
                "authors": [
                    {
                        "name": "Hai Ye"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01129v3",
                "updated": "2024-08-22T07:18:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    18,
                    1,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-02T09:18:41Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    18,
                    41,
                    4,
                    215,
                    0
                ],
                "title": "A Survey of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mamba"
                },
                "summary": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Tyler Derr"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14183v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14183v3",
                "updated": "2024-08-22T07:01:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    7,
                    1,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2023-12-19T14:35:04Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    14,
                    35,
                    4,
                    1,
                    353,
                    0
                ],
                "title": "On Early Detection of Hallucinations in Factual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Early Detection of Hallucinations in Factual Question Answering"
                },
                "summary": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs."
                },
                "authors": [
                    {
                        "name": "Ben Snyder"
                    },
                    {
                        "name": "Marius Moisescu"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "arxiv_doi": "10.1145/3637528.3671796",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671796",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.14183v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14183v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "KDD 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12159v1",
                "updated": "2024-08-22T06:59:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    59,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T06:59:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    59,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "Search-Based LLMs for Code Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-Based LLMs for Code Optimization"
                },
                "summary": "The code written by developers usually suffers from efficiency problems and\ncontain various performance bugs. These inefficiencies necessitate the research\nof automated refactoring methods for code optimization. Early research in code\noptimization employs rule-based methods and focuses on specific inefficiency\nissues, which are labor-intensive and suffer from the low coverage issue.\nRecent work regards the task as a sequence generation problem, and resorts to\ndeep learning (DL) techniques such as large language models (LLMs). These\nmethods typically prompt LLMs to directly generate optimized code. Although\nthese methods show state-of-the-art performance, such one-step generation\nparadigm is hard to achieve an optimal solution. First, complex optimization\nmethods such as combinatorial ones are hard to be captured by LLMs. Second, the\none-step generation paradigm poses challenge in precisely infusing the\nknowledge required for effective code optimization within LLMs, resulting in\nunder-optimized code.To address these problems, we propose to model this task\nfrom the search perspective, and propose a search-based LLMs framework named\nSBLLM that enables iterative refinement and discovery of improved optimization\nmethods. SBLLM synergistically integrate LLMs with evolutionary search and\nconsists of three key components: 1) an execution-based representative sample\nselection part that evaluates the fitness of each existing optimized code and\nprioritizes promising ones to pilot the generation of improved code; 2) an\nadaptive optimization pattern retrieval part that infuses targeted optimization\npatterns into the model for guiding LLMs towards rectifying and progressively\nenhancing their optimization methods; and 3) a genetic operator-inspired\nchain-of-thought prompting part that aids LLMs in combining different\noptimization methods and generating improved optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The code written by developers usually suffers from efficiency problems and\ncontain various performance bugs. These inefficiencies necessitate the research\nof automated refactoring methods for code optimization. Early research in code\noptimization employs rule-based methods and focuses on specific inefficiency\nissues, which are labor-intensive and suffer from the low coverage issue.\nRecent work regards the task as a sequence generation problem, and resorts to\ndeep learning (DL) techniques such as large language models (LLMs). These\nmethods typically prompt LLMs to directly generate optimized code. Although\nthese methods show state-of-the-art performance, such one-step generation\nparadigm is hard to achieve an optimal solution. First, complex optimization\nmethods such as combinatorial ones are hard to be captured by LLMs. Second, the\none-step generation paradigm poses challenge in precisely infusing the\nknowledge required for effective code optimization within LLMs, resulting in\nunder-optimized code.To address these problems, we propose to model this task\nfrom the search perspective, and propose a search-based LLMs framework named\nSBLLM that enables iterative refinement and discovery of improved optimization\nmethods. SBLLM synergistically integrate LLMs with evolutionary search and\nconsists of three key components: 1) an execution-based representative sample\nselection part that evaluates the fitness of each existing optimized code and\nprioritizes promising ones to pilot the generation of improved code; 2) an\nadaptive optimization pattern retrieval part that infuses targeted optimization\npatterns into the model for guiding LLMs towards rectifying and progressively\nenhancing their optimization methods; and 3) a genetic operator-inspired\nchain-of-thought prompting part that aids LLMs in combining different\noptimization methods and generating improved optimization methods."
                },
                "authors": [
                    {
                        "name": "Shuzheng Gao"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Wenchao Gu"
                    },
                    {
                        "name": "Michael Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lyu"
                },
                "author": "Michael Lyu",
                "arxiv_comment": "Accepted by 2025 IEEE/ACM 47th International Conference on Software\n  Engineering (ICSE'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12148v1",
                "updated": "2024-08-22T06:27:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    27,
                    10,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T06:27:10Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    27,
                    10,
                    3,
                    235,
                    0
                ],
                "title": "Multi-tool Integration Application for Math Reasoning Using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tool Integration Application for Math Reasoning Using Large\n  Language Model"
                },
                "summary": "Mathematical reasoning is an important research direction in the field of\nartificial intelligence. This article proposes a novel multi tool application\nframework for mathematical reasoning, aiming to achieve more comprehensive and\naccurate mathematical reasoning by utilizing the collaborative effect of large\nlanguage models (LLMs) and multiple external tools. Firstly, use a Math Tool to\nperform basic mathematical calculations during the inference process through\ninteraction with LLM. Secondly, Code Tool can generate code fragments that\ncomply with syntax rules and execute them, providing support for complex\nmathematical problems. Then, through the iterative reasoning of the CoT Tool,\nthe logical coherence and accuracy of mathematical reasoning are enhanced.\nUltimately, by using self consistency tools to select the final answer based on\ndifferent parameters, the consistency and reliability of reasoning are\nimproved. Through the synergistic effect of these tools, the framework has\nachieved significant performance improvement in mathematical reasoning tasks.\nWe conducted experiments on the NumGLUE Task 4 test set, which includes 220\nmathematical reasoning fill in the blank questions. The experimental results\nshowed that, based on Math Tool, Code Tool, and CoT Tool, in Task 4 task,our\nmethod achieved an accuracy of 89.09,compared with the GPT3+FewShot baseline,\nFew Shot+ERNIE-4.0+self consistency improved by 49.09%, and compared with\nfine-tuning the Fine tuning baseline, Few Shot+ERNIE-4.0+self consistency\nimproved by 52.29%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is an important research direction in the field of\nartificial intelligence. This article proposes a novel multi tool application\nframework for mathematical reasoning, aiming to achieve more comprehensive and\naccurate mathematical reasoning by utilizing the collaborative effect of large\nlanguage models (LLMs) and multiple external tools. Firstly, use a Math Tool to\nperform basic mathematical calculations during the inference process through\ninteraction with LLM. Secondly, Code Tool can generate code fragments that\ncomply with syntax rules and execute them, providing support for complex\nmathematical problems. Then, through the iterative reasoning of the CoT Tool,\nthe logical coherence and accuracy of mathematical reasoning are enhanced.\nUltimately, by using self consistency tools to select the final answer based on\ndifferent parameters, the consistency and reliability of reasoning are\nimproved. Through the synergistic effect of these tools, the framework has\nachieved significant performance improvement in mathematical reasoning tasks.\nWe conducted experiments on the NumGLUE Task 4 test set, which includes 220\nmathematical reasoning fill in the blank questions. The experimental results\nshowed that, based on Math Tool, Code Tool, and CoT Tool, in Task 4 task,our\nmethod achieved an accuracy of 89.09,compared with the GPT3+FewShot baseline,\nFew Shot+ERNIE-4.0+self consistency improved by 49.09%, and compared with\nfine-tuning the Fine tuning baseline, Few Shot+ERNIE-4.0+self consistency\nimproved by 52.29%"
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15960v3",
                "updated": "2024-08-22T06:24:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    24,
                    12,
                    3,
                    235,
                    0
                ],
                "published": "2023-12-26T08:49:57Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    8,
                    49,
                    57,
                    1,
                    360,
                    0
                ],
                "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks"
                },
                "summary": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Pengguang Chen"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:\n  https://github.com/dvlab-research/MoTCoder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07528v2",
                "updated": "2024-08-22T06:09:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    6,
                    9,
                    53,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-11T17:55:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    55,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models"
                },
                "summary": "The capacity of Large Language Models (LLMs) to comprehend and reason over\nlong contexts is pivotal for advancements in diverse fields. Yet, they still\nstuggle with capturing long-distance dependencies within sequences to deeply\nunderstand semantics. To address this issue, we introduce Query-aware Inference\nfor LLMs (Q-LLM), a system designed to process extensive sequences akin to\nhuman cognition. By focusing on memory data relevant to a given query, Q-LLM\ncan accurately capture pertinent information within a fixed window size and\nprovide precise answers to queries. It doesn't require extra training and can\nbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can\nread Harry Potter within 30s and accurately answer the questions. On widely\nrecognized benchmarks, Q-LLM improved by 7.17% compared to the current\nstate-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In\nthe Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current\nSOTA by 7.0% and 6.1%. Our code can be found in\nhttps://github.com/dvlab-research/Q-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of Large Language Models (LLMs) to comprehend and reason over\nlong contexts is pivotal for advancements in diverse fields. Yet, they still\nstuggle with capturing long-distance dependencies within sequences to deeply\nunderstand semantics. To address this issue, we introduce Query-aware Inference\nfor LLMs (Q-LLM), a system designed to process extensive sequences akin to\nhuman cognition. By focusing on memory data relevant to a given query, Q-LLM\ncan accurately capture pertinent information within a fixed window size and\nprovide precise answers to queries. It doesn't require extra training and can\nbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can\nread Harry Potter within 30s and accurately answer the questions. On widely\nrecognized benchmarks, Q-LLM improved by 7.17% compared to the current\nstate-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In\nthe Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current\nSOTA by 7.0% and 6.1%. Our code can be found in\nhttps://github.com/dvlab-research/Q-LLM."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12142v1",
                "updated": "2024-08-22T05:59:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    59,
                    47,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T05:59:47Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    59,
                    47,
                    3,
                    235,
                    0
                ],
                "title": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents"
                },
                "summary": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymous patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and achieves text generation under\nsymbolic control via a dynamic diagnosis tree from a tool agent. By applying\nthe proposed framework, we develop the largest Chinese mental disorders\ndiagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases\nby cooperating with a pioneering psychiatric hospital, and contains 5000\nhigh-quality long conversations with diagnosis results as labels. To the best\nof our knowledge, it's also the first labelled Chinese mental disorders\ndiagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset\nsuccessfully simulates human-like diagnostic process of mental disorders. The\ndataset and code will become publicly accessible in\nhttps://github.com/lemonsis/MDD-5k.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymous patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and achieves text generation under\nsymbolic control via a dynamic diagnosis tree from a tool agent. By applying\nthe proposed framework, we develop the largest Chinese mental disorders\ndiagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases\nby cooperating with a pioneering psychiatric hospital, and contains 5000\nhigh-quality long conversations with diagnosis results as labels. To the best\nof our knowledge, it's also the first labelled Chinese mental disorders\ndiagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset\nsuccessfully simulates human-like diagnostic process of mental disorders. The\ndataset and code will become publicly accessible in\nhttps://github.com/lemonsis/MDD-5k."
                },
                "authors": [
                    {
                        "name": "Congchi Yin"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Zike Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Piji Li"
                    },
                    {
                        "name": "Jianhua Chen"
                    },
                    {
                        "name": "Xun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Jiang"
                },
                "author": "Xun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12122v1",
                "updated": "2024-08-22T04:29:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    29,
                    48,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T04:29:48Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    29,
                    48,
                    3,
                    235,
                    0
                ],
                "title": "On the Credibility of Backdoor Attacks Against Object Detectors in the\n  Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Credibility of Backdoor Attacks Against Object Detectors in the\n  Physical World"
                },
                "summary": "Object detectors are vulnerable to backdoor attacks. In contrast to\nclassifiers, detectors possess unique characteristics, architecturally and in\ntask execution; often operating in challenging conditions, for instance,\ndetecting traffic signs in autonomous cars. But, our knowledge dominates\nattacks against classifiers and tests in the \"digital domain\".\n  To address this critical gap, we conducted an extensive empirical study\ntargeting multiple detector architectures and two challenging detection tasks\nin real-world settings: traffic signs and vehicles. Using the diverse,\nmethodically collected videos captured from driving cars and flying drones,\nincorporating physical object trigger deployments in authentic scenes, we\ninvestigated the viability of physical object-triggered backdoor attacks in\napplication settings.\n  Our findings revealed 8 key insights. Importantly, the prevalent \"digital\"\ndata poisoning method for injecting backdoors into models does not lead to\neffective attacks against detectors in the real world, although proven\neffective in classification tasks. We construct a new, cost-efficient attack\nmethod, dubbed MORPHING, incorporating the unique nature of detection tasks;\nours is remarkably successful in injecting physical object-triggered backdoors,\neven capable of poisoning triggers with clean label annotations or invisible\ntriggers without diminishing the success of physical object triggered\nbackdoors. We discovered that the defenses curated are ill-equipped to\nsafeguard detectors against such attacks. To underscore the severity of the\nthreat and foster further research, we, for the first time, release an\nextensive video test set of real-world backdoor attacks. Our study not only\nestablishes the credibility and seriousness of this threat but also serves as a\nclarion call to the research community to advance backdoor defenses in the\ncontext of object detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detectors are vulnerable to backdoor attacks. In contrast to\nclassifiers, detectors possess unique characteristics, architecturally and in\ntask execution; often operating in challenging conditions, for instance,\ndetecting traffic signs in autonomous cars. But, our knowledge dominates\nattacks against classifiers and tests in the \"digital domain\".\n  To address this critical gap, we conducted an extensive empirical study\ntargeting multiple detector architectures and two challenging detection tasks\nin real-world settings: traffic signs and vehicles. Using the diverse,\nmethodically collected videos captured from driving cars and flying drones,\nincorporating physical object trigger deployments in authentic scenes, we\ninvestigated the viability of physical object-triggered backdoor attacks in\napplication settings.\n  Our findings revealed 8 key insights. Importantly, the prevalent \"digital\"\ndata poisoning method for injecting backdoors into models does not lead to\neffective attacks against detectors in the real world, although proven\neffective in classification tasks. We construct a new, cost-efficient attack\nmethod, dubbed MORPHING, incorporating the unique nature of detection tasks;\nours is remarkably successful in injecting physical object-triggered backdoors,\neven capable of poisoning triggers with clean label annotations or invisible\ntriggers without diminishing the success of physical object triggered\nbackdoors. We discovered that the defenses curated are ill-equipped to\nsafeguard detectors against such attacks. To underscore the severity of the\nthreat and foster further research, we, for the first time, release an\nextensive video test set of real-world backdoor attacks. Our study not only\nestablishes the credibility and seriousness of this threat but also serves as a\nclarion call to the research community to advance backdoor defenses in the\ncontext of object detection."
                },
                "authors": [
                    {
                        "name": "Bao Gia Doan"
                    },
                    {
                        "name": "Dang Quang Nguyen"
                    },
                    {
                        "name": "Callum Lindquist"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Tamas Abraham"
                    },
                    {
                        "name": "Olivier De Vel"
                    },
                    {
                        "name": "Seyit Camtepe"
                    },
                    {
                        "name": "Salil S. Kanhere"
                    },
                    {
                        "name": "Ehsan Abbasnejad"
                    },
                    {
                        "name": "Damith C. Ranasinghe"
                    }
                ],
                "author_detail": {
                    "name": "Damith C. Ranasinghe"
                },
                "author": "Damith C. Ranasinghe",
                "arxiv_comment": "Accepted to appear at the 40th Annual Computer Security Applications\n  Conference (ACSAC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01109v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01109v4",
                "updated": "2024-08-22T04:29:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    29,
                    11,
                    3,
                    235,
                    0
                ],
                "published": "2024-02-02T02:56:50Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    2,
                    56,
                    50,
                    4,
                    33,
                    0
                ],
                "title": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning"
                },
                "summary": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01109v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01109v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12117v3",
                "updated": "2024-08-22T04:11:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    11,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-01-22T16:57:05Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    16,
                    57,
                    5,
                    0,
                    22,
                    0
                ],
                "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large\n  Language Models"
                },
                "summary": "While large language models (LLMs) are still being adopted to new domains and\nutilized in novel applications, we are experiencing an influx of the new\ngeneration of foundation models, namely multi-modal large language models\n(MLLMs). These models integrate verbal and visual information, opening new\npossibilities to demonstrate more complex reasoning abilities at the\nintersection of the two modalities. However, despite the revolutionizing\nprospect of MLLMs, our understanding of their reasoning abilities is limited.\nIn this study, we assess the nonverbal abstract reasoning abilities of\nopen-source and closed-source MLLMs using variations of Raven's Progressive\nMatrices. Our experiments reveal the challenging nature of such problems for\nMLLMs while showcasing the immense gap between open-source and closed-source\nmodels. We also uncover critical shortcomings of visual and textual\nperceptions, subjecting the models to low-performance ceilings. Finally, to\nimprove MLLMs' performance, we experiment with different methods, such as\nChain-of-Thought prompting, leading to a significant (up to 100%) boost in\nperformance. Our code and datasets are available at\nhttps://github.com/usc-isi-i2/isi-mmlm-rpm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are still being adopted to new domains and\nutilized in novel applications, we are experiencing an influx of the new\ngeneration of foundation models, namely multi-modal large language models\n(MLLMs). These models integrate verbal and visual information, opening new\npossibilities to demonstrate more complex reasoning abilities at the\nintersection of the two modalities. However, despite the revolutionizing\nprospect of MLLMs, our understanding of their reasoning abilities is limited.\nIn this study, we assess the nonverbal abstract reasoning abilities of\nopen-source and closed-source MLLMs using variations of Raven's Progressive\nMatrices. Our experiments reveal the challenging nature of such problems for\nMLLMs while showcasing the immense gap between open-source and closed-source\nmodels. We also uncover critical shortcomings of visual and textual\nperceptions, subjecting the models to low-performance ceilings. Finally, to\nimprove MLLMs' performance, we experiment with different methods, such as\nChain-of-Thought prompting, leading to a significant (up to 100%) boost in\nperformance. Our code and datasets are available at\nhttps://github.com/usc-isi-i2/isi-mmlm-rpm."
                },
                "authors": [
                    {
                        "name": "Kian Ahrabian"
                    },
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Kexuan Sun"
                    },
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Fred Morstatter"
                    },
                    {
                        "name": "Jay Pujara"
                    }
                ],
                "author_detail": {
                    "name": "Jay Pujara"
                },
                "author": "Jay Pujara",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12116v1",
                "updated": "2024-08-22T04:05:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    5,
                    2,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T04:05:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    5,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "Geolocation Representation from Large Language Models are Generic\n  Enhancers for Spatio-Temporal Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geolocation Representation from Large Language Models are Generic\n  Enhancers for Spatio-Temporal Learning"
                },
                "summary": "In the geospatial domain, universal representation models are significantly\nless prevalent than their extensive use in natural language processing and\ncomputer vision. This discrepancy arises primarily from the high costs\nassociated with the input of existing representation models, which often\nrequire street views and mobility data. To address this, we develop a novel,\ntraining-free method that leverages large language models (LLMs) and auxiliary\nmap data from OpenStreetMap to derive geolocation representations (LLMGeovec).\nLLMGeovec can represent the geographic semantics of city, country, and global\nscales, which acts as a generic enhancer for spatio-temporal learning.\nSpecifically, by direct feature concatenation, we introduce a simple yet\neffective paradigm for enhancing multiple spatio-temporal tasks including\ngeographic prediction (GP), long-term time series forecasting (LTSF), and\ngraph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly\nintegrate into a wide spectrum of spatio-temporal learning models, providing\nimmediate enhancements. Experimental results demonstrate that LLMGeovec\nachieves global coverage and significantly boosts the performance of leading\nGP, LTSF, and GSTF models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the geospatial domain, universal representation models are significantly\nless prevalent than their extensive use in natural language processing and\ncomputer vision. This discrepancy arises primarily from the high costs\nassociated with the input of existing representation models, which often\nrequire street views and mobility data. To address this, we develop a novel,\ntraining-free method that leverages large language models (LLMs) and auxiliary\nmap data from OpenStreetMap to derive geolocation representations (LLMGeovec).\nLLMGeovec can represent the geographic semantics of city, country, and global\nscales, which acts as a generic enhancer for spatio-temporal learning.\nSpecifically, by direct feature concatenation, we introduce a simple yet\neffective paradigm for enhancing multiple spatio-temporal tasks including\ngeographic prediction (GP), long-term time series forecasting (LTSF), and\ngraph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly\nintegrate into a wide spectrum of spatio-temporal learning models, providing\nimmediate enhancements. Experimental results demonstrate that LLMGeovec\nachieves global coverage and significantly boosts the performance of leading\nGP, LTSF, and GSTF models."
                },
                "authors": [
                    {
                        "name": "Junlin He"
                    },
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12112v1",
                "updated": "2024-08-22T03:54:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    54,
                    8,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T03:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    54,
                    8,
                    3,
                    235,
                    0
                ],
                "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards"
                },
                "summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Shresth Verma"
                    },
                    {
                        "name": "Niclas Boehmer"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11119v2",
                "updated": "2024-08-22T03:46:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    46,
                    25,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-20T18:21:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    18,
                    21,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Mistral-SPLADE: LLMs for better Learned Sparse Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mistral-SPLADE: LLMs for better Learned Sparse Retrieval"
                },
                "summary": "Learned Sparse Retrievers (LSR) have evolved into an effective retrieval\nstrategy that can bridge the gap between traditional keyword-based sparse\nretrievers and embedding-based dense retrievers. At its core, learned sparse\nretrievers try to learn the most important semantic keyword expansions from a\nquery and/or document which can facilitate better retrieval with overlapping\nkeyword expansions. LSR like SPLADE has typically been using encoder only\nmodels with MLM (masked language modeling) style objective in conjunction with\nknown ways of retrieval performance improvement such as hard negative mining,\ndistillation, etc. In this work, we propose to use decoder-only model for\nlearning semantic keyword expansion. We posit, decoder only models that have\nseen much higher magnitudes of data are better equipped to learn keyword\nexpansions needed for improved retrieval. We use Mistral as the backbone to\ndevelop our Learned Sparse Retriever similar to SPLADE and train it on a subset\nof sentence-transformer data which is often used for training text embedding\nmodels. Our experiments support the hypothesis that a sparse retrieval model\nbased on decoder only large language model (LLM) surpasses the performance of\nexisting LSR systems, including SPLADE and all its variants. The LLM based\nmodel (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse\nretrieval model on the BEIR text retrieval benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Sparse Retrievers (LSR) have evolved into an effective retrieval\nstrategy that can bridge the gap between traditional keyword-based sparse\nretrievers and embedding-based dense retrievers. At its core, learned sparse\nretrievers try to learn the most important semantic keyword expansions from a\nquery and/or document which can facilitate better retrieval with overlapping\nkeyword expansions. LSR like SPLADE has typically been using encoder only\nmodels with MLM (masked language modeling) style objective in conjunction with\nknown ways of retrieval performance improvement such as hard negative mining,\ndistillation, etc. In this work, we propose to use decoder-only model for\nlearning semantic keyword expansion. We posit, decoder only models that have\nseen much higher magnitudes of data are better equipped to learn keyword\nexpansions needed for improved retrieval. We use Mistral as the backbone to\ndevelop our Learned Sparse Retriever similar to SPLADE and train it on a subset\nof sentence-transformer data which is often used for training text embedding\nmodels. Our experiments support the hypothesis that a sparse retrieval model\nbased on decoder only large language model (LLM) surpasses the performance of\nexisting LSR systems, including SPLADE and all its variants. The LLM based\nmodel (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse\nretrieval model on the BEIR text retrieval benchmark."
                },
                "authors": [
                    {
                        "name": "Meet Doshi"
                    },
                    {
                        "name": "Vishwajeet Kumar"
                    },
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Vignesh P"
                    },
                    {
                        "name": "Jaydeep Sen"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep Sen"
                },
                "author": "Jaydeep Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.01728v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.01728v3",
                "updated": "2024-08-22T03:43:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    43,
                    30,
                    3,
                    235,
                    0
                ],
                "published": "2022-12-04T02:58:50Z",
                "published_parsed": [
                    2022,
                    12,
                    4,
                    2,
                    58,
                    50,
                    6,
                    338,
                    0
                ],
                "title": "ISAC-Enabled Beam Alignment for Terahertz Networks: Scheme Design and\n  Coverage Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISAC-Enabled Beam Alignment for Terahertz Networks: Scheme Design and\n  Coverage Analysis"
                },
                "summary": "As a key pillar technology for the future 6G networks, Terahertz (THz)\ncommunications can provide high-capacity transmissions, but suffers from severe\npropagation loss and line-of-sight (LoS) blockage that limits the network\ncoverage. Narrow beams are required to compensate for the loss, but they in\nturn bring in beam misalignment challenge and degrade the THz network coverage.\nThe high sensing resolution of THz signals enables integrated sensing and\ncommunications (ISAC) technology to assist the LoS blockage and user\nmobility-induced beam misalignment, enhancing THz network coverage. Based on\nthe 5G beam management, we propose a joint synchronization signal block (SSB)\nand reference signal (RS)-based sensing (JSRS) scheme to assist beam alignment.\nJSRS enables a predict-and-prevent procedure that provides early interventions\nfor timely beam switches. To maximize performance of JSRS, we provide an\noptimal sensing signal insertion and time-to-frequency allocation to improve\nthe joint range and velocity resolutions. We derive the coverage probability of\nthe JSRS-enabled network to evaluate its abilities in beam misalignment\nreduction and coverage enhancement. The expression also instructs the network\ndensity deployment and beamwidth selection. Numerical results show that the\nJSRS scheme is effective and highly compatible with the 5G air interface.\nAveraged in the tested urban use cases, JSRS achieves near-ideal performance\nand reduces around 80% of beam misalignment, and enhances the coverage\nprobability by about 75%, compared to the network with 5G-required positioning\nability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a key pillar technology for the future 6G networks, Terahertz (THz)\ncommunications can provide high-capacity transmissions, but suffers from severe\npropagation loss and line-of-sight (LoS) blockage that limits the network\ncoverage. Narrow beams are required to compensate for the loss, but they in\nturn bring in beam misalignment challenge and degrade the THz network coverage.\nThe high sensing resolution of THz signals enables integrated sensing and\ncommunications (ISAC) technology to assist the LoS blockage and user\nmobility-induced beam misalignment, enhancing THz network coverage. Based on\nthe 5G beam management, we propose a joint synchronization signal block (SSB)\nand reference signal (RS)-based sensing (JSRS) scheme to assist beam alignment.\nJSRS enables a predict-and-prevent procedure that provides early interventions\nfor timely beam switches. To maximize performance of JSRS, we provide an\noptimal sensing signal insertion and time-to-frequency allocation to improve\nthe joint range and velocity resolutions. We derive the coverage probability of\nthe JSRS-enabled network to evaluate its abilities in beam misalignment\nreduction and coverage enhancement. The expression also instructs the network\ndensity deployment and beamwidth selection. Numerical results show that the\nJSRS scheme is effective and highly compatible with the 5G air interface.\nAveraged in the tested urban use cases, JSRS achieves near-ideal performance\nand reduces around 80% of beam misalignment, and enhances the coverage\nprobability by about 75%, compared to the network with 5G-required positioning\nability."
                },
                "authors": [
                    {
                        "name": "Wenrong Chen"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Boyu Ning"
                    },
                    {
                        "name": "Tony Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Quek"
                },
                "author": "Tony Quek",
                "arxiv_doi": "10.1109/TVT.2024.3441534",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3441534",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.01728v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.01728v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Veh. Techno (2024)",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10529v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10529v3",
                "updated": "2024-08-22T03:40:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    40,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-20T04:06:58Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    6,
                    58,
                    1,
                    233,
                    0
                ],
                "title": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study"
                },
                "summary": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2."
                },
                "authors": [
                    {
                        "name": "Emmanuel Iko-Ojo Simon"
                    },
                    {
                        "name": "Chirath Hettiarachchi"
                    },
                    {
                        "name": "Alex Potanin"
                    },
                    {
                        "name": "Hanna Suominen"
                    },
                    {
                        "name": "Fatemeh Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Fard"
                },
                "author": "Fatemeh Fard",
                "arxiv_comment": "Accepted as Continuity Acceptance (CA) for a Stage 1 registration of\n  the Registered Report Track at 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME 2024), Flagstaff, USA, October 6-11, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10529v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10529v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05055v2",
                "updated": "2024-08-22T03:17:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    17,
                    42,
                    3,
                    235,
                    0
                ],
                "published": "2024-03-08T05:03:25Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    5,
                    3,
                    25,
                    4,
                    68,
                    0
                ],
                "title": "MUC: Mixture of Uncalibrated Cameras for Robust 3D Human Body\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUC: Mixture of Uncalibrated Cameras for Robust 3D Human Body\n  Reconstruction"
                },
                "summary": "Multiple cameras can provide comprehensive multi-view video coverage of a\nperson. Fusing this multi-view data is crucial for tasks like behavioral\nanalysis, although it traditionally requires camera calibration, a process that\nis often complex. Moreover, previous studies have overlooked the challenges\nposed by self-occlusion under multiple views and the continuity of human body\nshape estimation. In this study, we introduce a method to reconstruct the 3D\nhuman body from multiple uncalibrated camera views. Initially, we utilize a\npre-trained human body encoder to process each camera view individually,\nenabling the reconstruction of human body models and parameters for each view\nalong with predicted camera positions. Rather than merely averaging the models\nacross views, we develop a neural network trained to assign weights to\nindividual views for all human body joints, based on the estimated distribution\nof joint distances from each camera. Additionally, we focus on the mesh surface\nof the human body for dynamic fusion, allowing for the seamless integration of\nfacial expressions and body shape into a unified human body model. Our method\nhas shown excellent performance in reconstructing the human body on two public\ndatasets, advancing beyond previous work from the SMPL model to the SMPL-X\nmodel. This extension incorporates more complex hand poses and facial\nexpressions, enhancing the detail and accuracy of the reconstructions.\nCrucially, it supports the flexible ad-hoc deployment of any number of cameras,\noffering significant potential for various applications. Our code is available\nat https://github.com/AbsterZhu/MUC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple cameras can provide comprehensive multi-view video coverage of a\nperson. Fusing this multi-view data is crucial for tasks like behavioral\nanalysis, although it traditionally requires camera calibration, a process that\nis often complex. Moreover, previous studies have overlooked the challenges\nposed by self-occlusion under multiple views and the continuity of human body\nshape estimation. In this study, we introduce a method to reconstruct the 3D\nhuman body from multiple uncalibrated camera views. Initially, we utilize a\npre-trained human body encoder to process each camera view individually,\nenabling the reconstruction of human body models and parameters for each view\nalong with predicted camera positions. Rather than merely averaging the models\nacross views, we develop a neural network trained to assign weights to\nindividual views for all human body joints, based on the estimated distribution\nof joint distances from each camera. Additionally, we focus on the mesh surface\nof the human body for dynamic fusion, allowing for the seamless integration of\nfacial expressions and body shape into a unified human body model. Our method\nhas shown excellent performance in reconstructing the human body on two public\ndatasets, advancing beyond previous work from the SMPL model to the SMPL-X\nmodel. This extension incorporates more complex hand poses and facial\nexpressions, enhancing the detail and accuracy of the reconstructions.\nCrucially, it supports the flexible ad-hoc deployment of any number of cameras,\noffering significant potential for various applications. Our code is available\nat https://github.com/AbsterZhu/MUC."
                },
                "authors": [
                    {
                        "name": "Yitao Zhu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Mengjie Xu"
                    },
                    {
                        "name": "Zixu Zhuang"
                    },
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Kaidong Wang"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12097v1",
                "updated": "2024-08-22T03:10:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    10,
                    52,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T03:10:52Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    10,
                    52,
                    3,
                    235,
                    0
                ],
                "title": "Extraction of Research Objectives, Machine Learning Model Names, and\n  Dataset Names from Academic Papers and Analysis of Their Interrelationships\n  Using LLM and Network Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extraction of Research Objectives, Machine Learning Model Names, and\n  Dataset Names from Academic Papers and Analysis of Their Interrelationships\n  Using LLM and Network Analysis"
                },
                "summary": "Machine learning is widely utilized across various industries. Identifying\nthe appropriate machine learning models and datasets for specific tasks is\ncrucial for the effective industrial application of machine learning. However,\nthis requires expertise in both machine learning and the relevant domain,\nleading to a high learning cost. Therefore, research focused on extracting\ncombinations of tasks, machine learning models, and datasets from academic\npapers is critically important, as it can facilitate the automatic\nrecommendation of suitable methods. Conventional information extraction methods\nfrom academic papers have been limited to identifying machine learning models\nand other entities as named entities. To address this issue, this study\nproposes a methodology extracting tasks, machine learning methods, and dataset\nnames from scientific papers and analyzing the relationships between these\ninformation by using LLM, embedding model, and network clustering. The proposed\nmethod's expression extraction performance, when using Llama3, achieves an\nF-score exceeding 0.8 across various categories, confirming its practical\nutility. Benchmarking results on financial domain papers have demonstrated the\neffectiveness of this method, providing insights into the use of the latest\ndatasets, including those related to ESG (Environmental, Social, and\nGovernance) data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning is widely utilized across various industries. Identifying\nthe appropriate machine learning models and datasets for specific tasks is\ncrucial for the effective industrial application of machine learning. However,\nthis requires expertise in both machine learning and the relevant domain,\nleading to a high learning cost. Therefore, research focused on extracting\ncombinations of tasks, machine learning models, and datasets from academic\npapers is critically important, as it can facilitate the automatic\nrecommendation of suitable methods. Conventional information extraction methods\nfrom academic papers have been limited to identifying machine learning models\nand other entities as named entities. To address this issue, this study\nproposes a methodology extracting tasks, machine learning methods, and dataset\nnames from scientific papers and analyzing the relationships between these\ninformation by using LLM, embedding model, and network clustering. The proposed\nmethod's expression extraction performance, when using Llama3, achieves an\nF-score exceeding 0.8 across various categories, confirming its practical\nutility. Benchmarking results on financial domain papers have demonstrated the\neffectiveness of this method, providing insights into the use of the latest\ndatasets, including those related to ESG (Environmental, Social, and\nGovernance) data."
                },
                "authors": [
                    {
                        "name": "S. Nishio"
                    },
                    {
                        "name": "H. Nonaka"
                    },
                    {
                        "name": "N. Tsuchiya"
                    },
                    {
                        "name": "A. Migita"
                    },
                    {
                        "name": "Y. Banno"
                    },
                    {
                        "name": "T. Hayashi"
                    },
                    {
                        "name": "H. Sakaji"
                    },
                    {
                        "name": "T. Sakumoto"
                    },
                    {
                        "name": "K. Watabe"
                    }
                ],
                "author_detail": {
                    "name": "K. Watabe"
                },
                "author": "K. Watabe",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12093v1",
                "updated": "2024-08-22T03:03:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    3,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T03:03:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    3,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "LLM-enhanced Scene Graph Learning for Household Rearrangement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Scene Graph Learning for Household Rearrangement"
                },
                "summary": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Qijin She"
                    },
                    {
                        "name": "Zhinan Yu"
                    },
                    {
                        "name": "Yuqing Lan"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Ruizhen Hu"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu",
                "arxiv_comment": "SIGGRAPH ASIA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v3",
                "updated": "2024-08-22T02:52:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    52,
                    28,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "There are some mistakes in the experimental data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12081v1",
                "updated": "2024-08-22T02:41:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    41,
                    6,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T02:41:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    41,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "Towards Threat Modelling of IoT Context-Sharing Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Threat Modelling of IoT Context-Sharing Platforms"
                },
                "summary": "The Internet of Things (IoT) involves complex, interconnected systems and\ndevices that depend on context-sharing platforms for interoperability and\ninformation exchange. These platforms are, therefore, critical components of\nreal-world IoT deployments, making their security essential to ensure the\nresilience and reliability of these 'systems of systems'. In this paper, we\ntake the first steps toward systematically and comprehensively addressing the\nsecurity of IoT context-sharing platforms. We propose a framework for threat\nmodelling and security analysis of a generic IoT context-sharing solution,\nemploying the MITRE ATT&CK framework. Through an evaluation of various\nindustry-funded projects and academic research, we identify significant\nsecurity challenges in the design of IoT context-sharing platforms. Our threat\nmodelling provides an in-depth analysis of the techniques and sub-techniques\nadversaries may use to exploit these systems, offering valuable insights for\nfuture research aimed at developing resilient solutions. Additionally, we have\ndeveloped an open-source threat analysis tool that incorporates our detailed\nthreat modelling, which can be used to evaluate and enhance the security of\nexisting context-sharing platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Internet of Things (IoT) involves complex, interconnected systems and\ndevices that depend on context-sharing platforms for interoperability and\ninformation exchange. These platforms are, therefore, critical components of\nreal-world IoT deployments, making their security essential to ensure the\nresilience and reliability of these 'systems of systems'. In this paper, we\ntake the first steps toward systematically and comprehensively addressing the\nsecurity of IoT context-sharing platforms. We propose a framework for threat\nmodelling and security analysis of a generic IoT context-sharing solution,\nemploying the MITRE ATT&CK framework. Through an evaluation of various\nindustry-funded projects and academic research, we identify significant\nsecurity challenges in the design of IoT context-sharing platforms. Our threat\nmodelling provides an in-depth analysis of the techniques and sub-techniques\nadversaries may use to exploit these systems, offering valuable insights for\nfuture research aimed at developing resilient solutions. Additionally, we have\ndeveloped an open-source threat analysis tool that incorporates our detailed\nthreat modelling, which can be used to evaluate and enhance the security of\nexisting context-sharing platforms."
                },
                "authors": [
                    {
                        "name": "Mohammad Goudarzi"
                    },
                    {
                        "name": "Arash Shaghaghi"
                    },
                    {
                        "name": "Simon Finn"
                    },
                    {
                        "name": "Burkhard Stiller"
                    },
                    {
                        "name": "Sanjay Jha"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Jha"
                },
                "author": "Sanjay Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12080v1",
                "updated": "2024-08-22T02:40:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    40,
                    21,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T02:40:21Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    40,
                    21,
                    3,
                    235,
                    0
                ],
                "title": "Exploring the Feasibility of Automated Data Standardization using Large\n  Language Models for Seamless Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Feasibility of Automated Data Standardization using Large\n  Language Models for Seamless Positioning"
                },
                "summary": "We propose a feasibility study for real-time automated data standardization\nleveraging Large Language Models (LLMs) to enhance seamless positioning systems\nin IoT environments. By integrating and standardizing heterogeneous sensor data\nfrom smartphones, IoT devices, and dedicated systems such as Ultra-Wideband\n(UWB), our study ensures data compatibility and improves positioning accuracy\nusing the Extended Kalman Filter (EKF). The core components include the\nIntelligent Data Standardization Module (IDSM), which employs a fine-tuned LLM\nto convert varied sensor data into a standardized format, and the\nTransformation Rule Generation Module (TRGM), which automates the creation of\ntransformation rules and scripts for ongoing data standardization. Evaluated in\nreal-time environments, our study demonstrates adaptability and scalability,\nenhancing operational efficiency and accuracy in seamless navigation. This\nstudy underscores the potential of advanced LLMs in overcoming sensor data\nintegration complexities, paving the way for more scalable and precise IoT\nnavigation solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a feasibility study for real-time automated data standardization\nleveraging Large Language Models (LLMs) to enhance seamless positioning systems\nin IoT environments. By integrating and standardizing heterogeneous sensor data\nfrom smartphones, IoT devices, and dedicated systems such as Ultra-Wideband\n(UWB), our study ensures data compatibility and improves positioning accuracy\nusing the Extended Kalman Filter (EKF). The core components include the\nIntelligent Data Standardization Module (IDSM), which employs a fine-tuned LLM\nto convert varied sensor data into a standardized format, and the\nTransformation Rule Generation Module (TRGM), which automates the creation of\ntransformation rules and scripts for ongoing data standardization. Evaluated in\nreal-time environments, our study demonstrates adaptability and scalability,\nenhancing operational efficiency and accuracy in seamless navigation. This\nstudy underscores the potential of advanced LLMs in overcoming sensor data\nintegration complexities, paving the way for more scalable and precise IoT\nnavigation solutions."
                },
                "authors": [
                    {
                        "name": "Max J. L. Lee"
                    },
                    {
                        "name": "Ju Lin"
                    },
                    {
                        "name": "Li-Ta Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Li-Ta Hsu"
                },
                "author": "Li-Ta Hsu",
                "arxiv_comment": "Accepted at IPIN 2024. To be published in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11729v2",
                "updated": "2024-08-22T02:38:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    38,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-21T15:54:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites"
                },
                "summary": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes."
                },
                "authors": [
                    {
                        "name": "Zachariah Sollenberger"
                    },
                    {
                        "name": "Jay Patel"
                    },
                    {
                        "name": "Christian Munley"
                    },
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12076v1",
                "updated": "2024-08-22T02:33:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    33,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T02:33:13Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    33,
                    13,
                    3,
                    235,
                    0
                ],
                "title": "ConflictBank: A Benchmark for Evaluating the Influence of Knowledge\n  Conflicts in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConflictBank: A Benchmark for Evaluating the Influence of Knowledge\n  Conflicts in LLM"
                },
                "summary": "Large language models (LLMs) have achieved impressive advancements across\nnumerous disciplines, yet the critical issue of knowledge conflicts, a major\nsource of hallucinations, has rarely been studied. Only a few research explored\nthe conflicts between the inherent knowledge of LLMs and the retrieved\ncontextual knowledge. However, a thorough assessment of knowledge conflict in\nLLMs is still missing. Motivated by this research gap, we present ConflictBank,\nthe first comprehensive benchmark developed to systematically evaluate\nknowledge conflicts from three aspects: (i) conflicts encountered in retrieved\nknowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the\ninterplay between these conflict forms. Our investigation delves into four\nmodel families and twelve LLM instances, meticulously analyzing conflicts\nstemming from misinformation, temporal discrepancies, and semantic divergences.\nBased on our proposed novel construction framework, we create 7,453,853\nclaim-evidence pairs and 553,117 QA pairs. We present numerous findings on\nmodel scale, conflict causes, and conflict types. We hope our ConflictBank\nbenchmark will help the community better understand model behavior in conflicts\nand develop more reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive advancements across\nnumerous disciplines, yet the critical issue of knowledge conflicts, a major\nsource of hallucinations, has rarely been studied. Only a few research explored\nthe conflicts between the inherent knowledge of LLMs and the retrieved\ncontextual knowledge. However, a thorough assessment of knowledge conflict in\nLLMs is still missing. Motivated by this research gap, we present ConflictBank,\nthe first comprehensive benchmark developed to systematically evaluate\nknowledge conflicts from three aspects: (i) conflicts encountered in retrieved\nknowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the\ninterplay between these conflict forms. Our investigation delves into four\nmodel families and twelve LLM instances, meticulously analyzing conflicts\nstemming from misinformation, temporal discrepancies, and semantic divergences.\nBased on our proposed novel construction framework, we create 7,453,853\nclaim-evidence pairs and 553,117 QA pairs. We present numerous findings on\nmodel scale, conflict causes, and conflict types. We hope our ConflictBank\nbenchmark will help the community better understand model behavior in conflicts\nand develop more reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14845v2",
                "updated": "2024-08-22T02:23:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    23,
                    12,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-20T11:19:58Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    11,
                    19,
                    58,
                    5,
                    202,
                    0
                ],
                "title": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models"
                },
                "summary": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real datasets validate our\nproposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real datasets validate our\nproposed model."
                },
                "authors": [
                    {
                        "name": "Ze Yu Zhang"
                    },
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Finale Doshi-Velez"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12070v1",
                "updated": "2024-08-22T02:18:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    18,
                    35,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T02:18:35Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    18,
                    35,
                    3,
                    235,
                    0
                ],
                "title": "Better Debugging: Combining Static Analysis and LLMs for Explainable\n  Crashing Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Debugging: Combining Static Analysis and LLMs for Explainable\n  Crashing Fault Localization"
                },
                "summary": "Nowadays, many applications do not exist independently but rely on various\nframeworks or libraries. The frequent evolution and the complex implementation\nof framework APIs induce many unexpected post-release crashes. Starting from\nthe crash stack traces, existing approaches either perform direct call graph\n(CG) tracing or construct datasets with similar crash-fixing records to locate\nbuggy methods. However, these approaches are limited by the completeness of CG\nor dependent on historical fixing records. Moreover, they fail to explain the\nbuggy candidates by revealing their relationship with the crashing point.\n  To fill the gap, we propose an explainable crashing fault localization\napproach by combining static analysis and LLM techniques. Our primary insight\nis that understanding the semantics of exception-throwing statements in the\nframework code can help find and apprehend the buggy methods in the app code.\nBased on this idea, first, we design the exception-thrown summary (ETS) that\ndescribes the key elements related to each framework-specific exception and\nextract ETSs by performing static analysis. Then we make data-tracking of its\nkey elements to identify and sort buggy candidates for the given crash. After\nthat, we introduce LLMs to improve the explainability of the localization\nresults. To construct effective LLM prompts, we design the candidate\ninformation summary (CIS) that describes multiple types of explanation-related\ncontexts and then extract CISs via static analysis. We apply our approach to\none typical scenario, i.e., locating Android framework-specific crashing\nfaults, and implement a tool CrashTracker. For fault localization, it exhibited\nan overall MRR value of 0.91 in precision. For fault explanation, compared to\nthe naive one produced by static analysis only, the LLM-powered explanation\nachieved a 67.04% improvement in users' satisfaction score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, many applications do not exist independently but rely on various\nframeworks or libraries. The frequent evolution and the complex implementation\nof framework APIs induce many unexpected post-release crashes. Starting from\nthe crash stack traces, existing approaches either perform direct call graph\n(CG) tracing or construct datasets with similar crash-fixing records to locate\nbuggy methods. However, these approaches are limited by the completeness of CG\nor dependent on historical fixing records. Moreover, they fail to explain the\nbuggy candidates by revealing their relationship with the crashing point.\n  To fill the gap, we propose an explainable crashing fault localization\napproach by combining static analysis and LLM techniques. Our primary insight\nis that understanding the semantics of exception-throwing statements in the\nframework code can help find and apprehend the buggy methods in the app code.\nBased on this idea, first, we design the exception-thrown summary (ETS) that\ndescribes the key elements related to each framework-specific exception and\nextract ETSs by performing static analysis. Then we make data-tracking of its\nkey elements to identify and sort buggy candidates for the given crash. After\nthat, we introduce LLMs to improve the explainability of the localization\nresults. To construct effective LLM prompts, we design the candidate\ninformation summary (CIS) that describes multiple types of explanation-related\ncontexts and then extract CISs via static analysis. We apply our approach to\none typical scenario, i.e., locating Android framework-specific crashing\nfaults, and implement a tool CrashTracker. For fault localization, it exhibited\nan overall MRR value of 0.91 in precision. For fault explanation, compared to\nthe naive one produced by static analysis only, the LLM-powered explanation\nachieved a 67.04% improvement in users' satisfaction score."
                },
                "authors": [
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12067v1",
                "updated": "2024-08-22T02:11:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    11,
                    14,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T02:11:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    11,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Distributed Noncoherent Joint Transmission Based on Multi-Agent\n  Reinforcement Learning for Dense Small Cell MISO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Noncoherent Joint Transmission Based on Multi-Agent\n  Reinforcement Learning for Dense Small Cell MISO Systems"
                },
                "summary": "We consider a dense small cell (DSC) network where multi-antenna small cell\nbase stations (SBSs) transmit data to single-antenna users over a shared\nfrequency band. To enhance capacity, a state-of-the-art technique known as\nnoncoherent joint transmission (JT) is applied, enabling users to receive data\nfrom multiple coordinated SBSs. However, the sum rate maximization problem with\nnoncoherent JT is inherently nonconvex and NP-hard. While existing\noptimization-based noncoherent JT algorithms can provide near-optimal\nperformance, they require global channel state information (CSI) and multiple\niterations, which makes them difficult to be implemeted in DSC networks.To\novercome these challenges, we first prove that the optimal beamforming\nstructure is the same for both the power minimization problem and the sum rate\nmaximization problem, and then mathematically derive the optimal beamforming\nstructure for both problems by solving the power minimization problem.The\noptimal beamforming structure can effectively reduces the variable\ndimensions.By exploiting the optimal beamforming structure, we propose a deep\ndeterministic policy gradient-based distributed noncoherent JT scheme to\nmaximize the system sum rate.In the proposed scheme, each SBS utilizes global\ninformation for training and uses local CSI to determine beamforming vectors.\nSimulation results demonstrate that the proposed scheme achieves comparable\nperformance with considerably lower computational complexity and information\noverhead compared to centralized iterative optimization-based techniques,\nmaking it more attractive for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dense small cell (DSC) network where multi-antenna small cell\nbase stations (SBSs) transmit data to single-antenna users over a shared\nfrequency band. To enhance capacity, a state-of-the-art technique known as\nnoncoherent joint transmission (JT) is applied, enabling users to receive data\nfrom multiple coordinated SBSs. However, the sum rate maximization problem with\nnoncoherent JT is inherently nonconvex and NP-hard. While existing\noptimization-based noncoherent JT algorithms can provide near-optimal\nperformance, they require global channel state information (CSI) and multiple\niterations, which makes them difficult to be implemeted in DSC networks.To\novercome these challenges, we first prove that the optimal beamforming\nstructure is the same for both the power minimization problem and the sum rate\nmaximization problem, and then mathematically derive the optimal beamforming\nstructure for both problems by solving the power minimization problem.The\noptimal beamforming structure can effectively reduces the variable\ndimensions.By exploiting the optimal beamforming structure, we propose a deep\ndeterministic policy gradient-based distributed noncoherent JT scheme to\nmaximize the system sum rate.In the proposed scheme, each SBS utilizes global\ninformation for training and uses local CSI to determine beamforming vectors.\nSimulation results demonstrate that the proposed scheme achieves comparable\nperformance with considerably lower computational complexity and information\noverhead compared to centralized iterative optimization-based techniques,\nmaking it more attractive for practical deployment."
                },
                "authors": [
                    {
                        "name": "Shaozhuang Bai"
                    },
                    {
                        "name": "Zhenzhen Gao"
                    },
                    {
                        "name": "Xuewen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xuewen Liao"
                },
                "author": "Xuewen Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06777v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06777v4",
                "updated": "2024-08-22T02:06:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    6,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-10T20:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    20,
                    25,
                    18,
                    0,
                    162,
                    0
                ],
                "title": "MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension"
                },
                "summary": "Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving professional molecule-related\ntasks. This challenge is attributed to their inherent limitations in\ncomprehending molecules using only common textual representations, i.e., SMILES\nstrings. In this study, we seek to enhance the ability of LLMs to comprehend\nmolecules by equipping them with a multi-modal external module, namely MolX. In\nparticular, instead of directly using a SMILES string to represent a molecule,\nwe utilize specific encoders to extract fine-grained features from both SMILES\nstring and 2D molecular graph representations for feeding into an LLM.\nMoreover, a handcrafted molecular fingerprint is incorporated to leverage its\nembedded domain knowledge. Then, to establish an alignment between MolX and the\nLLM's textual input space, the whole model in which the LLM is frozen, is\npre-trained with a versatile strategy including a diverse set of tasks.\nExperimental evaluations show that our proposed method outperforms baselines\nacross 4 downstream molecule-related tasks ranging from molecule-to-text\ntranslation to retrosynthesis, with and without fine-tuning the LLM, while only\nintroducing a small number of trainable parameters 0.53% and 0.82%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving professional molecule-related\ntasks. This challenge is attributed to their inherent limitations in\ncomprehending molecules using only common textual representations, i.e., SMILES\nstrings. In this study, we seek to enhance the ability of LLMs to comprehend\nmolecules by equipping them with a multi-modal external module, namely MolX. In\nparticular, instead of directly using a SMILES string to represent a molecule,\nwe utilize specific encoders to extract fine-grained features from both SMILES\nstring and 2D molecular graph representations for feeding into an LLM.\nMoreover, a handcrafted molecular fingerprint is incorporated to leverage its\nembedded domain knowledge. Then, to establish an alignment between MolX and the\nLLM's textual input space, the whole model in which the LLM is frozen, is\npre-trained with a versatile strategy including a diverse set of tasks.\nExperimental evaluations show that our proposed method outperforms baselines\nacross 4 downstream molecule-related tasks ranging from molecule-to-text\ntranslation to retrosynthesis, with and without fine-tuning the LLM, while only\nintroducing a small number of trainable parameters 0.53% and 0.82%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Zhichun Guo"
                    },
                    {
                        "name": "Kaiwen Dong"
                    },
                    {
                        "name": "Xiaobao Huang"
                    },
                    {
                        "name": "Bozhao Nan"
                    },
                    {
                        "name": "Roshni Iyer"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Olaf Wiest"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06777v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06777v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12060v1",
                "updated": "2024-08-22T01:42:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    42,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T01:42:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    42,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning\n  with LLMs"
                },
                "summary": "Given the widespread dissemination of misinformation on social media,\nimplementing fact-checking mechanisms for online claims is essential. Manually\nverifying every claim is highly challenging, underscoring the need for an\nautomated fact-checking system. This paper presents our system designed to\naddress this issue. We utilize the Averitec dataset to assess the veracity of\nclaims. In addition to veracity prediction, our system provides supporting\nevidence, which is extracted from the dataset. We develop a Retrieve and\nGenerate (RAG) pipeline to extract relevant evidence sentences from a knowledge\nbase, which are then inputted along with the claim into a large language model\n(LLM) for classification. We also evaluate the few-shot In-Context Learning\n(ICL) capabilities of multiple LLMs. Our system achieves an 'Averitec' score of\n0.33, which is a 22% absolute improvement over the baseline. All code will be\nmade available on All code will be made available on\nhttps://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the widespread dissemination of misinformation on social media,\nimplementing fact-checking mechanisms for online claims is essential. Manually\nverifying every claim is highly challenging, underscoring the need for an\nautomated fact-checking system. This paper presents our system designed to\naddress this issue. We utilize the Averitec dataset to assess the veracity of\nclaims. In addition to veracity prediction, our system provides supporting\nevidence, which is extracted from the dataset. We develop a Retrieve and\nGenerate (RAG) pipeline to extract relevant evidence sentences from a knowledge\nbase, which are then inputted along with the claim into a large language model\n(LLM) for classification. We also evaluate the few-shot In-Context Learning\n(ICL) capabilities of multiple LLMs. Our system achieves an 'Averitec' score of\n0.33, which is a 22% absolute improvement over the baseline. All code will be\nmade available on All code will be made available on\nhttps://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms."
                },
                "authors": [
                    {
                        "name": "Ronit Singhal"
                    },
                    {
                        "name": "Pransh Patwa"
                    },
                    {
                        "name": "Parth Patwa"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitava Das"
                },
                "author": "Amitava Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12056v1",
                "updated": "2024-08-22T01:13:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    13,
                    2,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T01:13:02Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    13,
                    2,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing LLM-Based Automated Program Repair with Design Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Automated Program Repair with Design Rationales"
                },
                "summary": "Automatic Program Repair (APR) endeavors to autonomously rectify issues\nwithin specific projects, which generally encompasses three categories of\ntasks: bug resolution, new feature development, and feature enhancement.\nDespite extensive research proposing various methodologies, their efficacy in\naddressing real issues remains unsatisfactory. It's worth noting that,\ntypically, engineers have design rationales (DR) on solution-planed solutions\nand a set of underlying reasons-before they start patching code. In open-source\nprojects, these DRs are frequently captured in issue logs through project\nmanagement tools like Jira. This raises a compelling question: How can we\nleverage DR scattered across the issue logs to efficiently enhance APR? To\ninvestigate this premise, we introduce DRCodePilot, an approach designed to\naugment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt\ninstruction. Furthermore, given GPT-4's constraints in fully grasping the\nbroader project context and occasional shortcomings in generating precise\nidentifiers, we have devised a feedback-based self-reflective framework, in\nwhich we prompt GPT-4 to reconsider and refine its outputs by referencing a\nprovided patch and suggested identifiers. We have established a benchmark\ncomprising 938 issue-patch pairs sourced from two open-source repositories\nhosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot\nachieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is\nutilized directly. Additionally, the CodeBLEU scores also exhibit promising\nenhancements. Moreover, our findings reveal that the standalone application of\nDR can yield promising increase in the full-match ratio across CodeLlama,\nGPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot\ninitiative heralds a novel human-in-the-loop avenue for advancing the field of\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Program Repair (APR) endeavors to autonomously rectify issues\nwithin specific projects, which generally encompasses three categories of\ntasks: bug resolution, new feature development, and feature enhancement.\nDespite extensive research proposing various methodologies, their efficacy in\naddressing real issues remains unsatisfactory. It's worth noting that,\ntypically, engineers have design rationales (DR) on solution-planed solutions\nand a set of underlying reasons-before they start patching code. In open-source\nprojects, these DRs are frequently captured in issue logs through project\nmanagement tools like Jira. This raises a compelling question: How can we\nleverage DR scattered across the issue logs to efficiently enhance APR? To\ninvestigate this premise, we introduce DRCodePilot, an approach designed to\naugment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt\ninstruction. Furthermore, given GPT-4's constraints in fully grasping the\nbroader project context and occasional shortcomings in generating precise\nidentifiers, we have devised a feedback-based self-reflective framework, in\nwhich we prompt GPT-4 to reconsider and refine its outputs by referencing a\nprovided patch and suggested identifiers. We have established a benchmark\ncomprising 938 issue-patch pairs sourced from two open-source repositories\nhosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot\nachieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is\nutilized directly. Additionally, the CodeBLEU scores also exhibit promising\nenhancements. Moreover, our findings reveal that the standalone application of\nDR can yield promising increase in the full-match ratio across CodeLlama,\nGPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot\ninitiative heralds a novel human-in-the-loop avenue for advancing the field of\nAPR."
                },
                "authors": [
                    {
                        "name": "Jiuang Zhao"
                    },
                    {
                        "name": "Donghao Yang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Zitian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zitian Yang"
                },
                "author": "Zitian Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12055v1",
                "updated": "2024-08-22T01:11:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    11,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T01:11:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    1,
                    11,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning (Medical) LLMs for (Counterfactual) Fairness"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising solutions for a\nvariety of medical and clinical decision support applications. However, LLMs\nare often subject to different types of biases, which can lead to unfair\ntreatment of individuals, worsening health disparities, and reducing trust in\nAI-augmented medical tools. Aiming to address this important issue, in this\nstudy, we present a new model alignment approach for aligning LLMs using a\npreference optimization method within a knowledge distillation framework. Prior\nto presenting our proposed method, we first use an evaluation framework to\nconduct a comprehensive (largest to our knowledge) empirical evaluation to\nreveal the type and nature of existing biases in LLMs used for medical\napplications. We then offer a bias mitigation technique to reduce the unfair\npatterns in LLM outputs across different subgroups identified by the protected\nattributes. We show that our mitigation method is effective in significantly\nreducing observed biased patterns. Our code is publicly available at\n\\url{https://github.com/healthylaife/FairAlignmentLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising solutions for a\nvariety of medical and clinical decision support applications. However, LLMs\nare often subject to different types of biases, which can lead to unfair\ntreatment of individuals, worsening health disparities, and reducing trust in\nAI-augmented medical tools. Aiming to address this important issue, in this\nstudy, we present a new model alignment approach for aligning LLMs using a\npreference optimization method within a knowledge distillation framework. Prior\nto presenting our proposed method, we first use an evaluation framework to\nconduct a comprehensive (largest to our knowledge) empirical evaluation to\nreveal the type and nature of existing biases in LLMs used for medical\napplications. We then offer a bias mitigation technique to reduce the unfair\npatterns in LLM outputs across different subgroups identified by the protected\nattributes. We show that our mitigation method is effective in significantly\nreducing observed biased patterns. Our code is publicly available at\n\\url{https://github.com/healthylaife/FairAlignmentLLM}."
                },
                "authors": [
                    {
                        "name": "Raphael Poulain"
                    },
                    {
                        "name": "Hamed Fayyaz"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2404.15149",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12047v1",
                "updated": "2024-08-22T00:14:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    0,
                    14,
                    37,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T00:14:37Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    0,
                    14,
                    37,
                    3,
                    235,
                    0
                ],
                "title": "Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers\n  Perceived by Legal and Civil Stakeholders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers\n  Perceived by Legal and Civil Stakeholders"
                },
                "summary": "The responsible AI (RAI) community has introduced numerous processes and\nartifacts (e.g., Model Cards, Transparency Notes, Data Cards) to facilitate\ntransparency and support the governance of AI systems. While originally\ndesigned to scaffold and document AI development processes in technology\ncompanies, these artifacts are becoming central components of regulatory\ncompliance under recent regulations such as the EU AI Act. Much prior work has\nexplored the design of new RAI artifacts or their use by practitioners within\ntechnology companies. However, as RAI artifacts begin to play key roles in\nenabling external oversight, it becomes critical to understand how\nstakeholders--particularly those situated outside of technology companies who\ngovern and audit industry AI deployments--perceive the efficacy of RAI\nartifacts. In this study, we conduct semi-structured interviews and design\nactivities with 19 government, legal, and civil society stakeholders who inform\npolicy and advocacy around responsible AI efforts. While participants believe\nthat RAI artifacts are a valuable contribution to the broader AI governance\necosystem, many are concerned about their potential unintended, longer-term\nimpacts on actors outside of technology companies (e.g., downstream end-users,\npolicymakers, civil society stakeholders). We organize these beliefs into four\nbarriers that help explain how RAI artifacts may (inadvertently) reconfigure\npower relations across civil society, government, and industry, impeding civil\nsociety and legal stakeholders' ability to protect downstream end-users from\npotential AI harms. Participants envision how structural changes, along with\nchanges in how RAI artifacts are designed, used, and governed, could help\nredirect the role of artifacts to support more collaborative and proactive\nexternal oversight of AI systems. We discuss research and policy implications\nfor RAI artifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responsible AI (RAI) community has introduced numerous processes and\nartifacts (e.g., Model Cards, Transparency Notes, Data Cards) to facilitate\ntransparency and support the governance of AI systems. While originally\ndesigned to scaffold and document AI development processes in technology\ncompanies, these artifacts are becoming central components of regulatory\ncompliance under recent regulations such as the EU AI Act. Much prior work has\nexplored the design of new RAI artifacts or their use by practitioners within\ntechnology companies. However, as RAI artifacts begin to play key roles in\nenabling external oversight, it becomes critical to understand how\nstakeholders--particularly those situated outside of technology companies who\ngovern and audit industry AI deployments--perceive the efficacy of RAI\nartifacts. In this study, we conduct semi-structured interviews and design\nactivities with 19 government, legal, and civil society stakeholders who inform\npolicy and advocacy around responsible AI efforts. While participants believe\nthat RAI artifacts are a valuable contribution to the broader AI governance\necosystem, many are concerned about their potential unintended, longer-term\nimpacts on actors outside of technology companies (e.g., downstream end-users,\npolicymakers, civil society stakeholders). We organize these beliefs into four\nbarriers that help explain how RAI artifacts may (inadvertently) reconfigure\npower relations across civil society, government, and industry, impeding civil\nsociety and legal stakeholders' ability to protect downstream end-users from\npotential AI harms. Participants envision how structural changes, along with\nchanges in how RAI artifacts are designed, used, and governed, could help\nredirect the role of artifacts to support more collaborative and proactive\nexternal oversight of AI systems. We discuss research and policy implications\nfor RAI artifacts."
                },
                "authors": [
                    {
                        "name": "Anna Kawakami"
                    },
                    {
                        "name": "Daricia Wilkinson"
                    },
                    {
                        "name": "Alexandra Chouldechova"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Chouldechova"
                },
                "author": "Alexandra Chouldechova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15549v2",
                "updated": "2024-08-21T23:22:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    23,
                    22,
                    40,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-22T11:19:14Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    11,
                    19,
                    14,
                    0,
                    204,
                    0
                ],
                "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful\n  Behaviors in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Adversarial Training Improves Robustness to Persistent Harmful\n  Behaviors in LLMs"
                },
                "summary": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of 'jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of 'jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs."
                },
                "authors": [
                    {
                        "name": "Abhay Sheshadri"
                    },
                    {
                        "name": "Aidan Ewart"
                    },
                    {
                        "name": "Phillip Guo"
                    },
                    {
                        "name": "Aengus Lynch"
                    },
                    {
                        "name": "Cindy Wu"
                    },
                    {
                        "name": "Vivek Hebbar"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Asa Cooper Stickland"
                    },
                    {
                        "name": "Ethan Perez"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    },
                    {
                        "name": "Stephen Casper"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Casper"
                },
                "author": "Stephen Casper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07778v2",
                "updated": "2024-08-21T23:07:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    23,
                    7,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-12T00:01:32Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    0,
                    1,
                    32,
                    2,
                    164,
                    0
                ],
                "title": "A Study of Backdoors in Instruction Fine-tuned Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of Backdoors in Instruction Fine-tuned Language Models"
                },
                "summary": "Backdoor data poisoning, inserted within instruction examples used to\nfine-tune a foundation Large Language Model (LLM) for downstream tasks\n(\\textit{e.g.,} sentiment prediction), is a serious security concern due to the\nevasive nature of such attacks. The poisoning is usually in the form of a\n(seemingly innocuous) trigger word or phrase inserted into a very small\nfraction of the fine-tuning samples from a target class. Such backdoor attacks\ncan: alter response sentiment, violate censorship, over-refuse (invoke\ncensorship for legitimate queries), inject false content, or trigger nonsense\nresponses (hallucinations). In this work we investigate the efficacy of\ninstruction fine-tuning backdoor attacks as attack \"hyperparameters\" are varied\nunder a variety of scenarios, considering: the trigger location in the poisoned\nexamples; robustness to change in the trigger location, partial triggers, and\nsynonym substitutions at test time; attack transfer from one (fine-tuning)\ndomain to a related test domain; and clean-label vs. dirty-label poisoning.\nBased on our observations, we propose and evaluate two defenses against these\nattacks: i) a \\textit{during-fine-tuning defense} based on word-frequency\ncounts that assumes the (possibly poisoned) fine-tuning dataset is available\nand identifies the backdoor trigger tokens; and ii) a \\textit{post-fine-tuning\ndefense} based on downstream clean fine-tuning of the backdoored LLM with a\nsmall defense dataset. Finally, we provide a brief survey of related work on\nbackdoor attacks and defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor data poisoning, inserted within instruction examples used to\nfine-tune a foundation Large Language Model (LLM) for downstream tasks\n(\\textit{e.g.,} sentiment prediction), is a serious security concern due to the\nevasive nature of such attacks. The poisoning is usually in the form of a\n(seemingly innocuous) trigger word or phrase inserted into a very small\nfraction of the fine-tuning samples from a target class. Such backdoor attacks\ncan: alter response sentiment, violate censorship, over-refuse (invoke\ncensorship for legitimate queries), inject false content, or trigger nonsense\nresponses (hallucinations). In this work we investigate the efficacy of\ninstruction fine-tuning backdoor attacks as attack \"hyperparameters\" are varied\nunder a variety of scenarios, considering: the trigger location in the poisoned\nexamples; robustness to change in the trigger location, partial triggers, and\nsynonym substitutions at test time; attack transfer from one (fine-tuning)\ndomain to a related test domain; and clean-label vs. dirty-label poisoning.\nBased on our observations, we propose and evaluate two defenses against these\nattacks: i) a \\textit{during-fine-tuning defense} based on word-frequency\ncounts that assumes the (possibly poisoned) fine-tuning dataset is available\nand identifies the backdoor trigger tokens; and ii) a \\textit{post-fine-tuning\ndefense} based on downstream clean fine-tuning of the backdoored LLM with a\nsmall defense dataset. Finally, we provide a brief survey of related work on\nbackdoor attacks and defenses."
                },
                "authors": [
                    {
                        "name": "Jayaram Raghuram"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "David J. Miller"
                    }
                ],
                "author_detail": {
                    "name": "David J. Miller"
                },
                "author": "David J. Miller",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11901v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11901v4",
                "updated": "2024-08-21T22:54:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    54,
                    47,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-18T16:01:42Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    16,
                    1,
                    42,
                    0,
                    78,
                    0
                ],
                "title": "Larimar: Large Language Models with Episodic Memory Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larimar: Large Language Models with Episodic Memory Control"
                },
                "summary": "Efficient and accurate updating of knowledge stored in Large Language Models\n(LLMs) is one of the most pressing research challenges today. This paper\npresents Larimar - a novel, brain-inspired architecture for enhancing LLMs with\na distributed episodic memory. Larimar's memory allows for dynamic, one-shot\nupdates of knowledge without the need for computationally expensive re-training\nor fine-tuning. Experimental results on multiple fact editing benchmarks\ndemonstrate that Larimar attains accuracy comparable to most competitive\nbaselines, even in the challenging sequential editing setup, but also excels in\nspeed - yielding speed-ups of 8-10x depending on the base LLM - as well as\nflexibility due to the proposed architecture being simple, LLM-agnostic, and\nhence general. We further provide mechanisms for selective fact forgetting,\ninformation leakage prevention, and input context length generalization with\nLarimar and show their effectiveness. Our code is available at\nhttps://github.com/IBM/larimar",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and accurate updating of knowledge stored in Large Language Models\n(LLMs) is one of the most pressing research challenges today. This paper\npresents Larimar - a novel, brain-inspired architecture for enhancing LLMs with\na distributed episodic memory. Larimar's memory allows for dynamic, one-shot\nupdates of knowledge without the need for computationally expensive re-training\nor fine-tuning. Experimental results on multiple fact editing benchmarks\ndemonstrate that Larimar attains accuracy comparable to most competitive\nbaselines, even in the challenging sequential editing setup, but also excels in\nspeed - yielding speed-ups of 8-10x depending on the base LLM - as well as\nflexibility due to the proposed architecture being simple, LLM-agnostic, and\nhence general. We further provide mechanisms for selective fact forgetting,\ninformation leakage prevention, and input context length generalization with\nLarimar and show their effectiveness. Our code is available at\nhttps://github.com/IBM/larimar"
                },
                "authors": [
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Sarath Swaminathan"
                    },
                    {
                        "name": "Sihui Dai"
                    },
                    {
                        "name": "Aur√©lie Lozano"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Vijil Chenthamarakshan"
                    },
                    {
                        "name": "Ji≈ô√≠"
                    },
                    {
                        "name": "Navr√°til"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11901v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11901v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05961v2",
                "updated": "2024-08-21T22:46:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    46,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-09T02:51:05Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    2,
                    51,
                    5,
                    1,
                    100,
                    0
                ],
                "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
                },
                "summary": "Large decoder-only language models (LLMs) are the state-of-the-art models on\nmost of today's NLP tasks and benchmarks. Yet, the community is only slowly\nadopting these models for text embedding tasks, which require rich\ncontextualized representations. In this work, we introduce LLM2Vec, a simple\nunsupervised approach that can transform any decoder-only LLM into a strong\ntext encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional\nattention, 2) masked next token prediction, and 3) unsupervised contrastive\nlearning. We demonstrate the effectiveness of LLM2Vec by applying it to 4\npopular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed\nmodels on English word- and sequence-level tasks. We outperform encoder-only\nmodels by a large margin on word-level tasks and reach a new unsupervised\nstate-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).\nMoreover, when combining LLM2Vec with supervised contrastive learning, we\nachieve state-of-the-art performance on MTEB among models that train only on\npublicly available data (as of May 24, 2024). Our strong empirical results and\nextensive analysis demonstrate that LLMs can be effectively transformed into\nuniversal text encoders in a parameter-efficient manner without the need for\nexpensive adaptation or synthetic GPT-4 generated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large decoder-only language models (LLMs) are the state-of-the-art models on\nmost of today's NLP tasks and benchmarks. Yet, the community is only slowly\nadopting these models for text embedding tasks, which require rich\ncontextualized representations. In this work, we introduce LLM2Vec, a simple\nunsupervised approach that can transform any decoder-only LLM into a strong\ntext encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional\nattention, 2) masked next token prediction, and 3) unsupervised contrastive\nlearning. We demonstrate the effectiveness of LLM2Vec by applying it to 4\npopular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed\nmodels on English word- and sequence-level tasks. We outperform encoder-only\nmodels by a large margin on word-level tasks and reach a new unsupervised\nstate-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).\nMoreover, when combining LLM2Vec with supervised contrastive learning, we\nachieve state-of-the-art performance on MTEB among models that train only on\npublicly available data (as of May 24, 2024). Our strong empirical results and\nextensive analysis demonstrate that LLMs can be effectively transformed into\nuniversal text encoders in a parameter-efficient manner without the need for\nexpensive adaptation or synthetic GPT-4 generated data."
                },
                "authors": [
                    {
                        "name": "Parishad BehnamGhader"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Marius Mosbach"
                    },
                    {
                        "name": "Dzmitry Bahdanau"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "arxiv_comment": "Accepted to COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12025v1",
                "updated": "2024-08-21T22:35:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    35,
                    19,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T22:35:19Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    35,
                    19,
                    2,
                    234,
                    0
                ],
                "title": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires samples values to do statistical\ninference and text-based feature selection which utilizes prior knowledge of\nLLMs to do semantical associations using descriptive context. We conduct\nextensive experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires samples values to do statistical\ninference and text-based feature selection which utilizes prior knowledge of\nLLMs to do semantical associations using descriptive context. We conduct\nextensive experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01635v2",
                "updated": "2024-08-21T22:30:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-03T02:38:26Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    2,
                    38,
                    26,
                    5,
                    216,
                    0
                ],
                "title": "KTWIN: A Serverless Kubernetes-based Digital Twin Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTWIN: A Serverless Kubernetes-based Digital Twin Platform"
                },
                "summary": "Digital Twins (DTs) systems are virtual representations of physical assets\nallowing organizations to gain insights and improve existing processes. In\npractice, DTs require proper modeling, coherent development and seamless\ndeployment along cloud and edge landscapes relying on established patterns to\nreduce operational costs. In this work, we propose KTWIN a Kubernetes-based\nServerless Platform for Digital Twins. KTWIN was developed using the\nstate-of-the-art open-source Cloud Native tools, allowing DT operators to\neasily define models through open standards and configure details of the\nunderlying services and infrastructure. The experiments carried out with the\ndeveloped prototype show that KTWIN can provide a higher level of abstraction\nto model and deploy a Digital Twin use case without compromising the solution\nscalability. The tests performed also show cost savings ranging between 60% and\n80% compared to overprovisioned scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twins (DTs) systems are virtual representations of physical assets\nallowing organizations to gain insights and improve existing processes. In\npractice, DTs require proper modeling, coherent development and seamless\ndeployment along cloud and edge landscapes relying on established patterns to\nreduce operational costs. In this work, we propose KTWIN a Kubernetes-based\nServerless Platform for Digital Twins. KTWIN was developed using the\nstate-of-the-art open-source Cloud Native tools, allowing DT operators to\neasily define models through open standards and configure details of the\nunderlying services and infrastructure. The experiments carried out with the\ndeveloped prototype show that KTWIN can provide a higher level of abstraction\nto model and deploy a Digital Twin use case without compromising the solution\nscalability. The tests performed also show cost savings ranging between 60% and\n80% compared to overprovisioned scenarios."
                },
                "authors": [
                    {
                        "name": "Alexandre Gustavo Wermann"
                    },
                    {
                        "name": "Juliano Araujo Wickboldt"
                    }
                ],
                "author_detail": {
                    "name": "Juliano Araujo Wickboldt"
                },
                "author": "Juliano Araujo Wickboldt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12022v1",
                "updated": "2024-08-21T22:29:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    29,
                    56,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T22:29:56Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    29,
                    56,
                    2,
                    234,
                    0
                ],
                "title": "Understanding Epistemic Language with a Bayesian Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Epistemic Language with a Bayesian Theory of Mind"
                },
                "summary": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'', then evaluating these\ntranslations against the inferences produced by inverting a probabilistic\ngenerative model of rational action and perception, LaBToM captures graded\nplausibility judgments about epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'', then evaluating these\ntranslations against the inferences produced by inverting a probabilistic\ngenerative model of rational action and perception, LaBToM captures graded\nplausibility judgments about epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief."
                },
                "authors": [
                    {
                        "name": "Lance Ying"
                    },
                    {
                        "name": "Tan Zhi-Xuan"
                    },
                    {
                        "name": "Lionel Wong"
                    },
                    {
                        "name": "Vikash Mansinghka"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Joshua B. Tenenbaum"
                },
                "author": "Joshua B. Tenenbaum",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00854v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00854v4",
                "updated": "2024-08-21T22:07:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    7,
                    31,
                    2,
                    234,
                    0
                ],
                "published": "2024-02-01T18:50:50Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    18,
                    50,
                    50,
                    3,
                    32,
                    0
                ],
                "title": "SymbolicAI: A framework for logic-based approaches combining generative\n  models and solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymbolicAI: A framework for logic-based approaches combining generative\n  models and solvers"
                },
                "summary": "We introduce SymbolicAI, a versatile and modular framework employing a\nlogic-based approach to concept learning and flow management in generative\nprocesses. SymbolicAI enables the seamless integration of generative models\nwith a diverse range of solvers by treating large language models (LLMs) as\nsemantic parsers that execute tasks based on both natural and formal language\ninstructions, thus bridging the gap between symbolic reasoning and generative\nAI. We leverage probabilistic programming principles to tackle complex tasks,\nand utilize differentiable and classical programming paradigms with their\nrespective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for multi-modal data that\nconnects multi-step generative processes and aligns their outputs with user\nobjectives in complex workflows. As a result, we can transition between the\ncapabilities of various foundation models with in-context learning capabilities\nand specialized, fine-tuned models or solvers proficient in addressing specific\nproblems. Through these operations based on in-context learning our framework\nenables the creation and evaluation of explainable computational graphs.\nFinally, we introduce a quality measure and its empirical score for evaluating\nthese computational graphs, and propose a benchmark that compares various\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\nempirical score as the \"Vector Embedding for Relational Trajectory Evaluation\nthrough Cross-similarity\", or VERTEX score for short. The framework codebase\nand benchmark are linked below.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SymbolicAI, a versatile and modular framework employing a\nlogic-based approach to concept learning and flow management in generative\nprocesses. SymbolicAI enables the seamless integration of generative models\nwith a diverse range of solvers by treating large language models (LLMs) as\nsemantic parsers that execute tasks based on both natural and formal language\ninstructions, thus bridging the gap between symbolic reasoning and generative\nAI. We leverage probabilistic programming principles to tackle complex tasks,\nand utilize differentiable and classical programming paradigms with their\nrespective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for multi-modal data that\nconnects multi-step generative processes and aligns their outputs with user\nobjectives in complex workflows. As a result, we can transition between the\ncapabilities of various foundation models with in-context learning capabilities\nand specialized, fine-tuned models or solvers proficient in addressing specific\nproblems. Through these operations based on in-context learning our framework\nenables the creation and evaluation of explainable computational graphs.\nFinally, we introduce a quality measure and its empirical score for evaluating\nthese computational graphs, and propose a benchmark that compares various\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\nempirical score as the \"Vector Embedding for Relational Trajectory Evaluation\nthrough Cross-similarity\", or VERTEX score for short. The framework codebase\nand benchmark are linked below."
                },
                "authors": [
                    {
                        "name": "Marius-Constantin Dinu"
                    },
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    },
                    {
                        "name": "Markus Holzleitner"
                    },
                    {
                        "name": "Werner Zellinger"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "46 pages, 13 figures, external resources: framework is available at\n  https://github.com/ExtensityAI/symbolicai and benchmark at\n  https://github.com/ExtensityAI/benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00854v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00854v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12003v1",
                "updated": "2024-08-21T21:34:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    21,
                    34,
                    1,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T21:34:01Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    21,
                    34,
                    1,
                    2,
                    234,
                    0
                ],
                "title": "RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and\n  Personalization"
                },
                "summary": "With the development of the modern social economy, tourism has become an\nimportant way to meet people's spiritual needs, bringing development\nopportunities to the tourism industry. However, existing large language models\n(LLMs) face challenges in personalized recommendation capabilities and the\ngeneration of content that can sometimes produce hallucinations. This study\nproposes an optimization scheme for Tibet tourism LLMs based on\nretrieval-augmented generation (RAG) technology. By constructing a database of\ntourist viewpoints and processing the data using vectorization techniques, we\nhave significantly improved retrieval accuracy. The application of RAG\ntechnology effectively addresses the hallucination problem in content\ngeneration. The optimized model shows significant improvements in fluency,\naccuracy, and relevance of content generation. This research demonstrates the\npotential of RAG technology in the standardization of cultural tourism\ninformation and data analysis, providing theoretical and technical support for\nthe development of intelligent cultural tourism service systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of the modern social economy, tourism has become an\nimportant way to meet people's spiritual needs, bringing development\nopportunities to the tourism industry. However, existing large language models\n(LLMs) face challenges in personalized recommendation capabilities and the\ngeneration of content that can sometimes produce hallucinations. This study\nproposes an optimization scheme for Tibet tourism LLMs based on\nretrieval-augmented generation (RAG) technology. By constructing a database of\ntourist viewpoints and processing the data using vectorization techniques, we\nhave significantly improved retrieval accuracy. The application of RAG\ntechnology effectively addresses the hallucination problem in content\ngeneration. The optimized model shows significant improvements in fluency,\naccuracy, and relevance of content generation. This research demonstrates the\npotential of RAG technology in the standardization of cultural tourism\ninformation and data analysis, providing theoretical and technical support for\nthe development of intelligent cultural tourism service systems."
                },
                "authors": [
                    {
                        "name": "Jinhu Qi"
                    },
                    {
                        "name": "Shuai Yan"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "arxiv_comment": "Accepted by AIPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11993v1",
                "updated": "2024-08-21T21:07:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    21,
                    7,
                    46,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T21:07:46Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    21,
                    7,
                    46,
                    2,
                    234,
                    0
                ],
                "title": "Simulators for Quantum Network Modelling: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulators for Quantum Network Modelling: A Comprehensive Review"
                },
                "summary": "Quantum network research, is exploring new networking protocols,\nphysics-based hardware and novel experiments to demonstrate how quantum\ndistribution will work over large distances. Current work explores much of\nthese concepts in simulations, that are developed to understand how quantum\nnetworking will be set up and researchers can experiment virtually. Exposing\nflaws in network designs, like unsustainable topologies, or develop protocols\nthat efficiently utilize network resources, simulators can also help assess\nwhether workloads are balanced across virtual machines in the network. However,\nmuch of these simulation models come without reliable verification methods, for\ntesting performance in real deployments.\n  In this paper, we present a review of, to the best of our knowledge,\ncurrently used toolkits for modeling quantum networks. With these toolkits and\nstandardized validation techniques, we can lay down the foundations for more\naccurate and reliable quantum network simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum network research, is exploring new networking protocols,\nphysics-based hardware and novel experiments to demonstrate how quantum\ndistribution will work over large distances. Current work explores much of\nthese concepts in simulations, that are developed to understand how quantum\nnetworking will be set up and researchers can experiment virtually. Exposing\nflaws in network designs, like unsustainable topologies, or develop protocols\nthat efficiently utilize network resources, simulators can also help assess\nwhether workloads are balanced across virtual machines in the network. However,\nmuch of these simulation models come without reliable verification methods, for\ntesting performance in real deployments.\n  In this paper, we present a review of, to the best of our knowledge,\ncurrently used toolkits for modeling quantum networks. With these toolkits and\nstandardized validation techniques, we can lay down the foundations for more\naccurate and reliable quantum network simulators."
                },
                "authors": [
                    {
                        "name": "Oceane Bel"
                    },
                    {
                        "name": "Mariam Kiran"
                    }
                ],
                "author_detail": {
                    "name": "Mariam Kiran"
                },
                "author": "Mariam Kiran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11987v1",
                "updated": "2024-08-21T20:52:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    52,
                    32,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T20:52:32Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    52,
                    32,
                    2,
                    234,
                    0
                ],
                "title": "SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating\n  an LLM's Ability to Generate Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating\n  an LLM's Ability to Generate Digital Twins"
                },
                "summary": "We introduce SimBench, a benchmark designed to evaluate the proficiency of\nstudent large language models (S-LLMs) in generating digital twins (DTs) that\ncan be used in simulators for virtual testing. Given a collection of S-LLMs,\nthis benchmark enables the ranking of the S-LLMs based on their ability to\nproduce high-quality DTs. We demonstrate this by comparing over 20 open- and\nclosed-source S-LLMs. Using multi-turn interactions, SimBench employs a\nrule-based judge LLM (J-LLM) that leverages both predefined rules and\nhuman-in-the-loop guidance to assign scores for the DTs generated by the S-LLM,\nthus providing a consistent and expert-inspired evaluation protocol. The J-LLM\nis specific to a simulator, and herein the proposed benchmarking approach is\ndemonstrated in conjunction with the Chrono multi-physics simulator. Chrono\nprovided the backdrop used to assess an S-LLM in relation to the latter's\nability to create digital twins for multibody dynamics, finite element\nanalysis, vehicle dynamics, robotic dynamics, and sensor simulations. The\nproposed benchmarking principle is broadly applicable and enables the\nassessment of an S-LLM's ability to generate digital twins for other simulation\npackages. All code and data are available at\nhttps://github.com/uwsbel/SimBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SimBench, a benchmark designed to evaluate the proficiency of\nstudent large language models (S-LLMs) in generating digital twins (DTs) that\ncan be used in simulators for virtual testing. Given a collection of S-LLMs,\nthis benchmark enables the ranking of the S-LLMs based on their ability to\nproduce high-quality DTs. We demonstrate this by comparing over 20 open- and\nclosed-source S-LLMs. Using multi-turn interactions, SimBench employs a\nrule-based judge LLM (J-LLM) that leverages both predefined rules and\nhuman-in-the-loop guidance to assign scores for the DTs generated by the S-LLM,\nthus providing a consistent and expert-inspired evaluation protocol. The J-LLM\nis specific to a simulator, and herein the proposed benchmarking approach is\ndemonstrated in conjunction with the Chrono multi-physics simulator. Chrono\nprovided the backdrop used to assess an S-LLM in relation to the latter's\nability to create digital twins for multibody dynamics, finite element\nanalysis, vehicle dynamics, robotic dynamics, and sensor simulations. The\nproposed benchmarking principle is broadly applicable and enables the\nassessment of an S-LLM's ability to generate digital twins for other simulation\npackages. All code and data are available at\nhttps://github.com/uwsbel/SimBench."
                },
                "authors": [
                    {
                        "name": "Jingquan Wang"
                    },
                    {
                        "name": "Harry Zhang"
                    },
                    {
                        "name": "Huzaifa Mustafa Unjhawala"
                    },
                    {
                        "name": "Peter Negrut"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Khailanii Slaton"
                    },
                    {
                        "name": "Radu Serban"
                    },
                    {
                        "name": "Jin-Long Wu"
                    },
                    {
                        "name": "Dan Negrut"
                    }
                ],
                "author_detail": {
                    "name": "Dan Negrut"
                },
                "author": "Dan Negrut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11981v1",
                "updated": "2024-08-21T20:28:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    28,
                    42,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T20:28:42Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    28,
                    42,
                    2,
                    234,
                    0
                ],
                "title": "Large Language Models for Page Stream Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Page Stream Segmentation"
                },
                "summary": "Page Stream Segmentation (PSS) is an essential prerequisite for automated\ndocument processing at scale. However, research progress has been limited by\nthe absence of realistic public benchmarks. This paper works towards addressing\nthis gap by introducing TABME++, an enhanced benchmark featuring commercial\nOptical Character Recognition (OCR) annotations. We evaluate the performance of\nlarge language models (LLMs) on PSS, focusing on decoder-based models\nfine-tuned with parameter-efficient methods. Our results show that\ndecoder-based LLMs outperform smaller multimodal encoders. Through a review of\nexisting PSS research and datasets, we identify key challenges and advancements\nin the field. Our findings highlight the key importance of robust OCR,\nproviding valuable insights for the development of more effective document\nprocessing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Page Stream Segmentation (PSS) is an essential prerequisite for automated\ndocument processing at scale. However, research progress has been limited by\nthe absence of realistic public benchmarks. This paper works towards addressing\nthis gap by introducing TABME++, an enhanced benchmark featuring commercial\nOptical Character Recognition (OCR) annotations. We evaluate the performance of\nlarge language models (LLMs) on PSS, focusing on decoder-based models\nfine-tuned with parameter-efficient methods. Our results show that\ndecoder-based LLMs outperform smaller multimodal encoders. Through a review of\nexisting PSS research and datasets, we identify key challenges and advancements\nin the field. Our findings highlight the key importance of robust OCR,\nproviding valuable insights for the development of more effective document\nprocessing systems."
                },
                "authors": [
                    {
                        "name": "Hunter Heidenreich"
                    },
                    {
                        "name": "Ratish Dalvi"
                    },
                    {
                        "name": "Rohith Mukku"
                    },
                    {
                        "name": "Nikhil Verma"
                    },
                    {
                        "name": "Neven Piƒçuljan"
                    }
                ],
                "author_detail": {
                    "name": "Neven Piƒçuljan"
                },
                "author": "Neven Piƒçuljan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11975v1",
                "updated": "2024-08-21T20:15:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    15,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T20:15:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    20,
                    15,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "Automatic knowledge-graph creation from historical documents: The\n  Chilean dictatorship as a case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic knowledge-graph creation from historical documents: The\n  Chilean dictatorship as a case study"
                },
                "summary": "We present our results regarding the automatic construction of a knowledge\ngraph from historical documents related to the Chilean dictatorship period\n(1973-1990). Our approach consists on using LLMs to automatically recognize\nentities and relations between these entities, and also to perform resolution\nbetween these sets of values. In order to prevent hallucination, the\ninteraction with the LLM is grounded in a simple ontology with 4 types of\nentities and 7 types of relations. To evaluate our architecture, we use a gold\nstandard graph constructed using a small subset of the documents, and compare\nthis to the graph obtained from our approach when processing the same set of\ndocuments. Results show that the automatic construction manages to recognize a\ngood portion of all the entities in the gold standard, and that those not\nrecognized are mostly explained by the level of granularity in which the\ninformation is structured in the graph, and not because the automatic approach\nmisses an important entity in the graph. Looking forward, we expect this report\nwill encourage work on other similar projects focused on enhancing research in\nhumanities and social science, but we remark that better evaluation metrics are\nneeded in order to accurately fine-tune these types of architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present our results regarding the automatic construction of a knowledge\ngraph from historical documents related to the Chilean dictatorship period\n(1973-1990). Our approach consists on using LLMs to automatically recognize\nentities and relations between these entities, and also to perform resolution\nbetween these sets of values. In order to prevent hallucination, the\ninteraction with the LLM is grounded in a simple ontology with 4 types of\nentities and 7 types of relations. To evaluate our architecture, we use a gold\nstandard graph constructed using a small subset of the documents, and compare\nthis to the graph obtained from our approach when processing the same set of\ndocuments. Results show that the automatic construction manages to recognize a\ngood portion of all the entities in the gold standard, and that those not\nrecognized are mostly explained by the level of granularity in which the\ninformation is structured in the graph, and not because the automatic approach\nmisses an important entity in the graph. Looking forward, we expect this report\nwill encourage work on other similar projects focused on enhancing research in\nhumanities and social science, but we remark that better evaluation metrics are\nneeded in order to accurately fine-tune these types of architectures."
                },
                "authors": [
                    {
                        "name": "Camila D√≠az"
                    },
                    {
                        "name": "Jocelyn Dunstan"
                    },
                    {
                        "name": "Lorena Etcheverry"
                    },
                    {
                        "name": "Antonia Fonck"
                    },
                    {
                        "name": "Alejandro Grez"
                    },
                    {
                        "name": "Domingo Mery"
                    },
                    {
                        "name": "Juan Reutter"
                    },
                    {
                        "name": "Hugo Rojas"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Rojas"
                },
                "author": "Hugo Rojas",
                "arxiv_comment": "11 pages, 1 figure, 2 tables. Paper submitted to KBC-LM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11961v1",
                "updated": "2024-08-21T19:30:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    19,
                    30,
                    59,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T19:30:59Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    19,
                    30,
                    59,
                    2,
                    234,
                    0
                ],
                "title": "Decoding SEC Actions: Enforcement Trends through Analyzing Blockchain\n  litigation using LLM-based Thematic Factor Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding SEC Actions: Enforcement Trends through Analyzing Blockchain\n  litigation using LLM-based Thematic Factor Mapping"
                },
                "summary": "The proliferation of blockchain entities (persons or enterprises) exposes\nthem to potential regulatory actions (e.g., being litigated) by regulatory\nauthorities. Regulatory frameworks for crypto assets are actively being\ndeveloped and refined, increasing the likelihood of such actions. The lack of\nsystematic analysis of the factors driving litigation against blockchain\nentities leaves companies in need of clarity to navigate compliance risks. This\nabsence of insight also deprives investors of the information for informed\ndecision-making. This study focuses on U.S. litigation against blockchain\nentities, particularly by the U.S. Securities and Exchange Commission (SEC)\ngiven its influence on global crypto regulation. Utilizing frontier pretrained\nlanguage models and large language models, we systematically map all SEC\ncomplaints against blockchain companies from 2012 to 2024 to thematic factors\nconceptualized by our study to delineate the factors driving SEC actions. We\nquantify the thematic factors and assess their influence on specific legal Acts\ncited within the complaints on an annual basis, allowing us to discern the\nregulatory emphasis, patterns and conduct trend analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of blockchain entities (persons or enterprises) exposes\nthem to potential regulatory actions (e.g., being litigated) by regulatory\nauthorities. Regulatory frameworks for crypto assets are actively being\ndeveloped and refined, increasing the likelihood of such actions. The lack of\nsystematic analysis of the factors driving litigation against blockchain\nentities leaves companies in need of clarity to navigate compliance risks. This\nabsence of insight also deprives investors of the information for informed\ndecision-making. This study focuses on U.S. litigation against blockchain\nentities, particularly by the U.S. Securities and Exchange Commission (SEC)\ngiven its influence on global crypto regulation. Utilizing frontier pretrained\nlanguage models and large language models, we systematically map all SEC\ncomplaints against blockchain companies from 2012 to 2024 to thematic factors\nconceptualized by our study to delineate the factors driving SEC actions. We\nquantify the thematic factors and assess their influence on specific legal Acts\ncited within the complaints on an annual basis, allowing us to discern the\nregulatory emphasis, patterns and conduct trend analysis."
                },
                "authors": [
                    {
                        "name": "Junliang Luo"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "William Knottenbelt"
                    },
                    {
                        "name": "Xue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xue Liu"
                },
                "author": "Xue Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11939v1",
                "updated": "2024-08-21T18:44:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    44,
                    21,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T18:44:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    44,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "Matmul or No Matmal in the Era of 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matmul or No Matmal in the Era of 1-bit LLMs"
                },
                "summary": "The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jinendra Malekar"
                    },
                    {
                        "name": "Mohammed E. Elbtity"
                    },
                    {
                        "name": "Ramtin Zand Co"
                    }
                ],
                "author_detail": {
                    "name": "Ramtin Zand Co"
                },
                "author": "Ramtin Zand Co",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]